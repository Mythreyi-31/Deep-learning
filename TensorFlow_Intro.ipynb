{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ibsqZFNDOtcL",
      "metadata": {
        "id": "ibsqZFNDOtcL"
      },
      "source": [
        "From https://www.tensorflow.org/guide/basics\n",
        "\n",
        "TensorFlow (TF) is an end-to-end platform for machine learning. It supports the following:\n",
        "\n",
        "1. Multidimensional-array based numeric computation (similar to NumPy.)\n",
        "2. GPU and distributed processing\n",
        "3. Automatic differentiation\n",
        "4. Model construction, training, and export\n",
        "5. And more\n",
        "\n",
        "---\n",
        "\n",
        "Some important points about TF.\n",
        "\n",
        "* TensorFlow was developed by the Google Brain team for internal use at Google. Then, it was made open source in November 2015.\n",
        "* Keras is an API (application programming interface) for deep learning calculations. API means that Keras defines a specific interface to write codes.\n",
        "* Keras was written in Python.\n",
        "* Keras has multiple backends (libraries that are\n",
        "responsible for doing the actual calculations): TensorFlow,\n",
        "CNTK, Theano.\n",
        "* Keras is focused on easy and fast prototyping, through\n",
        "user friendliness, modularity, and extensibility.\n",
        "* Although TF can be used as a backend for Keras, it is\n",
        "recommended to use tf.keras, which is the implementation\n",
        "of Keras in TF.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CrW3wGfQN2KV",
      "metadata": {
        "id": "CrW3wGfQN2KV"
      },
      "source": [
        "---\n",
        "\n",
        "**Load essential libraries**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valued-lodging",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valued-lodging",
        "outputId": "cafbab3f-2286-41ec-ad11-f09759cbb488"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CZmMj92lN6Yc",
      "metadata": {
        "id": "CZmMj92lN6Yc"
      },
      "source": [
        "---\n",
        "\n",
        "**Check TensorFlow version**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2VpALYvOBoG",
      "metadata": {
        "id": "B2VpALYvOBoG"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V6MJcE8mQuxG",
      "metadata": {
        "id": "V6MJcE8mQuxG"
      },
      "source": [
        "---\n",
        "Introduction to tensors from https://www.tensorflow.org/guide/tensor\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4CbC5yVsQyth",
      "metadata": {
        "id": "4CbC5yVsQyth"
      },
      "outputs": [],
      "source": [
        "# Let's make this a float tensor.\n",
        "rank_1_tensor = tf.constant([2.0, 3.0, 4.0])\n",
        "print(rank_1_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CHCPJZG6QzKr",
      "metadata": {
        "id": "CHCPJZG6QzKr"
      },
      "source": [
        "---\n",
        "\n",
        "Introduction to variables from https://www.tensorflow.org/guide/variable\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gGjQqL8gQ8rk",
      "metadata": {
        "id": "gGjQqL8gQ8rk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6U1TokFxQ_w2",
      "metadata": {
        "id": "6U1TokFxQ_w2"
      },
      "source": [
        "---\n",
        "\n",
        "Automatic differentiation using TF (https://www.tensorflow.org/guide/autodiff)\n",
        "\n",
        "Example: calculate the sensitivity of $L(w) = 4w+w^3$ w.r.t. the input $w$ at $w=1.$\n",
        "\n",
        "Sensitivity $\\nabla_wL = 4+3w^2,$ which at $w=1$ is equal to $4+3\\times1^2=7.$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "banner-guatemala",
      "metadata": {
        "id": "banner-guatemala"
      },
      "outputs": [],
      "source": [
        "#plot a graph for the equation L(w) = 4*w + w^3 at w=1\n",
        "w = np.linspace(-2,2,1000)\n",
        "L = 4*w + w**3\n",
        "plt.plot(w,L)\n",
        "plt.title('L(w) = 4*w + w^3')\n",
        "plt.xlabel('w')\n",
        "plt.ylabel('L')\n",
        "plt.scatter(1,5, color='red')\n",
        "plt.scatter(1,0, color='red');\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "w = tf.Variable(1.5)\n",
        "with tf.GradientTape() as g:\n",
        "    L = 4*w + w**3\n",
        "\n",
        "gradL_w = g.gradient(L, w)\n",
        "print(\"THE GRADIENT OF L wrt w AT w = 1 is %f\"%(gradL_w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "better-restoration",
      "metadata": {
        "id": "better-restoration"
      },
      "source": [
        "---\n",
        "\n",
        "Example: calculate the sensitivity of $L(w_1,w_2) = w_1+w_2^2$ w.r.t. the inputs $w_1, w_2$ at $w_1=1, w_2=2.$\n",
        "\n",
        "Setting $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix},$ sensitivity $\\nabla_\\mathbf{w}L= \\begin{bmatrix}\\nabla_{w_1}(w_1+w_2^2)\\\\\\nabla_{w_2}(w_1+w_2^2)\\end{bmatrix} = \\begin{bmatrix}1\\\\2w_2\\end{bmatrix},$\n",
        "\n",
        " which at $w_1=1,w_2=2$ is equal to $\\begin{bmatrix}1\\\\4\\end{bmatrix}.$\n",
        "\n",
        " ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continuous-protocol",
      "metadata": {
        "id": "continuous-protocol"
      },
      "outputs": [],
      "source": [
        "w1 = tf.Variable(1.0)\n",
        "w2 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    L = w1 + w2**2 \n",
        "\n",
        "gradL_w = g.gradient(L, [w1,w2])\n",
        "print(gradL_w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alert-prairie",
      "metadata": {
        "id": "alert-prairie"
      },
      "source": [
        "---\n",
        "\n",
        "In TF, we can control which input is considered an independent variable versus a constant value.\n",
        "\n",
        "`gradient` will return `None` when the input is not a `tf.Variable`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ordered-midwest",
      "metadata": {
        "id": "ordered-midwest"
      },
      "outputs": [],
      "source": [
        "#independent variable\n",
        "w1 = tf.Variable(1.0, name='not survived')\n",
        "\n",
        "# A tf.constant is not a variable\n",
        "c1 = tf.constant(-2.0, name='constant 1')\n",
        "\n",
        "# treat a tf.Variable like a constant\n",
        "w2 = tf.Variable(2.0, name='second layer weights', trainable= False)\n",
        "\n",
        "# A tf.Variable + a tensor object is treated as a tensor object(constant)\n",
        "c2 = tf.Variable(10.0, name='constant 2') + 1.0\n",
        "\n",
        "# Deine a variable that will not be used in the computation \n",
        "w3 = tf.Variable(0.0, name='third layer weights')\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    L = (w1+c1)**2 + w2**2 + 4*c2\n",
        "\n",
        "gradL_w = g.gradient(L, [w1,w2,c1,w3,c2])\n",
        "for dw in gradL_w:\n",
        "    print(dw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d690a29",
      "metadata": {},
      "outputs": [],
      "source": [
        "w = tf.Variable([0.1, 0, 0.1, 0, 0.1, 0, 0.1, 0], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    #norm = tf.norm(w)\n",
        "    #f = 2 * norm\n",
        "    L = tf.norm(w)**2\n",
        "\n",
        "gradient = g.gradient(L, w)\n",
        "print(gradient)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11e8ad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the  softmax gradient for two matrices x and w\n",
        "x = tf.constant([76.0,120.0,37.5], dtype=tf.float32)\n",
        "x = tf.reshape(x, [3, 1])\n",
        "w = tf.Variable([[0.01, 0.01, 0.01, 0.01], [0.01, 0.01, 0.01, 0.01]], dtype=tf.float32) #0.1*tf.ones((2,4))\n",
        "z = tf.matmul(w[:, :-1], x)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "    L = tf.nn.softmax(z) \n",
        "    \n",
        "\n",
        "gradL_w = g.gradient(L, w)\n",
        "print(L.shape)\n",
        "print(gradL_w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "statewide-voluntary",
      "metadata": {
        "id": "statewide-voluntary"
      },
      "source": [
        "---\n",
        "\n",
        "A `tf.Tensor` can be used as a variable using the `watch` function.\n",
        "\n",
        "For example, consider calculating the sensitivity of $L(w) = w^4$ at $w=-3.$\n",
        "\n",
        "The sensitivity is $\\nabla_wL = 4w^3,$ which at $w=-3$ is equal to $4\\times(-3)^3 = -108.$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stable-macro",
      "metadata": {
        "id": "stable-macro"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "contrary-hollywood",
      "metadata": {
        "id": "contrary-hollywood"
      },
      "source": [
        "---\n",
        "\n",
        "We can use multiple tensor variables as input. This just means we calculate the sensitivity w.r.t. all the variables in the tensor.\n",
        "\n",
        "For example, consider calculating the sensitivity of $L(w) = w[0]^2+w[1]^2$ at $w[0] = 1, w[1] = -3.$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handled-mexico",
      "metadata": {
        "id": "handled-mexico"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "athletic-carnival",
      "metadata": {
        "id": "athletic-carnival"
      },
      "source": [
        "---\n",
        "\n",
        "When `g.gradient` is called with a tensor dependent variable (tensor target), it returns the sum of the sensitivities for each component of the target variable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fatal-calibration",
      "metadata": {
        "id": "fatal-calibration"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "latter-generic",
      "metadata": {
        "id": "latter-generic"
      },
      "source": [
        "---\n",
        "\n",
        "By default, when we call `g.gradient`, all resources required to compute the gradient are released. This allows saving memory. However, there are cases when we want to call `g.gradient` several times, for example, to differentiate different chained functions. In that case, we must use the option `persistent=True`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conscious-twins",
      "metadata": {
        "id": "conscious-twins"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "alternate-tournament",
      "metadata": {
        "id": "alternate-tournament"
      },
      "source": [
        "---\n",
        "\n",
        "We can easily calculate sensitivies of functions and plot them.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "difficult-documentary",
      "metadata": {
        "id": "difficult-documentary"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "430e6722",
      "metadata": {},
      "source": [
        "**Create training data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7c5a19",
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = np.arange(11)\n",
        "y_train = 2 * x_train + 1 + np.random.normal(0,0.5, len(x_train))\n",
        "plt.scatter(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7532f350",
      "metadata": {},
      "source": [
        "Initialise the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "045a887c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aac354a",
      "metadata": {},
      "outputs": [],
      "source": [
        "w1 = tf.Variable(0.01*np.random.normal())\n",
        "w2 = tf.Variable(0.01*np.random.normal())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191cc659",
      "metadata": {},
      "source": [
        "LOSS Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6634bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(y, y_hat):\n",
        "    return (y - y_hat)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "094403e5",
      "metadata": {},
      "source": [
        "Gradient and loss calculation for one step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2631644",
      "metadata": {},
      "outputs": [],
      "source": [
        "def onestep(x, y, learning_rate = 0.01):\n",
        "    with tf.GradientTape(persistent=True) as g:\n",
        "        # forward propagation\n",
        "        z1 = w1 * x + w2 # later rewrite as Wx_B\n",
        "        a = tf.tanh(z)\n",
        "        z2 = \n",
        "\n",
        "        loss = mse_loss(y, z)\n",
        "\n",
        "    # calculate gradient #gradL_w1, gradL_w2 = g.gradient(loss, [w1, w2])\n",
        "    \n",
        "    dw1, dw2 = g.gradient(loss, [w1, w2])\n",
        "    #print(dw1, dw2)\n",
        "\n",
        "    # update weights\n",
        "    #w1 = w1 + w.assign(dw1 * learning_rate)\n",
        "    #w2 = w2 + w.assign(dw1 * learning_rate)\n",
        "\n",
        "    w1.assign_add(-dw1 * learning_rate)\n",
        "    w2.assign_add(-dw2 * learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "    #w1.assign_sub(0.01 * dw1)\n",
        "    #w2.assign_sub(0.01 * dw2)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a055cca1",
      "metadata": {},
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263af9d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "niters = 10000\n",
        "for i in range(niters):\n",
        "    onestep(x_train, y_train)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91874943",
      "metadata": {},
      "source": [
        "print the trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf94715b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(w1.numpy())\n",
        "print(w2.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c593092",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

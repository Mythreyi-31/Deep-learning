{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "outputs": [],
      "source": [
        "## Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4T7eUtw7Mh0z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Q1e2N5S8MlCU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.15.0'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbaFOIn_CDuN"
      },
      "source": [
        "---\n",
        "\n",
        "Mount Google Drive if running in Colab\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "02xk1yt7CE1D"
      },
      "outputs": [],
      "source": [
        "## Mount Google drive folder if running in Colab\n",
        "if('google.colab' in sys.modules):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2024MAHE'\n",
        "    DATA_DIR = DIR + '/Data/'\n",
        "    os.chdir(DIR)\n",
        "else:\n",
        "    DATA_DIR = 'Data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "---\n",
        "\n",
        "Load diabetes data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "E5kaKFKSIQgu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diabetes dataset\n",
            "-----------\n",
            "Initial number of samples = 442\n",
            "Initial number of features = 11\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>BMI</th>\n",
              "      <th>BP</th>\n",
              "      <th>S1</th>\n",
              "      <th>S2</th>\n",
              "      <th>S3</th>\n",
              "      <th>S4</th>\n",
              "      <th>S5</th>\n",
              "      <th>S6</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>32.1</td>\n",
              "      <td>101.0</td>\n",
              "      <td>157</td>\n",
              "      <td>93.2</td>\n",
              "      <td>38.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.8598</td>\n",
              "      <td>87</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>21.6</td>\n",
              "      <td>87.0</td>\n",
              "      <td>183</td>\n",
              "      <td>103.2</td>\n",
              "      <td>70.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.8918</td>\n",
              "      <td>69</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>30.5</td>\n",
              "      <td>93.0</td>\n",
              "      <td>156</td>\n",
              "      <td>93.6</td>\n",
              "      <td>41.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.6728</td>\n",
              "      <td>85</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>25.3</td>\n",
              "      <td>84.0</td>\n",
              "      <td>198</td>\n",
              "      <td>131.4</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.8903</td>\n",
              "      <td>89</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>23.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>192</td>\n",
              "      <td>125.4</td>\n",
              "      <td>52.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.2905</td>\n",
              "      <td>80</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   AGE  GENDER   BMI     BP   S1     S2    S3   S4      S5  S6    Y\n",
              "0   59       2  32.1  101.0  157   93.2  38.0  4.0  4.8598  87  151\n",
              "1   48       1  21.6   87.0  183  103.2  70.0  3.0  3.8918  69   75\n",
              "2   72       2  30.5   93.0  156   93.6  41.0  4.0  4.6728  85  141\n",
              "3   24       1  25.3   84.0  198  131.4  40.0  5.0  4.8903  89  206\n",
              "4   50       1  23.0  101.0  192  125.4  52.0  4.0  4.2905  80  135"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Load diabetes data\n",
        "file = 'diabetes_regression1.csv'\n",
        "df= pd.read_csv(file, header = 0)\n",
        "\n",
        "print('Diabetes dataset')\n",
        "print('-----------')\n",
        "print('Initial number of samples = %d'%(df.shape[0]))\n",
        "print('Initial number of features = %d\\n'%(df.shape[1]))\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AJE5ehBOClXW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['GENDER']\n",
            "['AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y']\n"
          ]
        }
      ],
      "source": [
        "## Create lists of ordinal, categorical, and continuous features\n",
        "#categorical_features =  ['GENDER', 'BMILEVEL']\n",
        "categorical_features =  ['GENDER']\n",
        "continuous_features = df.drop(categorical_features, axis = 1).columns.tolist()\n",
        "print(categorical_features)\n",
        "print(continuous_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3NyP_EoDG1i"
      },
      "source": [
        "---\n",
        "\n",
        "Assign 'category' datatype to categorical columns\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-dVFfOBlDJ5n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AGE         int64\n",
            "GENDER      int64\n",
            "BMI       float64\n",
            "BP        float64\n",
            "S1          int64\n",
            "S2        float64\n",
            "S3        float64\n",
            "S4        float64\n",
            "S5        float64\n",
            "S6          int64\n",
            "Y           int64\n",
            "dtype: object\n",
            "----\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AGE          int64\n",
              "GENDER    category\n",
              "BMI        float64\n",
              "BP         float64\n",
              "S1           int64\n",
              "S2         float64\n",
              "S3         float64\n",
              "S4         float64\n",
              "S5         float64\n",
              "S6           int64\n",
              "Y            int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Assign 'category' datatype to ordinal and categorical columns\n",
        "print(df.dtypes)\n",
        "df[categorical_features] = df[categorical_features].astype('category')\n",
        "print('----')\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m95YNt2eDUJ8"
      },
      "source": [
        "---\n",
        "\n",
        "Remove the target variable column from the list of continuous features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XuQGzpefDUqE"
      },
      "outputs": [],
      "source": [
        "## Remove the target variable column from the list of continuous features\n",
        "continuous_features.remove('Y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tn-qjRocE8Lj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diabetes data set\n",
            "---------------------\n",
            "Number of training samples = 10\n",
            "Number of features = 353\n"
          ]
        }
      ],
      "source": [
        "## Train and test split of the data\n",
        "X = df.drop('Y', axis = 1)\n",
        "y = df['Y']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "num_features = X_train.shape[0]\n",
        "num_samples = X_train.shape[1]\n",
        "\n",
        "print('Diabetes data set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqSu9e7G-fh"
      },
      "source": [
        "---\n",
        "\n",
        "Build pipeline for categorical and continuous features\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "FKAqkz1GHGvm"
      },
      "outputs": [],
      "source": [
        "## Build pipeline for categorical and continuous features\n",
        "\n",
        "# Pipeline object for categorical (features\n",
        "categorical_transformer = Pipeline(steps = [('onehotenc', OneHotEncoder(handle_unknown = 'ignore'))])\n",
        "\n",
        "# Pipeline object for continuous features\n",
        "continuous_transformer = Pipeline(steps = [('scaler', RobustScaler())])\n",
        "\n",
        "# Create a preprocessor object for all features\n",
        "preprocessor = ColumnTransformer(transformers = [('continuous', continuous_transformer, continuous_features),\n",
        "                                                 ('categorical', categorical_transformer, categorical_features)\n",
        "                                                ],\n",
        "                                 remainder = 'passthrough'\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUWtfRseHv2O"
      },
      "source": [
        "---\n",
        "\n",
        "Fit and transform train data using preprocessor followed by transforming test data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "RvrUIw4SHzO_"
      },
      "outputs": [],
      "source": [
        "## Fit and transform train data using preprocessor\n",
        "X_train_transformed = preprocessor.fit_transform(X_train).T\n",
        "# Update number of features\n",
        "num_features = X_train_transformed.shape[0]\n",
        "# Transform training data using preprocessor\n",
        "X_test_transformed = preprocessor.transform(X_test).T\n",
        "# Convert Y_train and Y_test to numpy arrays\n",
        "Y_train = Y_train.to_numpy()\n",
        "Y_test = Y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYn_hOccDa3M"
      },
      "source": [
        "---\n",
        "\n",
        "A generic layer class with forward and backward methods\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMt81Faf9-bf"
      },
      "source": [
        "---\n",
        "\n",
        "Mean squared error (MSE) loss and its gradient\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def mse(Y, Yhat):\n",
        "  return(np.mean(0.5*(Y - Yhat)**2))\n",
        "  #TensorFlow in-built function for mean squared error loss\n",
        "  #mse = tf.keras.losses.MeanSquaredError()\n",
        "  #mse(Y, Yhat).numpy()\n",
        "\n",
        "def mse_gradient(Y, Yhat):\n",
        "  return(Yhat - Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmcNJTjS-BaW"
      },
      "source": [
        "---\n",
        "\n",
        "Generic activation layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "C21FcWIEwGCN"
      },
      "outputs": [],
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_gradient):\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.activation(self.input)\n",
        "        return(self.output)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate = None):\n",
        "        return(output_gradient[:-1, :] * self.activation_gradient(self.input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JheGWSoKxYWu"
      },
      "source": [
        "---\n",
        "\n",
        "Specific activation layer classes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "PQ5ybz_Yxbef"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "\n",
        "        def sigmoid_gradient(z):\n",
        "            a = sigmoid(z)\n",
        "            return a * (1 - a)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_gradient)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(z):\n",
        "            return np.tanh(z)\n",
        "\n",
        "        def tanh_gradient(z):\n",
        "            a = np.tanh(z)\n",
        "            return 1 - a**2\n",
        "\n",
        "        super().__init__(tanh, tanh_gradient)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(z):\n",
        "            return z * (z > 0)\n",
        "\n",
        "        def relu_gradient(z):\n",
        "            return 1. * (z > 0)\n",
        "\n",
        "        super().__init__(relu, relu_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKnqi7rf-MBn"
      },
      "source": [
        "---\n",
        "\n",
        "Dense layer class\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size, reg_strength):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
        "        self.reg_strength = reg_strength\n",
        "        self.reg_loss = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "        # Calculate regularization loss\n",
        "        self.reg_loss = self.reg_strength * np.sum(self.weights[:, :-1] * self.weights[:, :-1])\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the backward gradient\n",
        "        #weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
        "        #for b in range(output_gradient.shape[1]):\n",
        "        #  weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
        "        #weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
        "\n",
        "        ## Following is the efficient way of calculating the weights gradient w.r.t. data\n",
        "        weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "        # Add the regularization gradient here\n",
        "        weights_gradient += 2 * self.reg_strength * np.hstack([self.weights[:, :-1], np.zeros((self.weights.shape[0], 1))])\n",
        "\n",
        "\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "---\n",
        "\n",
        "Function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "---\n",
        "\n",
        "Train the 3-layer neural network (8/8/1 structure) using batch training with batch size = 16\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LGIzrN-rPuI4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train loss = 12503.221768, test loss = 13495.082980\n",
            "Epoch 2: train loss = 12501.083388, test loss = 13492.927971\n",
            "Epoch 3: train loss = 12498.945372, test loss = 13490.773318\n",
            "Epoch 4: train loss = 12496.807699, test loss = 13488.619004\n",
            "Epoch 5: train loss = 12494.670346, test loss = 13486.465012\n",
            "Epoch 6: train loss = 12492.533294, test loss = 13484.311324\n",
            "Epoch 7: train loss = 12490.396519, test loss = 13482.157924\n",
            "Epoch 8: train loss = 12488.260000, test loss = 13480.004794\n",
            "Epoch 9: train loss = 12486.123715, test loss = 13477.851917\n",
            "Epoch 10: train loss = 12483.987641, test loss = 13475.699274\n",
            "Epoch 11: train loss = 12481.851756, test loss = 13473.546849\n",
            "Epoch 12: train loss = 12479.716037, test loss = 13471.394621\n",
            "Epoch 13: train loss = 12477.580460, test loss = 13469.242573\n",
            "Epoch 14: train loss = 12475.445002, test loss = 13467.090685\n",
            "Epoch 15: train loss = 12473.309638, test loss = 13464.938938\n",
            "Epoch 16: train loss = 12471.174344, test loss = 13462.787311\n",
            "Epoch 17: train loss = 12469.039094, test loss = 13460.635786\n",
            "Epoch 18: train loss = 12466.903863, test loss = 13458.484340\n",
            "Epoch 19: train loss = 12464.768624, test loss = 13456.332953\n",
            "Epoch 20: train loss = 12462.633349, test loss = 13454.181602\n",
            "Epoch 21: train loss = 12460.498012, test loss = 13452.030265\n",
            "Epoch 22: train loss = 12458.362584, test loss = 13449.878919\n",
            "Epoch 23: train loss = 12456.227035, test loss = 13447.727540\n",
            "Epoch 24: train loss = 12454.091336, test loss = 13445.576103\n",
            "Epoch 25: train loss = 12451.955456, test loss = 13443.424584\n",
            "Epoch 26: train loss = 12449.819362, test loss = 13441.272957\n",
            "Epoch 27: train loss = 12447.683022, test loss = 13439.121194\n",
            "Epoch 28: train loss = 12445.546403, test loss = 13436.969268\n",
            "Epoch 29: train loss = 12443.409468, test loss = 13434.817151\n",
            "Epoch 30: train loss = 12441.272183, test loss = 13432.664813\n",
            "Epoch 31: train loss = 12439.134510, test loss = 13430.512224\n",
            "Epoch 32: train loss = 12436.996411, test loss = 13428.359352\n",
            "Epoch 33: train loss = 12434.857846, test loss = 13426.206165\n",
            "Epoch 34: train loss = 12432.718774, test loss = 13424.052629\n",
            "Epoch 35: train loss = 12430.579153, test loss = 13421.898709\n",
            "Epoch 36: train loss = 12428.438939, test loss = 13419.744370\n",
            "Epoch 37: train loss = 12426.298086, test loss = 13417.589573\n",
            "Epoch 38: train loss = 12424.156548, test loss = 13415.434281\n",
            "Epoch 39: train loss = 12422.014275, test loss = 13413.278453\n",
            "Epoch 40: train loss = 12419.871218, test loss = 13411.122047\n",
            "Epoch 41: train loss = 12417.727324, test loss = 13408.965021\n",
            "Epoch 42: train loss = 12415.582538, test loss = 13406.807329\n",
            "Epoch 43: train loss = 12413.436806, test loss = 13404.648926\n",
            "Epoch 44: train loss = 12411.290068, test loss = 13402.489764\n",
            "Epoch 45: train loss = 12409.142264, test loss = 13400.329793\n",
            "Epoch 46: train loss = 12406.993332, test loss = 13398.168960\n",
            "Epoch 47: train loss = 12404.843207, test loss = 13396.007213\n",
            "Epoch 48: train loss = 12402.691821, test loss = 13393.844496\n",
            "Epoch 49: train loss = 12400.539105, test loss = 13391.680750\n",
            "Epoch 50: train loss = 12398.384986, test loss = 13389.515917\n",
            "Epoch 51: train loss = 12396.229389, test loss = 13387.349934\n",
            "Epoch 52: train loss = 12394.072236, test loss = 13385.182736\n",
            "Epoch 53: train loss = 12391.913446, test loss = 13383.014256\n",
            "Epoch 54: train loss = 12389.752935, test loss = 13380.844424\n",
            "Epoch 55: train loss = 12387.590615, test loss = 13378.673169\n",
            "Epoch 56: train loss = 12385.426396, test loss = 13376.500415\n",
            "Epoch 57: train loss = 12383.260184, test loss = 13374.326085\n",
            "Epoch 58: train loss = 12381.091882, test loss = 13372.150097\n",
            "Epoch 59: train loss = 12378.921387, test loss = 13369.972367\n",
            "Epoch 60: train loss = 12376.748596, test loss = 13367.792809\n",
            "Epoch 61: train loss = 12374.573398, test loss = 13365.611330\n",
            "Epoch 62: train loss = 12372.395680, test loss = 13363.427839\n",
            "Epoch 63: train loss = 12370.215325, test loss = 13361.242236\n",
            "Epoch 64: train loss = 12368.032211, test loss = 13359.054420\n",
            "Epoch 65: train loss = 12365.846210, test loss = 13356.864287\n",
            "Epoch 66: train loss = 12363.657191, test loss = 13354.671726\n",
            "Epoch 67: train loss = 12361.465017, test loss = 13352.476624\n",
            "Epoch 68: train loss = 12359.269547, test loss = 13350.278864\n",
            "Epoch 69: train loss = 12357.070632, test loss = 13348.078322\n",
            "Epoch 70: train loss = 12354.868121, test loss = 13345.874871\n",
            "Epoch 71: train loss = 12352.661854, test loss = 13343.668381\n",
            "Epoch 72: train loss = 12350.451667, test loss = 13341.458713\n",
            "Epoch 73: train loss = 12348.237389, test loss = 13339.245725\n",
            "Epoch 74: train loss = 12346.018842, test loss = 13337.029270\n",
            "Epoch 75: train loss = 12343.795843, test loss = 13334.809194\n",
            "Epoch 76: train loss = 12341.568199, test loss = 13332.585339\n",
            "Epoch 77: train loss = 12339.335714, test loss = 13330.357540\n",
            "Epoch 78: train loss = 12337.098180, test loss = 13328.125624\n",
            "Epoch 79: train loss = 12334.855385, test loss = 13325.889414\n",
            "Epoch 80: train loss = 12332.607108, test loss = 13323.648725\n",
            "Epoch 81: train loss = 12330.353117, test loss = 13321.403367\n",
            "Epoch 82: train loss = 12328.093176, test loss = 13319.153140\n",
            "Epoch 83: train loss = 12325.827037, test loss = 13316.897838\n",
            "Epoch 84: train loss = 12323.554444, test loss = 13314.637248\n",
            "Epoch 85: train loss = 12321.275131, test loss = 13312.371147\n",
            "Epoch 86: train loss = 12318.988822, test loss = 13310.099306\n",
            "Epoch 87: train loss = 12316.695231, test loss = 13307.821486\n",
            "Epoch 88: train loss = 12314.394063, test loss = 13305.537439\n",
            "Epoch 89: train loss = 12312.085009, test loss = 13303.246910\n",
            "Epoch 90: train loss = 12309.767753, test loss = 13300.949631\n",
            "Epoch 91: train loss = 12307.441964, test loss = 13298.645328\n",
            "Epoch 92: train loss = 12305.107300, test loss = 13296.333715\n",
            "Epoch 93: train loss = 12302.763408, test loss = 13294.014494\n",
            "Epoch 94: train loss = 12300.409919, test loss = 13291.687361\n",
            "Epoch 95: train loss = 12298.046456, test loss = 13289.351996\n",
            "Epoch 96: train loss = 12295.672623, test loss = 13287.008069\n",
            "Epoch 97: train loss = 12293.288014, test loss = 13284.655241\n",
            "Epoch 98: train loss = 12290.892207, test loss = 13282.293157\n",
            "Epoch 99: train loss = 12288.484765, test loss = 13279.921452\n",
            "Epoch 100: train loss = 12286.065236, test loss = 13277.539746\n",
            "Epoch 101: train loss = 12283.633154, test loss = 13275.147647\n",
            "Epoch 102: train loss = 12281.188033, test loss = 13272.744749\n",
            "Epoch 103: train loss = 12278.729375, test loss = 13270.330633\n",
            "Epoch 104: train loss = 12276.256661, test loss = 13267.904862\n",
            "Epoch 105: train loss = 12273.769356, test loss = 13265.466989\n",
            "Epoch 106: train loss = 12271.266908, test loss = 13263.016548\n",
            "Epoch 107: train loss = 12268.748745, test loss = 13260.553057\n",
            "Epoch 108: train loss = 12266.214277, test loss = 13258.076020\n",
            "Epoch 109: train loss = 12263.662893, test loss = 13255.584924\n",
            "Epoch 110: train loss = 12261.093963, test loss = 13253.079236\n",
            "Epoch 111: train loss = 12258.506837, test loss = 13250.558410\n",
            "Epoch 112: train loss = 12255.900844, test loss = 13248.021877\n",
            "Epoch 113: train loss = 12253.275291, test loss = 13245.469054\n",
            "Epoch 114: train loss = 12250.629462, test loss = 13242.899337\n",
            "Epoch 115: train loss = 12247.962622, test loss = 13240.312101\n",
            "Epoch 116: train loss = 12245.274009, test loss = 13237.706706\n",
            "Epoch 117: train loss = 12242.562840, test loss = 13235.082486\n",
            "Epoch 118: train loss = 12239.828307, test loss = 13232.438759\n",
            "Epoch 119: train loss = 12237.069581, test loss = 13229.774819\n",
            "Epoch 120: train loss = 12234.285803, test loss = 13227.089940\n",
            "Epoch 121: train loss = 12231.476093, test loss = 13224.383374\n",
            "Epoch 122: train loss = 12228.639545, test loss = 13221.654350\n",
            "Epoch 123: train loss = 12225.775224, test loss = 13218.902075\n",
            "Epoch 124: train loss = 12222.882173, test loss = 13216.125732\n",
            "Epoch 125: train loss = 12219.959405, test loss = 13213.324482\n",
            "Epoch 126: train loss = 12217.005908, test loss = 13210.497461\n",
            "Epoch 127: train loss = 12214.020642, test loss = 13207.643780\n",
            "Epoch 128: train loss = 12211.002540, test loss = 13204.762529\n",
            "Epoch 129: train loss = 12207.950506, test loss = 13201.852769\n",
            "Epoch 130: train loss = 12204.863418, test loss = 13198.913540\n",
            "Epoch 131: train loss = 12201.740124, test loss = 13195.943854\n",
            "Epoch 132: train loss = 12198.579444, test loss = 13192.942699\n",
            "Epoch 133: train loss = 12195.380172, test loss = 13189.909036\n",
            "Epoch 134: train loss = 12192.141071, test loss = 13186.841802\n",
            "Epoch 135: train loss = 12188.860877, test loss = 13183.739909\n",
            "Epoch 136: train loss = 12185.538296, test loss = 13180.602239\n",
            "Epoch 137: train loss = 12182.172008, test loss = 13177.427653\n",
            "Epoch 138: train loss = 12178.760664, test loss = 13174.214983\n",
            "Epoch 139: train loss = 12175.302888, test loss = 13170.963037\n",
            "Epoch 140: train loss = 12171.797275, test loss = 13167.670595\n",
            "Epoch 141: train loss = 12168.242393, test loss = 13164.336414\n",
            "Epoch 142: train loss = 12164.636785, test loss = 13160.959226\n",
            "Epoch 143: train loss = 12160.978966, test loss = 13157.537734\n",
            "Epoch 144: train loss = 12157.267426, test loss = 13154.070621\n",
            "Epoch 145: train loss = 12153.500630, test loss = 13150.556544\n",
            "Epoch 146: train loss = 12149.677018, test loss = 13146.994134\n",
            "Epoch 147: train loss = 12145.795006, test loss = 13143.382002\n",
            "Epoch 148: train loss = 12141.852989, test loss = 13139.718734\n",
            "Epoch 149: train loss = 12137.849337, test loss = 13136.002895\n",
            "Epoch 150: train loss = 12133.782402, test loss = 13132.233026\n",
            "Epoch 151: train loss = 12129.650513, test loss = 13128.407651\n",
            "Epoch 152: train loss = 12125.451984, test loss = 13124.525272\n",
            "Epoch 153: train loss = 12121.185106, test loss = 13120.584371\n",
            "Epoch 154: train loss = 12116.848159, test loss = 13116.583413\n",
            "Epoch 155: train loss = 12112.439404, test loss = 13112.520845\n",
            "Epoch 156: train loss = 12107.957090, test loss = 13108.395099\n",
            "Epoch 157: train loss = 12103.399452, test loss = 13104.204591\n",
            "Epoch 158: train loss = 12098.764716, test loss = 13099.947723\n",
            "Epoch 159: train loss = 12094.051097, test loss = 13095.622885\n",
            "Epoch 160: train loss = 12089.256803, test loss = 13091.228455\n",
            "Epoch 161: train loss = 12084.380037, test loss = 13086.762800\n",
            "Epoch 162: train loss = 12079.418994, test loss = 13082.224280\n",
            "Epoch 163: train loss = 12074.371871, test loss = 13077.611246\n",
            "Epoch 164: train loss = 12069.236859, test loss = 13072.922044\n",
            "Epoch 165: train loss = 12064.012154, test loss = 13068.155015\n",
            "Epoch 166: train loss = 12058.695951, test loss = 13063.308497\n",
            "Epoch 167: train loss = 12053.286452, test loss = 13058.380826\n",
            "Epoch 168: train loss = 12047.781864, test loss = 13053.370339\n",
            "Epoch 169: train loss = 12042.180402, test loss = 13048.275374\n",
            "Epoch 170: train loss = 12036.480290, test loss = 13043.094272\n",
            "Epoch 171: train loss = 12030.679764, test loss = 13037.825378\n",
            "Epoch 172: train loss = 12024.777073, test loss = 13032.467045\n",
            "Epoch 173: train loss = 12018.770482, test loss = 13027.017632\n",
            "Epoch 174: train loss = 12012.658272, test loss = 13021.475508\n",
            "Epoch 175: train loss = 12006.438742, test loss = 13015.839053\n",
            "Epoch 176: train loss = 12000.110210, test loss = 13010.106658\n",
            "Epoch 177: train loss = 11993.671019, test loss = 13004.276729\n",
            "Epoch 178: train loss = 11987.119531, test loss = 12998.347687\n",
            "Epoch 179: train loss = 11980.454135, test loss = 12992.317970\n",
            "Epoch 180: train loss = 11973.673245, test loss = 12986.186033\n",
            "Epoch 181: train loss = 11966.775303, test loss = 12979.950351\n",
            "Epoch 182: train loss = 11959.758779, test loss = 12973.609419\n",
            "Epoch 183: train loss = 11952.622174, test loss = 12967.161753\n",
            "Epoch 184: train loss = 11945.364018, test loss = 12960.605895\n",
            "Epoch 185: train loss = 11937.982874, test loss = 12953.940408\n",
            "Epoch 186: train loss = 11930.477338, test loss = 12947.163882\n",
            "Epoch 187: train loss = 11922.846041, test loss = 12940.274931\n",
            "Epoch 188: train loss = 11915.087647, test loss = 12933.272198\n",
            "Epoch 189: train loss = 11907.200857, test loss = 12926.154354\n",
            "Epoch 190: train loss = 11899.184407, test loss = 12918.920097\n",
            "Epoch 191: train loss = 11891.037073, test loss = 12911.568158\n",
            "Epoch 192: train loss = 11882.757665, test loss = 12904.097296\n",
            "Epoch 193: train loss = 11874.345034, test loss = 12896.506300\n",
            "Epoch 194: train loss = 11865.798069, test loss = 12888.793994\n",
            "Epoch 195: train loss = 11857.115696, test loss = 12880.959232\n",
            "Epoch 196: train loss = 11848.296885, test loss = 12873.000901\n",
            "Epoch 197: train loss = 11839.340642, test loss = 12864.917923\n",
            "Epoch 198: train loss = 11830.246017, test loss = 12856.709252\n",
            "Epoch 199: train loss = 11821.012097, test loss = 12848.373877\n",
            "Epoch 200: train loss = 11811.638012, test loss = 12839.910823\n",
            "Epoch 201: train loss = 11802.122935, test loss = 12831.319149\n",
            "Epoch 202: train loss = 11792.466077, test loss = 12822.597950\n",
            "Epoch 203: train loss = 11782.666694, test loss = 12813.746357\n",
            "Epoch 204: train loss = 11772.724083, test loss = 12804.763537\n",
            "Epoch 205: train loss = 11762.637583, test loss = 12795.648694\n",
            "Epoch 206: train loss = 11752.406577, test loss = 12786.401070\n",
            "Epoch 207: train loss = 11742.030489, test loss = 12777.019943\n",
            "Epoch 208: train loss = 11731.508790, test loss = 12767.504629\n",
            "Epoch 209: train loss = 11720.840990, test loss = 12757.854484\n",
            "Epoch 210: train loss = 11710.026649, test loss = 12748.068900\n",
            "Epoch 211: train loss = 11699.065365, test loss = 12738.147310\n",
            "Epoch 212: train loss = 11687.956787, test loss = 12728.089185\n",
            "Epoch 213: train loss = 11676.700605, test loss = 12717.894038\n",
            "Epoch 214: train loss = 11665.296557, test loss = 12707.561419\n",
            "Epoch 215: train loss = 11653.744427, test loss = 12697.090921\n",
            "Epoch 216: train loss = 11642.044046, test loss = 12686.482179\n",
            "Epoch 217: train loss = 11630.195291, test loss = 12675.734868\n",
            "Epoch 218: train loss = 11618.198089, test loss = 12664.848705\n",
            "Epoch 219: train loss = 11606.052414, test loss = 12653.823451\n",
            "Epoch 220: train loss = 11593.758291, test loss = 12642.658910\n",
            "Epoch 221: train loss = 11581.315792, test loss = 12631.354930\n",
            "Epoch 222: train loss = 11568.725042, test loss = 12619.911402\n",
            "Epoch 223: train loss = 11555.986216, test loss = 12608.328265\n",
            "Epoch 224: train loss = 11543.099539, test loss = 12596.605500\n",
            "Epoch 225: train loss = 11530.065291, test loss = 12584.743137\n",
            "Epoch 226: train loss = 11516.883803, test loss = 12572.741251\n",
            "Epoch 227: train loss = 11503.555460, test loss = 12560.599964\n",
            "Epoch 228: train loss = 11490.080700, test loss = 12548.319448\n",
            "Epoch 229: train loss = 11476.460017, test loss = 12535.899921\n",
            "Epoch 230: train loss = 11462.693959, test loss = 12523.341652\n",
            "Epoch 231: train loss = 11448.783129, test loss = 12510.644956\n",
            "Epoch 232: train loss = 11434.728186, test loss = 12497.810200\n",
            "Epoch 233: train loss = 11420.529847, test loss = 12484.837803\n",
            "Epoch 234: train loss = 11406.188882, test loss = 12471.728230\n",
            "Epoch 235: train loss = 11391.706120, test loss = 12458.482000\n",
            "Epoch 236: train loss = 11377.082447, test loss = 12445.099682\n",
            "Epoch 237: train loss = 11362.318805, test loss = 12431.581897\n",
            "Epoch 238: train loss = 11347.416192, test loss = 12417.929318\n",
            "Epoch 239: train loss = 11332.375665, test loss = 12404.142667\n",
            "Epoch 240: train loss = 11317.198337, test loss = 12390.222719\n",
            "Epoch 241: train loss = 11301.885377, test loss = 12376.170303\n",
            "Epoch 242: train loss = 11286.438009, test loss = 12361.986296\n",
            "Epoch 243: train loss = 11270.857515, test loss = 12347.671628\n",
            "Epoch 244: train loss = 11255.145231, test loss = 12333.227280\n",
            "Epoch 245: train loss = 11239.302547, test loss = 12318.654283\n",
            "Epoch 246: train loss = 11223.330908, test loss = 12303.953719\n",
            "Epoch 247: train loss = 11207.231812, test loss = 12289.126720\n",
            "Epoch 248: train loss = 11191.006807, test loss = 12274.174467\n",
            "Epoch 249: train loss = 11174.657495, test loss = 12259.098188\n",
            "Epoch 250: train loss = 11158.185526, test loss = 12243.899163\n",
            "Epoch 251: train loss = 11141.592598, test loss = 12228.578715\n",
            "Epoch 252: train loss = 11124.880460, test loss = 12213.138216\n",
            "Epoch 253: train loss = 11108.050903, test loss = 12197.579080\n",
            "Epoch 254: train loss = 11091.105765, test loss = 12181.902768\n",
            "Epoch 255: train loss = 11074.046926, test loss = 12166.110783\n",
            "Epoch 256: train loss = 11056.876307, test loss = 12150.204670\n",
            "Epoch 257: train loss = 11039.595871, test loss = 12134.186015\n",
            "Epoch 258: train loss = 11022.207614, test loss = 12118.056441\n",
            "Epoch 259: train loss = 11004.713574, test loss = 12101.817611\n",
            "Epoch 260: train loss = 10987.115817, test loss = 12085.471224\n",
            "Epoch 261: train loss = 10969.416445, test loss = 12069.019013\n",
            "Epoch 262: train loss = 10951.617588, test loss = 12052.462744\n",
            "Epoch 263: train loss = 10933.721405, test loss = 12035.804216\n",
            "Epoch 264: train loss = 10915.730080, test loss = 12019.045254\n",
            "Epoch 265: train loss = 10897.645819, test loss = 12002.187717\n",
            "Epoch 266: train loss = 10879.470852, test loss = 11985.233485\n",
            "Epoch 267: train loss = 10861.207426, test loss = 11968.184464\n",
            "Epoch 268: train loss = 10842.857805, test loss = 11951.042584\n",
            "Epoch 269: train loss = 10824.424268, test loss = 11933.809795\n",
            "Epoch 270: train loss = 10805.909106, test loss = 11916.488065\n",
            "Epoch 271: train loss = 10787.314620, test loss = 11899.079379\n",
            "Epoch 272: train loss = 10768.643117, test loss = 11881.585738\n",
            "Epoch 273: train loss = 10749.896912, test loss = 11864.009155\n",
            "Epoch 274: train loss = 10731.078321, test loss = 11846.351656\n",
            "Epoch 275: train loss = 10712.189662, test loss = 11828.615275\n",
            "Epoch 276: train loss = 10693.233251, test loss = 11810.802053\n",
            "Epoch 277: train loss = 10674.211401, test loss = 11792.914037\n",
            "Epoch 278: train loss = 10655.126419, test loss = 11774.953278\n",
            "Epoch 279: train loss = 10635.980605, test loss = 11756.921828\n",
            "Epoch 280: train loss = 10616.776250, test loss = 11738.821740\n",
            "Epoch 281: train loss = 10597.515630, test loss = 11720.655064\n",
            "Epoch 282: train loss = 10578.201012, test loss = 11702.423849\n",
            "Epoch 283: train loss = 10558.834645, test loss = 11684.130135\n",
            "Epoch 284: train loss = 10539.418761, test loss = 11665.775957\n",
            "Epoch 285: train loss = 10519.955575, test loss = 11647.363343\n",
            "Epoch 286: train loss = 10500.447278, test loss = 11628.894309\n",
            "Epoch 287: train loss = 10480.896043, test loss = 11610.370859\n",
            "Epoch 288: train loss = 10461.304017, test loss = 11591.794985\n",
            "Epoch 289: train loss = 10441.673323, test loss = 11573.168665\n",
            "Epoch 290: train loss = 10422.006058, test loss = 11554.493860\n",
            "Epoch 291: train loss = 10402.304291, test loss = 11535.772515\n",
            "Epoch 292: train loss = 10382.570062, test loss = 11517.006556\n",
            "Epoch 293: train loss = 10362.805382, test loss = 11498.197889\n",
            "Epoch 294: train loss = 10343.012233, test loss = 11479.348401\n",
            "Epoch 295: train loss = 10323.192561, test loss = 11460.459955\n",
            "Epoch 296: train loss = 10303.348284, test loss = 11441.534395\n",
            "Epoch 297: train loss = 10283.481285, test loss = 11422.573538\n",
            "Epoch 298: train loss = 10263.593412, test loss = 11403.579180\n",
            "Epoch 299: train loss = 10243.686482, test loss = 11384.553087\n",
            "Epoch 300: train loss = 10223.762273, test loss = 11365.497005\n",
            "Epoch 301: train loss = 10203.822531, test loss = 11346.412650\n",
            "Epoch 302: train loss = 10183.868964, test loss = 11327.301712\n",
            "Epoch 303: train loss = 10163.903244, test loss = 11308.165853\n",
            "Epoch 304: train loss = 10143.927010, test loss = 11289.006708\n",
            "Epoch 305: train loss = 10123.941860, test loss = 11269.825883\n",
            "Epoch 306: train loss = 10103.949359, test loss = 11250.624954\n",
            "Epoch 307: train loss = 10083.951034, test loss = 11231.405471\n",
            "Epoch 308: train loss = 10063.948376, test loss = 11212.168951\n",
            "Epoch 309: train loss = 10043.942840, test loss = 11192.916886\n",
            "Epoch 310: train loss = 10023.935844, test loss = 11173.650735\n",
            "Epoch 311: train loss = 10003.928770, test loss = 11154.371929\n",
            "Epoch 312: train loss = 9983.922966, test loss = 11135.081869\n",
            "Epoch 313: train loss = 9963.919743, test loss = 11115.781926\n",
            "Epoch 314: train loss = 9943.920377, test loss = 11096.473442\n",
            "Epoch 315: train loss = 9923.926111, test loss = 11077.157729\n",
            "Epoch 316: train loss = 9903.938151, test loss = 11057.836072\n",
            "Epoch 317: train loss = 9883.957673, test loss = 11038.509723\n",
            "Epoch 318: train loss = 9863.985817, test loss = 11019.179910\n",
            "Epoch 319: train loss = 9844.023690, test loss = 10999.847828\n",
            "Epoch 320: train loss = 9824.072371, test loss = 10980.514646\n",
            "Epoch 321: train loss = 9804.132902, test loss = 10961.181505\n",
            "Epoch 322: train loss = 9784.206299, test loss = 10941.849518\n",
            "Epoch 323: train loss = 9764.293544, test loss = 10922.519771\n",
            "Epoch 324: train loss = 9744.395593, test loss = 10903.193324\n",
            "Epoch 325: train loss = 9724.513370, test loss = 10883.871208\n",
            "Epoch 326: train loss = 9704.647773, test loss = 10864.554432\n",
            "Epoch 327: train loss = 9684.799672, test loss = 10845.243976\n",
            "Epoch 328: train loss = 9664.969910, test loss = 10825.940797\n",
            "Epoch 329: train loss = 9645.159303, test loss = 10806.645827\n",
            "Epoch 330: train loss = 9625.368645, test loss = 10787.359974\n",
            "Epoch 331: train loss = 9605.598701, test loss = 10768.084122\n",
            "Epoch 332: train loss = 9585.850216, test loss = 10748.819134\n",
            "Epoch 333: train loss = 9566.123910, test loss = 10729.565849\n",
            "Epoch 334: train loss = 9546.420482, test loss = 10710.325083\n",
            "Epoch 335: train loss = 9526.740607, test loss = 10691.097634\n",
            "Epoch 336: train loss = 9507.084941, test loss = 10671.884276\n",
            "Epoch 337: train loss = 9487.454119, test loss = 10652.685765\n",
            "Epoch 338: train loss = 9467.848759, test loss = 10633.502836\n",
            "Epoch 339: train loss = 9448.269455, test loss = 10614.336206\n",
            "Epoch 340: train loss = 9428.716789, test loss = 10595.186572\n",
            "Epoch 341: train loss = 9409.191320, test loss = 10576.054614\n",
            "Epoch 342: train loss = 9389.693594, test loss = 10556.940995\n",
            "Epoch 343: train loss = 9370.224139, test loss = 10537.846360\n",
            "Epoch 344: train loss = 9350.783469, test loss = 10518.771337\n",
            "Epoch 345: train loss = 9331.372081, test loss = 10499.716540\n",
            "Epoch 346: train loss = 9311.990458, test loss = 10480.682566\n",
            "Epoch 347: train loss = 9292.639072, test loss = 10461.669998\n",
            "Epoch 348: train loss = 9273.318378, test loss = 10442.679403\n",
            "Epoch 349: train loss = 9254.028821, test loss = 10423.711335\n",
            "Epoch 350: train loss = 9234.770832, test loss = 10404.766336\n",
            "Epoch 351: train loss = 9215.544831, test loss = 10385.844931\n",
            "Epoch 352: train loss = 9196.351227, test loss = 10366.947637\n",
            "Epoch 353: train loss = 9177.190418, test loss = 10348.074955\n",
            "Epoch 354: train loss = 9158.062791, test loss = 10329.227376\n",
            "Epoch 355: train loss = 9138.968725, test loss = 10310.405379\n",
            "Epoch 356: train loss = 9119.908586, test loss = 10291.609431\n",
            "Epoch 357: train loss = 9100.882735, test loss = 10272.839991\n",
            "Epoch 358: train loss = 9081.891520, test loss = 10254.097504\n",
            "Epoch 359: train loss = 9062.935283, test loss = 10235.382407\n",
            "Epoch 360: train loss = 9044.014357, test loss = 10216.695128\n",
            "Epoch 361: train loss = 9025.129067, test loss = 10198.036082\n",
            "Epoch 362: train loss = 9006.279730, test loss = 10179.405680\n",
            "Epoch 363: train loss = 8987.466656, test loss = 10160.804319\n",
            "Epoch 364: train loss = 8968.690148, test loss = 10142.232391\n",
            "Epoch 365: train loss = 8949.950500, test loss = 10123.690276\n",
            "Epoch 366: train loss = 8931.248002, test loss = 10105.178350\n",
            "Epoch 367: train loss = 8912.582936, test loss = 10086.696977\n",
            "Epoch 368: train loss = 8893.955576, test loss = 10068.246516\n",
            "Epoch 369: train loss = 8875.366191, test loss = 10049.827317\n",
            "Epoch 370: train loss = 8856.815044, test loss = 10031.439722\n",
            "Epoch 371: train loss = 8838.302392, test loss = 10013.084066\n",
            "Epoch 372: train loss = 8819.828485, test loss = 9994.760679\n",
            "Epoch 373: train loss = 8801.393566, test loss = 9976.469880\n",
            "Epoch 374: train loss = 8782.997876, test loss = 9958.211985\n",
            "Epoch 375: train loss = 8764.641645, test loss = 9939.987299\n",
            "Epoch 376: train loss = 8746.325101, test loss = 9921.796124\n",
            "Epoch 377: train loss = 8728.048464, test loss = 9903.638754\n",
            "Epoch 378: train loss = 8709.811951, test loss = 9885.515475\n",
            "Epoch 379: train loss = 8691.615771, test loss = 9867.426569\n",
            "Epoch 380: train loss = 8673.460127, test loss = 9849.372310\n",
            "Epoch 381: train loss = 8655.345218, test loss = 9831.352965\n",
            "Epoch 382: train loss = 8637.271237, test loss = 9813.368797\n",
            "Epoch 383: train loss = 8619.238371, test loss = 9795.420060\n",
            "Epoch 384: train loss = 8601.246800, test loss = 9777.507004\n",
            "Epoch 385: train loss = 8583.296702, test loss = 9759.629872\n",
            "Epoch 386: train loss = 8565.388246, test loss = 9741.788901\n",
            "Epoch 387: train loss = 8547.521596, test loss = 9723.984321\n",
            "Epoch 388: train loss = 8529.696912, test loss = 9706.216356\n",
            "Epoch 389: train loss = 8511.914347, test loss = 9688.485226\n",
            "Epoch 390: train loss = 8494.174048, test loss = 9670.791143\n",
            "Epoch 391: train loss = 8476.476158, test loss = 9653.134312\n",
            "Epoch 392: train loss = 8458.820813, test loss = 9635.514935\n",
            "Epoch 393: train loss = 8441.208142, test loss = 9617.933206\n",
            "Epoch 394: train loss = 8423.638272, test loss = 9600.389313\n",
            "Epoch 395: train loss = 8406.111322, test loss = 9582.883439\n",
            "Epoch 396: train loss = 8388.627404, test loss = 9565.415760\n",
            "Epoch 397: train loss = 8371.186627, test loss = 9547.986447\n",
            "Epoch 398: train loss = 8353.789093, test loss = 9530.595665\n",
            "Epoch 399: train loss = 8336.434897, test loss = 9513.243571\n",
            "Epoch 400: train loss = 8319.124132, test loss = 9495.930320\n",
            "Epoch 401: train loss = 8301.856881, test loss = 9478.656057\n",
            "Epoch 402: train loss = 8284.633224, test loss = 9461.420925\n",
            "Epoch 403: train loss = 8267.453234, test loss = 9444.225058\n",
            "Epoch 404: train loss = 8250.316980, test loss = 9427.068585\n",
            "Epoch 405: train loss = 8233.224523, test loss = 9409.951632\n",
            "Epoch 406: train loss = 8216.175921, test loss = 9392.874315\n",
            "Epoch 407: train loss = 8199.171223, test loss = 9375.836746\n",
            "Epoch 408: train loss = 8182.210476, test loss = 9358.839033\n",
            "Epoch 409: train loss = 8165.293719, test loss = 9341.881276\n",
            "Epoch 410: train loss = 8148.420987, test loss = 9324.963570\n",
            "Epoch 411: train loss = 8131.592308, test loss = 9308.086005\n",
            "Epoch 412: train loss = 8114.807705, test loss = 9291.248665\n",
            "Epoch 413: train loss = 8098.067197, test loss = 9274.451628\n",
            "Epoch 414: train loss = 8081.370795, test loss = 9257.694967\n",
            "Epoch 415: train loss = 8064.718508, test loss = 9240.978750\n",
            "Epoch 416: train loss = 8048.110336, test loss = 9224.303038\n",
            "Epoch 417: train loss = 8031.546277, test loss = 9207.667889\n",
            "Epoch 418: train loss = 8015.026321, test loss = 9191.073355\n",
            "Epoch 419: train loss = 7998.550456, test loss = 9174.519480\n",
            "Epoch 420: train loss = 7982.118662, test loss = 9158.006306\n",
            "Epoch 421: train loss = 7965.730915, test loss = 9141.533869\n",
            "Epoch 422: train loss = 7949.387187, test loss = 9125.102200\n",
            "Epoch 423: train loss = 7933.087444, test loss = 9108.711324\n",
            "Epoch 424: train loss = 7916.831647, test loss = 9092.361262\n",
            "Epoch 425: train loss = 7900.619754, test loss = 9076.052029\n",
            "Epoch 426: train loss = 7884.451717, test loss = 9059.783636\n",
            "Epoch 427: train loss = 7868.327482, test loss = 9043.556090\n",
            "Epoch 428: train loss = 7852.246993, test loss = 9027.369392\n",
            "Epoch 429: train loss = 7836.210189, test loss = 9011.223538\n",
            "Epoch 430: train loss = 7820.217003, test loss = 8995.118520\n",
            "Epoch 431: train loss = 7804.267365, test loss = 8979.054326\n",
            "Epoch 432: train loss = 7788.361202, test loss = 8963.030940\n",
            "Epoch 433: train loss = 7772.498434, test loss = 8947.048338\n",
            "Epoch 434: train loss = 7756.678979, test loss = 8931.106497\n",
            "Epoch 435: train loss = 7740.902751, test loss = 8915.205386\n",
            "Epoch 436: train loss = 7725.169659, test loss = 8899.344971\n",
            "Epoch 437: train loss = 7709.479610, test loss = 8883.525214\n",
            "Epoch 438: train loss = 7693.832504, test loss = 8867.746073\n",
            "Epoch 439: train loss = 7678.228242, test loss = 8852.007501\n",
            "Epoch 440: train loss = 7662.666718, test loss = 8836.309450\n",
            "Epoch 441: train loss = 7647.147824, test loss = 8820.651864\n",
            "Epoch 442: train loss = 7631.671448, test loss = 8805.034688\n",
            "Epoch 443: train loss = 7616.237476, test loss = 8789.457860\n",
            "Epoch 444: train loss = 7600.845789, test loss = 8773.921315\n",
            "Epoch 445: train loss = 7585.496268, test loss = 8758.424986\n",
            "Epoch 446: train loss = 7570.188788, test loss = 8742.968802\n",
            "Epoch 447: train loss = 7554.923222, test loss = 8727.552689\n",
            "Epoch 448: train loss = 7539.699442, test loss = 8712.176568\n",
            "Epoch 449: train loss = 7524.517316, test loss = 8696.840360\n",
            "Epoch 450: train loss = 7509.376710, test loss = 8681.543980\n",
            "Epoch 451: train loss = 7494.277486, test loss = 8666.287343\n",
            "Epoch 452: train loss = 7479.219506, test loss = 8651.070358\n",
            "Epoch 453: train loss = 7464.202629, test loss = 8635.892936\n",
            "Epoch 454: train loss = 7449.226712, test loss = 8620.754980\n",
            "Epoch 455: train loss = 7434.291609, test loss = 8605.656394\n",
            "Epoch 456: train loss = 7419.397175, test loss = 8590.597078\n",
            "Epoch 457: train loss = 7404.543261, test loss = 8575.576931\n",
            "Epoch 458: train loss = 7389.729717, test loss = 8560.595850\n",
            "Epoch 459: train loss = 7374.956390, test loss = 8545.653728\n",
            "Epoch 460: train loss = 7360.223129, test loss = 8530.750457\n",
            "Epoch 461: train loss = 7345.529779, test loss = 8515.885928\n",
            "Epoch 462: train loss = 7330.876185, test loss = 8501.060029\n",
            "Epoch 463: train loss = 7316.262191, test loss = 8486.272646\n",
            "Epoch 464: train loss = 7301.687640, test loss = 8471.523666\n",
            "Epoch 465: train loss = 7287.152375, test loss = 8456.812970\n",
            "Epoch 466: train loss = 7272.656236, test loss = 8442.140443\n",
            "Epoch 467: train loss = 7258.199065, test loss = 8427.505965\n",
            "Epoch 468: train loss = 7243.780703, test loss = 8412.909415\n",
            "Epoch 469: train loss = 7229.400990, test loss = 8398.350673\n",
            "Epoch 470: train loss = 7215.059767, test loss = 8383.829616\n",
            "Epoch 471: train loss = 7200.756873, test loss = 8369.346122\n",
            "Epoch 472: train loss = 7186.492150, test loss = 8354.900066\n",
            "Epoch 473: train loss = 7172.265438, test loss = 8340.491325\n",
            "Epoch 474: train loss = 7158.076579, test loss = 8326.119773\n",
            "Epoch 475: train loss = 7143.925412, test loss = 8311.785286\n",
            "Epoch 476: train loss = 7129.811782, test loss = 8297.487736\n",
            "Epoch 477: train loss = 7115.735531, test loss = 8283.227000\n",
            "Epoch 478: train loss = 7101.696501, test loss = 8269.002950\n",
            "Epoch 479: train loss = 7087.694539, test loss = 8254.815461\n",
            "Epoch 480: train loss = 7073.729489, test loss = 8240.664406\n",
            "Epoch 481: train loss = 7059.801199, test loss = 8226.549661\n",
            "Epoch 482: train loss = 7045.909517, test loss = 8212.471100\n",
            "Epoch 483: train loss = 7032.054292, test loss = 8198.428599\n",
            "Epoch 484: train loss = 7018.235375, test loss = 8184.422032\n",
            "Epoch 485: train loss = 7004.452620, test loss = 8170.451275\n",
            "Epoch 486: train loss = 6990.705880, test loss = 8156.516207\n",
            "Epoch 487: train loss = 6976.995012, test loss = 8142.616704\n",
            "Epoch 488: train loss = 6963.319875, test loss = 8128.752646\n",
            "Epoch 489: train loss = 6949.680328, test loss = 8114.923911\n",
            "Epoch 490: train loss = 6936.076234, test loss = 8101.130381\n",
            "Epoch 491: train loss = 6922.507458, test loss = 8087.371938\n",
            "Epoch 492: train loss = 6908.973866, test loss = 8073.648464\n",
            "Epoch 493: train loss = 6895.475329, test loss = 8059.959845\n",
            "Epoch 494: train loss = 6882.011717, test loss = 8046.305966\n",
            "Epoch 495: train loss = 6868.582906, test loss = 8032.686714\n",
            "Epoch 496: train loss = 6855.188772, test loss = 8019.101980\n",
            "Epoch 497: train loss = 6841.829196, test loss = 8005.551653\n",
            "Epoch 498: train loss = 6828.504059, test loss = 7992.035626\n",
            "Epoch 499: train loss = 6815.213248, test loss = 7978.553793\n",
            "Epoch 500: train loss = 6801.956651, test loss = 7965.106051\n",
            "Epoch 501: train loss = 6788.734158, test loss = 7951.692298\n",
            "Epoch 502: train loss = 6775.545665, test loss = 7938.312433\n",
            "Epoch 503: train loss = 6762.391068, test loss = 7924.966360\n",
            "Epoch 504: train loss = 6749.270268, test loss = 7911.653981\n",
            "Epoch 505: train loss = 6736.183170, test loss = 7898.375205\n",
            "Epoch 506: train loss = 6723.129678, test loss = 7885.129939\n",
            "Epoch 507: train loss = 6710.109704, test loss = 7871.918094\n",
            "Epoch 508: train loss = 6697.123161, test loss = 7858.739584\n",
            "Epoch 509: train loss = 6684.169965, test loss = 7845.594324\n",
            "Epoch 510: train loss = 6671.250036, test loss = 7832.482232\n",
            "Epoch 511: train loss = 6658.363296, test loss = 7819.403229\n",
            "Epoch 512: train loss = 6645.509672, test loss = 7806.357237\n",
            "Epoch 513: train loss = 6632.689093, test loss = 7793.344181\n",
            "Epoch 514: train loss = 6619.901492, test loss = 7780.363990\n",
            "Epoch 515: train loss = 6607.146805, test loss = 7767.416592\n",
            "Epoch 516: train loss = 6594.424971, test loss = 7754.501921\n",
            "Epoch 517: train loss = 6581.735932, test loss = 7741.619911\n",
            "Epoch 518: train loss = 6569.079634, test loss = 7728.770501\n",
            "Epoch 519: train loss = 6556.456026, test loss = 7715.953630\n",
            "Epoch 520: train loss = 6543.865058, test loss = 7703.169240\n",
            "Epoch 521: train loss = 6531.306687, test loss = 7690.417277\n",
            "Epoch 522: train loss = 6518.780869, test loss = 7677.697688\n",
            "Epoch 523: train loss = 6506.287565, test loss = 7665.010423\n",
            "Epoch 524: train loss = 6493.826740, test loss = 7652.355433\n",
            "Epoch 525: train loss = 6481.398360, test loss = 7639.732674\n",
            "Epoch 526: train loss = 6469.002393, test loss = 7627.142102\n",
            "Epoch 527: train loss = 6456.638813, test loss = 7614.583677\n",
            "Epoch 528: train loss = 6444.307594, test loss = 7602.057360\n",
            "Epoch 529: train loss = 6432.008714, test loss = 7589.563115\n",
            "Epoch 530: train loss = 6419.742152, test loss = 7577.100908\n",
            "Epoch 531: train loss = 6407.507891, test loss = 7564.670708\n",
            "Epoch 532: train loss = 6395.305915, test loss = 7552.272483\n",
            "Epoch 533: train loss = 6383.136213, test loss = 7539.906208\n",
            "Epoch 534: train loss = 6370.998772, test loss = 7527.571855\n",
            "Epoch 535: train loss = 6358.893585, test loss = 7515.269403\n",
            "Epoch 536: train loss = 6346.820645, test loss = 7502.998827\n",
            "Epoch 537: train loss = 6334.779947, test loss = 7490.760110\n",
            "Epoch 538: train loss = 6322.771489, test loss = 7478.553233\n",
            "Epoch 539: train loss = 6310.795271, test loss = 7466.378179\n",
            "Epoch 540: train loss = 6298.851292, test loss = 7454.234934\n",
            "Epoch 541: train loss = 6286.939555, test loss = 7442.123485\n",
            "Epoch 542: train loss = 6275.060065, test loss = 7430.043820\n",
            "Epoch 543: train loss = 6263.212826, test loss = 7417.995929\n",
            "Epoch 544: train loss = 6251.397846, test loss = 7405.979804\n",
            "Epoch 545: train loss = 6239.615131, test loss = 7393.995437\n",
            "Epoch 546: train loss = 6227.864692, test loss = 7382.042821\n",
            "Epoch 547: train loss = 6216.146539, test loss = 7370.121953\n",
            "Epoch 548: train loss = 6204.460682, test loss = 7358.232829\n",
            "Epoch 549: train loss = 6192.807133, test loss = 7346.375446\n",
            "Epoch 550: train loss = 6181.185905, test loss = 7334.549801\n",
            "Epoch 551: train loss = 6169.597012, test loss = 7322.755895\n",
            "Epoch 552: train loss = 6158.040466, test loss = 7310.993728\n",
            "Epoch 553: train loss = 6146.516284, test loss = 7299.263300\n",
            "Epoch 554: train loss = 6135.024480, test loss = 7287.564613\n",
            "Epoch 555: train loss = 6123.565068, test loss = 7275.897669\n",
            "Epoch 556: train loss = 6112.138064, test loss = 7264.262470\n",
            "Epoch 557: train loss = 6100.743485, test loss = 7252.659021\n",
            "Epoch 558: train loss = 6089.381344, test loss = 7241.087325\n",
            "Epoch 559: train loss = 6078.051659, test loss = 7229.547385\n",
            "Epoch 560: train loss = 6066.754445, test loss = 7218.039206\n",
            "Epoch 561: train loss = 6055.489717, test loss = 7206.562793\n",
            "Epoch 562: train loss = 6044.257491, test loss = 7195.118151\n",
            "Epoch 563: train loss = 6033.057781, test loss = 7183.705283\n",
            "Epoch 564: train loss = 6021.890602, test loss = 7172.324196\n",
            "Epoch 565: train loss = 6010.755967, test loss = 7160.974894\n",
            "Epoch 566: train loss = 5999.653892, test loss = 7149.657381\n",
            "Epoch 567: train loss = 5988.584389, test loss = 7138.371663\n",
            "Epoch 568: train loss = 5977.547469, test loss = 7127.117743\n",
            "Epoch 569: train loss = 5966.543146, test loss = 7115.895627\n",
            "Epoch 570: train loss = 5955.571430, test loss = 7104.705317\n",
            "Epoch 571: train loss = 5944.632331, test loss = 7093.546818\n",
            "Epoch 572: train loss = 5933.725860, test loss = 7082.420132\n",
            "Epoch 573: train loss = 5922.852024, test loss = 7071.325263\n",
            "Epoch 574: train loss = 5912.010830, test loss = 7060.262212\n",
            "Epoch 575: train loss = 5901.202287, test loss = 7049.230982\n",
            "Epoch 576: train loss = 5890.426399, test loss = 7038.231573\n",
            "Epoch 577: train loss = 5879.683172, test loss = 7027.263986\n",
            "Epoch 578: train loss = 5868.972608, test loss = 7016.328221\n",
            "Epoch 579: train loss = 5858.294711, test loss = 7005.424276\n",
            "Epoch 580: train loss = 5847.649481, test loss = 6994.552152\n",
            "Epoch 581: train loss = 5837.036919, test loss = 6983.711844\n",
            "Epoch 582: train loss = 5826.457023, test loss = 6972.903351\n",
            "Epoch 583: train loss = 5815.909792, test loss = 6962.126668\n",
            "Epoch 584: train loss = 5805.395221, test loss = 6951.381790\n",
            "Epoch 585: train loss = 5794.913307, test loss = 6940.668712\n",
            "Epoch 586: train loss = 5784.464042, test loss = 6929.987427\n",
            "Epoch 587: train loss = 5774.047421, test loss = 6919.337928\n",
            "Epoch 588: train loss = 5763.663433, test loss = 6908.720206\n",
            "Epoch 589: train loss = 5753.312069, test loss = 6898.134252\n",
            "Epoch 590: train loss = 5742.993317, test loss = 6887.580057\n",
            "Epoch 591: train loss = 5732.707166, test loss = 6877.057607\n",
            "Epoch 592: train loss = 5722.453600, test loss = 6866.566892\n",
            "Epoch 593: train loss = 5712.232604, test loss = 6856.107897\n",
            "Epoch 594: train loss = 5702.044162, test loss = 6845.680609\n",
            "Epoch 595: train loss = 5691.888256, test loss = 6835.285011\n",
            "Epoch 596: train loss = 5681.764865, test loss = 6824.921088\n",
            "Epoch 597: train loss = 5671.673969, test loss = 6814.588822\n",
            "Epoch 598: train loss = 5661.615546, test loss = 6804.288194\n",
            "Epoch 599: train loss = 5651.589572, test loss = 6794.019184\n",
            "Epoch 600: train loss = 5641.596022, test loss = 6783.781773\n",
            "Epoch 601: train loss = 5631.634870, test loss = 6773.575936\n",
            "Epoch 602: train loss = 5621.706088, test loss = 6763.401653\n",
            "Epoch 603: train loss = 5611.809647, test loss = 6753.258898\n",
            "Epoch 604: train loss = 5601.945516, test loss = 6743.147647\n",
            "Epoch 605: train loss = 5592.113665, test loss = 6733.067873\n",
            "Epoch 606: train loss = 5582.314059, test loss = 6723.019549\n",
            "Epoch 607: train loss = 5572.546665, test loss = 6713.002647\n",
            "Epoch 608: train loss = 5562.811447, test loss = 6703.017137\n",
            "Epoch 609: train loss = 5553.108368, test loss = 6693.062989\n",
            "Epoch 610: train loss = 5543.437390, test loss = 6683.140170\n",
            "Epoch 611: train loss = 5533.798474, test loss = 6673.248649\n",
            "Epoch 612: train loss = 5524.191579, test loss = 6663.388391\n",
            "Epoch 613: train loss = 5514.616663, test loss = 6653.559362\n",
            "Epoch 614: train loss = 5505.073684, test loss = 6643.761525\n",
            "Epoch 615: train loss = 5495.562596, test loss = 6633.994845\n",
            "Epoch 616: train loss = 5486.083355, test loss = 6624.259283\n",
            "Epoch 617: train loss = 5476.635914, test loss = 6614.554800\n",
            "Epoch 618: train loss = 5467.220225, test loss = 6604.881356\n",
            "Epoch 619: train loss = 5457.836240, test loss = 6595.238911\n",
            "Epoch 620: train loss = 5448.483908, test loss = 6585.627422\n",
            "Epoch 621: train loss = 5439.163179, test loss = 6576.046847\n",
            "Epoch 622: train loss = 5429.874001, test loss = 6566.497142\n",
            "Epoch 623: train loss = 5420.616321, test loss = 6556.978262\n",
            "Epoch 624: train loss = 5411.390084, test loss = 6547.490161\n",
            "Epoch 625: train loss = 5402.195236, test loss = 6538.032793\n",
            "Epoch 626: train loss = 5393.031720, test loss = 6528.606111\n",
            "Epoch 627: train loss = 5383.899479, test loss = 6519.210066\n",
            "Epoch 628: train loss = 5374.798456, test loss = 6509.844608\n",
            "Epoch 629: train loss = 5365.728592, test loss = 6500.509688\n",
            "Epoch 630: train loss = 5356.689826, test loss = 6491.205255\n",
            "Epoch 631: train loss = 5347.682098, test loss = 6481.931257\n",
            "Epoch 632: train loss = 5338.705347, test loss = 6472.687641\n",
            "Epoch 633: train loss = 5329.759510, test loss = 6463.474353\n",
            "Epoch 634: train loss = 5320.844524, test loss = 6454.291341\n",
            "Epoch 635: train loss = 5311.960325, test loss = 6445.138548\n",
            "Epoch 636: train loss = 5303.106848, test loss = 6436.015919\n",
            "Epoch 637: train loss = 5294.284028, test loss = 6426.923397\n",
            "Epoch 638: train loss = 5285.491798, test loss = 6417.860926\n",
            "Epoch 639: train loss = 5276.730091, test loss = 6408.828447\n",
            "Epoch 640: train loss = 5267.998840, test loss = 6399.825902\n",
            "Epoch 641: train loss = 5259.297976, test loss = 6390.853230\n",
            "Epoch 642: train loss = 5250.627431, test loss = 6381.910374\n",
            "Epoch 643: train loss = 5241.987133, test loss = 6372.997271\n",
            "Epoch 644: train loss = 5233.377014, test loss = 6364.113860\n",
            "Epoch 645: train loss = 5224.797001, test loss = 6355.260080\n",
            "Epoch 646: train loss = 5216.247024, test loss = 6346.435868\n",
            "Epoch 647: train loss = 5207.727010, test loss = 6337.641161\n",
            "Epoch 648: train loss = 5199.236887, test loss = 6328.875895\n",
            "Epoch 649: train loss = 5190.776581, test loss = 6320.140006\n",
            "Epoch 650: train loss = 5182.346018, test loss = 6311.433430\n",
            "Epoch 651: train loss = 5173.945125, test loss = 6302.756100\n",
            "Epoch 652: train loss = 5165.573826, test loss = 6294.107951\n",
            "Epoch 653: train loss = 5157.232046, test loss = 6285.488917\n",
            "Epoch 654: train loss = 5148.919710, test loss = 6276.898930\n",
            "Epoch 655: train loss = 5140.636740, test loss = 6268.337924\n",
            "Epoch 656: train loss = 5132.383062, test loss = 6259.805830\n",
            "Epoch 657: train loss = 5124.158597, test loss = 6251.302581\n",
            "Epoch 658: train loss = 5115.963269, test loss = 6242.828107\n",
            "Epoch 659: train loss = 5107.796999, test loss = 6234.382340\n",
            "Epoch 660: train loss = 5099.659709, test loss = 6225.965209\n",
            "Epoch 661: train loss = 5091.551321, test loss = 6217.576646\n",
            "Epoch 662: train loss = 5083.471756, test loss = 6209.216579\n",
            "Epoch 663: train loss = 5075.420935, test loss = 6200.884937\n",
            "Epoch 664: train loss = 5067.398778, test loss = 6192.581651\n",
            "Epoch 665: train loss = 5059.405206, test loss = 6184.306648\n",
            "Epoch 666: train loss = 5051.440138, test loss = 6176.059856\n",
            "Epoch 667: train loss = 5043.503494, test loss = 6167.841204\n",
            "Epoch 668: train loss = 5035.595194, test loss = 6159.650618\n",
            "Epoch 669: train loss = 5027.715157, test loss = 6151.488027\n",
            "Epoch 670: train loss = 5019.863301, test loss = 6143.353357\n",
            "Epoch 671: train loss = 5012.039546, test loss = 6135.246535\n",
            "Epoch 672: train loss = 5004.243809, test loss = 6127.167487\n",
            "Epoch 673: train loss = 4996.476010, test loss = 6119.116140\n",
            "Epoch 674: train loss = 4988.736066, test loss = 6111.092419\n",
            "Epoch 675: train loss = 4981.023895, test loss = 6103.096250\n",
            "Epoch 676: train loss = 4973.339415, test loss = 6095.127558\n",
            "Epoch 677: train loss = 4965.682544, test loss = 6087.186269\n",
            "Epoch 678: train loss = 4958.053199, test loss = 6079.272307\n",
            "Epoch 679: train loss = 4950.451297, test loss = 6071.385598\n",
            "Epoch 680: train loss = 4942.876755, test loss = 6063.526066\n",
            "Epoch 681: train loss = 4935.329492, test loss = 6055.693635\n",
            "Epoch 682: train loss = 4927.809422, test loss = 6047.888230\n",
            "Epoch 683: train loss = 4920.316465, test loss = 6040.109775\n",
            "Epoch 684: train loss = 4912.850536, test loss = 6032.358194\n",
            "Epoch 685: train loss = 4905.411552, test loss = 6024.633410\n",
            "Epoch 686: train loss = 4897.999429, test loss = 6016.935347\n",
            "Epoch 687: train loss = 4890.614085, test loss = 6009.263928\n",
            "Epoch 688: train loss = 4883.255436, test loss = 6001.619078\n",
            "Epoch 689: train loss = 4875.923398, test loss = 5994.000719\n",
            "Epoch 690: train loss = 4868.617888, test loss = 5986.408774\n",
            "Epoch 691: train loss = 4861.338822, test loss = 5978.843167\n",
            "Epoch 692: train loss = 4854.086116, test loss = 5971.303821\n",
            "Epoch 693: train loss = 4846.859688, test loss = 5963.790658\n",
            "Epoch 694: train loss = 4839.659453, test loss = 5956.303601\n",
            "Epoch 695: train loss = 4832.485328, test loss = 5948.842573\n",
            "Epoch 696: train loss = 4825.337229, test loss = 5941.407497\n",
            "Epoch 697: train loss = 4818.215072, test loss = 5933.998296\n",
            "Epoch 698: train loss = 4811.118775, test loss = 5926.614891\n",
            "Epoch 699: train loss = 4804.048252, test loss = 5919.257206\n",
            "Epoch 700: train loss = 4797.003422, test loss = 5911.925163\n",
            "Epoch 701: train loss = 4789.984200, test loss = 5904.618684\n",
            "Epoch 702: train loss = 4782.990503, test loss = 5897.337693\n",
            "Epoch 703: train loss = 4776.022248, test loss = 5890.082111\n",
            "Epoch 704: train loss = 4769.079350, test loss = 5882.851860\n",
            "Epoch 705: train loss = 4762.161728, test loss = 5875.646864\n",
            "Epoch 706: train loss = 4755.269297, test loss = 5868.467045\n",
            "Epoch 707: train loss = 4748.401975, test loss = 5861.312325\n",
            "Epoch 708: train loss = 4741.559678, test loss = 5854.182627\n",
            "Epoch 709: train loss = 4734.742324, test loss = 5847.077872\n",
            "Epoch 710: train loss = 4727.949829, test loss = 5839.997984\n",
            "Epoch 711: train loss = 4721.182111, test loss = 5832.942886\n",
            "Epoch 712: train loss = 4714.439087, test loss = 5825.912499\n",
            "Epoch 713: train loss = 4707.720675, test loss = 5818.906746\n",
            "Epoch 714: train loss = 4701.026791, test loss = 5811.925550\n",
            "Epoch 715: train loss = 4694.357353, test loss = 5804.968833\n",
            "Epoch 716: train loss = 4687.712280, test loss = 5798.036519\n",
            "Epoch 717: train loss = 4681.091488, test loss = 5791.128530\n",
            "Epoch 718: train loss = 4674.494897, test loss = 5784.244788\n",
            "Epoch 719: train loss = 4667.922423, test loss = 5777.385217\n",
            "Epoch 720: train loss = 4661.373985, test loss = 5770.549740\n",
            "Epoch 721: train loss = 4654.849501, test loss = 5763.738280\n",
            "Epoch 722: train loss = 4648.348890, test loss = 5756.950759\n",
            "Epoch 723: train loss = 4641.872071, test loss = 5750.187101\n",
            "Epoch 724: train loss = 4635.418961, test loss = 5743.447230\n",
            "Epoch 725: train loss = 4628.989480, test loss = 5736.731068\n",
            "Epoch 726: train loss = 4622.583547, test loss = 5730.038539\n",
            "Epoch 727: train loss = 4616.201081, test loss = 5723.369566\n",
            "Epoch 728: train loss = 4609.842001, test loss = 5716.724074\n",
            "Epoch 729: train loss = 4603.506227, test loss = 5710.101986\n",
            "Epoch 730: train loss = 4597.193677, test loss = 5703.503225\n",
            "Epoch 731: train loss = 4590.904273, test loss = 5696.927716\n",
            "Epoch 732: train loss = 4584.637934, test loss = 5690.375382\n",
            "Epoch 733: train loss = 4578.394579, test loss = 5683.846148\n",
            "Epoch 734: train loss = 4572.174130, test loss = 5677.339939\n",
            "Epoch 735: train loss = 4565.976506, test loss = 5670.856677\n",
            "Epoch 736: train loss = 4559.801628, test loss = 5664.396289\n",
            "Epoch 737: train loss = 4553.649417, test loss = 5657.958698\n",
            "Epoch 738: train loss = 4547.519794, test loss = 5651.543829\n",
            "Epoch 739: train loss = 4541.412679, test loss = 5645.151607\n",
            "Epoch 740: train loss = 4535.327995, test loss = 5638.781957\n",
            "Epoch 741: train loss = 4529.265662, test loss = 5632.434805\n",
            "Epoch 742: train loss = 4523.225602, test loss = 5626.110075\n",
            "Epoch 743: train loss = 4517.207737, test loss = 5619.807693\n",
            "Epoch 744: train loss = 4511.211989, test loss = 5613.527584\n",
            "Epoch 745: train loss = 4505.238280, test loss = 5607.269674\n",
            "Epoch 746: train loss = 4499.286533, test loss = 5601.033889\n",
            "Epoch 747: train loss = 4493.356669, test loss = 5594.820155\n",
            "Epoch 748: train loss = 4487.448613, test loss = 5588.628397\n",
            "Epoch 749: train loss = 4481.562285, test loss = 5582.458543\n",
            "Epoch 750: train loss = 4475.697611, test loss = 5576.310518\n",
            "Epoch 751: train loss = 4469.854513, test loss = 5570.184250\n",
            "Epoch 752: train loss = 4464.032914, test loss = 5564.079664\n",
            "Epoch 753: train loss = 4458.232738, test loss = 5557.996687\n",
            "Epoch 754: train loss = 4452.453910, test loss = 5551.935248\n",
            "Epoch 755: train loss = 4446.696353, test loss = 5545.895271\n",
            "Epoch 756: train loss = 4440.959992, test loss = 5539.876687\n",
            "Epoch 757: train loss = 4435.244750, test loss = 5533.879420\n",
            "Epoch 758: train loss = 4429.550554, test loss = 5527.903400\n",
            "Epoch 759: train loss = 4423.877327, test loss = 5521.948554\n",
            "Epoch 760: train loss = 4418.224994, test loss = 5516.014810\n",
            "Epoch 761: train loss = 4412.593482, test loss = 5510.102095\n",
            "Epoch 762: train loss = 4406.982716, test loss = 5504.210340\n",
            "Epoch 763: train loss = 4401.392621, test loss = 5498.339471\n",
            "Epoch 764: train loss = 4395.823123, test loss = 5492.489418\n",
            "Epoch 765: train loss = 4390.274148, test loss = 5486.660109\n",
            "Epoch 766: train loss = 4384.745624, test loss = 5480.851473\n",
            "Epoch 767: train loss = 4379.237475, test loss = 5475.063440\n",
            "Epoch 768: train loss = 4373.749630, test loss = 5469.295939\n",
            "Epoch 769: train loss = 4368.282014, test loss = 5463.548899\n",
            "Epoch 770: train loss = 4362.834556, test loss = 5457.822250\n",
            "Epoch 771: train loss = 4357.407182, test loss = 5452.115922\n",
            "Epoch 772: train loss = 4351.999821, test loss = 5446.429845\n",
            "Epoch 773: train loss = 4346.612399, test loss = 5440.763948\n",
            "Epoch 774: train loss = 4341.244845, test loss = 5435.118164\n",
            "Epoch 775: train loss = 4335.897088, test loss = 5429.492421\n",
            "Epoch 776: train loss = 4330.569055, test loss = 5423.886651\n",
            "Epoch 777: train loss = 4325.260676, test loss = 5418.300785\n",
            "Epoch 778: train loss = 4319.971878, test loss = 5412.734754\n",
            "Epoch 779: train loss = 4314.702592, test loss = 5407.188489\n",
            "Epoch 780: train loss = 4309.452747, test loss = 5401.661921\n",
            "Epoch 781: train loss = 4304.222271, test loss = 5396.154983\n",
            "Epoch 782: train loss = 4299.011096, test loss = 5390.667606\n",
            "Epoch 783: train loss = 4293.819150, test loss = 5385.199722\n",
            "Epoch 784: train loss = 4288.646364, test loss = 5379.751263\n",
            "Epoch 785: train loss = 4283.492668, test loss = 5374.322162\n",
            "Epoch 786: train loss = 4278.357993, test loss = 5368.912352\n",
            "Epoch 787: train loss = 4273.242270, test loss = 5363.521764\n",
            "Epoch 788: train loss = 4268.145429, test loss = 5358.150333\n",
            "Epoch 789: train loss = 4263.067402, test loss = 5352.797991\n",
            "Epoch 790: train loss = 4258.008120, test loss = 5347.464671\n",
            "Epoch 791: train loss = 4252.967516, test loss = 5342.150308\n",
            "Epoch 792: train loss = 4247.945520, test loss = 5336.854834\n",
            "Epoch 793: train loss = 4242.942065, test loss = 5331.578184\n",
            "Epoch 794: train loss = 4237.957083, test loss = 5326.320292\n",
            "Epoch 795: train loss = 4232.990507, test loss = 5321.081092\n",
            "Epoch 796: train loss = 4228.042269, test loss = 5315.860518\n",
            "Epoch 797: train loss = 4223.112302, test loss = 5310.658506\n",
            "Epoch 798: train loss = 4218.200540, test loss = 5305.474990\n",
            "Epoch 799: train loss = 4213.306916, test loss = 5300.309904\n",
            "Epoch 800: train loss = 4208.431363, test loss = 5295.163186\n",
            "Epoch 801: train loss = 4203.573816, test loss = 5290.034768\n",
            "Epoch 802: train loss = 4198.734208, test loss = 5284.924588\n",
            "Epoch 803: train loss = 4193.912473, test loss = 5279.832582\n",
            "Epoch 804: train loss = 4189.108546, test loss = 5274.758684\n",
            "Epoch 805: train loss = 4184.322362, test loss = 5269.702832\n",
            "Epoch 806: train loss = 4179.553855, test loss = 5264.664961\n",
            "Epoch 807: train loss = 4174.802960, test loss = 5259.645009\n",
            "Epoch 808: train loss = 4170.069613, test loss = 5254.642912\n",
            "Epoch 809: train loss = 4165.353750, test loss = 5249.658606\n",
            "Epoch 810: train loss = 4160.655305, test loss = 5244.692030\n",
            "Epoch 811: train loss = 4155.974216, test loss = 5239.743120\n",
            "Epoch 812: train loss = 4151.310417, test loss = 5234.811814\n",
            "Epoch 813: train loss = 4146.663846, test loss = 5229.898050\n",
            "Epoch 814: train loss = 4142.034439, test loss = 5225.001766\n",
            "Epoch 815: train loss = 4137.422132, test loss = 5220.122899\n",
            "Epoch 816: train loss = 4132.826863, test loss = 5215.261388\n",
            "Epoch 817: train loss = 4128.248569, test loss = 5210.417171\n",
            "Epoch 818: train loss = 4123.687187, test loss = 5205.590188\n",
            "Epoch 819: train loss = 4119.142655, test loss = 5200.780377\n",
            "Epoch 820: train loss = 4114.614910, test loss = 5195.987676\n",
            "Epoch 821: train loss = 4110.103892, test loss = 5191.212026\n",
            "Epoch 822: train loss = 4105.609537, test loss = 5186.453366\n",
            "Epoch 823: train loss = 4101.131785, test loss = 5181.711635\n",
            "Epoch 824: train loss = 4096.670573, test loss = 5176.986773\n",
            "Epoch 825: train loss = 4092.225842, test loss = 5172.278720\n",
            "Epoch 826: train loss = 4087.797529, test loss = 5167.587417\n",
            "Epoch 827: train loss = 4083.385574, test loss = 5162.912803\n",
            "Epoch 828: train loss = 4078.989918, test loss = 5158.254819\n",
            "Epoch 829: train loss = 4074.610498, test loss = 5153.613407\n",
            "Epoch 830: train loss = 4070.247256, test loss = 5148.988507\n",
            "Epoch 831: train loss = 4065.900131, test loss = 5144.380060\n",
            "Epoch 832: train loss = 4061.569064, test loss = 5139.788007\n",
            "Epoch 833: train loss = 4057.253994, test loss = 5135.212290\n",
            "Epoch 834: train loss = 4052.954864, test loss = 5130.652851\n",
            "Epoch 835: train loss = 4048.671613, test loss = 5126.109632\n",
            "Epoch 836: train loss = 4044.404183, test loss = 5121.582574\n",
            "Epoch 837: train loss = 4040.152515, test loss = 5117.071620\n",
            "Epoch 838: train loss = 4035.916551, test loss = 5112.576713\n",
            "Epoch 839: train loss = 4031.696232, test loss = 5108.097795\n",
            "Epoch 840: train loss = 4027.491500, test loss = 5103.634808\n",
            "Epoch 841: train loss = 4023.302298, test loss = 5099.187697\n",
            "Epoch 842: train loss = 4019.128567, test loss = 5094.756404\n",
            "Epoch 843: train loss = 4014.970251, test loss = 5090.340872\n",
            "Epoch 844: train loss = 4010.827291, test loss = 5085.941045\n",
            "Epoch 845: train loss = 4006.699632, test loss = 5081.556868\n",
            "Epoch 846: train loss = 4002.587215, test loss = 5077.188283\n",
            "Epoch 847: train loss = 3998.489984, test loss = 5072.835235\n",
            "Epoch 848: train loss = 3994.407883, test loss = 5068.497668\n",
            "Epoch 849: train loss = 3990.340855, test loss = 5064.175527\n",
            "Epoch 850: train loss = 3986.288845, test loss = 5059.868757\n",
            "Epoch 851: train loss = 3982.251796, test loss = 5055.577301\n",
            "Epoch 852: train loss = 3978.229653, test loss = 5051.301107\n",
            "Epoch 853: train loss = 3974.222360, test loss = 5047.040117\n",
            "Epoch 854: train loss = 3970.229862, test loss = 5042.794279\n",
            "Epoch 855: train loss = 3966.252103, test loss = 5038.563538\n",
            "Epoch 856: train loss = 3962.289029, test loss = 5034.347839\n",
            "Epoch 857: train loss = 3958.340585, test loss = 5030.147128\n",
            "Epoch 858: train loss = 3954.406717, test loss = 5025.961352\n",
            "Epoch 859: train loss = 3950.487369, test loss = 5021.790457\n",
            "Epoch 860: train loss = 3946.582489, test loss = 5017.634389\n",
            "Epoch 861: train loss = 3942.692021, test loss = 5013.493095\n",
            "Epoch 862: train loss = 3938.815913, test loss = 5009.366522\n",
            "Epoch 863: train loss = 3934.954110, test loss = 5005.254617\n",
            "Epoch 864: train loss = 3931.106559, test loss = 5001.157327\n",
            "Epoch 865: train loss = 3927.273207, test loss = 4997.074600\n",
            "Epoch 866: train loss = 3923.454000, test loss = 4993.006383\n",
            "Epoch 867: train loss = 3919.648887, test loss = 4988.952624\n",
            "Epoch 868: train loss = 3915.857814, test loss = 4984.913271\n",
            "Epoch 869: train loss = 3912.080729, test loss = 4980.888271\n",
            "Epoch 870: train loss = 3908.317579, test loss = 4976.877574\n",
            "Epoch 871: train loss = 3904.568313, test loss = 4972.881128\n",
            "Epoch 872: train loss = 3900.832878, test loss = 4968.898881\n",
            "Epoch 873: train loss = 3897.111223, test loss = 4964.930783\n",
            "Epoch 874: train loss = 3893.403296, test loss = 4960.976781\n",
            "Epoch 875: train loss = 3889.709046, test loss = 4957.036826\n",
            "Epoch 876: train loss = 3886.028421, test loss = 4953.110867\n",
            "Epoch 877: train loss = 3882.361371, test loss = 4949.198852\n",
            "Epoch 878: train loss = 3878.707845, test loss = 4945.300733\n",
            "Epoch 879: train loss = 3875.067792, test loss = 4941.416459\n",
            "Epoch 880: train loss = 3871.441161, test loss = 4937.545979\n",
            "Epoch 881: train loss = 3867.827903, test loss = 4933.689244\n",
            "Epoch 882: train loss = 3864.227967, test loss = 4929.846204\n",
            "Epoch 883: train loss = 3860.641304, test loss = 4926.016811\n",
            "Epoch 884: train loss = 3857.067863, test loss = 4922.201014\n",
            "Epoch 885: train loss = 3853.507595, test loss = 4918.398764\n",
            "Epoch 886: train loss = 3849.960450, test loss = 4914.610014\n",
            "Epoch 887: train loss = 3846.426380, test loss = 4910.834712\n",
            "Epoch 888: train loss = 3842.905335, test loss = 4907.072812\n",
            "Epoch 889: train loss = 3839.397267, test loss = 4903.324265\n",
            "Epoch 890: train loss = 3835.902126, test loss = 4899.589023\n",
            "Epoch 891: train loss = 3832.419864, test loss = 4895.867036\n",
            "Epoch 892: train loss = 3828.950433, test loss = 4892.158258\n",
            "Epoch 893: train loss = 3825.493785, test loss = 4888.462641\n",
            "Epoch 894: train loss = 3822.049872, test loss = 4884.780137\n",
            "Epoch 895: train loss = 3818.618645, test loss = 4881.110698\n",
            "Epoch 896: train loss = 3815.200057, test loss = 4877.454278\n",
            "Epoch 897: train loss = 3811.794061, test loss = 4873.810829\n",
            "Epoch 898: train loss = 3808.400610, test loss = 4870.180305\n",
            "Epoch 899: train loss = 3805.019655, test loss = 4866.562657\n",
            "Epoch 900: train loss = 3801.651151, test loss = 4862.957841\n",
            "Epoch 901: train loss = 3798.295050, test loss = 4859.365810\n",
            "Epoch 902: train loss = 3794.951306, test loss = 4855.786516\n",
            "Epoch 903: train loss = 3791.619872, test loss = 4852.219914\n",
            "Epoch 904: train loss = 3788.300702, test loss = 4848.665959\n",
            "Epoch 905: train loss = 3784.993750, test loss = 4845.124604\n",
            "Epoch 906: train loss = 3781.698970, test loss = 4841.595803\n",
            "Epoch 907: train loss = 3778.416315, test loss = 4838.079512\n",
            "Epoch 908: train loss = 3775.145741, test loss = 4834.575684\n",
            "Epoch 909: train loss = 3771.887202, test loss = 4831.084275\n",
            "Epoch 910: train loss = 3768.640652, test loss = 4827.605240\n",
            "Epoch 911: train loss = 3765.406047, test loss = 4824.138533\n",
            "Epoch 912: train loss = 3762.183341, test loss = 4820.684111\n",
            "Epoch 913: train loss = 3758.972489, test loss = 4817.241928\n",
            "Epoch 914: train loss = 3755.773448, test loss = 4813.811940\n",
            "Epoch 915: train loss = 3752.586171, test loss = 4810.394104\n",
            "Epoch 916: train loss = 3749.410616, test loss = 4806.988374\n",
            "Epoch 917: train loss = 3746.246737, test loss = 4803.594707\n",
            "Epoch 918: train loss = 3743.094492, test loss = 4800.213060\n",
            "Epoch 919: train loss = 3739.953835, test loss = 4796.843388\n",
            "Epoch 920: train loss = 3736.824723, test loss = 4793.485649\n",
            "Epoch 921: train loss = 3733.707113, test loss = 4790.139798\n",
            "Epoch 922: train loss = 3730.600961, test loss = 4786.805793\n",
            "Epoch 923: train loss = 3727.506224, test loss = 4783.483591\n",
            "Epoch 924: train loss = 3724.422858, test loss = 4780.173149\n",
            "Epoch 925: train loss = 3721.350822, test loss = 4776.874424\n",
            "Epoch 926: train loss = 3718.290072, test loss = 4773.587374\n",
            "Epoch 927: train loss = 3715.240565, test loss = 4770.311956\n",
            "Epoch 928: train loss = 3712.202260, test loss = 4767.048129\n",
            "Epoch 929: train loss = 3709.175113, test loss = 4763.795850\n",
            "Epoch 930: train loss = 3706.159082, test loss = 4760.555076\n",
            "Epoch 931: train loss = 3703.154127, test loss = 4757.325767\n",
            "Epoch 932: train loss = 3700.160203, test loss = 4754.107881\n",
            "Epoch 933: train loss = 3697.177271, test loss = 4750.901377\n",
            "Epoch 934: train loss = 3694.205288, test loss = 4747.706212\n",
            "Epoch 935: train loss = 3691.244214, test loss = 4744.522346\n",
            "Epoch 936: train loss = 3688.294006, test loss = 4741.349737\n",
            "Epoch 937: train loss = 3685.354623, test loss = 4738.188346\n",
            "Epoch 938: train loss = 3682.426025, test loss = 4735.038130\n",
            "Epoch 939: train loss = 3679.508171, test loss = 4731.899050\n",
            "Epoch 940: train loss = 3676.601021, test loss = 4728.771065\n",
            "Epoch 941: train loss = 3673.704533, test loss = 4725.654135\n",
            "Epoch 942: train loss = 3670.818667, test loss = 4722.548220\n",
            "Epoch 943: train loss = 3667.943383, test loss = 4719.453278\n",
            "Epoch 944: train loss = 3665.078641, test loss = 4716.369272\n",
            "Epoch 945: train loss = 3662.224401, test loss = 4713.296160\n",
            "Epoch 946: train loss = 3659.380624, test loss = 4710.233904\n",
            "Epoch 947: train loss = 3656.547269, test loss = 4707.182463\n",
            "Epoch 948: train loss = 3653.724297, test loss = 4704.141800\n",
            "Epoch 949: train loss = 3650.911669, test loss = 4701.111873\n",
            "Epoch 950: train loss = 3648.109345, test loss = 4698.092645\n",
            "Epoch 951: train loss = 3645.317287, test loss = 4695.084076\n",
            "Epoch 952: train loss = 3642.535455, test loss = 4692.086128\n",
            "Epoch 953: train loss = 3639.763811, test loss = 4689.098762\n",
            "Epoch 954: train loss = 3637.002316, test loss = 4686.121940\n",
            "Epoch 955: train loss = 3634.250932, test loss = 4683.155623\n",
            "Epoch 956: train loss = 3631.509619, test loss = 4680.199773\n",
            "Epoch 957: train loss = 3628.778341, test loss = 4677.254352\n",
            "Epoch 958: train loss = 3626.057058, test loss = 4674.319322\n",
            "Epoch 959: train loss = 3623.345733, test loss = 4671.394645\n",
            "Epoch 960: train loss = 3620.644328, test loss = 4668.480285\n",
            "Epoch 961: train loss = 3617.952805, test loss = 4665.576202\n",
            "Epoch 962: train loss = 3615.271128, test loss = 4662.682360\n",
            "Epoch 963: train loss = 3612.599257, test loss = 4659.798722\n",
            "Epoch 964: train loss = 3609.937157, test loss = 4656.925249\n",
            "Epoch 965: train loss = 3607.284789, test loss = 4654.061907\n",
            "Epoch 966: train loss = 3604.642117, test loss = 4651.208657\n",
            "Epoch 967: train loss = 3602.009105, test loss = 4648.365463\n",
            "Epoch 968: train loss = 3599.385714, test loss = 4645.532288\n",
            "Epoch 969: train loss = 3596.771910, test loss = 4642.709096\n",
            "Epoch 970: train loss = 3594.167654, test loss = 4639.895851\n",
            "Epoch 971: train loss = 3591.572911, test loss = 4637.092515\n",
            "Epoch 972: train loss = 3588.987645, test loss = 4634.299055\n",
            "Epoch 973: train loss = 3586.411820, test loss = 4631.515432\n",
            "Epoch 974: train loss = 3583.845399, test loss = 4628.741612\n",
            "Epoch 975: train loss = 3581.288347, test loss = 4625.977559\n",
            "Epoch 976: train loss = 3578.740627, test loss = 4623.223238\n",
            "Epoch 977: train loss = 3576.202206, test loss = 4620.478612\n",
            "Epoch 978: train loss = 3573.673046, test loss = 4617.743647\n",
            "Epoch 979: train loss = 3571.153113, test loss = 4615.018308\n",
            "Epoch 980: train loss = 3568.642371, test loss = 4612.302559\n",
            "Epoch 981: train loss = 3566.140786, test loss = 4609.596366\n",
            "Epoch 982: train loss = 3563.648323, test loss = 4606.899694\n",
            "Epoch 983: train loss = 3561.164946, test loss = 4604.212508\n",
            "Epoch 984: train loss = 3558.690622, test loss = 4601.534773\n",
            "Epoch 985: train loss = 3556.225314, test loss = 4598.866456\n",
            "Epoch 986: train loss = 3553.768990, test loss = 4596.207522\n",
            "Epoch 987: train loss = 3551.321615, test loss = 4593.557937\n",
            "Epoch 988: train loss = 3548.883154, test loss = 4590.917667\n",
            "Epoch 989: train loss = 3546.453574, test loss = 4588.286677\n",
            "Epoch 990: train loss = 3544.032840, test loss = 4585.664935\n",
            "Epoch 991: train loss = 3541.620919, test loss = 4583.052406\n",
            "Epoch 992: train loss = 3539.217778, test loss = 4580.449058\n",
            "Epoch 993: train loss = 3536.823381, test loss = 4577.854856\n",
            "Epoch 994: train loss = 3534.437697, test loss = 4575.269767\n",
            "Epoch 995: train loss = 3532.060692, test loss = 4572.693759\n",
            "Epoch 996: train loss = 3529.692332, test loss = 4570.126797\n",
            "Epoch 997: train loss = 3527.332584, test loss = 4567.568850\n",
            "Epoch 998: train loss = 3524.981416, test loss = 4565.019884\n",
            "Epoch 999: train loss = 3522.638794, test loss = 4562.479867\n",
            "Epoch 1000: train loss = 3520.304686, test loss = 4559.948766\n",
            "Epoch 1001: train loss = 3517.979060, test loss = 4557.426549\n",
            "Epoch 1002: train loss = 3515.661882, test loss = 4554.913183\n",
            "Epoch 1003: train loss = 3513.353121, test loss = 4552.408636\n",
            "Epoch 1004: train loss = 3511.052743, test loss = 4549.912876\n",
            "Epoch 1005: train loss = 3508.760718, test loss = 4547.425871\n",
            "Epoch 1006: train loss = 3506.477013, test loss = 4544.947590\n",
            "Epoch 1007: train loss = 3504.201595, test loss = 4542.477999\n",
            "Epoch 1008: train loss = 3501.934434, test loss = 4540.017069\n",
            "Epoch 1009: train loss = 3499.675497, test loss = 4537.564767\n",
            "Epoch 1010: train loss = 3497.424753, test loss = 4535.121062\n",
            "Epoch 1011: train loss = 3495.182170, test loss = 4532.685923\n",
            "Epoch 1012: train loss = 3492.947717, test loss = 4530.259318\n",
            "Epoch 1013: train loss = 3490.721364, test loss = 4527.841216\n",
            "Epoch 1014: train loss = 3488.503078, test loss = 4525.431587\n",
            "Epoch 1015: train loss = 3486.292828, test loss = 4523.030399\n",
            "Epoch 1016: train loss = 3484.090584, test loss = 4520.637623\n",
            "Epoch 1017: train loss = 3481.896316, test loss = 4518.253226\n",
            "Epoch 1018: train loss = 3479.709991, test loss = 4515.877180\n",
            "Epoch 1019: train loss = 3477.531581, test loss = 4513.509453\n",
            "Epoch 1020: train loss = 3475.361053, test loss = 4511.150015\n",
            "Epoch 1021: train loss = 3473.198378, test loss = 4508.798836\n",
            "Epoch 1022: train loss = 3471.043526, test loss = 4506.455885\n",
            "Epoch 1023: train loss = 3468.896467, test loss = 4504.121134\n",
            "Epoch 1024: train loss = 3466.757169, test loss = 4501.794552\n",
            "Epoch 1025: train loss = 3464.625605, test loss = 4499.476110\n",
            "Epoch 1026: train loss = 3462.501743, test loss = 4497.165777\n",
            "Epoch 1027: train loss = 3460.385554, test loss = 4494.863525\n",
            "Epoch 1028: train loss = 3458.277008, test loss = 4492.569323\n",
            "Epoch 1029: train loss = 3456.176076, test loss = 4490.283143\n",
            "Epoch 1030: train loss = 3454.082728, test loss = 4488.004956\n",
            "Epoch 1031: train loss = 3451.996936, test loss = 4485.734732\n",
            "Epoch 1032: train loss = 3449.918669, test loss = 4483.472442\n",
            "Epoch 1033: train loss = 3447.847900, test loss = 4481.218057\n",
            "Epoch 1034: train loss = 3445.784598, test loss = 4478.971550\n",
            "Epoch 1035: train loss = 3443.728735, test loss = 4476.732891\n",
            "Epoch 1036: train loss = 3441.680282, test loss = 4474.502051\n",
            "Epoch 1037: train loss = 3439.639211, test loss = 4472.279002\n",
            "Epoch 1038: train loss = 3437.605493, test loss = 4470.063716\n",
            "Epoch 1039: train loss = 3435.579099, test loss = 4467.856164\n",
            "Epoch 1040: train loss = 3433.560001, test loss = 4465.656319\n",
            "Epoch 1041: train loss = 3431.548171, test loss = 4463.464153\n",
            "Epoch 1042: train loss = 3429.543581, test loss = 4461.279636\n",
            "Epoch 1043: train loss = 3427.546202, test loss = 4459.102743\n",
            "Epoch 1044: train loss = 3425.556006, test loss = 4456.933444\n",
            "Epoch 1045: train loss = 3423.572967, test loss = 4454.771712\n",
            "Epoch 1046: train loss = 3421.597055, test loss = 4452.617520\n",
            "Epoch 1047: train loss = 3419.628243, test loss = 4450.470841\n",
            "Epoch 1048: train loss = 3417.666504, test loss = 4448.331646\n",
            "Epoch 1049: train loss = 3415.711810, test loss = 4446.199910\n",
            "Epoch 1050: train loss = 3413.764134, test loss = 4444.075604\n",
            "Epoch 1051: train loss = 3411.823448, test loss = 4441.958701\n",
            "Epoch 1052: train loss = 3409.889725, test loss = 4439.849176\n",
            "Epoch 1053: train loss = 3407.962938, test loss = 4437.747000\n",
            "Epoch 1054: train loss = 3406.043061, test loss = 4435.652147\n",
            "Epoch 1055: train loss = 3404.130065, test loss = 4433.564591\n",
            "Epoch 1056: train loss = 3402.223925, test loss = 4431.484305\n",
            "Epoch 1057: train loss = 3400.324614, test loss = 4429.411263\n",
            "Epoch 1058: train loss = 3398.432104, test loss = 4427.345437\n",
            "Epoch 1059: train loss = 3396.546370, test loss = 4425.286803\n",
            "Epoch 1060: train loss = 3394.667385, test loss = 4423.235334\n",
            "Epoch 1061: train loss = 3392.795122, test loss = 4421.191004\n",
            "Epoch 1062: train loss = 3390.929556, test loss = 4419.153786\n",
            "Epoch 1063: train loss = 3389.070659, test loss = 4417.123655\n",
            "Epoch 1064: train loss = 3387.218407, test loss = 4415.100586\n",
            "Epoch 1065: train loss = 3385.372773, test loss = 4413.084553\n",
            "Epoch 1066: train loss = 3383.533731, test loss = 4411.075529\n",
            "Epoch 1067: train loss = 3381.701256, test loss = 4409.073490\n",
            "Epoch 1068: train loss = 3379.875320, test loss = 4407.078410\n",
            "Epoch 1069: train loss = 3378.055900, test loss = 4405.090264\n",
            "Epoch 1070: train loss = 3376.242969, test loss = 4403.109027\n",
            "Epoch 1071: train loss = 3374.436503, test loss = 4401.134673\n",
            "Epoch 1072: train loss = 3372.636474, test loss = 4399.167178\n",
            "Epoch 1073: train loss = 3370.842859, test loss = 4397.206517\n",
            "Epoch 1074: train loss = 3369.055632, test loss = 4395.252664\n",
            "Epoch 1075: train loss = 3367.274768, test loss = 4393.305596\n",
            "Epoch 1076: train loss = 3365.500242, test loss = 4391.365287\n",
            "Epoch 1077: train loss = 3363.732028, test loss = 4389.431712\n",
            "Epoch 1078: train loss = 3361.970103, test loss = 4387.504849\n",
            "Epoch 1079: train loss = 3360.214441, test loss = 4385.584671\n",
            "Epoch 1080: train loss = 3358.465018, test loss = 4383.671155\n",
            "Epoch 1081: train loss = 3356.721809, test loss = 4381.764277\n",
            "Epoch 1082: train loss = 3354.984789, test loss = 4379.864012\n",
            "Epoch 1083: train loss = 3353.253934, test loss = 4377.970337\n",
            "Epoch 1084: train loss = 3351.529220, test loss = 4376.083227\n",
            "Epoch 1085: train loss = 3349.810622, test loss = 4374.202659\n",
            "Epoch 1086: train loss = 3348.098116, test loss = 4372.328609\n",
            "Epoch 1087: train loss = 3346.391679, test loss = 4370.461053\n",
            "Epoch 1088: train loss = 3344.691286, test loss = 4368.599968\n",
            "Epoch 1089: train loss = 3342.996912, test loss = 4366.745330\n",
            "Epoch 1090: train loss = 3341.308535, test loss = 4364.897116\n",
            "Epoch 1091: train loss = 3339.626131, test loss = 4363.055302\n",
            "Epoch 1092: train loss = 3337.949675, test loss = 4361.219866\n",
            "Epoch 1093: train loss = 3336.279144, test loss = 4359.390783\n",
            "Epoch 1094: train loss = 3334.614515, test loss = 4357.568032\n",
            "Epoch 1095: train loss = 3332.955764, test loss = 4355.751589\n",
            "Epoch 1096: train loss = 3331.302867, test loss = 4353.941431\n",
            "Epoch 1097: train loss = 3329.655802, test loss = 4352.137535\n",
            "Epoch 1098: train loss = 3328.014545, test loss = 4350.339879\n",
            "Epoch 1099: train loss = 3326.379073, test loss = 4348.548440\n",
            "Epoch 1100: train loss = 3324.749363, test loss = 4346.763195\n",
            "Epoch 1101: train loss = 3323.125391, test loss = 4344.984122\n",
            "Epoch 1102: train loss = 3321.507136, test loss = 4343.211199\n",
            "Epoch 1103: train loss = 3319.894573, test loss = 4341.444403\n",
            "Epoch 1104: train loss = 3318.287681, test loss = 4339.683712\n",
            "Epoch 1105: train loss = 3316.686437, test loss = 4337.929104\n",
            "Epoch 1106: train loss = 3315.090817, test loss = 4336.180556\n",
            "Epoch 1107: train loss = 3313.500800, test loss = 4334.438048\n",
            "Epoch 1108: train loss = 3311.916363, test loss = 4332.701556\n",
            "Epoch 1109: train loss = 3310.337483, test loss = 4330.971060\n",
            "Epoch 1110: train loss = 3308.764138, test loss = 4329.246537\n",
            "Epoch 1111: train loss = 3307.196306, test loss = 4327.527965\n",
            "Epoch 1112: train loss = 3305.633964, test loss = 4325.815324\n",
            "Epoch 1113: train loss = 3304.077092, test loss = 4324.108591\n",
            "Epoch 1114: train loss = 3302.525665, test loss = 4322.407746\n",
            "Epoch 1115: train loss = 3300.979663, test loss = 4320.712766\n",
            "Epoch 1116: train loss = 3299.439064, test loss = 4319.023631\n",
            "Epoch 1117: train loss = 3297.903845, test loss = 4317.340320\n",
            "Epoch 1118: train loss = 3296.373986, test loss = 4315.662810\n",
            "Epoch 1119: train loss = 3294.849463, test loss = 4313.991082\n",
            "Epoch 1120: train loss = 3293.330256, test loss = 4312.325114\n",
            "Epoch 1121: train loss = 3291.816343, test loss = 4310.664886\n",
            "Epoch 1122: train loss = 3290.307703, test loss = 4309.010376\n",
            "Epoch 1123: train loss = 3288.804314, test loss = 4307.361564\n",
            "Epoch 1124: train loss = 3287.306154, test loss = 4305.718430\n",
            "Epoch 1125: train loss = 3285.813203, test loss = 4304.080952\n",
            "Epoch 1126: train loss = 3284.325439, test loss = 4302.449110\n",
            "Epoch 1127: train loss = 3282.842841, test loss = 4300.822884\n",
            "Epoch 1128: train loss = 3281.365388, test loss = 4299.202253\n",
            "Epoch 1129: train loss = 3279.893059, test loss = 4297.587198\n",
            "Epoch 1130: train loss = 3278.425832, test loss = 4295.977698\n",
            "Epoch 1131: train loss = 3276.963688, test loss = 4294.373732\n",
            "Epoch 1132: train loss = 3275.506605, test loss = 4292.775281\n",
            "Epoch 1133: train loss = 3274.054562, test loss = 4291.182325\n",
            "Epoch 1134: train loss = 3272.607539, test loss = 4289.594844\n",
            "Epoch 1135: train loss = 3271.165515, test loss = 4288.012818\n",
            "Epoch 1136: train loss = 3269.728469, test loss = 4286.436227\n",
            "Epoch 1137: train loss = 3268.296381, test loss = 4284.865052\n",
            "Epoch 1138: train loss = 3266.869230, test loss = 4283.299273\n",
            "Epoch 1139: train loss = 3265.446997, test loss = 4281.738871\n",
            "Epoch 1140: train loss = 3264.029660, test loss = 4280.183825\n",
            "Epoch 1141: train loss = 3262.617200, test loss = 4278.634117\n",
            "Epoch 1142: train loss = 3261.209595, test loss = 4277.089728\n",
            "Epoch 1143: train loss = 3259.806827, test loss = 4275.550637\n",
            "Epoch 1144: train loss = 3258.408874, test loss = 4274.016827\n",
            "Epoch 1145: train loss = 3257.015718, test loss = 4272.488277\n",
            "Epoch 1146: train loss = 3255.627337, test loss = 4270.964969\n",
            "Epoch 1147: train loss = 3254.243712, test loss = 4269.446884\n",
            "Epoch 1148: train loss = 3252.864823, test loss = 4267.934002\n",
            "Epoch 1149: train loss = 3251.490650, test loss = 4266.426306\n",
            "Epoch 1150: train loss = 3250.121174, test loss = 4264.923776\n",
            "Epoch 1151: train loss = 3248.756375, test loss = 4263.426394\n",
            "Epoch 1152: train loss = 3247.396232, test loss = 4261.934140\n",
            "Epoch 1153: train loss = 3246.040727, test loss = 4260.446997\n",
            "Epoch 1154: train loss = 3244.689840, test loss = 4258.964946\n",
            "Epoch 1155: train loss = 3243.343552, test loss = 4257.487968\n",
            "Epoch 1156: train loss = 3242.001842, test loss = 4256.016046\n",
            "Epoch 1157: train loss = 3240.664693, test loss = 4254.549160\n",
            "Epoch 1158: train loss = 3239.332083, test loss = 4253.087293\n",
            "Epoch 1159: train loss = 3238.003995, test loss = 4251.630426\n",
            "Epoch 1160: train loss = 3236.680409, test loss = 4250.178542\n",
            "Epoch 1161: train loss = 3235.361305, test loss = 4248.731622\n",
            "Epoch 1162: train loss = 3234.046665, test loss = 4247.289649\n",
            "Epoch 1163: train loss = 3232.736469, test loss = 4245.852604\n",
            "Epoch 1164: train loss = 3231.430699, test loss = 4244.420470\n",
            "Epoch 1165: train loss = 3230.129336, test loss = 4242.993229\n",
            "Epoch 1166: train loss = 3228.832360, test loss = 4241.570863\n",
            "Epoch 1167: train loss = 3227.539753, test loss = 4240.153354\n",
            "Epoch 1168: train loss = 3226.251496, test loss = 4238.740686\n",
            "Epoch 1169: train loss = 3224.967570, test loss = 4237.332840\n",
            "Epoch 1170: train loss = 3223.687957, test loss = 4235.929800\n",
            "Epoch 1171: train loss = 3222.412637, test loss = 4234.531547\n",
            "Epoch 1172: train loss = 3221.141593, test loss = 4233.138065\n",
            "Epoch 1173: train loss = 3219.874805, test loss = 4231.749336\n",
            "Epoch 1174: train loss = 3218.612256, test loss = 4230.365342\n",
            "Epoch 1175: train loss = 3217.353926, test loss = 4228.986068\n",
            "Epoch 1176: train loss = 3216.099797, test loss = 4227.611496\n",
            "Epoch 1177: train loss = 3214.849851, test loss = 4226.241608\n",
            "Epoch 1178: train loss = 3213.604069, test loss = 4224.876389\n",
            "Epoch 1179: train loss = 3212.362434, test loss = 4223.515820\n",
            "Epoch 1180: train loss = 3211.124927, test loss = 4222.159886\n",
            "Epoch 1181: train loss = 3209.891529, test loss = 4220.808569\n",
            "Epoch 1182: train loss = 3208.662222, test loss = 4219.461853\n",
            "Epoch 1183: train loss = 3207.436989, test loss = 4218.119721\n",
            "Epoch 1184: train loss = 3206.215812, test loss = 4216.782156\n",
            "Epoch 1185: train loss = 3204.998671, test loss = 4215.449143\n",
            "Epoch 1186: train loss = 3203.785550, test loss = 4214.120664\n",
            "Epoch 1187: train loss = 3202.576430, test loss = 4212.796704\n",
            "Epoch 1188: train loss = 3201.371293, test loss = 4211.477245\n",
            "Epoch 1189: train loss = 3200.170122, test loss = 4210.162272\n",
            "Epoch 1190: train loss = 3198.972898, test loss = 4208.851768\n",
            "Epoch 1191: train loss = 3197.779604, test loss = 4207.545718\n",
            "Epoch 1192: train loss = 3196.590221, test loss = 4206.244105\n",
            "Epoch 1193: train loss = 3195.404733, test loss = 4204.946912\n",
            "Epoch 1194: train loss = 3194.223122, test loss = 4203.654125\n",
            "Epoch 1195: train loss = 3193.045369, test loss = 4202.365728\n",
            "Epoch 1196: train loss = 3191.871458, test loss = 4201.081703\n",
            "Epoch 1197: train loss = 3190.701370, test loss = 4199.802036\n",
            "Epoch 1198: train loss = 3189.535088, test loss = 4198.526711\n",
            "Epoch 1199: train loss = 3188.372595, test loss = 4197.255712\n",
            "Epoch 1200: train loss = 3187.213873, test loss = 4195.989023\n",
            "Epoch 1201: train loss = 3186.058904, test loss = 4194.726629\n",
            "Epoch 1202: train loss = 3184.907671, test loss = 4193.468514\n",
            "Epoch 1203: train loss = 3183.760157, test loss = 4192.214664\n",
            "Epoch 1204: train loss = 3182.616345, test loss = 4190.965061\n",
            "Epoch 1205: train loss = 3181.476216, test loss = 4189.719692\n",
            "Epoch 1206: train loss = 3180.339754, test loss = 4188.478540\n",
            "Epoch 1207: train loss = 3179.206942, test loss = 4187.241591\n",
            "Epoch 1208: train loss = 3178.077761, test loss = 4186.008829\n",
            "Epoch 1209: train loss = 3176.952195, test loss = 4184.780239\n",
            "Epoch 1210: train loss = 3175.830228, test loss = 4183.555806\n",
            "Epoch 1211: train loss = 3174.711840, test loss = 4182.335515\n",
            "Epoch 1212: train loss = 3173.597016, test loss = 4181.119351\n",
            "Epoch 1213: train loss = 3172.485738, test loss = 4179.907299\n",
            "Epoch 1214: train loss = 3171.377990, test loss = 4178.699345\n",
            "Epoch 1215: train loss = 3170.273753, test loss = 4177.495472\n",
            "Epoch 1216: train loss = 3169.173012, test loss = 4176.295667\n",
            "Epoch 1217: train loss = 3168.075748, test loss = 4175.099915\n",
            "Epoch 1218: train loss = 3166.981946, test loss = 4173.908201\n",
            "Epoch 1219: train loss = 3165.891587, test loss = 4172.720510\n",
            "Epoch 1220: train loss = 3164.804656, test loss = 4171.536829\n",
            "Epoch 1221: train loss = 3163.721134, test loss = 4170.357141\n",
            "Epoch 1222: train loss = 3162.641006, test loss = 4169.181434\n",
            "Epoch 1223: train loss = 3161.564254, test loss = 4168.009692\n",
            "Epoch 1224: train loss = 3160.490862, test loss = 4166.841901\n",
            "Epoch 1225: train loss = 3159.420812, test loss = 4165.678047\n",
            "Epoch 1226: train loss = 3158.354087, test loss = 4164.518115\n",
            "Epoch 1227: train loss = 3157.290672, test loss = 4163.362091\n",
            "Epoch 1228: train loss = 3156.230548, test loss = 4162.209962\n",
            "Epoch 1229: train loss = 3155.173700, test loss = 4161.061712\n",
            "Epoch 1230: train loss = 3154.120110, test loss = 4159.917329\n",
            "Epoch 1231: train loss = 3153.069761, test loss = 4158.776797\n",
            "Epoch 1232: train loss = 3152.022638, test loss = 4157.640102\n",
            "Epoch 1233: train loss = 3150.978723, test loss = 4156.507232\n",
            "Epoch 1234: train loss = 3149.937999, test loss = 4155.378172\n",
            "Epoch 1235: train loss = 3148.900449, test loss = 4154.252907\n",
            "Epoch 1236: train loss = 3147.866058, test loss = 4153.131425\n",
            "Epoch 1237: train loss = 3146.834807, test loss = 4152.013711\n",
            "Epoch 1238: train loss = 3145.806681, test loss = 4150.899752\n",
            "Epoch 1239: train loss = 3144.781663, test loss = 4149.789535\n",
            "Epoch 1240: train loss = 3143.759736, test loss = 4148.683044\n",
            "Epoch 1241: train loss = 3142.740883, test loss = 4147.580268\n",
            "Epoch 1242: train loss = 3141.725088, test loss = 4146.481192\n",
            "Epoch 1243: train loss = 3140.712334, test loss = 4145.385803\n",
            "Epoch 1244: train loss = 3139.702604, test loss = 4144.294087\n",
            "Epoch 1245: train loss = 3138.695882, test loss = 4143.206031\n",
            "Epoch 1246: train loss = 3137.692151, test loss = 4142.121622\n",
            "Epoch 1247: train loss = 3136.691394, test loss = 4141.040846\n",
            "Epoch 1248: train loss = 3135.693594, test loss = 4139.963690\n",
            "Epoch 1249: train loss = 3134.698736, test loss = 4138.890140\n",
            "Epoch 1250: train loss = 3133.706801, test loss = 4137.820185\n",
            "Epoch 1251: train loss = 3132.717774, test loss = 4136.753809\n",
            "Epoch 1252: train loss = 3131.731638, test loss = 4135.691001\n",
            "Epoch 1253: train loss = 3130.748376, test loss = 4134.631747\n",
            "Epoch 1254: train loss = 3129.767972, test loss = 4133.576035\n",
            "Epoch 1255: train loss = 3128.790408, test loss = 4132.523850\n",
            "Epoch 1256: train loss = 3127.815668, test loss = 4131.475181\n",
            "Epoch 1257: train loss = 3126.843735, test loss = 4130.430014\n",
            "Epoch 1258: train loss = 3125.874593, test loss = 4129.388336\n",
            "Epoch 1259: train loss = 3124.908224, test loss = 4128.350135\n",
            "Epoch 1260: train loss = 3123.944613, test loss = 4127.315398\n",
            "Epoch 1261: train loss = 3122.983741, test loss = 4126.284111\n",
            "Epoch 1262: train loss = 3122.025593, test loss = 4125.256263\n",
            "Epoch 1263: train loss = 3121.070152, test loss = 4124.231840\n",
            "Epoch 1264: train loss = 3120.117400, test loss = 4123.210830\n",
            "Epoch 1265: train loss = 3119.167321, test loss = 4122.193221\n",
            "Epoch 1266: train loss = 3118.219898, test loss = 4121.178999\n",
            "Epoch 1267: train loss = 3117.275114, test loss = 4120.168152\n",
            "Epoch 1268: train loss = 3116.332952, test loss = 4119.160668\n",
            "Epoch 1269: train loss = 3115.393396, test loss = 4118.156533\n",
            "Epoch 1270: train loss = 3114.456427, test loss = 4117.155737\n",
            "Epoch 1271: train loss = 3113.522030, test loss = 4116.158265\n",
            "Epoch 1272: train loss = 3112.590188, test loss = 4115.164106\n",
            "Epoch 1273: train loss = 3111.660882, test loss = 4114.173248\n",
            "Epoch 1274: train loss = 3110.734097, test loss = 4113.185678\n",
            "Epoch 1275: train loss = 3109.809815, test loss = 4112.201384\n",
            "Epoch 1276: train loss = 3108.888019, test loss = 4111.220353\n",
            "Epoch 1277: train loss = 3107.968691, test loss = 4110.242574\n",
            "Epoch 1278: train loss = 3107.051815, test loss = 4109.268033\n",
            "Epoch 1279: train loss = 3106.137374, test loss = 4108.296720\n",
            "Epoch 1280: train loss = 3105.225350, test loss = 4107.328621\n",
            "Epoch 1281: train loss = 3104.315725, test loss = 4106.363725\n",
            "Epoch 1282: train loss = 3103.408483, test loss = 4105.402020\n",
            "Epoch 1283: train loss = 3102.503606, test loss = 4104.443493\n",
            "Epoch 1284: train loss = 3101.601076, test loss = 4103.488133\n",
            "Epoch 1285: train loss = 3100.700877, test loss = 4102.535927\n",
            "Epoch 1286: train loss = 3099.802990, test loss = 4101.586863\n",
            "Epoch 1287: train loss = 3098.907399, test loss = 4100.640930\n",
            "Epoch 1288: train loss = 3098.014085, test loss = 4099.698116\n",
            "Epoch 1289: train loss = 3097.123031, test loss = 4098.758408\n",
            "Epoch 1290: train loss = 3096.234219, test loss = 4097.821795\n",
            "Epoch 1291: train loss = 3095.347631, test loss = 4096.888265\n",
            "Epoch 1292: train loss = 3094.463250, test loss = 4095.957806\n",
            "Epoch 1293: train loss = 3093.581058, test loss = 4095.030406\n",
            "Epoch 1294: train loss = 3092.701036, test loss = 4094.106054\n",
            "Epoch 1295: train loss = 3091.823168, test loss = 4093.184737\n",
            "Epoch 1296: train loss = 3090.947434, test loss = 4092.266444\n",
            "Epoch 1297: train loss = 3090.073816, test loss = 4091.351163\n",
            "Epoch 1298: train loss = 3089.202298, test loss = 4090.438882\n",
            "Epoch 1299: train loss = 3088.332860, test loss = 4089.529590\n",
            "Epoch 1300: train loss = 3087.465483, test loss = 4088.623275\n",
            "Epoch 1301: train loss = 3086.600151, test loss = 4087.719925\n",
            "Epoch 1302: train loss = 3085.736844, test loss = 4086.819529\n",
            "Epoch 1303: train loss = 3084.875543, test loss = 4085.922075\n",
            "Epoch 1304: train loss = 3084.016231, test loss = 4085.027551\n",
            "Epoch 1305: train loss = 3083.158889, test loss = 4084.135945\n",
            "Epoch 1306: train loss = 3082.303498, test loss = 4083.247247\n",
            "Epoch 1307: train loss = 3081.450038, test loss = 4082.361444\n",
            "Epoch 1308: train loss = 3080.598493, test loss = 4081.478525\n",
            "Epoch 1309: train loss = 3079.748841, test loss = 4080.598478\n",
            "Epoch 1310: train loss = 3078.901065, test loss = 4079.721291\n",
            "Epoch 1311: train loss = 3078.055146, test loss = 4078.846954\n",
            "Epoch 1312: train loss = 3077.211063, test loss = 4077.975455\n",
            "Epoch 1313: train loss = 3076.368799, test loss = 4077.106781\n",
            "Epoch 1314: train loss = 3075.528333, test loss = 4076.240922\n",
            "Epoch 1315: train loss = 3074.689647, test loss = 4075.377865\n",
            "Epoch 1316: train loss = 3073.852720, test loss = 4074.517600\n",
            "Epoch 1317: train loss = 3073.017534, test loss = 4073.660114\n",
            "Epoch 1318: train loss = 3072.184068, test loss = 4072.805397\n",
            "Epoch 1319: train loss = 3071.352302, test loss = 4071.953436\n",
            "Epoch 1320: train loss = 3070.522218, test loss = 4071.104221\n",
            "Epoch 1321: train loss = 3069.693794, test loss = 4070.257739\n",
            "Epoch 1322: train loss = 3068.867012, test loss = 4069.413979\n",
            "Epoch 1323: train loss = 3068.041850, test loss = 4068.572929\n",
            "Epoch 1324: train loss = 3067.218289, test loss = 4067.734579\n",
            "Epoch 1325: train loss = 3066.396308, test loss = 4066.898915\n",
            "Epoch 1326: train loss = 3065.575887, test loss = 4066.065927\n",
            "Epoch 1327: train loss = 3064.757005, test loss = 4065.235604\n",
            "Epoch 1328: train loss = 3063.939642, test loss = 4064.407933\n",
            "Epoch 1329: train loss = 3063.123776, test loss = 4063.582903\n",
            "Epoch 1330: train loss = 3062.309388, test loss = 4062.760502\n",
            "Epoch 1331: train loss = 3061.496456, test loss = 4061.940719\n",
            "Epoch 1332: train loss = 3060.684960, test loss = 4061.123542\n",
            "Epoch 1333: train loss = 3059.874877, test loss = 4060.308959\n",
            "Epoch 1334: train loss = 3059.066188, test loss = 4059.496959\n",
            "Epoch 1335: train loss = 3058.258870, test loss = 4058.687530\n",
            "Epoch 1336: train loss = 3057.452902, test loss = 4057.880660\n",
            "Epoch 1337: train loss = 3056.648263, test loss = 4057.076338\n",
            "Epoch 1338: train loss = 3055.844931, test loss = 4056.274552\n",
            "Epoch 1339: train loss = 3055.042884, test loss = 4055.475289\n",
            "Epoch 1340: train loss = 3054.242101, test loss = 4054.678539\n",
            "Epoch 1341: train loss = 3053.442559, test loss = 4053.884289\n",
            "Epoch 1342: train loss = 3052.644237, test loss = 4053.092528\n",
            "Epoch 1343: train loss = 3051.847112, test loss = 4052.303244\n",
            "Epoch 1344: train loss = 3051.051163, test loss = 4051.516424\n",
            "Epoch 1345: train loss = 3050.256366, test loss = 4050.732057\n",
            "Epoch 1346: train loss = 3049.462700, test loss = 4049.950130\n",
            "Epoch 1347: train loss = 3048.670142, test loss = 4049.170633\n",
            "Epoch 1348: train loss = 3047.878670, test loss = 4048.393552\n",
            "Epoch 1349: train loss = 3047.088260, test loss = 4047.618876\n",
            "Epoch 1350: train loss = 3046.298891, test loss = 4046.846593\n",
            "Epoch 1351: train loss = 3045.510538, test loss = 4046.076690\n",
            "Epoch 1352: train loss = 3044.723181, test loss = 4045.309155\n",
            "Epoch 1353: train loss = 3043.936794, test loss = 4044.543976\n",
            "Epoch 1354: train loss = 3043.151356, test loss = 4043.781141\n",
            "Epoch 1355: train loss = 3042.366843, test loss = 4043.020637\n",
            "Epoch 1356: train loss = 3041.583232, test loss = 4042.262452\n",
            "Epoch 1357: train loss = 3040.800500, test loss = 4041.506574\n",
            "Epoch 1358: train loss = 3040.018623, test loss = 4040.752990\n",
            "Epoch 1359: train loss = 3039.237579, test loss = 4040.001688\n",
            "Epoch 1360: train loss = 3038.457343, test loss = 4039.252654\n",
            "Epoch 1361: train loss = 3037.677893, test loss = 4038.505878\n",
            "Epoch 1362: train loss = 3036.899204, test loss = 4037.761345\n",
            "Epoch 1363: train loss = 3036.121254, test loss = 4037.019043\n",
            "Epoch 1364: train loss = 3035.344018, test loss = 4036.278959\n",
            "Epoch 1365: train loss = 3034.567475, test loss = 4035.541081\n",
            "Epoch 1366: train loss = 3033.791598, test loss = 4034.805396\n",
            "Epoch 1367: train loss = 3033.016367, test loss = 4034.071890\n",
            "Epoch 1368: train loss = 3032.241756, test loss = 4033.340552\n",
            "Epoch 1369: train loss = 3031.467742, test loss = 4032.611366\n",
            "Epoch 1370: train loss = 3030.694303, test loss = 4031.884322\n",
            "Epoch 1371: train loss = 3029.921414, test loss = 4031.159405\n",
            "Epoch 1372: train loss = 3029.149052, test loss = 4030.436602\n",
            "Epoch 1373: train loss = 3028.377195, test loss = 4029.715901\n",
            "Epoch 1374: train loss = 3027.605818, test loss = 4028.997287\n",
            "Epoch 1375: train loss = 3026.834899, test loss = 4028.280747\n",
            "Epoch 1376: train loss = 3026.064414, test loss = 4027.566268\n",
            "Epoch 1377: train loss = 3025.294341, test loss = 4026.853837\n",
            "Epoch 1378: train loss = 3024.524658, test loss = 4026.143439\n",
            "Epoch 1379: train loss = 3023.755341, test loss = 4025.435061\n",
            "Epoch 1380: train loss = 3022.986368, test loss = 4024.728690\n",
            "Epoch 1381: train loss = 3022.217717, test loss = 4024.024312\n",
            "Epoch 1382: train loss = 3021.449366, test loss = 4023.321913\n",
            "Epoch 1383: train loss = 3020.681293, test loss = 4022.621478\n",
            "Epoch 1384: train loss = 3019.913477, test loss = 4021.922996\n",
            "Epoch 1385: train loss = 3019.145895, test loss = 4021.226450\n",
            "Epoch 1386: train loss = 3018.378528, test loss = 4020.531828\n",
            "Epoch 1387: train loss = 3017.611353, test loss = 4019.839115\n",
            "Epoch 1388: train loss = 3016.844352, test loss = 4019.148297\n",
            "Epoch 1389: train loss = 3016.077503, test loss = 4018.459361\n",
            "Epoch 1390: train loss = 3015.310787, test loss = 4017.772291\n",
            "Epoch 1391: train loss = 3014.544185, test loss = 4017.087074\n",
            "Epoch 1392: train loss = 3013.777676, test loss = 4016.403696\n",
            "Epoch 1393: train loss = 3013.011244, test loss = 4015.722142\n",
            "Epoch 1394: train loss = 3012.244870, test loss = 4015.042398\n",
            "Epoch 1395: train loss = 3011.478535, test loss = 4014.364450\n",
            "Epoch 1396: train loss = 3010.712224, test loss = 4013.688283\n",
            "Epoch 1397: train loss = 3009.945919, test loss = 4013.013883\n",
            "Epoch 1398: train loss = 3009.179604, test loss = 4012.341235\n",
            "Epoch 1399: train loss = 3008.413264, test loss = 4011.670326\n",
            "Epoch 1400: train loss = 3007.646883, test loss = 4011.001141\n",
            "Epoch 1401: train loss = 3006.880448, test loss = 4010.333666\n",
            "Epoch 1402: train loss = 3006.113945, test loss = 4009.667886\n",
            "Epoch 1403: train loss = 3005.347360, test loss = 4009.003787\n",
            "Epoch 1404: train loss = 3004.580682, test loss = 4008.341354\n",
            "Epoch 1405: train loss = 3003.813899, test loss = 4007.680574\n",
            "Epoch 1406: train loss = 3003.047001, test loss = 4007.021432\n",
            "Epoch 1407: train loss = 3002.279977, test loss = 4006.363915\n",
            "Epoch 1408: train loss = 3001.512818, test loss = 4005.708007\n",
            "Epoch 1409: train loss = 3000.745516, test loss = 4005.053695\n",
            "Epoch 1410: train loss = 2999.978063, test loss = 4004.400966\n",
            "Epoch 1411: train loss = 2999.210454, test loss = 4003.749804\n",
            "Epoch 1412: train loss = 2998.442681, test loss = 4003.100197\n",
            "Epoch 1413: train loss = 2997.674741, test loss = 4002.452130\n",
            "Epoch 1414: train loss = 2996.906630, test loss = 4001.805591\n",
            "Epoch 1415: train loss = 2996.138345, test loss = 4001.160565\n",
            "Epoch 1416: train loss = 2995.369884, test loss = 4000.517039\n",
            "Epoch 1417: train loss = 2994.601246, test loss = 3999.875001\n",
            "Epoch 1418: train loss = 2993.832433, test loss = 3999.234436\n",
            "Epoch 1419: train loss = 2993.063445, test loss = 3998.595333\n",
            "Epoch 1420: train loss = 2992.294285, test loss = 3997.957679\n",
            "Epoch 1421: train loss = 2991.524956, test loss = 3997.321460\n",
            "Epoch 1422: train loss = 2990.755463, test loss = 3996.686665\n",
            "Epoch 1423: train loss = 2989.985812, test loss = 3996.053282\n",
            "Epoch 1424: train loss = 2989.216010, test loss = 3995.421299\n",
            "Epoch 1425: train loss = 2988.446065, test loss = 3994.790704\n",
            "Epoch 1426: train loss = 2987.675986, test loss = 3994.161486\n",
            "Epoch 1427: train loss = 2986.905783, test loss = 3993.533633\n",
            "Epoch 1428: train loss = 2986.135469, test loss = 3992.907136\n",
            "Epoch 1429: train loss = 2985.365055, test loss = 3992.281982\n",
            "Epoch 1430: train loss = 2984.594555, test loss = 3991.658162\n",
            "Epoch 1431: train loss = 2983.823985, test loss = 3991.035666\n",
            "Epoch 1432: train loss = 2983.053361, test loss = 3990.414485\n",
            "Epoch 1433: train loss = 2982.282699, test loss = 3989.794608\n",
            "Epoch 1434: train loss = 2981.512019, test loss = 3989.176027\n",
            "Epoch 1435: train loss = 2980.741339, test loss = 3988.558733\n",
            "Epoch 1436: train loss = 2979.970680, test loss = 3987.942718\n",
            "Epoch 1437: train loss = 2979.200064, test loss = 3987.327973\n",
            "Epoch 1438: train loss = 2978.429513, test loss = 3986.714492\n",
            "Epoch 1439: train loss = 2977.659050, test loss = 3986.102267\n",
            "Epoch 1440: train loss = 2976.888701, test loss = 3985.491290\n",
            "Epoch 1441: train loss = 2976.118490, test loss = 3984.881557\n",
            "Epoch 1442: train loss = 2975.348445, test loss = 3984.273059\n",
            "Epoch 1443: train loss = 2974.578592, test loss = 3983.665793\n",
            "Epoch 1444: train loss = 2973.808960, test loss = 3983.059752\n",
            "Epoch 1445: train loss = 2973.039577, test loss = 3982.454932\n",
            "Epoch 1446: train loss = 2972.270473, test loss = 3981.851328\n",
            "Epoch 1447: train loss = 2971.501678, test loss = 3981.248936\n",
            "Epoch 1448: train loss = 2970.733224, test loss = 3980.647754\n",
            "Epoch 1449: train loss = 2969.965143, test loss = 3980.047776\n",
            "Epoch 1450: train loss = 2969.197465, test loss = 3979.449001\n",
            "Epoch 1451: train loss = 2968.430224, test loss = 3978.851427\n",
            "Epoch 1452: train loss = 2967.663454, test loss = 3978.255051\n",
            "Epoch 1453: train loss = 2966.897187, test loss = 3977.659872\n",
            "Epoch 1454: train loss = 2966.131457, test loss = 3977.065889\n",
            "Epoch 1455: train loss = 2965.366299, test loss = 3976.473101\n",
            "Epoch 1456: train loss = 2964.601747, test loss = 3975.881509\n",
            "Epoch 1457: train loss = 2963.837836, test loss = 3975.291111\n",
            "Epoch 1458: train loss = 2963.074599, test loss = 3974.701908\n",
            "Epoch 1459: train loss = 2962.312073, test loss = 3974.113903\n",
            "Epoch 1460: train loss = 2961.550291, test loss = 3973.527095\n",
            "Epoch 1461: train loss = 2960.789289, test loss = 3972.941486\n",
            "Epoch 1462: train loss = 2960.029100, test loss = 3972.357079\n",
            "Epoch 1463: train loss = 2959.269759, test loss = 3971.773876\n",
            "Epoch 1464: train loss = 2958.511301, test loss = 3971.191879\n",
            "Epoch 1465: train loss = 2957.753759, test loss = 3970.611092\n",
            "Epoch 1466: train loss = 2956.997166, test loss = 3970.031517\n",
            "Epoch 1467: train loss = 2956.241556, test loss = 3969.453159\n",
            "Epoch 1468: train loss = 2955.486962, test loss = 3968.876022\n",
            "Epoch 1469: train loss = 2954.733415, test loss = 3968.300110\n",
            "Epoch 1470: train loss = 2953.980947, test loss = 3967.725426\n",
            "Epoch 1471: train loss = 2953.229589, test loss = 3967.151977\n",
            "Epoch 1472: train loss = 2952.479373, test loss = 3966.579767\n",
            "Epoch 1473: train loss = 2951.730327, test loss = 3966.008800\n",
            "Epoch 1474: train loss = 2950.982480, test loss = 3965.439084\n",
            "Epoch 1475: train loss = 2950.235861, test loss = 3964.870623\n",
            "Epoch 1476: train loss = 2949.490498, test loss = 3964.303422\n",
            "Epoch 1477: train loss = 2948.746417, test loss = 3963.737489\n",
            "Epoch 1478: train loss = 2948.003645, test loss = 3963.172829\n",
            "Epoch 1479: train loss = 2947.262206, test loss = 3962.609448\n",
            "Epoch 1480: train loss = 2946.522124, test loss = 3962.047353\n",
            "Epoch 1481: train loss = 2945.783423, test loss = 3961.486551\n",
            "Epoch 1482: train loss = 2945.046126, test loss = 3960.927047\n",
            "Epoch 1483: train loss = 2944.310253, test loss = 3960.368848\n",
            "Epoch 1484: train loss = 2943.575825, test loss = 3959.811961\n",
            "Epoch 1485: train loss = 2942.842862, test loss = 3959.256394\n",
            "Epoch 1486: train loss = 2942.111383, test loss = 3958.702152\n",
            "Epoch 1487: train loss = 2941.381404, test loss = 3958.149243\n",
            "Epoch 1488: train loss = 2940.652943, test loss = 3957.597673\n",
            "Epoch 1489: train loss = 2939.926015, test loss = 3957.047449\n",
            "Epoch 1490: train loss = 2939.200635, test loss = 3956.498578\n",
            "Epoch 1491: train loss = 2938.476816, test loss = 3955.951067\n",
            "Epoch 1492: train loss = 2937.754572, test loss = 3955.404922\n",
            "Epoch 1493: train loss = 2937.033913, test loss = 3954.860150\n",
            "Epoch 1494: train loss = 2936.314851, test loss = 3954.316759\n",
            "Epoch 1495: train loss = 2935.597395, test loss = 3953.774753\n",
            "Epoch 1496: train loss = 2934.881555, test loss = 3953.234141\n",
            "Epoch 1497: train loss = 2934.167337, test loss = 3952.694928\n",
            "Epoch 1498: train loss = 2933.454750, test loss = 3952.157120\n",
            "Epoch 1499: train loss = 2932.743799, test loss = 3951.620724\n",
            "Epoch 1500: train loss = 2932.034490, test loss = 3951.085746\n",
            "Epoch 1501: train loss = 2931.326826, test loss = 3950.552192\n",
            "Epoch 1502: train loss = 2930.620812, test loss = 3950.020067\n",
            "Epoch 1503: train loss = 2929.916450, test loss = 3949.489378\n",
            "Epoch 1504: train loss = 2929.213742, test loss = 3948.960130\n",
            "Epoch 1505: train loss = 2928.512688, test loss = 3948.432328\n",
            "Epoch 1506: train loss = 2927.813290, test loss = 3947.905978\n",
            "Epoch 1507: train loss = 2927.115548, test loss = 3947.381084\n",
            "Epoch 1508: train loss = 2926.419458, test loss = 3946.857652\n",
            "Epoch 1509: train loss = 2925.725021, test loss = 3946.335687\n",
            "Epoch 1510: train loss = 2925.032233, test loss = 3945.815193\n",
            "Epoch 1511: train loss = 2924.341092, test loss = 3945.296175\n",
            "Epoch 1512: train loss = 2923.651593, test loss = 3944.778637\n",
            "Epoch 1513: train loss = 2922.963732, test loss = 3944.262582\n",
            "Epoch 1514: train loss = 2922.277504, test loss = 3943.748016\n",
            "Epoch 1515: train loss = 2921.592905, test loss = 3943.234942\n",
            "Epoch 1516: train loss = 2920.909926, test loss = 3942.723364\n",
            "Epoch 1517: train loss = 2920.228563, test loss = 3942.213284\n",
            "Epoch 1518: train loss = 2919.548808, test loss = 3941.704707\n",
            "Epoch 1519: train loss = 2918.870654, test loss = 3941.197635\n",
            "Epoch 1520: train loss = 2918.194092, test loss = 3940.692072\n",
            "Epoch 1521: train loss = 2917.519114, test loss = 3940.188019\n",
            "Epoch 1522: train loss = 2916.845713, test loss = 3939.685480\n",
            "Epoch 1523: train loss = 2916.173877, test loss = 3939.184458\n",
            "Epoch 1524: train loss = 2915.503599, test loss = 3938.684953\n",
            "Epoch 1525: train loss = 2914.834869, test loss = 3938.186969\n",
            "Epoch 1526: train loss = 2914.167676, test loss = 3937.690507\n",
            "Epoch 1527: train loss = 2913.502009, test loss = 3937.195569\n",
            "Epoch 1528: train loss = 2912.837859, test loss = 3936.702157\n",
            "Epoch 1529: train loss = 2912.175215, test loss = 3936.210272\n",
            "Epoch 1530: train loss = 2911.514065, test loss = 3935.719915\n",
            "Epoch 1531: train loss = 2910.854398, test loss = 3935.231087\n",
            "Epoch 1532: train loss = 2910.196203, test loss = 3934.743789\n",
            "Epoch 1533: train loss = 2909.539467, test loss = 3934.258022\n",
            "Epoch 1534: train loss = 2908.884180, test loss = 3933.773787\n",
            "Epoch 1535: train loss = 2908.230329, test loss = 3933.291083\n",
            "Epoch 1536: train loss = 2907.577902, test loss = 3932.809912\n",
            "Epoch 1537: train loss = 2906.926887, test loss = 3932.330273\n",
            "Epoch 1538: train loss = 2906.277271, test loss = 3931.852167\n",
            "Epoch 1539: train loss = 2905.629042, test loss = 3931.375593\n",
            "Epoch 1540: train loss = 2904.982188, test loss = 3930.900551\n",
            "Epoch 1541: train loss = 2904.336696, test loss = 3930.427040\n",
            "Epoch 1542: train loss = 2903.692553, test loss = 3929.955061\n",
            "Epoch 1543: train loss = 2903.049747, test loss = 3929.484612\n",
            "Epoch 1544: train loss = 2902.408265, test loss = 3929.015692\n",
            "Epoch 1545: train loss = 2901.768095, test loss = 3928.548301\n",
            "Epoch 1546: train loss = 2901.129223, test loss = 3928.082437\n",
            "Epoch 1547: train loss = 2900.491637, test loss = 3927.618100\n",
            "Epoch 1548: train loss = 2899.855325, test loss = 3927.155288\n",
            "Epoch 1549: train loss = 2899.220273, test loss = 3926.693999\n",
            "Epoch 1550: train loss = 2898.586469, test loss = 3926.234232\n",
            "Epoch 1551: train loss = 2897.953901, test loss = 3925.775986\n",
            "Epoch 1552: train loss = 2897.322555, test loss = 3925.319258\n",
            "Epoch 1553: train loss = 2896.692420, test loss = 3924.864047\n",
            "Epoch 1554: train loss = 2896.063483, test loss = 3924.410351\n",
            "Epoch 1555: train loss = 2895.435730, test loss = 3923.958167\n",
            "Epoch 1556: train loss = 2894.809151, test loss = 3923.507493\n",
            "Epoch 1557: train loss = 2894.183733, test loss = 3923.058328\n",
            "Epoch 1558: train loss = 2893.559463, test loss = 3922.610668\n",
            "Epoch 1559: train loss = 2892.936330, test loss = 3922.164512\n",
            "Epoch 1560: train loss = 2892.314321, test loss = 3921.719856\n",
            "Epoch 1561: train loss = 2891.693425, test loss = 3921.276697\n",
            "Epoch 1562: train loss = 2891.073629, test loss = 3920.835034\n",
            "Epoch 1563: train loss = 2890.454922, test loss = 3920.394863\n",
            "Epoch 1564: train loss = 2889.837292, test loss = 3919.956181\n",
            "Epoch 1565: train loss = 2889.220728, test loss = 3919.518986\n",
            "Epoch 1566: train loss = 2888.605218, test loss = 3919.083273\n",
            "Epoch 1567: train loss = 2887.990752, test loss = 3918.649040\n",
            "Epoch 1568: train loss = 2887.377317, test loss = 3918.216284\n",
            "Epoch 1569: train loss = 2886.764902, test loss = 3917.785001\n",
            "Epoch 1570: train loss = 2886.153498, test loss = 3917.355187\n",
            "Epoch 1571: train loss = 2885.543092, test loss = 3916.926839\n",
            "Epoch 1572: train loss = 2884.933674, test loss = 3916.499954\n",
            "Epoch 1573: train loss = 2884.325234, test loss = 3916.074527\n",
            "Epoch 1574: train loss = 2883.717761, test loss = 3915.650555\n",
            "Epoch 1575: train loss = 2883.111245, test loss = 3915.228034\n",
            "Epoch 1576: train loss = 2882.505675, test loss = 3914.806960\n",
            "Epoch 1577: train loss = 2881.901042, test loss = 3914.387328\n",
            "Epoch 1578: train loss = 2881.297335, test loss = 3913.969136\n",
            "Epoch 1579: train loss = 2880.694545, test loss = 3913.552378\n",
            "Epoch 1580: train loss = 2880.092662, test loss = 3913.137051\n",
            "Epoch 1581: train loss = 2879.491677, test loss = 3912.723150\n",
            "Epoch 1582: train loss = 2878.891580, test loss = 3912.310670\n",
            "Epoch 1583: train loss = 2878.292362, test loss = 3911.899608\n",
            "Epoch 1584: train loss = 2877.694014, test loss = 3911.489959\n",
            "Epoch 1585: train loss = 2877.096527, test loss = 3911.081717\n",
            "Epoch 1586: train loss = 2876.499892, test loss = 3910.674879\n",
            "Epoch 1587: train loss = 2875.904100, test loss = 3910.269439\n",
            "Epoch 1588: train loss = 2875.309144, test loss = 3909.865393\n",
            "Epoch 1589: train loss = 2874.715014, test loss = 3909.462736\n",
            "Epoch 1590: train loss = 2874.121703, test loss = 3909.061463\n",
            "Epoch 1591: train loss = 2873.529202, test loss = 3908.661568\n",
            "Epoch 1592: train loss = 2872.937504, test loss = 3908.263048\n",
            "Epoch 1593: train loss = 2872.346601, test loss = 3907.865896\n",
            "Epoch 1594: train loss = 2871.756485, test loss = 3907.470107\n",
            "Epoch 1595: train loss = 2871.167148, test loss = 3907.075676\n",
            "Epoch 1596: train loss = 2870.578585, test loss = 3906.682598\n",
            "Epoch 1597: train loss = 2869.990786, test loss = 3906.290867\n",
            "Epoch 1598: train loss = 2869.403747, test loss = 3905.900477\n",
            "Epoch 1599: train loss = 2868.817458, test loss = 3905.511424\n",
            "Epoch 1600: train loss = 2868.231915, test loss = 3905.123701\n",
            "Epoch 1601: train loss = 2867.647111, test loss = 3904.737303\n",
            "Epoch 1602: train loss = 2867.063038, test loss = 3904.352223\n",
            "Epoch 1603: train loss = 2866.479692, test loss = 3903.968457\n",
            "Epoch 1604: train loss = 2865.897066, test loss = 3903.585997\n",
            "Epoch 1605: train loss = 2865.315154, test loss = 3903.204839\n",
            "Epoch 1606: train loss = 2864.733951, test loss = 3902.824975\n",
            "Epoch 1607: train loss = 2864.153451, test loss = 3902.446401\n",
            "Epoch 1608: train loss = 2863.573649, test loss = 3902.069109\n",
            "Epoch 1609: train loss = 2862.994539, test loss = 3901.693093\n",
            "Epoch 1610: train loss = 2862.416117, test loss = 3901.318347\n",
            "Epoch 1611: train loss = 2861.838378, test loss = 3900.944865\n",
            "Epoch 1612: train loss = 2861.261317, test loss = 3900.572640\n",
            "Epoch 1613: train loss = 2860.684929, test loss = 3900.201666\n",
            "Epoch 1614: train loss = 2860.109211, test loss = 3899.831935\n",
            "Epoch 1615: train loss = 2859.534158, test loss = 3899.463442\n",
            "Epoch 1616: train loss = 2858.959767, test loss = 3899.096179\n",
            "Epoch 1617: train loss = 2858.386032, test loss = 3898.730140\n",
            "Epoch 1618: train loss = 2857.812952, test loss = 3898.365318\n",
            "Epoch 1619: train loss = 2857.240522, test loss = 3898.001706\n",
            "Epoch 1620: train loss = 2856.668738, test loss = 3897.639296\n",
            "Epoch 1621: train loss = 2856.097599, test loss = 3897.278082\n",
            "Epoch 1622: train loss = 2855.527100, test loss = 3896.918057\n",
            "Epoch 1623: train loss = 2854.957240, test loss = 3896.559214\n",
            "Epoch 1624: train loss = 2854.388014, test loss = 3896.201544\n",
            "Epoch 1625: train loss = 2853.819422, test loss = 3895.845041\n",
            "Epoch 1626: train loss = 2853.251461, test loss = 3895.489698\n",
            "Epoch 1627: train loss = 2852.684128, test loss = 3895.135507\n",
            "Epoch 1628: train loss = 2852.117422, test loss = 3894.782460\n",
            "Epoch 1629: train loss = 2851.551340, test loss = 3894.430550\n",
            "Epoch 1630: train loss = 2850.985882, test loss = 3894.079769\n",
            "Epoch 1631: train loss = 2850.421046, test loss = 3893.730110\n",
            "Epoch 1632: train loss = 2849.856830, test loss = 3893.381564\n",
            "Epoch 1633: train loss = 2849.293233, test loss = 3893.034125\n",
            "Epoch 1634: train loss = 2848.730255, test loss = 3892.687784\n",
            "Epoch 1635: train loss = 2848.167894, test loss = 3892.342533\n",
            "Epoch 1636: train loss = 2847.606150, test loss = 3891.998364\n",
            "Epoch 1637: train loss = 2847.045023, test loss = 3891.655270\n",
            "Epoch 1638: train loss = 2846.484511, test loss = 3891.313242\n",
            "Epoch 1639: train loss = 2845.924615, test loss = 3890.972272\n",
            "Epoch 1640: train loss = 2845.365335, test loss = 3890.632352\n",
            "Epoch 1641: train loss = 2844.806671, test loss = 3890.293473\n",
            "Epoch 1642: train loss = 2844.248623, test loss = 3889.955629\n",
            "Epoch 1643: train loss = 2843.691191, test loss = 3889.618809\n",
            "Epoch 1644: train loss = 2843.134376, test loss = 3889.283007\n",
            "Epoch 1645: train loss = 2842.578178, test loss = 3888.948213\n",
            "Epoch 1646: train loss = 2842.022599, test loss = 3888.614420\n",
            "Epoch 1647: train loss = 2841.467638, test loss = 3888.281618\n",
            "Epoch 1648: train loss = 2840.913298, test loss = 3887.949799\n",
            "Epoch 1649: train loss = 2840.359579, test loss = 3887.618956\n",
            "Epoch 1650: train loss = 2839.806483, test loss = 3887.289079\n",
            "Epoch 1651: train loss = 2839.254010, test loss = 3886.960160\n",
            "Epoch 1652: train loss = 2838.702162, test loss = 3886.632190\n",
            "Epoch 1653: train loss = 2838.150941, test loss = 3886.305161\n",
            "Epoch 1654: train loss = 2837.600349, test loss = 3885.979064\n",
            "Epoch 1655: train loss = 2837.050386, test loss = 3885.653891\n",
            "Epoch 1656: train loss = 2836.501055, test loss = 3885.329633\n",
            "Epoch 1657: train loss = 2835.952358, test loss = 3885.006281\n",
            "Epoch 1658: train loss = 2835.404297, test loss = 3884.683827\n",
            "Epoch 1659: train loss = 2834.856873, test loss = 3884.362262\n",
            "Epoch 1660: train loss = 2834.310088, test loss = 3884.041578\n",
            "Epoch 1661: train loss = 2833.763946, test loss = 3883.721765\n",
            "Epoch 1662: train loss = 2833.218447, test loss = 3883.402816\n",
            "Epoch 1663: train loss = 2832.673595, test loss = 3883.084722\n",
            "Epoch 1664: train loss = 2832.129391, test loss = 3882.767473\n",
            "Epoch 1665: train loss = 2831.585837, test loss = 3882.451062\n",
            "Epoch 1666: train loss = 2831.042936, test loss = 3882.135480\n",
            "Epoch 1667: train loss = 2830.500691, test loss = 3881.820718\n",
            "Epoch 1668: train loss = 2829.959102, test loss = 3881.506768\n",
            "Epoch 1669: train loss = 2829.418174, test loss = 3881.193621\n",
            "Epoch 1670: train loss = 2828.877908, test loss = 3880.881268\n",
            "Epoch 1671: train loss = 2828.338305, test loss = 3880.569702\n",
            "Epoch 1672: train loss = 2827.799370, test loss = 3880.258914\n",
            "Epoch 1673: train loss = 2827.261103, test loss = 3879.948895\n",
            "Epoch 1674: train loss = 2826.723508, test loss = 3879.639637\n",
            "Epoch 1675: train loss = 2826.186585, test loss = 3879.331131\n",
            "Epoch 1676: train loss = 2825.650338, test loss = 3879.023370\n",
            "Epoch 1677: train loss = 2825.114769, test loss = 3878.716345\n",
            "Epoch 1678: train loss = 2824.579878, test loss = 3878.410047\n",
            "Epoch 1679: train loss = 2824.045669, test loss = 3878.104470\n",
            "Epoch 1680: train loss = 2823.512143, test loss = 3877.799603\n",
            "Epoch 1681: train loss = 2822.979302, test loss = 3877.495440\n",
            "Epoch 1682: train loss = 2822.447148, test loss = 3877.191972\n",
            "Epoch 1683: train loss = 2821.915682, test loss = 3876.889192\n",
            "Epoch 1684: train loss = 2821.384906, test loss = 3876.587091\n",
            "Epoch 1685: train loss = 2820.854821, test loss = 3876.285661\n",
            "Epoch 1686: train loss = 2820.325428, test loss = 3875.984895\n",
            "Epoch 1687: train loss = 2819.796729, test loss = 3875.684784\n",
            "Epoch 1688: train loss = 2819.268725, test loss = 3875.385322\n",
            "Epoch 1689: train loss = 2818.741416, test loss = 3875.086500\n",
            "Epoch 1690: train loss = 2818.214804, test loss = 3874.788311\n",
            "Epoch 1691: train loss = 2817.688889, test loss = 3874.490748\n",
            "Epoch 1692: train loss = 2817.163671, test loss = 3874.193802\n",
            "Epoch 1693: train loss = 2816.639151, test loss = 3873.897466\n",
            "Epoch 1694: train loss = 2816.115330, test loss = 3873.601734\n",
            "Epoch 1695: train loss = 2815.592206, test loss = 3873.306597\n",
            "Epoch 1696: train loss = 2815.069781, test loss = 3873.012048\n",
            "Epoch 1697: train loss = 2814.548054, test loss = 3872.718081\n",
            "Epoch 1698: train loss = 2814.027023, test loss = 3872.424689\n",
            "Epoch 1699: train loss = 2813.506690, test loss = 3872.131863\n",
            "Epoch 1700: train loss = 2812.987053, test loss = 3871.839598\n",
            "Epoch 1701: train loss = 2812.468110, test loss = 3871.547885\n",
            "Epoch 1702: train loss = 2811.949862, test loss = 3871.256720\n",
            "Epoch 1703: train loss = 2811.432306, test loss = 3870.966094\n",
            "Epoch 1704: train loss = 2810.915441, test loss = 3870.676001\n",
            "Epoch 1705: train loss = 2810.399266, test loss = 3870.386434\n",
            "Epoch 1706: train loss = 2809.883778, test loss = 3870.097387\n",
            "Epoch 1707: train loss = 2809.368975, test loss = 3869.808853\n",
            "Epoch 1708: train loss = 2808.854856, test loss = 3869.520826\n",
            "Epoch 1709: train loss = 2808.341417, test loss = 3869.233299\n",
            "Epoch 1710: train loss = 2807.828656, test loss = 3868.946266\n",
            "Epoch 1711: train loss = 2807.316570, test loss = 3868.659721\n",
            "Epoch 1712: train loss = 2806.805157, test loss = 3868.373657\n",
            "Epoch 1713: train loss = 2806.294412, test loss = 3868.088069\n",
            "Epoch 1714: train loss = 2805.784333, test loss = 3867.802949\n",
            "Epoch 1715: train loss = 2805.274916, test loss = 3867.518293\n",
            "Epoch 1716: train loss = 2804.766157, test loss = 3867.234093\n",
            "Epoch 1717: train loss = 2804.258052, test loss = 3866.950345\n",
            "Epoch 1718: train loss = 2803.750597, test loss = 3866.667042\n",
            "Epoch 1719: train loss = 2803.243787, test loss = 3866.384177\n",
            "Epoch 1720: train loss = 2802.737618, test loss = 3866.101747\n",
            "Epoch 1721: train loss = 2802.232085, test loss = 3865.819744\n",
            "Epoch 1722: train loss = 2801.727184, test loss = 3865.538162\n",
            "Epoch 1723: train loss = 2801.222908, test loss = 3865.256997\n",
            "Epoch 1724: train loss = 2800.719253, test loss = 3864.976242\n",
            "Epoch 1725: train loss = 2800.216214, test loss = 3864.695893\n",
            "Epoch 1726: train loss = 2799.713784, test loss = 3864.415942\n",
            "Epoch 1727: train loss = 2799.211957, test loss = 3864.136385\n",
            "Epoch 1728: train loss = 2798.710729, test loss = 3863.857216\n",
            "Epoch 1729: train loss = 2798.210092, test loss = 3863.578430\n",
            "Epoch 1730: train loss = 2797.710040, test loss = 3863.300021\n",
            "Epoch 1731: train loss = 2797.210567, test loss = 3863.021983\n",
            "Epoch 1732: train loss = 2796.711666, test loss = 3862.744312\n",
            "Epoch 1733: train loss = 2796.213329, test loss = 3862.467002\n",
            "Epoch 1734: train loss = 2795.715551, test loss = 3862.190047\n",
            "Epoch 1735: train loss = 2795.218325, test loss = 3861.913442\n",
            "Epoch 1736: train loss = 2794.721641, test loss = 3861.637182\n",
            "Epoch 1737: train loss = 2794.225495, test loss = 3861.361261\n",
            "Epoch 1738: train loss = 2793.729876, test loss = 3861.085674\n",
            "Epoch 1739: train loss = 2793.234779, test loss = 3860.810415\n",
            "Epoch 1740: train loss = 2792.740196, test loss = 3860.535480\n",
            "Epoch 1741: train loss = 2792.246117, test loss = 3860.260863\n",
            "Epoch 1742: train loss = 2791.752536, test loss = 3859.986558\n",
            "Epoch 1743: train loss = 2791.259443, test loss = 3859.712561\n",
            "Epoch 1744: train loss = 2790.766832, test loss = 3859.438865\n",
            "Epoch 1745: train loss = 2790.274692, test loss = 3859.165467\n",
            "Epoch 1746: train loss = 2789.783017, test loss = 3858.892359\n",
            "Epoch 1747: train loss = 2789.291796, test loss = 3858.619538\n",
            "Epoch 1748: train loss = 2788.801022, test loss = 3858.346997\n",
            "Epoch 1749: train loss = 2788.310685, test loss = 3858.074731\n",
            "Epoch 1750: train loss = 2787.820777, test loss = 3857.802734\n",
            "Epoch 1751: train loss = 2787.331289, test loss = 3857.531003\n",
            "Epoch 1752: train loss = 2786.842211, test loss = 3857.259530\n",
            "Epoch 1753: train loss = 2786.353535, test loss = 3856.988310\n",
            "Epoch 1754: train loss = 2785.865251, test loss = 3856.717339\n",
            "Epoch 1755: train loss = 2785.377350, test loss = 3856.446610\n",
            "Epoch 1756: train loss = 2784.889822, test loss = 3856.176118\n",
            "Epoch 1757: train loss = 2784.402659, test loss = 3855.905858\n",
            "Epoch 1758: train loss = 2783.915850, test loss = 3855.635823\n",
            "Epoch 1759: train loss = 2783.429387, test loss = 3855.366008\n",
            "Epoch 1760: train loss = 2782.943260, test loss = 3855.096408\n",
            "Epoch 1761: train loss = 2782.457459, test loss = 3854.827017\n",
            "Epoch 1762: train loss = 2781.971974, test loss = 3854.557829\n",
            "Epoch 1763: train loss = 2781.486797, test loss = 3854.288839\n",
            "Epoch 1764: train loss = 2781.001917, test loss = 3854.020040\n",
            "Epoch 1765: train loss = 2780.517324, test loss = 3853.751427\n",
            "Epoch 1766: train loss = 2780.033010, test loss = 3853.482993\n",
            "Epoch 1767: train loss = 2779.548964, test loss = 3853.214734\n",
            "Epoch 1768: train loss = 2779.065177, test loss = 3852.946643\n",
            "Epoch 1769: train loss = 2778.581638, test loss = 3852.678713\n",
            "Epoch 1770: train loss = 2778.098340, test loss = 3852.410939\n",
            "Epoch 1771: train loss = 2777.615271, test loss = 3852.143315\n",
            "Epoch 1772: train loss = 2777.132422, test loss = 3851.875835\n",
            "Epoch 1773: train loss = 2776.649785, test loss = 3851.608492\n",
            "Epoch 1774: train loss = 2776.167348, test loss = 3851.341280\n",
            "Epoch 1775: train loss = 2775.685103, test loss = 3851.074192\n",
            "Epoch 1776: train loss = 2775.203040, test loss = 3850.807223\n",
            "Epoch 1777: train loss = 2774.721150, test loss = 3850.540365\n",
            "Epoch 1778: train loss = 2774.239423, test loss = 3850.273613\n",
            "Epoch 1779: train loss = 2773.757851, test loss = 3850.006960\n",
            "Epoch 1780: train loss = 2773.276424, test loss = 3849.740398\n",
            "Epoch 1781: train loss = 2772.795133, test loss = 3849.473922\n",
            "Epoch 1782: train loss = 2772.313968, test loss = 3849.207524\n",
            "Epoch 1783: train loss = 2771.832922, test loss = 3848.941198\n",
            "Epoch 1784: train loss = 2771.351985, test loss = 3848.674937\n",
            "Epoch 1785: train loss = 2770.871148, test loss = 3848.408733\n",
            "Epoch 1786: train loss = 2770.390403, test loss = 3848.142581\n",
            "Epoch 1787: train loss = 2769.909742, test loss = 3847.876471\n",
            "Epoch 1788: train loss = 2769.429155, test loss = 3847.610399\n",
            "Epoch 1789: train loss = 2768.948635, test loss = 3847.344355\n",
            "Epoch 1790: train loss = 2768.468173, test loss = 3847.078333\n",
            "Epoch 1791: train loss = 2767.987763, test loss = 3846.812326\n",
            "Epoch 1792: train loss = 2767.507394, test loss = 3846.546326\n",
            "Epoch 1793: train loss = 2767.027061, test loss = 3846.280324\n",
            "Epoch 1794: train loss = 2766.546756, test loss = 3846.014315\n",
            "Epoch 1795: train loss = 2766.066470, test loss = 3845.748290\n",
            "Epoch 1796: train loss = 2765.586198, test loss = 3845.482241\n",
            "Epoch 1797: train loss = 2765.105932, test loss = 3845.216160\n",
            "Epoch 1798: train loss = 2764.625665, test loss = 3844.950040\n",
            "Epoch 1799: train loss = 2764.145391, test loss = 3844.683873\n",
            "Epoch 1800: train loss = 2763.665103, test loss = 3844.417649\n",
            "Epoch 1801: train loss = 2763.184796, test loss = 3844.151362\n",
            "Epoch 1802: train loss = 2762.704463, test loss = 3843.885003\n",
            "Epoch 1803: train loss = 2762.224098, test loss = 3843.618564\n",
            "Epoch 1804: train loss = 2761.743697, test loss = 3843.352036\n",
            "Epoch 1805: train loss = 2761.263253, test loss = 3843.085411\n",
            "Epoch 1806: train loss = 2760.782762, test loss = 3842.818679\n",
            "Epoch 1807: train loss = 2760.302220, test loss = 3842.551834\n",
            "Epoch 1808: train loss = 2759.821621, test loss = 3842.284866\n",
            "Epoch 1809: train loss = 2759.340961, test loss = 3842.017766\n",
            "Epoch 1810: train loss = 2758.860237, test loss = 3841.750525\n",
            "Epoch 1811: train loss = 2758.379445, test loss = 3841.483134\n",
            "Epoch 1812: train loss = 2757.898581, test loss = 3841.215585\n",
            "Epoch 1813: train loss = 2757.417642, test loss = 3840.947869\n",
            "Epoch 1814: train loss = 2756.936626, test loss = 3840.679975\n",
            "Epoch 1815: train loss = 2756.455530, test loss = 3840.411896\n",
            "Epoch 1816: train loss = 2755.974351, test loss = 3840.143622\n",
            "Epoch 1817: train loss = 2755.493088, test loss = 3839.875143\n",
            "Epoch 1818: train loss = 2755.011740, test loss = 3839.606450\n",
            "Epoch 1819: train loss = 2754.530305, test loss = 3839.337534\n",
            "Epoch 1820: train loss = 2754.048781, test loss = 3839.068385\n",
            "Epoch 1821: train loss = 2753.567169, test loss = 3838.798994\n",
            "Epoch 1822: train loss = 2753.085469, test loss = 3838.529350\n",
            "Epoch 1823: train loss = 2752.603679, test loss = 3838.259445\n",
            "Epoch 1824: train loss = 2752.121801, test loss = 3837.989268\n",
            "Epoch 1825: train loss = 2751.639835, test loss = 3837.718810\n",
            "Epoch 1826: train loss = 2751.157782, test loss = 3837.448061\n",
            "Epoch 1827: train loss = 2750.675644, test loss = 3837.177011\n",
            "Epoch 1828: train loss = 2750.193422, test loss = 3836.905650\n",
            "Epoch 1829: train loss = 2749.711118, test loss = 3836.633967\n",
            "Epoch 1830: train loss = 2749.228735, test loss = 3836.361955\n",
            "Epoch 1831: train loss = 2748.746275, test loss = 3836.089601\n",
            "Epoch 1832: train loss = 2748.263742, test loss = 3835.816896\n",
            "Epoch 1833: train loss = 2747.781139, test loss = 3835.543830\n",
            "Epoch 1834: train loss = 2747.298469, test loss = 3835.270393\n",
            "Epoch 1835: train loss = 2746.815737, test loss = 3834.996574\n",
            "Epoch 1836: train loss = 2746.332947, test loss = 3834.722364\n",
            "Epoch 1837: train loss = 2745.850103, test loss = 3834.447753\n",
            "Epoch 1838: train loss = 2745.367211, test loss = 3834.172729\n",
            "Epoch 1839: train loss = 2744.884275, test loss = 3833.897284\n",
            "Epoch 1840: train loss = 2744.401303, test loss = 3833.621406\n",
            "Epoch 1841: train loss = 2743.918298, test loss = 3833.345087\n",
            "Epoch 1842: train loss = 2743.435268, test loss = 3833.068314\n",
            "Epoch 1843: train loss = 2742.952220, test loss = 3832.791080\n",
            "Epoch 1844: train loss = 2742.469159, test loss = 3832.513372\n",
            "Epoch 1845: train loss = 2741.986093, test loss = 3832.235182\n",
            "Epoch 1846: train loss = 2741.503029, test loss = 3831.956499\n",
            "Epoch 1847: train loss = 2741.019975, test loss = 3831.677313\n",
            "Epoch 1848: train loss = 2740.536938, test loss = 3831.397614\n",
            "Epoch 1849: train loss = 2740.053927, test loss = 3831.117392\n",
            "Epoch 1850: train loss = 2739.570950, test loss = 3830.836638\n",
            "Epoch 1851: train loss = 2739.088015, test loss = 3830.555341\n",
            "Epoch 1852: train loss = 2738.605131, test loss = 3830.273492\n",
            "Epoch 1853: train loss = 2738.122307, test loss = 3829.991081\n",
            "Epoch 1854: train loss = 2737.639551, test loss = 3829.708099\n",
            "Epoch 1855: train loss = 2737.156873, test loss = 3829.424535\n",
            "Epoch 1856: train loss = 2736.674283, test loss = 3829.140381\n",
            "Epoch 1857: train loss = 2736.191789, test loss = 3828.855627\n",
            "Epoch 1858: train loss = 2735.709401, test loss = 3828.570263\n",
            "Epoch 1859: train loss = 2735.227130, test loss = 3828.284282\n",
            "Epoch 1860: train loss = 2734.744984, test loss = 3827.997673\n",
            "Epoch 1861: train loss = 2734.262974, test loss = 3827.710427\n",
            "Epoch 1862: train loss = 2733.781109, test loss = 3827.422536\n",
            "Epoch 1863: train loss = 2733.299400, test loss = 3827.133992\n",
            "Epoch 1864: train loss = 2732.817856, test loss = 3826.844785\n",
            "Epoch 1865: train loss = 2732.336488, test loss = 3826.554907\n",
            "Epoch 1866: train loss = 2731.855305, test loss = 3826.264349\n",
            "Epoch 1867: train loss = 2731.374318, test loss = 3825.973105\n",
            "Epoch 1868: train loss = 2730.893537, test loss = 3825.681165\n",
            "Epoch 1869: train loss = 2730.412972, test loss = 3825.388522\n",
            "Epoch 1870: train loss = 2729.932632, test loss = 3825.095169\n",
            "Epoch 1871: train loss = 2729.452528, test loss = 3824.801097\n",
            "Epoch 1872: train loss = 2728.972670, test loss = 3824.506299\n",
            "Epoch 1873: train loss = 2728.493066, test loss = 3824.210769\n",
            "Epoch 1874: train loss = 2728.013728, test loss = 3823.914499\n",
            "Epoch 1875: train loss = 2727.534663, test loss = 3823.617483\n",
            "Epoch 1876: train loss = 2727.055883, test loss = 3823.319714\n",
            "Epoch 1877: train loss = 2726.577395, test loss = 3823.021186\n",
            "Epoch 1878: train loss = 2726.099210, test loss = 3822.721892\n",
            "Epoch 1879: train loss = 2725.621335, test loss = 3822.421827\n",
            "Epoch 1880: train loss = 2725.143781, test loss = 3822.120985\n",
            "Epoch 1881: train loss = 2724.666555, test loss = 3821.819360\n",
            "Epoch 1882: train loss = 2724.189666, test loss = 3821.516947\n",
            "Epoch 1883: train loss = 2723.713123, test loss = 3821.213741\n",
            "Epoch 1884: train loss = 2723.236933, test loss = 3820.909738\n",
            "Epoch 1885: train loss = 2722.761104, test loss = 3820.604932\n",
            "Epoch 1886: train loss = 2722.285643, test loss = 3820.299319\n",
            "Epoch 1887: train loss = 2721.810559, test loss = 3819.992896\n",
            "Epoch 1888: train loss = 2721.335858, test loss = 3819.685657\n",
            "Epoch 1889: train loss = 2720.861547, test loss = 3819.377601\n",
            "Epoch 1890: train loss = 2720.387633, test loss = 3819.068722\n",
            "Epoch 1891: train loss = 2719.914123, test loss = 3818.759019\n",
            "Epoch 1892: train loss = 2719.441022, test loss = 3818.448488\n",
            "Epoch 1893: train loss = 2718.968336, test loss = 3818.137126\n",
            "Epoch 1894: train loss = 2718.496072, test loss = 3817.824931\n",
            "Epoch 1895: train loss = 2718.024234, test loss = 3817.511902\n",
            "Epoch 1896: train loss = 2717.552828, test loss = 3817.198035\n",
            "Epoch 1897: train loss = 2717.081858, test loss = 3816.883330\n",
            "Epoch 1898: train loss = 2716.611330, test loss = 3816.567785\n",
            "Epoch 1899: train loss = 2716.141247, test loss = 3816.251400\n",
            "Epoch 1900: train loss = 2715.671614, test loss = 3815.934172\n",
            "Epoch 1901: train loss = 2715.202434, test loss = 3815.616102\n",
            "Epoch 1902: train loss = 2714.733712, test loss = 3815.297189\n",
            "Epoch 1903: train loss = 2714.265449, test loss = 3814.977433\n",
            "Epoch 1904: train loss = 2713.797651, test loss = 3814.656834\n",
            "Epoch 1905: train loss = 2713.330319, test loss = 3814.335394\n",
            "Epoch 1906: train loss = 2712.863455, test loss = 3814.013111\n",
            "Epoch 1907: train loss = 2712.397063, test loss = 3813.689988\n",
            "Epoch 1908: train loss = 2711.931144, test loss = 3813.366026\n",
            "Epoch 1909: train loss = 2711.465700, test loss = 3813.041226\n",
            "Epoch 1910: train loss = 2711.000733, test loss = 3812.715590\n",
            "Epoch 1911: train loss = 2710.536244, test loss = 3812.389119\n",
            "Epoch 1912: train loss = 2710.072235, test loss = 3812.061816\n",
            "Epoch 1913: train loss = 2709.608705, test loss = 3811.733684\n",
            "Epoch 1914: train loss = 2709.145656, test loss = 3811.404724\n",
            "Epoch 1915: train loss = 2708.683089, test loss = 3811.074941\n",
            "Epoch 1916: train loss = 2708.221003, test loss = 3810.744337\n",
            "Epoch 1917: train loss = 2707.759399, test loss = 3810.412915\n",
            "Epoch 1918: train loss = 2707.298277, test loss = 3810.080680\n",
            "Epoch 1919: train loss = 2706.837635, test loss = 3809.747634\n",
            "Epoch 1920: train loss = 2706.377475, test loss = 3809.413783\n",
            "Epoch 1921: train loss = 2705.917795, test loss = 3809.079130\n",
            "Epoch 1922: train loss = 2705.458594, test loss = 3808.743681\n",
            "Epoch 1923: train loss = 2704.999872, test loss = 3808.407439\n",
            "Epoch 1924: train loss = 2704.541627, test loss = 3808.070410\n",
            "Epoch 1925: train loss = 2704.083858, test loss = 3807.732599\n",
            "Epoch 1926: train loss = 2703.626563, test loss = 3807.394011\n",
            "Epoch 1927: train loss = 2703.169742, test loss = 3807.054652\n",
            "Epoch 1928: train loss = 2702.713391, test loss = 3806.714528\n",
            "Epoch 1929: train loss = 2702.257510, test loss = 3806.373645\n",
            "Epoch 1930: train loss = 2701.802097, test loss = 3806.032009\n",
            "Epoch 1931: train loss = 2701.347149, test loss = 3805.689626\n",
            "Epoch 1932: train loss = 2700.892665, test loss = 3805.346503\n",
            "Epoch 1933: train loss = 2700.438642, test loss = 3805.002646\n",
            "Epoch 1934: train loss = 2699.985077, test loss = 3804.658063\n",
            "Epoch 1935: train loss = 2699.531969, test loss = 3804.312761\n",
            "Epoch 1936: train loss = 2699.079315, test loss = 3803.966747\n",
            "Epoch 1937: train loss = 2698.627111, test loss = 3803.620028\n",
            "Epoch 1938: train loss = 2698.175357, test loss = 3803.272612\n",
            "Epoch 1939: train loss = 2697.724048, test loss = 3802.924507\n",
            "Epoch 1940: train loss = 2697.273182, test loss = 3802.575720\n",
            "Epoch 1941: train loss = 2696.822757, test loss = 3802.226260\n",
            "Epoch 1942: train loss = 2696.372769, test loss = 3801.876135\n",
            "Epoch 1943: train loss = 2695.923215, test loss = 3801.525353\n",
            "Epoch 1944: train loss = 2695.474093, test loss = 3801.173923\n",
            "Epoch 1945: train loss = 2695.025400, test loss = 3800.821853\n",
            "Epoch 1946: train loss = 2694.577132, test loss = 3800.469152\n",
            "Epoch 1947: train loss = 2694.129286, test loss = 3800.115829\n",
            "Epoch 1948: train loss = 2693.681860, test loss = 3799.761893\n",
            "Epoch 1949: train loss = 2693.234850, test loss = 3799.407352\n",
            "Epoch 1950: train loss = 2692.788253, test loss = 3799.052217\n",
            "Epoch 1951: train loss = 2692.342066, test loss = 3798.696497\n",
            "Epoch 1952: train loss = 2691.896287, test loss = 3798.340200\n",
            "Epoch 1953: train loss = 2691.450911, test loss = 3797.983338\n",
            "Epoch 1954: train loss = 2691.005936, test loss = 3797.625918\n",
            "Epoch 1955: train loss = 2690.561359, test loss = 3797.267951\n",
            "Epoch 1956: train loss = 2690.117176, test loss = 3796.909446\n",
            "Epoch 1957: train loss = 2689.673385, test loss = 3796.550414\n",
            "Epoch 1958: train loss = 2689.229983, test loss = 3796.190865\n",
            "Epoch 1959: train loss = 2688.786966, test loss = 3795.830808\n",
            "Epoch 1960: train loss = 2688.344332, test loss = 3795.470254\n",
            "Epoch 1961: train loss = 2687.902077, test loss = 3795.109213\n",
            "Epoch 1962: train loss = 2687.460198, test loss = 3794.747695\n",
            "Epoch 1963: train loss = 2687.018693, test loss = 3794.385711\n",
            "Epoch 1964: train loss = 2686.577559, test loss = 3794.023270\n",
            "Epoch 1965: train loss = 2686.136793, test loss = 3793.660384\n",
            "Epoch 1966: train loss = 2685.696392, test loss = 3793.297063\n",
            "Epoch 1967: train loss = 2685.256353, test loss = 3792.933317\n",
            "Epoch 1968: train loss = 2684.816673, test loss = 3792.569157\n",
            "Epoch 1969: train loss = 2684.377350, test loss = 3792.204594\n",
            "Epoch 1970: train loss = 2683.938382, test loss = 3791.839639\n",
            "Epoch 1971: train loss = 2683.499764, test loss = 3791.474301\n",
            "Epoch 1972: train loss = 2683.061496, test loss = 3791.108592\n",
            "Epoch 1973: train loss = 2682.623574, test loss = 3790.742523\n",
            "Epoch 1974: train loss = 2682.185996, test loss = 3790.376105\n",
            "Epoch 1975: train loss = 2681.748759, test loss = 3790.009348\n",
            "Epoch 1976: train loss = 2681.311861, test loss = 3789.642263\n",
            "Epoch 1977: train loss = 2680.875300, test loss = 3789.274861\n",
            "Epoch 1978: train loss = 2680.439073, test loss = 3788.907154\n",
            "Epoch 1979: train loss = 2680.003178, test loss = 3788.539151\n",
            "Epoch 1980: train loss = 2679.567613, test loss = 3788.170864\n",
            "Epoch 1981: train loss = 2679.132376, test loss = 3787.802304\n",
            "Epoch 1982: train loss = 2678.697465, test loss = 3787.433482\n",
            "Epoch 1983: train loss = 2678.262877, test loss = 3787.064409\n",
            "Epoch 1984: train loss = 2677.828611, test loss = 3786.695095\n",
            "Epoch 1985: train loss = 2677.394665, test loss = 3786.325552\n",
            "Epoch 1986: train loss = 2676.961037, test loss = 3785.955791\n",
            "Epoch 1987: train loss = 2676.527725, test loss = 3785.585823\n",
            "Epoch 1988: train loss = 2676.094727, test loss = 3785.215658\n",
            "Epoch 1989: train loss = 2675.662041, test loss = 3784.845308\n",
            "Epoch 1990: train loss = 2675.229667, test loss = 3784.474783\n",
            "Epoch 1991: train loss = 2674.797602, test loss = 3784.104095\n",
            "Epoch 1992: train loss = 2674.365845, test loss = 3783.733255\n",
            "Epoch 1993: train loss = 2673.934394, test loss = 3783.362272\n",
            "Epoch 1994: train loss = 2673.503248, test loss = 3782.991159\n",
            "Epoch 1995: train loss = 2673.072405, test loss = 3782.619925\n",
            "Epoch 1996: train loss = 2672.641865, test loss = 3782.248583\n",
            "Epoch 1997: train loss = 2672.211625, test loss = 3781.877142\n",
            "Epoch 1998: train loss = 2671.781685, test loss = 3781.505613\n",
            "Epoch 1999: train loss = 2671.352044, test loss = 3781.134008\n",
            "Epoch 2000: train loss = 2670.922700, test loss = 3780.762336\n",
            "Epoch 2001: train loss = 2670.493652, test loss = 3780.390610\n",
            "Epoch 2002: train loss = 2670.064899, test loss = 3780.018838\n",
            "Epoch 2003: train loss = 2669.636441, test loss = 3779.647032\n",
            "Epoch 2004: train loss = 2669.208276, test loss = 3779.275203\n",
            "Epoch 2005: train loss = 2668.780404, test loss = 3778.903361\n",
            "Epoch 2006: train loss = 2668.352824, test loss = 3778.531517\n",
            "Epoch 2007: train loss = 2667.925534, test loss = 3778.159680\n",
            "Epoch 2008: train loss = 2667.498534, test loss = 3777.787862\n",
            "Epoch 2009: train loss = 2667.071824, test loss = 3777.416073\n",
            "Epoch 2010: train loss = 2666.645403, test loss = 3777.044323\n",
            "Epoch 2011: train loss = 2666.219271, test loss = 3776.672623\n",
            "Epoch 2012: train loss = 2665.793426, test loss = 3776.300982\n",
            "Epoch 2013: train loss = 2665.367868, test loss = 3775.929412\n",
            "Epoch 2014: train loss = 2664.942597, test loss = 3775.557922\n",
            "Epoch 2015: train loss = 2664.517613, test loss = 3775.186522\n",
            "Epoch 2016: train loss = 2664.092915, test loss = 3774.815222\n",
            "Epoch 2017: train loss = 2663.668502, test loss = 3774.444033\n",
            "Epoch 2018: train loss = 2663.244375, test loss = 3774.072964\n",
            "Epoch 2019: train loss = 2662.820534, test loss = 3773.702025\n",
            "Epoch 2020: train loss = 2662.396977, test loss = 3773.331226\n",
            "Epoch 2021: train loss = 2661.973705, test loss = 3772.960577\n",
            "Epoch 2022: train loss = 2661.550718, test loss = 3772.590088\n",
            "Epoch 2023: train loss = 2661.128016, test loss = 3772.219768\n",
            "Epoch 2024: train loss = 2660.705598, test loss = 3771.849627\n",
            "Epoch 2025: train loss = 2660.283465, test loss = 3771.479675\n",
            "Epoch 2026: train loss = 2659.861617, test loss = 3771.109920\n",
            "Epoch 2027: train loss = 2659.440053, test loss = 3770.740373\n",
            "Epoch 2028: train loss = 2659.018775, test loss = 3770.371042\n",
            "Epoch 2029: train loss = 2658.597781, test loss = 3770.001937\n",
            "Epoch 2030: train loss = 2658.177073, test loss = 3769.633068\n",
            "Epoch 2031: train loss = 2657.756649, test loss = 3769.264443\n",
            "Epoch 2032: train loss = 2657.336512, test loss = 3768.896072\n",
            "Epoch 2033: train loss = 2656.916660, test loss = 3768.527963\n",
            "Epoch 2034: train loss = 2656.497095, test loss = 3768.160126\n",
            "Epoch 2035: train loss = 2656.077816, test loss = 3767.792569\n",
            "Epoch 2036: train loss = 2655.658823, test loss = 3767.425302\n",
            "Epoch 2037: train loss = 2655.240118, test loss = 3767.058332\n",
            "Epoch 2038: train loss = 2654.821701, test loss = 3766.691670\n",
            "Epoch 2039: train loss = 2654.403571, test loss = 3766.325323\n",
            "Epoch 2040: train loss = 2653.985730, test loss = 3765.959299\n",
            "Epoch 2041: train loss = 2653.568179, test loss = 3765.593609\n",
            "Epoch 2042: train loss = 2653.150916, test loss = 3765.228259\n",
            "Epoch 2043: train loss = 2652.733944, test loss = 3764.863258\n",
            "Epoch 2044: train loss = 2652.317262, test loss = 3764.498615\n",
            "Epoch 2045: train loss = 2651.900872, test loss = 3764.134337\n",
            "Epoch 2046: train loss = 2651.484773, test loss = 3763.770434\n",
            "Epoch 2047: train loss = 2651.068966, test loss = 3763.406912\n",
            "Epoch 2048: train loss = 2650.653453, test loss = 3763.043779\n",
            "Epoch 2049: train loss = 2650.238234, test loss = 3762.681045\n",
            "Epoch 2050: train loss = 2649.823308, test loss = 3762.318716\n",
            "Epoch 2051: train loss = 2649.408678, test loss = 3761.956800\n",
            "Epoch 2052: train loss = 2648.994344, test loss = 3761.595305\n",
            "Epoch 2053: train loss = 2648.580307, test loss = 3761.234238\n",
            "Epoch 2054: train loss = 2648.166567, test loss = 3760.873608\n",
            "Epoch 2055: train loss = 2647.753125, test loss = 3760.513420\n",
            "Epoch 2056: train loss = 2647.339981, test loss = 3760.153684\n",
            "Epoch 2057: train loss = 2646.927138, test loss = 3759.794406\n",
            "Epoch 2058: train loss = 2646.514595, test loss = 3759.435592\n",
            "Epoch 2059: train loss = 2646.102354, test loss = 3759.077251\n",
            "Epoch 2060: train loss = 2645.690415, test loss = 3758.719390\n",
            "Epoch 2061: train loss = 2645.278779, test loss = 3758.362014\n",
            "Epoch 2062: train loss = 2644.867447, test loss = 3758.005132\n",
            "Epoch 2063: train loss = 2644.456420, test loss = 3757.648750\n",
            "Epoch 2064: train loss = 2644.045699, test loss = 3757.292874\n",
            "Epoch 2065: train loss = 2643.635284, test loss = 3756.937512\n",
            "Epoch 2066: train loss = 2643.225178, test loss = 3756.582669\n",
            "Epoch 2067: train loss = 2642.815379, test loss = 3756.228353\n",
            "Epoch 2068: train loss = 2642.405891, test loss = 3755.874570\n",
            "Epoch 2069: train loss = 2641.996713, test loss = 3755.521325\n",
            "Epoch 2070: train loss = 2641.587846, test loss = 3755.168626\n",
            "Epoch 2071: train loss = 2641.179292, test loss = 3754.816478\n",
            "Epoch 2072: train loss = 2640.771051, test loss = 3754.464887\n",
            "Epoch 2073: train loss = 2640.363125, test loss = 3754.113860\n",
            "Epoch 2074: train loss = 2639.955514, test loss = 3753.763401\n",
            "Epoch 2075: train loss = 2639.548220, test loss = 3753.413518\n",
            "Epoch 2076: train loss = 2639.141243, test loss = 3753.064216\n",
            "Epoch 2077: train loss = 2638.734585, test loss = 3752.715500\n",
            "Epoch 2078: train loss = 2638.328246, test loss = 3752.367377\n",
            "Epoch 2079: train loss = 2637.922228, test loss = 3752.019850\n",
            "Epoch 2080: train loss = 2637.516531, test loss = 3751.672927\n",
            "Epoch 2081: train loss = 2637.111157, test loss = 3751.326611\n",
            "Epoch 2082: train loss = 2636.706106, test loss = 3750.980910\n",
            "Epoch 2083: train loss = 2636.301380, test loss = 3750.635826\n",
            "Epoch 2084: train loss = 2635.896980, test loss = 3750.291366\n",
            "Epoch 2085: train loss = 2635.492907, test loss = 3749.947535\n",
            "Epoch 2086: train loss = 2635.089161, test loss = 3749.604338\n",
            "Epoch 2087: train loss = 2634.685745, test loss = 3749.261779\n",
            "Epoch 2088: train loss = 2634.282658, test loss = 3748.919862\n",
            "Epoch 2089: train loss = 2633.879903, test loss = 3748.578594\n",
            "Epoch 2090: train loss = 2633.477479, test loss = 3748.237978\n",
            "Epoch 2091: train loss = 2633.075389, test loss = 3747.898019\n",
            "Epoch 2092: train loss = 2632.673633, test loss = 3747.558721\n",
            "Epoch 2093: train loss = 2632.272212, test loss = 3747.220088\n",
            "Epoch 2094: train loss = 2631.871128, test loss = 3746.882125\n",
            "Epoch 2095: train loss = 2631.470381, test loss = 3746.544836\n",
            "Epoch 2096: train loss = 2631.069972, test loss = 3746.208226\n",
            "Epoch 2097: train loss = 2630.669903, test loss = 3745.872297\n",
            "Epoch 2098: train loss = 2630.270174, test loss = 3745.537054\n",
            "Epoch 2099: train loss = 2629.870787, test loss = 3745.202501\n",
            "Epoch 2100: train loss = 2629.471743, test loss = 3744.868642\n",
            "Epoch 2101: train loss = 2629.073043, test loss = 3744.535480\n",
            "Epoch 2102: train loss = 2628.674687, test loss = 3744.203019\n",
            "Epoch 2103: train loss = 2628.276677, test loss = 3743.871262\n",
            "Epoch 2104: train loss = 2627.879014, test loss = 3743.540214\n",
            "Epoch 2105: train loss = 2627.481698, test loss = 3743.209876\n",
            "Epoch 2106: train loss = 2627.084732, test loss = 3742.880253\n",
            "Epoch 2107: train loss = 2626.688116, test loss = 3742.551348\n",
            "Epoch 2108: train loss = 2626.291850, test loss = 3742.223163\n",
            "Epoch 2109: train loss = 2625.895937, test loss = 3741.895703\n",
            "Epoch 2110: train loss = 2625.500376, test loss = 3741.568970\n",
            "Epoch 2111: train loss = 2625.105170, test loss = 3741.242966\n",
            "Epoch 2112: train loss = 2624.710318, test loss = 3740.917695\n",
            "Epoch 2113: train loss = 2624.315823, test loss = 3740.593159\n",
            "Epoch 2114: train loss = 2623.921685, test loss = 3740.269362\n",
            "Epoch 2115: train loss = 2623.527904, test loss = 3739.946305\n",
            "Epoch 2116: train loss = 2623.134483, test loss = 3739.623992\n",
            "Epoch 2117: train loss = 2622.741422, test loss = 3739.302424\n",
            "Epoch 2118: train loss = 2622.348721, test loss = 3738.981604\n",
            "Epoch 2119: train loss = 2621.956382, test loss = 3738.661535\n",
            "Epoch 2120: train loss = 2621.564407, test loss = 3738.342218\n",
            "Epoch 2121: train loss = 2621.172795, test loss = 3738.023656\n",
            "Epoch 2122: train loss = 2620.781547, test loss = 3737.705851\n",
            "Epoch 2123: train loss = 2620.390666, test loss = 3737.388805\n",
            "Epoch 2124: train loss = 2620.000151, test loss = 3737.072520\n",
            "Epoch 2125: train loss = 2619.610003, test loss = 3736.756997\n",
            "Epoch 2126: train loss = 2619.220224, test loss = 3736.442239\n",
            "Epoch 2127: train loss = 2618.830815, test loss = 3736.128248\n",
            "Epoch 2128: train loss = 2618.441775, test loss = 3735.815024\n",
            "Epoch 2129: train loss = 2618.053107, test loss = 3735.502570\n",
            "Epoch 2130: train loss = 2617.664810, test loss = 3735.190887\n",
            "Epoch 2131: train loss = 2617.276887, test loss = 3734.879976\n",
            "Epoch 2132: train loss = 2616.889337, test loss = 3734.569840\n",
            "Epoch 2133: train loss = 2616.502161, test loss = 3734.260478\n",
            "Epoch 2134: train loss = 2616.115361, test loss = 3733.951894\n",
            "Epoch 2135: train loss = 2615.728938, test loss = 3733.644087\n",
            "Epoch 2136: train loss = 2615.342891, test loss = 3733.337059\n",
            "Epoch 2137: train loss = 2614.957222, test loss = 3733.030811\n",
            "Epoch 2138: train loss = 2614.571932, test loss = 3732.725344\n",
            "Epoch 2139: train loss = 2614.187021, test loss = 3732.420658\n",
            "Epoch 2140: train loss = 2613.802491, test loss = 3732.116756\n",
            "Epoch 2141: train loss = 2613.418341, test loss = 3731.813637\n",
            "Epoch 2142: train loss = 2613.034574, test loss = 3731.511303\n",
            "Epoch 2143: train loss = 2612.651188, test loss = 3731.209754\n",
            "Epoch 2144: train loss = 2612.268186, test loss = 3730.908990\n",
            "Epoch 2145: train loss = 2611.885569, test loss = 3730.609013\n",
            "Epoch 2146: train loss = 2611.503335, test loss = 3730.309823\n",
            "Epoch 2147: train loss = 2611.121488, test loss = 3730.011420\n",
            "Epoch 2148: train loss = 2610.740026, test loss = 3729.713804\n",
            "Epoch 2149: train loss = 2610.358951, test loss = 3729.416977\n",
            "Epoch 2150: train loss = 2609.978264, test loss = 3729.120938\n",
            "Epoch 2151: train loss = 2609.597965, test loss = 3728.825688\n",
            "Epoch 2152: train loss = 2609.218055, test loss = 3728.531226\n",
            "Epoch 2153: train loss = 2608.838534, test loss = 3728.237553\n",
            "Epoch 2154: train loss = 2608.459403, test loss = 3727.944670\n",
            "Epoch 2155: train loss = 2608.080663, test loss = 3727.652575\n",
            "Epoch 2156: train loss = 2607.702315, test loss = 3727.361269\n",
            "Epoch 2157: train loss = 2607.324358, test loss = 3727.070752\n",
            "Epoch 2158: train loss = 2606.946794, test loss = 3726.781023\n",
            "Epoch 2159: train loss = 2606.569623, test loss = 3726.492084\n",
            "Epoch 2160: train loss = 2606.192845, test loss = 3726.203932\n",
            "Epoch 2161: train loss = 2605.816462, test loss = 3725.916568\n",
            "Epoch 2162: train loss = 2605.440474, test loss = 3725.629992\n",
            "Epoch 2163: train loss = 2605.064881, test loss = 3725.344204\n",
            "Epoch 2164: train loss = 2604.689684, test loss = 3725.059202\n",
            "Epoch 2165: train loss = 2604.314883, test loss = 3724.774986\n",
            "Epoch 2166: train loss = 2603.940479, test loss = 3724.491556\n",
            "Epoch 2167: train loss = 2603.566473, test loss = 3724.208911\n",
            "Epoch 2168: train loss = 2603.192864, test loss = 3723.927051\n",
            "Epoch 2169: train loss = 2602.819654, test loss = 3723.645975\n",
            "Epoch 2170: train loss = 2602.446842, test loss = 3723.365682\n",
            "Epoch 2171: train loss = 2602.074429, test loss = 3723.086171\n",
            "Epoch 2172: train loss = 2601.702416, test loss = 3722.807443\n",
            "Epoch 2173: train loss = 2601.330803, test loss = 3722.529494\n",
            "Epoch 2174: train loss = 2600.959591, test loss = 3722.252326\n",
            "Epoch 2175: train loss = 2600.588779, test loss = 3721.975937\n",
            "Epoch 2176: train loss = 2600.218368, test loss = 3721.700326\n",
            "Epoch 2177: train loss = 2599.848359, test loss = 3721.425491\n",
            "Epoch 2178: train loss = 2599.478752, test loss = 3721.151433\n",
            "Epoch 2179: train loss = 2599.109547, test loss = 3720.878150\n",
            "Epoch 2180: train loss = 2598.740745, test loss = 3720.605640\n",
            "Epoch 2181: train loss = 2598.372345, test loss = 3720.333903\n",
            "Epoch 2182: train loss = 2598.004349, test loss = 3720.062937\n",
            "Epoch 2183: train loss = 2597.636756, test loss = 3719.792742\n",
            "Epoch 2184: train loss = 2597.269568, test loss = 3719.523315\n",
            "Epoch 2185: train loss = 2596.902783, test loss = 3719.254656\n",
            "Epoch 2186: train loss = 2596.536402, test loss = 3718.986763\n",
            "Epoch 2187: train loss = 2596.170426, test loss = 3718.719635\n",
            "Epoch 2188: train loss = 2595.804855, test loss = 3718.453270\n",
            "Epoch 2189: train loss = 2595.439689, test loss = 3718.187668\n",
            "Epoch 2190: train loss = 2595.074928, test loss = 3717.922825\n",
            "Epoch 2191: train loss = 2594.710573, test loss = 3717.658742\n",
            "Epoch 2192: train loss = 2594.346623, test loss = 3717.395417\n",
            "Epoch 2193: train loss = 2593.983079, test loss = 3717.132847\n",
            "Epoch 2194: train loss = 2593.619941, test loss = 3716.871032\n",
            "Epoch 2195: train loss = 2593.257209, test loss = 3716.609969\n",
            "Epoch 2196: train loss = 2592.894883, test loss = 3716.349657\n",
            "Epoch 2197: train loss = 2592.532964, test loss = 3716.090095\n",
            "Epoch 2198: train loss = 2592.171451, test loss = 3715.831280\n",
            "Epoch 2199: train loss = 2591.810345, test loss = 3715.573211\n",
            "Epoch 2200: train loss = 2591.449645, test loss = 3715.315886\n",
            "Epoch 2201: train loss = 2591.089352, test loss = 3715.059304\n",
            "Epoch 2202: train loss = 2590.729466, test loss = 3714.803462\n",
            "Epoch 2203: train loss = 2590.369987, test loss = 3714.548358\n",
            "Epoch 2204: train loss = 2590.010915, test loss = 3714.293992\n",
            "Epoch 2205: train loss = 2589.652250, test loss = 3714.040360\n",
            "Epoch 2206: train loss = 2589.293992, test loss = 3713.787461\n",
            "Epoch 2207: train loss = 2588.936141, test loss = 3713.535293\n",
            "Epoch 2208: train loss = 2588.578697, test loss = 3713.283855\n",
            "Epoch 2209: train loss = 2588.221660, test loss = 3713.033143\n",
            "Epoch 2210: train loss = 2587.865030, test loss = 3712.783156\n",
            "Epoch 2211: train loss = 2587.508807, test loss = 3712.533892\n",
            "Epoch 2212: train loss = 2587.152991, test loss = 3712.285349\n",
            "Epoch 2213: train loss = 2586.797582, test loss = 3712.037525\n",
            "Epoch 2214: train loss = 2586.442579, test loss = 3711.790418\n",
            "Epoch 2215: train loss = 2586.087983, test loss = 3711.544025\n",
            "Epoch 2216: train loss = 2585.733794, test loss = 3711.298345\n",
            "Epoch 2217: train loss = 2585.380011, test loss = 3711.053375\n",
            "Epoch 2218: train loss = 2585.026635, test loss = 3710.809113\n",
            "Epoch 2219: train loss = 2584.673664, test loss = 3710.565558\n",
            "Epoch 2220: train loss = 2584.321100, test loss = 3710.322706\n",
            "Epoch 2221: train loss = 2583.968942, test loss = 3710.080555\n",
            "Epoch 2222: train loss = 2583.617190, test loss = 3709.839104\n",
            "Epoch 2223: train loss = 2583.265843, test loss = 3709.598350\n",
            "Epoch 2224: train loss = 2582.914902, test loss = 3709.358290\n",
            "Epoch 2225: train loss = 2582.564366, test loss = 3709.118924\n",
            "Epoch 2226: train loss = 2582.214234, test loss = 3708.880247\n",
            "Epoch 2227: train loss = 2581.864508, test loss = 3708.642258\n",
            "Epoch 2228: train loss = 2581.515187, test loss = 3708.404954\n",
            "Epoch 2229: train loss = 2581.166269, test loss = 3708.168334\n",
            "Epoch 2230: train loss = 2580.817756, test loss = 3707.932394\n",
            "Epoch 2231: train loss = 2580.469647, test loss = 3707.697133\n",
            "Epoch 2232: train loss = 2580.121941, test loss = 3707.462548\n",
            "Epoch 2233: train loss = 2579.774638, test loss = 3707.228637\n",
            "Epoch 2234: train loss = 2579.427739, test loss = 3706.995396\n",
            "Epoch 2235: train loss = 2579.081242, test loss = 3706.762825\n",
            "Epoch 2236: train loss = 2578.735148, test loss = 3706.530919\n",
            "Epoch 2237: train loss = 2578.389455, test loss = 3706.299678\n",
            "Epoch 2238: train loss = 2578.044164, test loss = 3706.069098\n",
            "Epoch 2239: train loss = 2577.699275, test loss = 3705.839177\n",
            "Epoch 2240: train loss = 2577.354787, test loss = 3705.609912\n",
            "Epoch 2241: train loss = 2577.010699, test loss = 3705.381302\n",
            "Epoch 2242: train loss = 2576.667012, test loss = 3705.153343\n",
            "Epoch 2243: train loss = 2576.323724, test loss = 3704.926033\n",
            "Epoch 2244: train loss = 2575.980836, test loss = 3704.699369\n",
            "Epoch 2245: train loss = 2575.638348, test loss = 3704.473349\n",
            "Epoch 2246: train loss = 2575.296257, test loss = 3704.247970\n",
            "Epoch 2247: train loss = 2574.954565, test loss = 3704.023231\n",
            "Epoch 2248: train loss = 2574.613271, test loss = 3703.799127\n",
            "Epoch 2249: train loss = 2574.272374, test loss = 3703.575658\n",
            "Epoch 2250: train loss = 2573.931874, test loss = 3703.352819\n",
            "Epoch 2251: train loss = 2573.591771, test loss = 3703.130609\n",
            "Epoch 2252: train loss = 2573.252063, test loss = 3702.909025\n",
            "Epoch 2253: train loss = 2572.912751, test loss = 3702.688065\n",
            "Epoch 2254: train loss = 2572.573834, test loss = 3702.467725\n",
            "Epoch 2255: train loss = 2572.235311, test loss = 3702.248003\n",
            "Epoch 2256: train loss = 2571.897183, test loss = 3702.028897\n",
            "Epoch 2257: train loss = 2571.559447, test loss = 3701.810404\n",
            "Epoch 2258: train loss = 2571.222105, test loss = 3701.592522\n",
            "Epoch 2259: train loss = 2570.885155, test loss = 3701.375247\n",
            "Epoch 2260: train loss = 2570.548597, test loss = 3701.158577\n",
            "Epoch 2261: train loss = 2570.212430, test loss = 3700.942510\n",
            "Epoch 2262: train loss = 2569.876653, test loss = 3700.727042\n",
            "Epoch 2263: train loss = 2569.541267, test loss = 3700.512172\n",
            "Epoch 2264: train loss = 2569.206270, test loss = 3700.297896\n",
            "Epoch 2265: train loss = 2568.871662, test loss = 3700.084212\n",
            "Epoch 2266: train loss = 2568.537443, test loss = 3699.871118\n",
            "Epoch 2267: train loss = 2568.203611, test loss = 3699.658610\n",
            "Epoch 2268: train loss = 2567.870166, test loss = 3699.446686\n",
            "Epoch 2269: train loss = 2567.537107, test loss = 3699.235343\n",
            "Epoch 2270: train loss = 2567.204434, test loss = 3699.024578\n",
            "Epoch 2271: train loss = 2566.872147, test loss = 3698.814390\n",
            "Epoch 2272: train loss = 2566.540243, test loss = 3698.604775\n",
            "Epoch 2273: train loss = 2566.208724, test loss = 3698.395730\n",
            "Epoch 2274: train loss = 2565.877587, test loss = 3698.187254\n",
            "Epoch 2275: train loss = 2565.546833, test loss = 3697.979342\n",
            "Epoch 2276: train loss = 2565.216461, test loss = 3697.771993\n",
            "Epoch 2277: train loss = 2564.886469, test loss = 3697.565204\n",
            "Epoch 2278: train loss = 2564.556858, test loss = 3697.358972\n",
            "Epoch 2279: train loss = 2564.227627, test loss = 3697.153294\n",
            "Epoch 2280: train loss = 2563.898774, test loss = 3696.948169\n",
            "Epoch 2281: train loss = 2563.570299, test loss = 3696.743592\n",
            "Epoch 2282: train loss = 2563.242202, test loss = 3696.539562\n",
            "Epoch 2283: train loss = 2562.914481, test loss = 3696.336075\n",
            "Epoch 2284: train loss = 2562.587136, test loss = 3696.133129\n",
            "Epoch 2285: train loss = 2562.260166, test loss = 3695.930721\n",
            "Epoch 2286: train loss = 2561.933571, test loss = 3695.728850\n",
            "Epoch 2287: train loss = 2561.607349, test loss = 3695.527510\n",
            "Epoch 2288: train loss = 2561.281499, test loss = 3695.326702\n",
            "Epoch 2289: train loss = 2560.956022, test loss = 3695.126420\n",
            "Epoch 2290: train loss = 2560.630916, test loss = 3694.926663\n",
            "Epoch 2291: train loss = 2560.306179, test loss = 3694.727429\n",
            "Epoch 2292: train loss = 2559.981813, test loss = 3694.528713\n",
            "Epoch 2293: train loss = 2559.657815, test loss = 3694.330515\n",
            "Epoch 2294: train loss = 2559.334184, test loss = 3694.132830\n",
            "Epoch 2295: train loss = 2559.010921, test loss = 3693.935656\n",
            "Epoch 2296: train loss = 2558.688024, test loss = 3693.738991\n",
            "Epoch 2297: train loss = 2558.365492, test loss = 3693.542832\n",
            "Epoch 2298: train loss = 2558.043324, test loss = 3693.347176\n",
            "Epoch 2299: train loss = 2557.721520, test loss = 3693.152020\n",
            "Epoch 2300: train loss = 2557.400078, test loss = 3692.957362\n",
            "Epoch 2301: train loss = 2557.078998, test loss = 3692.763199\n",
            "Epoch 2302: train loss = 2556.758279, test loss = 3692.569528\n",
            "Epoch 2303: train loss = 2556.437920, test loss = 3692.376346\n",
            "Epoch 2304: train loss = 2556.117920, test loss = 3692.183652\n",
            "Epoch 2305: train loss = 2555.798278, test loss = 3691.991441\n",
            "Epoch 2306: train loss = 2555.478993, test loss = 3691.799712\n",
            "Epoch 2307: train loss = 2555.160064, test loss = 3691.608462\n",
            "Epoch 2308: train loss = 2554.841491, test loss = 3691.417687\n",
            "Epoch 2309: train loss = 2554.523272, test loss = 3691.227386\n",
            "Epoch 2310: train loss = 2554.205407, test loss = 3691.037556\n",
            "Epoch 2311: train loss = 2553.887894, test loss = 3690.848193\n",
            "Epoch 2312: train loss = 2553.570732, test loss = 3690.659295\n",
            "Epoch 2313: train loss = 2553.253921, test loss = 3690.470860\n",
            "Epoch 2314: train loss = 2552.937460, test loss = 3690.282884\n",
            "Epoch 2315: train loss = 2552.621347, test loss = 3690.095366\n",
            "Epoch 2316: train loss = 2552.305582, test loss = 3689.908302\n",
            "Epoch 2317: train loss = 2551.990164, test loss = 3689.721689\n",
            "Epoch 2318: train loss = 2551.675091, test loss = 3689.535525\n",
            "Epoch 2319: train loss = 2551.360363, test loss = 3689.349807\n",
            "Epoch 2320: train loss = 2551.045978, test loss = 3689.164533\n",
            "Epoch 2321: train loss = 2550.731936, test loss = 3688.979700\n",
            "Epoch 2322: train loss = 2550.418236, test loss = 3688.795304\n",
            "Epoch 2323: train loss = 2550.104877, test loss = 3688.611344\n",
            "Epoch 2324: train loss = 2549.791857, test loss = 3688.427816\n",
            "Epoch 2325: train loss = 2549.479176, test loss = 3688.244718\n",
            "Epoch 2326: train loss = 2549.166832, test loss = 3688.062048\n",
            "Epoch 2327: train loss = 2548.854825, test loss = 3687.879801\n",
            "Epoch 2328: train loss = 2548.543153, test loss = 3687.697977\n",
            "Epoch 2329: train loss = 2548.231816, test loss = 3687.516571\n",
            "Epoch 2330: train loss = 2547.920812, test loss = 3687.335582\n",
            "Epoch 2331: train loss = 2547.610140, test loss = 3687.155006\n",
            "Epoch 2332: train loss = 2547.299800, test loss = 3686.974841\n",
            "Epoch 2333: train loss = 2546.989790, test loss = 3686.795085\n",
            "Epoch 2334: train loss = 2546.680109, test loss = 3686.615733\n",
            "Epoch 2335: train loss = 2546.370756, test loss = 3686.436785\n",
            "Epoch 2336: train loss = 2546.061730, test loss = 3686.258236\n",
            "Epoch 2337: train loss = 2545.753030, test loss = 3686.080085\n",
            "Epoch 2338: train loss = 2545.444655, test loss = 3685.902328\n",
            "Epoch 2339: train loss = 2545.136604, test loss = 3685.724963\n",
            "Epoch 2340: train loss = 2544.828876, test loss = 3685.547986\n",
            "Epoch 2341: train loss = 2544.521469, test loss = 3685.371397\n",
            "Epoch 2342: train loss = 2544.214383, test loss = 3685.195191\n",
            "Epoch 2343: train loss = 2543.907616, test loss = 3685.019365\n",
            "Epoch 2344: train loss = 2543.601167, test loss = 3684.843918\n",
            "Epoch 2345: train loss = 2543.295036, test loss = 3684.668847\n",
            "Epoch 2346: train loss = 2542.989221, test loss = 3684.494148\n",
            "Epoch 2347: train loss = 2542.683721, test loss = 3684.319819\n",
            "Epoch 2348: train loss = 2542.378535, test loss = 3684.145857\n",
            "Epoch 2349: train loss = 2542.073661, test loss = 3683.972260\n",
            "Epoch 2350: train loss = 2541.769100, test loss = 3683.799024\n",
            "Epoch 2351: train loss = 2541.464849, test loss = 3683.626148\n",
            "Epoch 2352: train loss = 2541.160907, test loss = 3683.453628\n",
            "Epoch 2353: train loss = 2540.857274, test loss = 3683.281461\n",
            "Epoch 2354: train loss = 2540.553949, test loss = 3683.109645\n",
            "Epoch 2355: train loss = 2540.250929, test loss = 3682.938177\n",
            "Epoch 2356: train loss = 2539.948214, test loss = 3682.767054\n",
            "Epoch 2357: train loss = 2539.645804, test loss = 3682.596274\n",
            "Epoch 2358: train loss = 2539.343696, test loss = 3682.425833\n",
            "Epoch 2359: train loss = 2539.041890, test loss = 3682.255730\n",
            "Epoch 2360: train loss = 2538.740384, test loss = 3682.085961\n",
            "Epoch 2361: train loss = 2538.439178, test loss = 3681.916523\n",
            "Epoch 2362: train loss = 2538.138270, test loss = 3681.747414\n",
            "Epoch 2363: train loss = 2537.837660, test loss = 3681.578630\n",
            "Epoch 2364: train loss = 2537.537345, test loss = 3681.410170\n",
            "Epoch 2365: train loss = 2537.237326, test loss = 3681.242031\n",
            "Epoch 2366: train loss = 2536.937600, test loss = 3681.074208\n",
            "Epoch 2367: train loss = 2536.638167, test loss = 3680.906701\n",
            "Epoch 2368: train loss = 2536.339026, test loss = 3680.739506\n",
            "Epoch 2369: train loss = 2536.040175, test loss = 3680.572619\n",
            "Epoch 2370: train loss = 2535.741614, test loss = 3680.406040\n",
            "Epoch 2371: train loss = 2535.443341, test loss = 3680.239764\n",
            "Epoch 2372: train loss = 2535.145355, test loss = 3680.073788\n",
            "Epoch 2373: train loss = 2534.847655, test loss = 3679.908111\n",
            "Epoch 2374: train loss = 2534.550240, test loss = 3679.742729\n",
            "Epoch 2375: train loss = 2534.253108, test loss = 3679.577639\n",
            "Epoch 2376: train loss = 2533.956260, test loss = 3679.412839\n",
            "Epoch 2377: train loss = 2533.659693, test loss = 3679.248326\n",
            "Epoch 2378: train loss = 2533.363406, test loss = 3679.084096\n",
            "Epoch 2379: train loss = 2533.067399, test loss = 3678.920148\n",
            "Epoch 2380: train loss = 2532.771671, test loss = 3678.756478\n",
            "Epoch 2381: train loss = 2532.476219, test loss = 3678.593083\n",
            "Epoch 2382: train loss = 2532.181044, test loss = 3678.429961\n",
            "Epoch 2383: train loss = 2531.886143, test loss = 3678.267109\n",
            "Epoch 2384: train loss = 2531.591516, test loss = 3678.104523\n",
            "Epoch 2385: train loss = 2531.297163, test loss = 3677.942202\n",
            "Epoch 2386: train loss = 2531.003081, test loss = 3677.780141\n",
            "Epoch 2387: train loss = 2530.709269, test loss = 3677.618339\n",
            "Epoch 2388: train loss = 2530.415728, test loss = 3677.456793\n",
            "Epoch 2389: train loss = 2530.122454, test loss = 3677.295499\n",
            "Epoch 2390: train loss = 2529.829448, test loss = 3677.134455\n",
            "Epoch 2391: train loss = 2529.536709, test loss = 3676.973657\n",
            "Epoch 2392: train loss = 2529.244235, test loss = 3676.813104\n",
            "Epoch 2393: train loss = 2528.952025, test loss = 3676.652791\n",
            "Epoch 2394: train loss = 2528.660078, test loss = 3676.492717\n",
            "Epoch 2395: train loss = 2528.368394, test loss = 3676.332877\n",
            "Epoch 2396: train loss = 2528.076971, test loss = 3676.173270\n",
            "Epoch 2397: train loss = 2527.785807, test loss = 3676.013893\n",
            "Epoch 2398: train loss = 2527.494903, test loss = 3675.854742\n",
            "Epoch 2399: train loss = 2527.204257, test loss = 3675.695814\n",
            "Epoch 2400: train loss = 2526.913868, test loss = 3675.537107\n",
            "Epoch 2401: train loss = 2526.623735, test loss = 3675.378618\n",
            "Epoch 2402: train loss = 2526.333856, test loss = 3675.220344\n",
            "Epoch 2403: train loss = 2526.044232, test loss = 3675.062281\n",
            "Epoch 2404: train loss = 2525.754861, test loss = 3674.904428\n",
            "Epoch 2405: train loss = 2525.465742, test loss = 3674.746780\n",
            "Epoch 2406: train loss = 2525.176873, test loss = 3674.589335\n",
            "Epoch 2407: train loss = 2524.888255, test loss = 3674.432090\n",
            "Epoch 2408: train loss = 2524.599886, test loss = 3674.275042\n",
            "Epoch 2409: train loss = 2524.311765, test loss = 3674.118188\n",
            "Epoch 2410: train loss = 2524.023890, test loss = 3673.961526\n",
            "Epoch 2411: train loss = 2523.736262, test loss = 3673.805051\n",
            "Epoch 2412: train loss = 2523.448880, test loss = 3673.648762\n",
            "Epoch 2413: train loss = 2523.161741, test loss = 3673.492654\n",
            "Epoch 2414: train loss = 2522.874846, test loss = 3673.336726\n",
            "Epoch 2415: train loss = 2522.588193, test loss = 3673.180974\n",
            "Epoch 2416: train loss = 2522.301781, test loss = 3673.025395\n",
            "Epoch 2417: train loss = 2522.015610, test loss = 3672.869986\n",
            "Epoch 2418: train loss = 2521.729679, test loss = 3672.714744\n",
            "Epoch 2419: train loss = 2521.443986, test loss = 3672.559666\n",
            "Epoch 2420: train loss = 2521.158532, test loss = 3672.404750\n",
            "Epoch 2421: train loss = 2520.873314, test loss = 3672.249991\n",
            "Epoch 2422: train loss = 2520.588332, test loss = 3672.095388\n",
            "Epoch 2423: train loss = 2520.303586, test loss = 3671.940936\n",
            "Epoch 2424: train loss = 2520.019073, test loss = 3671.786633\n",
            "Epoch 2425: train loss = 2519.734795, test loss = 3671.632477\n",
            "Epoch 2426: train loss = 2519.450748, test loss = 3671.478463\n",
            "Epoch 2427: train loss = 2519.166934, test loss = 3671.324589\n",
            "Epoch 2428: train loss = 2518.883351, test loss = 3671.170851\n",
            "Epoch 2429: train loss = 2518.599997, test loss = 3671.017248\n",
            "Epoch 2430: train loss = 2518.316873, test loss = 3670.863775\n",
            "Epoch 2431: train loss = 2518.033978, test loss = 3670.710430\n",
            "Epoch 2432: train loss = 2517.751310, test loss = 3670.557209\n",
            "Epoch 2433: train loss = 2517.468869, test loss = 3670.404110\n",
            "Epoch 2434: train loss = 2517.186654, test loss = 3670.251129\n",
            "Epoch 2435: train loss = 2516.904664, test loss = 3670.098263\n",
            "Epoch 2436: train loss = 2516.622899, test loss = 3669.945510\n",
            "Epoch 2437: train loss = 2516.341358, test loss = 3669.792866\n",
            "Epoch 2438: train loss = 2516.060040, test loss = 3669.640329\n",
            "Epoch 2439: train loss = 2515.778944, test loss = 3669.487894\n",
            "Epoch 2440: train loss = 2515.498069, test loss = 3669.335559\n",
            "Epoch 2441: train loss = 2515.217416, test loss = 3669.183322\n",
            "Epoch 2442: train loss = 2514.936982, test loss = 3669.031178\n",
            "Epoch 2443: train loss = 2514.656768, test loss = 3668.879125\n",
            "Epoch 2444: train loss = 2514.376772, test loss = 3668.727160\n",
            "Epoch 2445: train loss = 2514.096994, test loss = 3668.575280\n",
            "Epoch 2446: train loss = 2513.817434, test loss = 3668.423481\n",
            "Epoch 2447: train loss = 2513.538090, test loss = 3668.271761\n",
            "Epoch 2448: train loss = 2513.258962, test loss = 3668.120116\n",
            "Epoch 2449: train loss = 2512.980049, test loss = 3667.968544\n",
            "Epoch 2450: train loss = 2512.701351, test loss = 3667.817041\n",
            "Epoch 2451: train loss = 2512.422866, test loss = 3667.665604\n",
            "Epoch 2452: train loss = 2512.144595, test loss = 3667.514231\n",
            "Epoch 2453: train loss = 2511.866536, test loss = 3667.362918\n",
            "Epoch 2454: train loss = 2511.588689, test loss = 3667.211662\n",
            "Epoch 2455: train loss = 2511.311054, test loss = 3667.060461\n",
            "Epoch 2456: train loss = 2511.033629, test loss = 3666.909310\n",
            "Epoch 2457: train loss = 2510.756414, test loss = 3666.758208\n",
            "Epoch 2458: train loss = 2510.479408, test loss = 3666.607150\n",
            "Epoch 2459: train loss = 2510.202611, test loss = 3666.456135\n",
            "Epoch 2460: train loss = 2509.926022, test loss = 3666.305158\n",
            "Epoch 2461: train loss = 2509.649641, test loss = 3666.154218\n",
            "Epoch 2462: train loss = 2509.373467, test loss = 3666.003311\n",
            "Epoch 2463: train loss = 2509.097500, test loss = 3665.852434\n",
            "Epoch 2464: train loss = 2508.821738, test loss = 3665.701583\n",
            "Epoch 2465: train loss = 2508.546181, test loss = 3665.550757\n",
            "Epoch 2466: train loss = 2508.270829, test loss = 3665.399952\n",
            "Epoch 2467: train loss = 2507.995681, test loss = 3665.249165\n",
            "Epoch 2468: train loss = 2507.720736, test loss = 3665.098394\n",
            "Epoch 2469: train loss = 2507.445995, test loss = 3664.947634\n",
            "Epoch 2470: train loss = 2507.171456, test loss = 3664.796884\n",
            "Epoch 2471: train loss = 2506.897118, test loss = 3664.646140\n",
            "Epoch 2472: train loss = 2506.622982, test loss = 3664.495400\n",
            "Epoch 2473: train loss = 2506.349047, test loss = 3664.344660\n",
            "Epoch 2474: train loss = 2506.075312, test loss = 3664.193918\n",
            "Epoch 2475: train loss = 2505.801776, test loss = 3664.043171\n",
            "Epoch 2476: train loss = 2505.528440, test loss = 3663.892416\n",
            "Epoch 2477: train loss = 2505.255302, test loss = 3663.741649\n",
            "Epoch 2478: train loss = 2504.982362, test loss = 3663.590870\n",
            "Epoch 2479: train loss = 2504.709619, test loss = 3663.440073\n",
            "Epoch 2480: train loss = 2504.437074, test loss = 3663.289258\n",
            "Epoch 2481: train loss = 2504.164725, test loss = 3663.138420\n",
            "Epoch 2482: train loss = 2503.892572, test loss = 3662.987557\n",
            "Epoch 2483: train loss = 2503.620614, test loss = 3662.836667\n",
            "Epoch 2484: train loss = 2503.348851, test loss = 3662.685746\n",
            "Epoch 2485: train loss = 2503.077282, test loss = 3662.534792\n",
            "Epoch 2486: train loss = 2502.805907, test loss = 3662.383803\n",
            "Epoch 2487: train loss = 2502.534726, test loss = 3662.232774\n",
            "Epoch 2488: train loss = 2502.263737, test loss = 3662.081705\n",
            "Epoch 2489: train loss = 2501.992940, test loss = 3661.930592\n",
            "Epoch 2490: train loss = 2501.722335, test loss = 3661.779433\n",
            "Epoch 2491: train loss = 2501.451921, test loss = 3661.628224\n",
            "Epoch 2492: train loss = 2501.181698, test loss = 3661.476964\n",
            "Epoch 2493: train loss = 2500.911665, test loss = 3661.325650\n",
            "Epoch 2494: train loss = 2500.641821, test loss = 3661.174279\n",
            "Epoch 2495: train loss = 2500.372166, test loss = 3661.022848\n",
            "Epoch 2496: train loss = 2500.102700, test loss = 3660.871356\n",
            "Epoch 2497: train loss = 2499.833422, test loss = 3660.719800\n",
            "Epoch 2498: train loss = 2499.564331, test loss = 3660.568177\n",
            "Epoch 2499: train loss = 2499.295427, test loss = 3660.416485\n",
            "Epoch 2500: train loss = 2499.026709, test loss = 3660.264722\n",
            "Epoch 2501: train loss = 2498.758177, test loss = 3660.112885\n",
            "Epoch 2502: train loss = 2498.489831, test loss = 3659.960971\n",
            "Epoch 2503: train loss = 2498.221669, test loss = 3659.808979\n",
            "Epoch 2504: train loss = 2497.953691, test loss = 3659.656906\n",
            "Epoch 2505: train loss = 2497.685897, test loss = 3659.504750\n",
            "Epoch 2506: train loss = 2497.418285, test loss = 3659.352509\n",
            "Epoch 2507: train loss = 2497.150856, test loss = 3659.200181\n",
            "Epoch 2508: train loss = 2496.883609, test loss = 3659.047762\n",
            "Epoch 2509: train loss = 2496.616543, test loss = 3658.895252\n",
            "Epoch 2510: train loss = 2496.349658, test loss = 3658.742647\n",
            "Epoch 2511: train loss = 2496.082953, test loss = 3658.589947\n",
            "Epoch 2512: train loss = 2495.816427, test loss = 3658.437148\n",
            "Epoch 2513: train loss = 2495.550080, test loss = 3658.284250\n",
            "Epoch 2514: train loss = 2495.283912, test loss = 3658.131249\n",
            "Epoch 2515: train loss = 2495.017921, test loss = 3657.978144\n",
            "Epoch 2516: train loss = 2494.752107, test loss = 3657.824933\n",
            "Epoch 2517: train loss = 2494.486470, test loss = 3657.671613\n",
            "Epoch 2518: train loss = 2494.221008, test loss = 3657.518184\n",
            "Epoch 2519: train loss = 2493.955722, test loss = 3657.364644\n",
            "Epoch 2520: train loss = 2493.690610, test loss = 3657.210990\n",
            "Epoch 2521: train loss = 2493.425673, test loss = 3657.057220\n",
            "Epoch 2522: train loss = 2493.160908, test loss = 3656.903334\n",
            "Epoch 2523: train loss = 2492.896316, test loss = 3656.749329\n",
            "Epoch 2524: train loss = 2492.631896, test loss = 3656.595203\n",
            "Epoch 2525: train loss = 2492.367648, test loss = 3656.440956\n",
            "Epoch 2526: train loss = 2492.103569, test loss = 3656.286585\n",
            "Epoch 2527: train loss = 2491.839661, test loss = 3656.132089\n",
            "Epoch 2528: train loss = 2491.575922, test loss = 3655.977466\n",
            "Epoch 2529: train loss = 2491.312352, test loss = 3655.822715\n",
            "Epoch 2530: train loss = 2491.048950, test loss = 3655.667834\n",
            "Epoch 2531: train loss = 2490.785714, test loss = 3655.512823\n",
            "Epoch 2532: train loss = 2490.522645, test loss = 3655.357679\n",
            "Epoch 2533: train loss = 2490.259742, test loss = 3655.202402\n",
            "Epoch 2534: train loss = 2489.997004, test loss = 3655.046989\n",
            "Epoch 2535: train loss = 2489.734430, test loss = 3654.891441\n",
            "Epoch 2536: train loss = 2489.472020, test loss = 3654.735755\n",
            "Epoch 2537: train loss = 2489.209772, test loss = 3654.579931\n",
            "Epoch 2538: train loss = 2488.947687, test loss = 3654.423966\n",
            "Epoch 2539: train loss = 2488.685763, test loss = 3654.267862\n",
            "Epoch 2540: train loss = 2488.423999, test loss = 3654.111615\n",
            "Epoch 2541: train loss = 2488.162395, test loss = 3653.955225\n",
            "Epoch 2542: train loss = 2487.900950, test loss = 3653.798692\n",
            "Epoch 2543: train loss = 2487.639663, test loss = 3653.642014\n",
            "Epoch 2544: train loss = 2487.378534, test loss = 3653.485190\n",
            "Epoch 2545: train loss = 2487.117561, test loss = 3653.328220\n",
            "Epoch 2546: train loss = 2486.856744, test loss = 3653.171103\n",
            "Epoch 2547: train loss = 2486.596082, test loss = 3653.013838\n",
            "Epoch 2548: train loss = 2486.335574, test loss = 3652.856424\n",
            "Epoch 2549: train loss = 2486.075220, test loss = 3652.698861\n",
            "Epoch 2550: train loss = 2485.815018, test loss = 3652.541147\n",
            "Epoch 2551: train loss = 2485.554968, test loss = 3652.383284\n",
            "Epoch 2552: train loss = 2485.295069, test loss = 3652.225269\n",
            "Epoch 2553: train loss = 2485.035319, test loss = 3652.067102\n",
            "Epoch 2554: train loss = 2484.775720, test loss = 3651.908784\n",
            "Epoch 2555: train loss = 2484.516268, test loss = 3651.750313\n",
            "Epoch 2556: train loss = 2484.256964, test loss = 3651.591690\n",
            "Epoch 2557: train loss = 2483.997806, test loss = 3651.432913\n",
            "Epoch 2558: train loss = 2483.738795, test loss = 3651.273983\n",
            "Epoch 2559: train loss = 2483.479928, test loss = 3651.114899\n",
            "Epoch 2560: train loss = 2483.221205, test loss = 3650.955662\n",
            "Epoch 2561: train loss = 2482.962626, test loss = 3650.796270\n",
            "Epoch 2562: train loss = 2482.704189, test loss = 3650.636725\n",
            "Epoch 2563: train loss = 2482.445893, test loss = 3650.477026\n",
            "Epoch 2564: train loss = 2482.187738, test loss = 3650.317172\n",
            "Epoch 2565: train loss = 2481.929722, test loss = 3650.157165\n",
            "Epoch 2566: train loss = 2481.671845, test loss = 3649.997003\n",
            "Epoch 2567: train loss = 2481.414106, test loss = 3649.836688\n",
            "Epoch 2568: train loss = 2481.156504, test loss = 3649.676220\n",
            "Epoch 2569: train loss = 2480.899038, test loss = 3649.515598\n",
            "Epoch 2570: train loss = 2480.641707, test loss = 3649.354822\n",
            "Epoch 2571: train loss = 2480.384510, test loss = 3649.193895\n",
            "Epoch 2572: train loss = 2480.127447, test loss = 3649.032814\n",
            "Epoch 2573: train loss = 2479.870516, test loss = 3648.871582\n",
            "Epoch 2574: train loss = 2479.613716, test loss = 3648.710198\n",
            "Epoch 2575: train loss = 2479.357048, test loss = 3648.548664\n",
            "Epoch 2576: train loss = 2479.100508, test loss = 3648.386979\n",
            "Epoch 2577: train loss = 2478.844098, test loss = 3648.225144\n",
            "Epoch 2578: train loss = 2478.587815, test loss = 3648.063159\n",
            "Epoch 2579: train loss = 2478.331660, test loss = 3647.901027\n",
            "Epoch 2580: train loss = 2478.075630, test loss = 3647.738746\n",
            "Epoch 2581: train loss = 2477.819725, test loss = 3647.576319\n",
            "Epoch 2582: train loss = 2477.563945, test loss = 3647.413745\n",
            "Epoch 2583: train loss = 2477.308288, test loss = 3647.251027\n",
            "Epoch 2584: train loss = 2477.052753, test loss = 3647.088164\n",
            "Epoch 2585: train loss = 2476.797339, test loss = 3646.925157\n",
            "Epoch 2586: train loss = 2476.542046, test loss = 3646.762008\n",
            "Epoch 2587: train loss = 2476.286873, test loss = 3646.598718\n",
            "Epoch 2588: train loss = 2476.031818, test loss = 3646.435288\n",
            "Epoch 2589: train loss = 2475.776881, test loss = 3646.271719\n",
            "Epoch 2590: train loss = 2475.522061, test loss = 3646.108012\n",
            "Epoch 2591: train loss = 2475.267357, test loss = 3645.944169\n",
            "Epoch 2592: train loss = 2475.012768, test loss = 3645.780191\n",
            "Epoch 2593: train loss = 2474.758293, test loss = 3645.616078\n",
            "Epoch 2594: train loss = 2474.503931, test loss = 3645.451833\n",
            "Epoch 2595: train loss = 2474.249682, test loss = 3645.287458\n",
            "Epoch 2596: train loss = 2473.995543, test loss = 3645.122953\n",
            "Epoch 2597: train loss = 2473.741516, test loss = 3644.958319\n",
            "Epoch 2598: train loss = 2473.487598, test loss = 3644.793560\n",
            "Epoch 2599: train loss = 2473.233789, test loss = 3644.628676\n",
            "Epoch 2600: train loss = 2472.980087, test loss = 3644.463668\n",
            "Epoch 2601: train loss = 2472.726493, test loss = 3644.298540\n",
            "Epoch 2602: train loss = 2472.473005, test loss = 3644.133292\n",
            "Epoch 2603: train loss = 2472.219622, test loss = 3643.967926\n",
            "Epoch 2604: train loss = 2471.966343, test loss = 3643.802444\n",
            "Epoch 2605: train loss = 2471.713168, test loss = 3643.636849\n",
            "Epoch 2606: train loss = 2471.460096, test loss = 3643.471141\n",
            "Epoch 2607: train loss = 2471.207126, test loss = 3643.305324\n",
            "Epoch 2608: train loss = 2470.954256, test loss = 3643.139399\n",
            "Epoch 2609: train loss = 2470.701487, test loss = 3642.973368\n",
            "Epoch 2610: train loss = 2470.448816, test loss = 3642.807233\n",
            "Epoch 2611: train loss = 2470.196245, test loss = 3642.640997\n",
            "Epoch 2612: train loss = 2469.943771, test loss = 3642.474662\n",
            "Epoch 2613: train loss = 2469.691394, test loss = 3642.308229\n",
            "Epoch 2614: train loss = 2469.439113, test loss = 3642.141703\n",
            "Epoch 2615: train loss = 2469.186927, test loss = 3641.975084\n",
            "Epoch 2616: train loss = 2468.934836, test loss = 3641.808375\n",
            "Epoch 2617: train loss = 2468.682838, test loss = 3641.641579\n",
            "Epoch 2618: train loss = 2468.430933, test loss = 3641.474699\n",
            "Epoch 2619: train loss = 2468.179121, test loss = 3641.307736\n",
            "Epoch 2620: train loss = 2467.927399, test loss = 3641.140694\n",
            "Epoch 2621: train loss = 2467.675769, test loss = 3640.973575\n",
            "Epoch 2622: train loss = 2467.424228, test loss = 3640.806381\n",
            "Epoch 2623: train loss = 2467.172777, test loss = 3640.639116\n",
            "Epoch 2624: train loss = 2466.921413, test loss = 3640.471783\n",
            "Epoch 2625: train loss = 2466.670138, test loss = 3640.304384\n",
            "Epoch 2626: train loss = 2466.418950, test loss = 3640.136921\n",
            "Epoch 2627: train loss = 2466.167847, test loss = 3639.969399\n",
            "Epoch 2628: train loss = 2465.916831, test loss = 3639.801820\n",
            "Epoch 2629: train loss = 2465.665899, test loss = 3639.634186\n",
            "Epoch 2630: train loss = 2465.415052, test loss = 3639.466502\n",
            "Epoch 2631: train loss = 2465.164288, test loss = 3639.298770\n",
            "Epoch 2632: train loss = 2464.913608, test loss = 3639.130993\n",
            "Epoch 2633: train loss = 2464.663009, test loss = 3638.963175\n",
            "Epoch 2634: train loss = 2464.412493, test loss = 3638.795318\n",
            "Epoch 2635: train loss = 2464.162057, test loss = 3638.627427\n",
            "Epoch 2636: train loss = 2463.911702, test loss = 3638.459503\n",
            "Epoch 2637: train loss = 2463.661427, test loss = 3638.291552\n",
            "Epoch 2638: train loss = 2463.411231, test loss = 3638.123575\n",
            "Epoch 2639: train loss = 2463.161114, test loss = 3637.955577\n",
            "Epoch 2640: train loss = 2462.911075, test loss = 3637.787561\n",
            "Epoch 2641: train loss = 2462.661114, test loss = 3637.619531\n",
            "Epoch 2642: train loss = 2462.411230, test loss = 3637.451489\n",
            "Epoch 2643: train loss = 2462.161423, test loss = 3637.283441\n",
            "Epoch 2644: train loss = 2461.911691, test loss = 3637.115389\n",
            "Epoch 2645: train loss = 2461.662036, test loss = 3636.947337\n",
            "Epoch 2646: train loss = 2461.412455, test loss = 3636.779288\n",
            "Epoch 2647: train loss = 2461.162949, test loss = 3636.611247\n",
            "Epoch 2648: train loss = 2460.913517, test loss = 3636.443218\n",
            "Epoch 2649: train loss = 2460.664159, test loss = 3636.275204\n",
            "Epoch 2650: train loss = 2460.414875, test loss = 3636.107209\n",
            "Epoch 2651: train loss = 2460.165663, test loss = 3635.939237\n",
            "Epoch 2652: train loss = 2459.916524, test loss = 3635.771292\n",
            "Epoch 2653: train loss = 2459.667457, test loss = 3635.603378\n",
            "Epoch 2654: train loss = 2459.418461, test loss = 3635.435498\n",
            "Epoch 2655: train loss = 2459.169537, test loss = 3635.267658\n",
            "Epoch 2656: train loss = 2458.920685, test loss = 3635.099862\n",
            "Epoch 2657: train loss = 2458.671903, test loss = 3634.932112\n",
            "Epoch 2658: train loss = 2458.423191, test loss = 3634.764414\n",
            "Epoch 2659: train loss = 2458.174549, test loss = 3634.596772\n",
            "Epoch 2660: train loss = 2457.925978, test loss = 3634.429190\n",
            "Epoch 2661: train loss = 2457.677476, test loss = 3634.261672\n",
            "Epoch 2662: train loss = 2457.429043, test loss = 3634.094222\n",
            "Epoch 2663: train loss = 2457.180679, test loss = 3633.926845\n",
            "Epoch 2664: train loss = 2456.932385, test loss = 3633.759546\n",
            "Epoch 2665: train loss = 2456.684159, test loss = 3633.592328\n",
            "Epoch 2666: train loss = 2456.436001, test loss = 3633.425196\n",
            "Epoch 2667: train loss = 2456.187912, test loss = 3633.258155\n",
            "Epoch 2668: train loss = 2455.939891, test loss = 3633.091208\n",
            "Epoch 2669: train loss = 2455.691938, test loss = 3632.924362\n",
            "Epoch 2670: train loss = 2455.444053, test loss = 3632.757619\n",
            "Epoch 2671: train loss = 2455.196235, test loss = 3632.590985\n",
            "Epoch 2672: train loss = 2454.948486, test loss = 3632.424464\n",
            "Epoch 2673: train loss = 2454.700804, test loss = 3632.258061\n",
            "Epoch 2674: train loss = 2454.453189, test loss = 3632.091780\n",
            "Epoch 2675: train loss = 2454.205642, test loss = 3631.925626\n",
            "Epoch 2676: train loss = 2453.958163, test loss = 3631.759605\n",
            "Epoch 2677: train loss = 2453.710751, test loss = 3631.593720\n",
            "Epoch 2678: train loss = 2453.463407, test loss = 3631.427976\n",
            "Epoch 2679: train loss = 2453.216130, test loss = 3631.262378\n",
            "Epoch 2680: train loss = 2452.968921, test loss = 3631.096931\n",
            "Epoch 2681: train loss = 2452.721780, test loss = 3630.931639\n",
            "Epoch 2682: train loss = 2452.474706, test loss = 3630.766508\n",
            "Epoch 2683: train loss = 2452.227700, test loss = 3630.601542\n",
            "Epoch 2684: train loss = 2451.980762, test loss = 3630.436746\n",
            "Epoch 2685: train loss = 2451.733893, test loss = 3630.272125\n",
            "Epoch 2686: train loss = 2451.487091, test loss = 3630.107684\n",
            "Epoch 2687: train loss = 2451.240359, test loss = 3629.943427\n",
            "Epoch 2688: train loss = 2450.993694, test loss = 3629.779360\n",
            "Epoch 2689: train loss = 2450.747099, test loss = 3629.615487\n",
            "Epoch 2690: train loss = 2450.500573, test loss = 3629.451814\n",
            "Epoch 2691: train loss = 2450.254116, test loss = 3629.288345\n",
            "Epoch 2692: train loss = 2450.007728, test loss = 3629.125085\n",
            "Epoch 2693: train loss = 2449.761411, test loss = 3628.962040\n",
            "Epoch 2694: train loss = 2449.515164, test loss = 3628.799214\n",
            "Epoch 2695: train loss = 2449.268987, test loss = 3628.636611\n",
            "Epoch 2696: train loss = 2449.022881, test loss = 3628.474238\n",
            "Epoch 2697: train loss = 2448.776847, test loss = 3628.312099\n",
            "Epoch 2698: train loss = 2448.530884, test loss = 3628.150199\n",
            "Epoch 2699: train loss = 2448.284993, test loss = 3627.988542\n",
            "Epoch 2700: train loss = 2448.039174, test loss = 3627.827135\n",
            "Epoch 2701: train loss = 2447.793429, test loss = 3627.665982\n",
            "Epoch 2702: train loss = 2447.547757, test loss = 3627.505087\n",
            "Epoch 2703: train loss = 2447.302158, test loss = 3627.344456\n",
            "Epoch 2704: train loss = 2447.056634, test loss = 3627.184094\n",
            "Epoch 2705: train loss = 2446.811184, test loss = 3627.024006\n",
            "Epoch 2706: train loss = 2446.565810, test loss = 3626.864196\n",
            "Epoch 2707: train loss = 2446.320512, test loss = 3626.704670\n",
            "Epoch 2708: train loss = 2446.075290, test loss = 3626.545433\n",
            "Epoch 2709: train loss = 2445.830146, test loss = 3626.386489\n",
            "Epoch 2710: train loss = 2445.585078, test loss = 3626.227844\n",
            "Epoch 2711: train loss = 2445.340090, test loss = 3626.069501\n",
            "Epoch 2712: train loss = 2445.095180, test loss = 3625.911467\n",
            "Epoch 2713: train loss = 2444.850349, test loss = 3625.753746\n",
            "Epoch 2714: train loss = 2444.605599, test loss = 3625.596343\n",
            "Epoch 2715: train loss = 2444.360930, test loss = 3625.439263\n",
            "Epoch 2716: train loss = 2444.116342, test loss = 3625.282510\n",
            "Epoch 2717: train loss = 2443.871837, test loss = 3625.126090\n",
            "Epoch 2718: train loss = 2443.627415, test loss = 3624.970007\n",
            "Epoch 2719: train loss = 2443.383077, test loss = 3624.814266\n",
            "Epoch 2720: train loss = 2443.138824, test loss = 3624.658872\n",
            "Epoch 2721: train loss = 2442.894656, test loss = 3624.503829\n",
            "Epoch 2722: train loss = 2442.650574, test loss = 3624.349142\n",
            "Epoch 2723: train loss = 2442.406580, test loss = 3624.194816\n",
            "Epoch 2724: train loss = 2442.162673, test loss = 3624.040855\n",
            "Epoch 2725: train loss = 2441.918856, test loss = 3623.887264\n",
            "Epoch 2726: train loss = 2441.675128, test loss = 3623.734048\n",
            "Epoch 2727: train loss = 2441.431492, test loss = 3623.581212\n",
            "Epoch 2728: train loss = 2441.187946, test loss = 3623.428759\n",
            "Epoch 2729: train loss = 2440.944494, test loss = 3623.276694\n",
            "Epoch 2730: train loss = 2440.701135, test loss = 3623.125022\n",
            "Epoch 2731: train loss = 2440.457870, test loss = 3622.973747\n",
            "Epoch 2732: train loss = 2440.214701, test loss = 3622.822874\n",
            "Epoch 2733: train loss = 2439.971629, test loss = 3622.672406\n",
            "Epoch 2734: train loss = 2439.728654, test loss = 3622.522349\n",
            "Epoch 2735: train loss = 2439.485778, test loss = 3622.372706\n",
            "Epoch 2736: train loss = 2439.243002, test loss = 3622.223482\n",
            "Epoch 2737: train loss = 2439.000326, test loss = 3622.074681\n",
            "Epoch 2738: train loss = 2438.757752, test loss = 3621.926307\n",
            "Epoch 2739: train loss = 2438.515282, test loss = 3621.778364\n",
            "Epoch 2740: train loss = 2438.272915, test loss = 3621.630857\n",
            "Epoch 2741: train loss = 2438.030654, test loss = 3621.483788\n",
            "Epoch 2742: train loss = 2437.788499, test loss = 3621.337164\n",
            "Epoch 2743: train loss = 2437.546451, test loss = 3621.190986\n",
            "Epoch 2744: train loss = 2437.304513, test loss = 3621.045260\n",
            "Epoch 2745: train loss = 2437.062684, test loss = 3620.899988\n",
            "Epoch 2746: train loss = 2436.820966, test loss = 3620.755175\n",
            "Epoch 2747: train loss = 2436.579361, test loss = 3620.610825\n",
            "Epoch 2748: train loss = 2436.337870, test loss = 3620.466941\n",
            "Epoch 2749: train loss = 2436.096493, test loss = 3620.323527\n",
            "Epoch 2750: train loss = 2435.855233, test loss = 3620.180587\n",
            "Epoch 2751: train loss = 2435.614090, test loss = 3620.038123\n",
            "Epoch 2752: train loss = 2435.373065, test loss = 3619.896140\n",
            "Epoch 2753: train loss = 2435.132161, test loss = 3619.754640\n",
            "Epoch 2754: train loss = 2434.891378, test loss = 3619.613628\n",
            "Epoch 2755: train loss = 2434.650718, test loss = 3619.473107\n",
            "Epoch 2756: train loss = 2434.410182, test loss = 3619.333080\n",
            "Epoch 2757: train loss = 2434.169771, test loss = 3619.193549\n",
            "Epoch 2758: train loss = 2433.929486, test loss = 3619.054519\n",
            "Epoch 2759: train loss = 2433.689330, test loss = 3618.915992\n",
            "Epoch 2760: train loss = 2433.449303, test loss = 3618.777971\n",
            "Epoch 2761: train loss = 2433.209407, test loss = 3618.640459\n",
            "Epoch 2762: train loss = 2432.969644, test loss = 3618.503460\n",
            "Epoch 2763: train loss = 2432.730014, test loss = 3618.366976\n",
            "Epoch 2764: train loss = 2432.490519, test loss = 3618.231009\n",
            "Epoch 2765: train loss = 2432.251160, test loss = 3618.095563\n",
            "Epoch 2766: train loss = 2432.011939, test loss = 3617.960640\n",
            "Epoch 2767: train loss = 2431.772858, test loss = 3617.826243\n",
            "Epoch 2768: train loss = 2431.533918, test loss = 3617.692374\n",
            "Epoch 2769: train loss = 2431.295120, test loss = 3617.559036\n",
            "Epoch 2770: train loss = 2431.056465, test loss = 3617.426231\n",
            "Epoch 2771: train loss = 2430.817956, test loss = 3617.293961\n",
            "Epoch 2772: train loss = 2430.579593, test loss = 3617.162229\n",
            "Epoch 2773: train loss = 2430.341379, test loss = 3617.031037\n",
            "Epoch 2774: train loss = 2430.103314, test loss = 3616.900387\n",
            "Epoch 2775: train loss = 2429.865400, test loss = 3616.770282\n",
            "Epoch 2776: train loss = 2429.627639, test loss = 3616.640722\n",
            "Epoch 2777: train loss = 2429.390032, test loss = 3616.511710\n",
            "Epoch 2778: train loss = 2429.152581, test loss = 3616.383249\n",
            "Epoch 2779: train loss = 2428.915287, test loss = 3616.255339\n",
            "Epoch 2780: train loss = 2428.678151, test loss = 3616.127982\n",
            "Epoch 2781: train loss = 2428.441176, test loss = 3616.001181\n",
            "Epoch 2782: train loss = 2428.204362, test loss = 3615.874936\n",
            "Epoch 2783: train loss = 2427.967712, test loss = 3615.749249\n",
            "Epoch 2784: train loss = 2427.731226, test loss = 3615.624123\n",
            "Epoch 2785: train loss = 2427.494906, test loss = 3615.499557\n",
            "Epoch 2786: train loss = 2427.258755, test loss = 3615.375554\n",
            "Epoch 2787: train loss = 2427.022772, test loss = 3615.252114\n",
            "Epoch 2788: train loss = 2426.786961, test loss = 3615.129240\n",
            "Epoch 2789: train loss = 2426.551322, test loss = 3615.006931\n",
            "Epoch 2790: train loss = 2426.315857, test loss = 3614.885189\n",
            "Epoch 2791: train loss = 2426.080567, test loss = 3614.764016\n",
            "Epoch 2792: train loss = 2425.845454, test loss = 3614.643411\n",
            "Epoch 2793: train loss = 2425.610520, test loss = 3614.523376\n",
            "Epoch 2794: train loss = 2425.375766, test loss = 3614.403912\n",
            "Epoch 2795: train loss = 2425.141194, test loss = 3614.285019\n",
            "Epoch 2796: train loss = 2424.906804, test loss = 3614.166698\n",
            "Epoch 2797: train loss = 2424.672599, test loss = 3614.048949\n",
            "Epoch 2798: train loss = 2424.438580, test loss = 3613.931774\n",
            "Epoch 2799: train loss = 2424.204749, test loss = 3613.815172\n",
            "Epoch 2800: train loss = 2423.971107, test loss = 3613.699144\n",
            "Epoch 2801: train loss = 2423.737656, test loss = 3613.583690\n",
            "Epoch 2802: train loss = 2423.504397, test loss = 3613.468811\n",
            "Epoch 2803: train loss = 2423.271332, test loss = 3613.354506\n",
            "Epoch 2804: train loss = 2423.038462, test loss = 3613.240776\n",
            "Epoch 2805: train loss = 2422.805788, test loss = 3613.127621\n",
            "Epoch 2806: train loss = 2422.573313, test loss = 3613.015041\n",
            "Epoch 2807: train loss = 2422.341037, test loss = 3612.903036\n",
            "Epoch 2808: train loss = 2422.108962, test loss = 3612.791605\n",
            "Epoch 2809: train loss = 2421.877090, test loss = 3612.680749\n",
            "Epoch 2810: train loss = 2421.645423, test loss = 3612.570467\n",
            "Epoch 2811: train loss = 2421.413960, test loss = 3612.460759\n",
            "Epoch 2812: train loss = 2421.182705, test loss = 3612.351624\n",
            "Epoch 2813: train loss = 2420.951658, test loss = 3612.243063\n",
            "Epoch 2814: train loss = 2420.720821, test loss = 3612.135074\n",
            "Epoch 2815: train loss = 2420.490195, test loss = 3612.027657\n",
            "Epoch 2816: train loss = 2420.259782, test loss = 3611.920812\n",
            "Epoch 2817: train loss = 2420.029583, test loss = 3611.814538\n",
            "Epoch 2818: train loss = 2419.799599, test loss = 3611.708833\n",
            "Epoch 2819: train loss = 2419.569833, test loss = 3611.603698\n",
            "Epoch 2820: train loss = 2419.340285, test loss = 3611.499132\n",
            "Epoch 2821: train loss = 2419.110956, test loss = 3611.395133\n",
            "Epoch 2822: train loss = 2418.881849, test loss = 3611.291700\n",
            "Epoch 2823: train loss = 2418.652964, test loss = 3611.188834\n",
            "Epoch 2824: train loss = 2418.424302, test loss = 3611.086531\n",
            "Epoch 2825: train loss = 2418.195866, test loss = 3610.984793\n",
            "Epoch 2826: train loss = 2417.967657, test loss = 3610.883616\n",
            "Epoch 2827: train loss = 2417.739675, test loss = 3610.783001\n",
            "Epoch 2828: train loss = 2417.511922, test loss = 3610.682945\n",
            "Epoch 2829: train loss = 2417.284399, test loss = 3610.583448\n",
            "Epoch 2830: train loss = 2417.057109, test loss = 3610.484508\n",
            "Epoch 2831: train loss = 2416.830051, test loss = 3610.386124\n",
            "Epoch 2832: train loss = 2416.603227, test loss = 3610.288294\n",
            "Epoch 2833: train loss = 2416.376639, test loss = 3610.191017\n",
            "Epoch 2834: train loss = 2416.150287, test loss = 3610.094291\n",
            "Epoch 2835: train loss = 2415.924174, test loss = 3609.998115\n",
            "Epoch 2836: train loss = 2415.698299, test loss = 3609.902486\n",
            "Epoch 2837: train loss = 2415.472665, test loss = 3609.807405\n",
            "Epoch 2838: train loss = 2415.247272, test loss = 3609.712867\n",
            "Epoch 2839: train loss = 2415.022122, test loss = 3609.618873\n",
            "Epoch 2840: train loss = 2414.797216, test loss = 3609.525419\n",
            "Epoch 2841: train loss = 2414.572555, test loss = 3609.432505\n",
            "Epoch 2842: train loss = 2414.348140, test loss = 3609.340128\n",
            "Epoch 2843: train loss = 2414.123972, test loss = 3609.248286\n",
            "Epoch 2844: train loss = 2413.900053, test loss = 3609.156978\n",
            "Epoch 2845: train loss = 2413.676383, test loss = 3609.066201\n",
            "Epoch 2846: train loss = 2413.452964, test loss = 3608.975953\n",
            "Epoch 2847: train loss = 2413.229797, test loss = 3608.886232\n",
            "Epoch 2848: train loss = 2413.006882, test loss = 3608.797037\n",
            "Epoch 2849: train loss = 2412.784221, test loss = 3608.708365\n",
            "Epoch 2850: train loss = 2412.561815, test loss = 3608.620213\n",
            "Epoch 2851: train loss = 2412.339664, test loss = 3608.532580\n",
            "Epoch 2852: train loss = 2412.117771, test loss = 3608.445463\n",
            "Epoch 2853: train loss = 2411.896135, test loss = 3608.358861\n",
            "Epoch 2854: train loss = 2411.674758, test loss = 3608.272770\n",
            "Epoch 2855: train loss = 2411.453641, test loss = 3608.187188\n",
            "Epoch 2856: train loss = 2411.232784, test loss = 3608.102114\n",
            "Epoch 2857: train loss = 2411.012189, test loss = 3608.017544\n",
            "Epoch 2858: train loss = 2410.791857, test loss = 3607.933476\n",
            "Epoch 2859: train loss = 2410.571788, test loss = 3607.849909\n",
            "Epoch 2860: train loss = 2410.351983, test loss = 3607.766838\n",
            "Epoch 2861: train loss = 2410.132443, test loss = 3607.684262\n",
            "Epoch 2862: train loss = 2409.913170, test loss = 3607.602179\n",
            "Epoch 2863: train loss = 2409.694163, test loss = 3607.520585\n",
            "Epoch 2864: train loss = 2409.475425, test loss = 3607.439479\n",
            "Epoch 2865: train loss = 2409.256955, test loss = 3607.358856\n",
            "Epoch 2866: train loss = 2409.038754, test loss = 3607.278716\n",
            "Epoch 2867: train loss = 2408.820823, test loss = 3607.199056\n",
            "Epoch 2868: train loss = 2408.603163, test loss = 3607.119872\n",
            "Epoch 2869: train loss = 2408.385775, test loss = 3607.041161\n",
            "Epoch 2870: train loss = 2408.168659, test loss = 3606.962923\n",
            "Epoch 2871: train loss = 2407.951817, test loss = 3606.885153\n",
            "Epoch 2872: train loss = 2407.735248, test loss = 3606.807848\n",
            "Epoch 2873: train loss = 2407.518954, test loss = 3606.731007\n",
            "Epoch 2874: train loss = 2407.302935, test loss = 3606.654626\n",
            "Epoch 2875: train loss = 2407.087193, test loss = 3606.578703\n",
            "Epoch 2876: train loss = 2406.871726, test loss = 3606.503235\n",
            "Epoch 2877: train loss = 2406.656537, test loss = 3606.428219\n",
            "Epoch 2878: train loss = 2406.441626, test loss = 3606.353652\n",
            "Epoch 2879: train loss = 2406.226993, test loss = 3606.279531\n",
            "Epoch 2880: train loss = 2406.012640, test loss = 3606.205854\n",
            "Epoch 2881: train loss = 2405.798566, test loss = 3606.132618\n",
            "Epoch 2882: train loss = 2405.584772, test loss = 3606.059819\n",
            "Epoch 2883: train loss = 2405.371259, test loss = 3605.987456\n",
            "Epoch 2884: train loss = 2405.158027, test loss = 3605.915525\n",
            "Epoch 2885: train loss = 2404.945077, test loss = 3605.844023\n",
            "Epoch 2886: train loss = 2404.732410, test loss = 3605.772947\n",
            "Epoch 2887: train loss = 2404.520025, test loss = 3605.702296\n",
            "Epoch 2888: train loss = 2404.307924, test loss = 3605.632064\n",
            "Epoch 2889: train loss = 2404.096107, test loss = 3605.562251\n",
            "Epoch 2890: train loss = 2403.884574, test loss = 3605.492852\n",
            "Epoch 2891: train loss = 2403.673326, test loss = 3605.423865\n",
            "Epoch 2892: train loss = 2403.462363, test loss = 3605.355287\n",
            "Epoch 2893: train loss = 2403.251685, test loss = 3605.287115\n",
            "Epoch 2894: train loss = 2403.041294, test loss = 3605.219347\n",
            "Epoch 2895: train loss = 2402.831189, test loss = 3605.151979\n",
            "Epoch 2896: train loss = 2402.621371, test loss = 3605.085007\n",
            "Epoch 2897: train loss = 2402.411840, test loss = 3605.018431\n",
            "Epoch 2898: train loss = 2402.202597, test loss = 3604.952246\n",
            "Epoch 2899: train loss = 2401.993641, test loss = 3604.886449\n",
            "Epoch 2900: train loss = 2401.784974, test loss = 3604.821038\n",
            "Epoch 2901: train loss = 2401.576595, test loss = 3604.756010\n",
            "Epoch 2902: train loss = 2401.368505, test loss = 3604.691361\n",
            "Epoch 2903: train loss = 2401.160704, test loss = 3604.627090\n",
            "Epoch 2904: train loss = 2400.953193, test loss = 3604.563192\n",
            "Epoch 2905: train loss = 2400.745971, test loss = 3604.499666\n",
            "Epoch 2906: train loss = 2400.539039, test loss = 3604.436507\n",
            "Epoch 2907: train loss = 2400.332397, test loss = 3604.373714\n",
            "Epoch 2908: train loss = 2400.126046, test loss = 3604.311283\n",
            "Epoch 2909: train loss = 2399.919986, test loss = 3604.249211\n",
            "Epoch 2910: train loss = 2399.714216, test loss = 3604.187496\n",
            "Epoch 2911: train loss = 2399.508737, test loss = 3604.126135\n",
            "Epoch 2912: train loss = 2399.303550, test loss = 3604.065124\n",
            "Epoch 2913: train loss = 2399.098654, test loss = 3604.004462\n",
            "Epoch 2914: train loss = 2398.894050, test loss = 3603.944144\n",
            "Epoch 2915: train loss = 2398.689737, test loss = 3603.884169\n",
            "Epoch 2916: train loss = 2398.485716, test loss = 3603.824532\n",
            "Epoch 2917: train loss = 2398.281987, test loss = 3603.765233\n",
            "Epoch 2918: train loss = 2398.078551, test loss = 3603.706267\n",
            "Epoch 2919: train loss = 2397.875406, test loss = 3603.647631\n",
            "Epoch 2920: train loss = 2397.672554, test loss = 3603.589324\n",
            "Epoch 2921: train loss = 2397.469995, test loss = 3603.531342\n",
            "Epoch 2922: train loss = 2397.267728, test loss = 3603.473682\n",
            "Epoch 2923: train loss = 2397.065753, test loss = 3603.416342\n",
            "Epoch 2924: train loss = 2396.864072, test loss = 3603.359318\n",
            "Epoch 2925: train loss = 2396.662683, test loss = 3603.302609\n",
            "Epoch 2926: train loss = 2396.461586, test loss = 3603.246211\n",
            "Epoch 2927: train loss = 2396.260783, test loss = 3603.190121\n",
            "Epoch 2928: train loss = 2396.060272, test loss = 3603.134337\n",
            "Epoch 2929: train loss = 2395.860054, test loss = 3603.078856\n",
            "Epoch 2930: train loss = 2395.660129, test loss = 3603.023675\n",
            "Epoch 2931: train loss = 2395.460497, test loss = 3602.968792\n",
            "Epoch 2932: train loss = 2395.261158, test loss = 3602.914204\n",
            "Epoch 2933: train loss = 2395.062111, test loss = 3602.859908\n",
            "Epoch 2934: train loss = 2394.863357, test loss = 3602.805902\n",
            "Epoch 2935: train loss = 2394.664896, test loss = 3602.752182\n",
            "Epoch 2936: train loss = 2394.466727, test loss = 3602.698747\n",
            "Epoch 2937: train loss = 2394.268851, test loss = 3602.645593\n",
            "Epoch 2938: train loss = 2394.071268, test loss = 3602.592719\n",
            "Epoch 2939: train loss = 2393.873977, test loss = 3602.540120\n",
            "Epoch 2940: train loss = 2393.676979, test loss = 3602.487796\n",
            "Epoch 2941: train loss = 2393.480272, test loss = 3602.435742\n",
            "Epoch 2942: train loss = 2393.283858, test loss = 3602.383958\n",
            "Epoch 2943: train loss = 2393.087737, test loss = 3602.332439\n",
            "Epoch 2944: train loss = 2392.891907, test loss = 3602.281184\n",
            "Epoch 2945: train loss = 2392.696368, test loss = 3602.230190\n",
            "Epoch 2946: train loss = 2392.501122, test loss = 3602.179454\n",
            "Epoch 2947: train loss = 2392.306167, test loss = 3602.128975\n",
            "Epoch 2948: train loss = 2392.111504, test loss = 3602.078749\n",
            "Epoch 2949: train loss = 2391.917132, test loss = 3602.028774\n",
            "Epoch 2950: train loss = 2391.723051, test loss = 3601.979048\n",
            "Epoch 2951: train loss = 2391.529261, test loss = 3601.929568\n",
            "Epoch 2952: train loss = 2391.335762, test loss = 3601.880331\n",
            "Epoch 2953: train loss = 2391.142554, test loss = 3601.831336\n",
            "Epoch 2954: train loss = 2390.949636, test loss = 3601.782580\n",
            "Epoch 2955: train loss = 2390.757008, test loss = 3601.734061\n",
            "Epoch 2956: train loss = 2390.564670, test loss = 3601.685775\n",
            "Epoch 2957: train loss = 2390.372622, test loss = 3601.637722\n",
            "Epoch 2958: train loss = 2390.180864, test loss = 3601.589898\n",
            "Epoch 2959: train loss = 2389.989396, test loss = 3601.542301\n",
            "Epoch 2960: train loss = 2389.798216, test loss = 3601.494929\n",
            "Epoch 2961: train loss = 2389.607326, test loss = 3601.447780\n",
            "Epoch 2962: train loss = 2389.416724, test loss = 3601.400852\n",
            "Epoch 2963: train loss = 2389.226411, test loss = 3601.354141\n",
            "Epoch 2964: train loss = 2389.036386, test loss = 3601.307646\n",
            "Epoch 2965: train loss = 2388.846649, test loss = 3601.261365\n",
            "Epoch 2966: train loss = 2388.657199, test loss = 3601.215295\n",
            "Epoch 2967: train loss = 2388.468038, test loss = 3601.169435\n",
            "Epoch 2968: train loss = 2388.279163, test loss = 3601.123781\n",
            "Epoch 2969: train loss = 2388.090576, test loss = 3601.078333\n",
            "Epoch 2970: train loss = 2387.902275, test loss = 3601.033087\n",
            "Epoch 2971: train loss = 2387.714261, test loss = 3600.988042\n",
            "Epoch 2972: train loss = 2387.526533, test loss = 3600.943196\n",
            "Epoch 2973: train loss = 2387.339091, test loss = 3600.898546\n",
            "Epoch 2974: train loss = 2387.151934, test loss = 3600.854090\n",
            "Epoch 2975: train loss = 2386.965062, test loss = 3600.809827\n",
            "Epoch 2976: train loss = 2386.778476, test loss = 3600.765753\n",
            "Epoch 2977: train loss = 2386.592174, test loss = 3600.721868\n",
            "Epoch 2978: train loss = 2386.406156, test loss = 3600.678170\n",
            "Epoch 2979: train loss = 2386.220423, test loss = 3600.634655\n",
            "Epoch 2980: train loss = 2386.034973, test loss = 3600.591323\n",
            "Epoch 2981: train loss = 2385.849806, test loss = 3600.548170\n",
            "Epoch 2982: train loss = 2385.664923, test loss = 3600.505196\n",
            "Epoch 2983: train loss = 2385.480322, test loss = 3600.462399\n",
            "Epoch 2984: train loss = 2385.296004, test loss = 3600.419776\n",
            "Epoch 2985: train loss = 2385.111968, test loss = 3600.377325\n",
            "Epoch 2986: train loss = 2384.928213, test loss = 3600.335045\n",
            "Epoch 2987: train loss = 2384.744740, test loss = 3600.292934\n",
            "Epoch 2988: train loss = 2384.561548, test loss = 3600.250989\n",
            "Epoch 2989: train loss = 2384.378636, test loss = 3600.209210\n",
            "Epoch 2990: train loss = 2384.196005, test loss = 3600.167594\n",
            "Epoch 2991: train loss = 2384.013653, test loss = 3600.126139\n",
            "Epoch 2992: train loss = 2383.831581, test loss = 3600.084844\n",
            "Epoch 2993: train loss = 2383.649789, test loss = 3600.043706\n",
            "Epoch 2994: train loss = 2383.468275, test loss = 3600.002725\n",
            "Epoch 2995: train loss = 2383.287039, test loss = 3599.961898\n",
            "Epoch 2996: train loss = 2383.106082, test loss = 3599.921223\n",
            "Epoch 2997: train loss = 2382.925402, test loss = 3599.880699\n",
            "Epoch 2998: train loss = 2382.745000, test loss = 3599.840325\n",
            "Epoch 2999: train loss = 2382.564875, test loss = 3599.800097\n",
            "Epoch 3000: train loss = 2382.385026, test loss = 3599.760016\n",
            "Epoch 3001: train loss = 2382.205453, test loss = 3599.720078\n",
            "Epoch 3002: train loss = 2382.026156, test loss = 3599.680283\n",
            "Epoch 3003: train loss = 2381.847135, test loss = 3599.640629\n",
            "Epoch 3004: train loss = 2381.668388, test loss = 3599.601114\n",
            "Epoch 3005: train loss = 2381.489916, test loss = 3599.561736\n",
            "Epoch 3006: train loss = 2381.311718, test loss = 3599.522495\n",
            "Epoch 3007: train loss = 2381.133794, test loss = 3599.483387\n",
            "Epoch 3008: train loss = 2380.956143, test loss = 3599.444413\n",
            "Epoch 3009: train loss = 2380.778765, test loss = 3599.405570\n",
            "Epoch 3010: train loss = 2380.601659, test loss = 3599.366856\n",
            "Epoch 3011: train loss = 2380.424826, test loss = 3599.328271\n",
            "Epoch 3012: train loss = 2380.248264, test loss = 3599.289813\n",
            "Epoch 3013: train loss = 2380.071974, test loss = 3599.251479\n",
            "Epoch 3014: train loss = 2379.895955, test loss = 3599.213270\n",
            "Epoch 3015: train loss = 2379.720205, test loss = 3599.175183\n",
            "Epoch 3016: train loss = 2379.544726, test loss = 3599.137216\n",
            "Epoch 3017: train loss = 2379.369517, test loss = 3599.099369\n",
            "Epoch 3018: train loss = 2379.194576, test loss = 3599.061640\n",
            "Epoch 3019: train loss = 2379.019905, test loss = 3599.024027\n",
            "Epoch 3020: train loss = 2378.845501, test loss = 3598.986530\n",
            "Epoch 3021: train loss = 2378.671366, test loss = 3598.949146\n",
            "Epoch 3022: train loss = 2378.497497, test loss = 3598.911875\n",
            "Epoch 3023: train loss = 2378.323896, test loss = 3598.874715\n",
            "Epoch 3024: train loss = 2378.150562, test loss = 3598.837664\n",
            "Epoch 3025: train loss = 2377.977493, test loss = 3598.800722\n",
            "Epoch 3026: train loss = 2377.804690, test loss = 3598.763887\n",
            "Epoch 3027: train loss = 2377.632153, test loss = 3598.727157\n",
            "Epoch 3028: train loss = 2377.459880, test loss = 3598.690532\n",
            "Epoch 3029: train loss = 2377.287871, test loss = 3598.654010\n",
            "Epoch 3030: train loss = 2377.116126, test loss = 3598.617590\n",
            "Epoch 3031: train loss = 2376.944645, test loss = 3598.581271\n",
            "Epoch 3032: train loss = 2376.773427, test loss = 3598.545051\n",
            "Epoch 3033: train loss = 2376.602471, test loss = 3598.508929\n",
            "Epoch 3034: train loss = 2376.431777, test loss = 3598.472904\n",
            "Epoch 3035: train loss = 2376.261345, test loss = 3598.436975\n",
            "Epoch 3036: train loss = 2376.091174, test loss = 3598.401140\n",
            "Epoch 3037: train loss = 2375.921264, test loss = 3598.365399\n",
            "Epoch 3038: train loss = 2375.751614, test loss = 3598.329750\n",
            "Epoch 3039: train loss = 2375.582224, test loss = 3598.294192\n",
            "Epoch 3040: train loss = 2375.413094, test loss = 3598.258723\n",
            "Epoch 3041: train loss = 2375.244222, test loss = 3598.223344\n",
            "Epoch 3042: train loss = 2375.075608, test loss = 3598.188052\n",
            "Epoch 3043: train loss = 2374.907253, test loss = 3598.152846\n",
            "Epoch 3044: train loss = 2374.739155, test loss = 3598.117726\n",
            "Epoch 3045: train loss = 2374.571314, test loss = 3598.082690\n",
            "Epoch 3046: train loss = 2374.403730, test loss = 3598.047738\n",
            "Epoch 3047: train loss = 2374.236402, test loss = 3598.012867\n",
            "Epoch 3048: train loss = 2374.069329, test loss = 3597.978078\n",
            "Epoch 3049: train loss = 2373.902512, test loss = 3597.943368\n",
            "Epoch 3050: train loss = 2373.735950, test loss = 3597.908738\n",
            "Epoch 3051: train loss = 2373.569641, test loss = 3597.874185\n",
            "Epoch 3052: train loss = 2373.403587, test loss = 3597.839710\n",
            "Epoch 3053: train loss = 2373.237786, test loss = 3597.805310\n",
            "Epoch 3054: train loss = 2373.072237, test loss = 3597.770986\n",
            "Epoch 3055: train loss = 2372.906941, test loss = 3597.736735\n",
            "Epoch 3056: train loss = 2372.741898, test loss = 3597.702557\n",
            "Epoch 3057: train loss = 2372.577105, test loss = 3597.668451\n",
            "Epoch 3058: train loss = 2372.412564, test loss = 3597.634417\n",
            "Epoch 3059: train loss = 2372.248273, test loss = 3597.600452\n",
            "Epoch 3060: train loss = 2372.084232, test loss = 3597.566557\n",
            "Epoch 3061: train loss = 2371.920440, test loss = 3597.532730\n",
            "Epoch 3062: train loss = 2371.756898, test loss = 3597.498970\n",
            "Epoch 3063: train loss = 2371.593605, test loss = 3597.465277\n",
            "Epoch 3064: train loss = 2371.430559, test loss = 3597.431649\n",
            "Epoch 3065: train loss = 2371.267761, test loss = 3597.398086\n",
            "Epoch 3066: train loss = 2371.105211, test loss = 3597.364586\n",
            "Epoch 3067: train loss = 2370.942907, test loss = 3597.331150\n",
            "Epoch 3068: train loss = 2370.780850, test loss = 3597.297775\n",
            "Epoch 3069: train loss = 2370.619038, test loss = 3597.264462\n",
            "Epoch 3070: train loss = 2370.457472, test loss = 3597.231209\n",
            "Epoch 3071: train loss = 2370.296150, test loss = 3597.198016\n",
            "Epoch 3072: train loss = 2370.135073, test loss = 3597.164881\n",
            "Epoch 3073: train loss = 2369.974240, test loss = 3597.131804\n",
            "Epoch 3074: train loss = 2369.813651, test loss = 3597.098785\n",
            "Epoch 3075: train loss = 2369.653304, test loss = 3597.065821\n",
            "Epoch 3076: train loss = 2369.493200, test loss = 3597.032913\n",
            "Epoch 3077: train loss = 2369.333338, test loss = 3597.000060\n",
            "Epoch 3078: train loss = 2369.173717, test loss = 3596.967261\n",
            "Epoch 3079: train loss = 2369.014337, test loss = 3596.934515\n",
            "Epoch 3080: train loss = 2368.855198, test loss = 3596.901821\n",
            "Epoch 3081: train loss = 2368.696300, test loss = 3596.869179\n",
            "Epoch 3082: train loss = 2368.537640, test loss = 3596.836588\n",
            "Epoch 3083: train loss = 2368.379220, test loss = 3596.804047\n",
            "Epoch 3084: train loss = 2368.221039, test loss = 3596.771556\n",
            "Epoch 3085: train loss = 2368.063096, test loss = 3596.739113\n",
            "Epoch 3086: train loss = 2367.905391, test loss = 3596.706718\n",
            "Epoch 3087: train loss = 2367.747923, test loss = 3596.674371\n",
            "Epoch 3088: train loss = 2367.590692, test loss = 3596.642071\n",
            "Epoch 3089: train loss = 2367.433697, test loss = 3596.609816\n",
            "Epoch 3090: train loss = 2367.276938, test loss = 3596.577607\n",
            "Epoch 3091: train loss = 2367.120414, test loss = 3596.545442\n",
            "Epoch 3092: train loss = 2366.964126, test loss = 3596.513322\n",
            "Epoch 3093: train loss = 2366.808072, test loss = 3596.481244\n",
            "Epoch 3094: train loss = 2366.652251, test loss = 3596.449210\n",
            "Epoch 3095: train loss = 2366.496665, test loss = 3596.417217\n",
            "Epoch 3096: train loss = 2366.341311, test loss = 3596.385266\n",
            "Epoch 3097: train loss = 2366.186190, test loss = 3596.353356\n",
            "Epoch 3098: train loss = 2366.031301, test loss = 3596.321486\n",
            "Epoch 3099: train loss = 2365.876644, test loss = 3596.289655\n",
            "Epoch 3100: train loss = 2365.722217, test loss = 3596.257863\n",
            "Epoch 3101: train loss = 2365.568022, test loss = 3596.226110\n",
            "Epoch 3102: train loss = 2365.414057, test loss = 3596.194394\n",
            "Epoch 3103: train loss = 2365.260321, test loss = 3596.162716\n",
            "Epoch 3104: train loss = 2365.106815, test loss = 3596.131074\n",
            "Epoch 3105: train loss = 2364.953537, test loss = 3596.099468\n",
            "Epoch 3106: train loss = 2364.800488, test loss = 3596.067898\n",
            "Epoch 3107: train loss = 2364.647667, test loss = 3596.036362\n",
            "Epoch 3108: train loss = 2364.495073, test loss = 3596.004861\n",
            "Epoch 3109: train loss = 2364.342706, test loss = 3595.973394\n",
            "Epoch 3110: train loss = 2364.190566, test loss = 3595.941960\n",
            "Epoch 3111: train loss = 2364.038651, test loss = 3595.910559\n",
            "Epoch 3112: train loss = 2363.886962, test loss = 3595.879190\n",
            "Epoch 3113: train loss = 2363.735498, test loss = 3595.847852\n",
            "Epoch 3114: train loss = 2363.584259, test loss = 3595.816546\n",
            "Epoch 3115: train loss = 2363.433244, test loss = 3595.785270\n",
            "Epoch 3116: train loss = 2363.282453, test loss = 3595.754025\n",
            "Epoch 3117: train loss = 2363.131884, test loss = 3595.722809\n",
            "Epoch 3118: train loss = 2362.981539, test loss = 3595.691622\n",
            "Epoch 3119: train loss = 2362.831416, test loss = 3595.660464\n",
            "Epoch 3120: train loss = 2362.681514, test loss = 3595.629335\n",
            "Epoch 3121: train loss = 2362.531834, test loss = 3595.598233\n",
            "Epoch 3122: train loss = 2362.382375, test loss = 3595.567158\n",
            "Epoch 3123: train loss = 2362.233136, test loss = 3595.536110\n",
            "Epoch 3124: train loss = 2362.084118, test loss = 3595.505088\n",
            "Epoch 3125: train loss = 2361.935318, test loss = 3595.474092\n",
            "Epoch 3126: train loss = 2361.786738, test loss = 3595.443122\n",
            "Epoch 3127: train loss = 2361.638377, test loss = 3595.412176\n",
            "Epoch 3128: train loss = 2361.490233, test loss = 3595.381255\n",
            "Epoch 3129: train loss = 2361.342307, test loss = 3595.350359\n",
            "Epoch 3130: train loss = 2361.194599, test loss = 3595.319485\n",
            "Epoch 3131: train loss = 2361.047107, test loss = 3595.288636\n",
            "Epoch 3132: train loss = 2360.899831, test loss = 3595.257808\n",
            "Epoch 3133: train loss = 2360.752772, test loss = 3595.227004\n",
            "Epoch 3134: train loss = 2360.605927, test loss = 3595.196221\n",
            "Epoch 3135: train loss = 2360.459298, test loss = 3595.165460\n",
            "Epoch 3136: train loss = 2360.312883, test loss = 3595.134720\n",
            "Epoch 3137: train loss = 2360.166682, test loss = 3595.104001\n",
            "Epoch 3138: train loss = 2360.020695, test loss = 3595.073303\n",
            "Epoch 3139: train loss = 2359.874921, test loss = 3595.042624\n",
            "Epoch 3140: train loss = 2359.729359, test loss = 3595.011966\n",
            "Epoch 3141: train loss = 2359.584010, test loss = 3594.981326\n",
            "Epoch 3142: train loss = 2359.438873, test loss = 3594.950705\n",
            "Epoch 3143: train loss = 2359.293946, test loss = 3594.920103\n",
            "Epoch 3144: train loss = 2359.149231, test loss = 3594.889519\n",
            "Epoch 3145: train loss = 2359.004726, test loss = 3594.858953\n",
            "Epoch 3146: train loss = 2358.860431, test loss = 3594.828405\n",
            "Epoch 3147: train loss = 2358.716345, test loss = 3594.797873\n",
            "Epoch 3148: train loss = 2358.572469, test loss = 3594.767359\n",
            "Epoch 3149: train loss = 2358.428801, test loss = 3594.736860\n",
            "Epoch 3150: train loss = 2358.285342, test loss = 3594.706378\n",
            "Epoch 3151: train loss = 2358.142090, test loss = 3594.675912\n",
            "Epoch 3152: train loss = 2357.999045, test loss = 3594.645461\n",
            "Epoch 3153: train loss = 2357.856207, test loss = 3594.615025\n",
            "Epoch 3154: train loss = 2357.713576, test loss = 3594.584604\n",
            "Epoch 3155: train loss = 2357.571151, test loss = 3594.554197\n",
            "Epoch 3156: train loss = 2357.428931, test loss = 3594.523805\n",
            "Epoch 3157: train loss = 2357.286916, test loss = 3594.493427\n",
            "Epoch 3158: train loss = 2357.145106, test loss = 3594.463062\n",
            "Epoch 3159: train loss = 2357.003500, test loss = 3594.432710\n",
            "Epoch 3160: train loss = 2356.862097, test loss = 3594.402371\n",
            "Epoch 3161: train loss = 2356.720898, test loss = 3594.372045\n",
            "Epoch 3162: train loss = 2356.579902, test loss = 3594.341731\n",
            "Epoch 3163: train loss = 2356.439109, test loss = 3594.311429\n",
            "Epoch 3164: train loss = 2356.298517, test loss = 3594.281140\n",
            "Epoch 3165: train loss = 2356.158127, test loss = 3594.250861\n",
            "Epoch 3166: train loss = 2356.017938, test loss = 3594.220594\n",
            "Epoch 3167: train loss = 2355.877950, test loss = 3594.190338\n",
            "Epoch 3168: train loss = 2355.738162, test loss = 3594.160093\n",
            "Epoch 3169: train loss = 2355.598573, test loss = 3594.129857\n",
            "Epoch 3170: train loss = 2355.459185, test loss = 3594.099633\n",
            "Epoch 3171: train loss = 2355.319995, test loss = 3594.069418\n",
            "Epoch 3172: train loss = 2355.181003, test loss = 3594.039212\n",
            "Epoch 3173: train loss = 2355.042210, test loss = 3594.009016\n",
            "Epoch 3174: train loss = 2354.903615, test loss = 3593.978830\n",
            "Epoch 3175: train loss = 2354.765216, test loss = 3593.948652\n",
            "Epoch 3176: train loss = 2354.627015, test loss = 3593.918483\n",
            "Epoch 3177: train loss = 2354.489010, test loss = 3593.888322\n",
            "Epoch 3178: train loss = 2354.351200, test loss = 3593.858169\n",
            "Epoch 3179: train loss = 2354.213587, test loss = 3593.828024\n",
            "Epoch 3180: train loss = 2354.076168, test loss = 3593.797888\n",
            "Epoch 3181: train loss = 2353.938944, test loss = 3593.767758\n",
            "Epoch 3182: train loss = 2353.801915, test loss = 3593.737636\n",
            "Epoch 3183: train loss = 2353.665079, test loss = 3593.707521\n",
            "Epoch 3184: train loss = 2353.528437, test loss = 3593.677412\n",
            "Epoch 3185: train loss = 2353.391987, test loss = 3593.647310\n",
            "Epoch 3186: train loss = 2353.255730, test loss = 3593.617215\n",
            "Epoch 3187: train loss = 2353.119666, test loss = 3593.587126\n",
            "Epoch 3188: train loss = 2352.983793, test loss = 3593.557043\n",
            "Epoch 3189: train loss = 2352.848111, test loss = 3593.526965\n",
            "Epoch 3190: train loss = 2352.712620, test loss = 3593.496893\n",
            "Epoch 3191: train loss = 2352.577320, test loss = 3593.466827\n",
            "Epoch 3192: train loss = 2352.442210, test loss = 3593.436765\n",
            "Epoch 3193: train loss = 2352.307289, test loss = 3593.406709\n",
            "Epoch 3194: train loss = 2352.172558, test loss = 3593.376657\n",
            "Epoch 3195: train loss = 2352.038016, test loss = 3593.346610\n",
            "Epoch 3196: train loss = 2351.903662, test loss = 3593.316567\n",
            "Epoch 3197: train loss = 2351.769495, test loss = 3593.286529\n",
            "Epoch 3198: train loss = 2351.635517, test loss = 3593.256494\n",
            "Epoch 3199: train loss = 2351.501725, test loss = 3593.226464\n",
            "Epoch 3200: train loss = 2351.368121, test loss = 3593.196437\n",
            "Epoch 3201: train loss = 2351.234703, test loss = 3593.166413\n",
            "Epoch 3202: train loss = 2351.101470, test loss = 3593.136393\n",
            "Epoch 3203: train loss = 2350.968423, test loss = 3593.106376\n",
            "Epoch 3204: train loss = 2350.835562, test loss = 3593.076362\n",
            "Epoch 3205: train loss = 2350.702885, test loss = 3593.046350\n",
            "Epoch 3206: train loss = 2350.570392, test loss = 3593.016342\n",
            "Epoch 3207: train loss = 2350.438083, test loss = 3592.986336\n",
            "Epoch 3208: train loss = 2350.305958, test loss = 3592.956332\n",
            "Epoch 3209: train loss = 2350.174016, test loss = 3592.926330\n",
            "Epoch 3210: train loss = 2350.042257, test loss = 3592.896331\n",
            "Epoch 3211: train loss = 2349.910679, test loss = 3592.866333\n",
            "Epoch 3212: train loss = 2349.779284, test loss = 3592.836337\n",
            "Epoch 3213: train loss = 2349.648070, test loss = 3592.806342\n",
            "Epoch 3214: train loss = 2349.517038, test loss = 3592.776349\n",
            "Epoch 3215: train loss = 2349.386186, test loss = 3592.746357\n",
            "Epoch 3216: train loss = 2349.255514, test loss = 3592.716367\n",
            "Epoch 3217: train loss = 2349.125022, test loss = 3592.686377\n",
            "Epoch 3218: train loss = 2348.994710, test loss = 3592.656388\n",
            "Epoch 3219: train loss = 2348.864577, test loss = 3592.626400\n",
            "Epoch 3220: train loss = 2348.734623, test loss = 3592.596412\n",
            "Epoch 3221: train loss = 2348.604846, test loss = 3592.566425\n",
            "Epoch 3222: train loss = 2348.475248, test loss = 3592.536438\n",
            "Epoch 3223: train loss = 2348.345828, test loss = 3592.506451\n",
            "Epoch 3224: train loss = 2348.216584, test loss = 3592.476465\n",
            "Epoch 3225: train loss = 2348.087517, test loss = 3592.446478\n",
            "Epoch 3226: train loss = 2347.958627, test loss = 3592.416491\n",
            "Epoch 3227: train loss = 2347.829913, test loss = 3592.386504\n",
            "Epoch 3228: train loss = 2347.701374, test loss = 3592.356516\n",
            "Epoch 3229: train loss = 2347.573010, test loss = 3592.326528\n",
            "Epoch 3230: train loss = 2347.444822, test loss = 3592.296539\n",
            "Epoch 3231: train loss = 2347.316807, test loss = 3592.266550\n",
            "Epoch 3232: train loss = 2347.188967, test loss = 3592.236559\n",
            "Epoch 3233: train loss = 2347.061300, test loss = 3592.206567\n",
            "Epoch 3234: train loss = 2346.933807, test loss = 3592.176575\n",
            "Epoch 3235: train loss = 2346.806486, test loss = 3592.146581\n",
            "Epoch 3236: train loss = 2346.679338, test loss = 3592.116586\n",
            "Epoch 3237: train loss = 2346.552363, test loss = 3592.086589\n",
            "Epoch 3238: train loss = 2346.425559, test loss = 3592.056591\n",
            "Epoch 3239: train loss = 2346.298926, test loss = 3592.026591\n",
            "Epoch 3240: train loss = 2346.172464, test loss = 3591.996589\n",
            "Epoch 3241: train loss = 2346.046173, test loss = 3591.966585\n",
            "Epoch 3242: train loss = 2345.920052, test loss = 3591.936580\n",
            "Epoch 3243: train loss = 2345.794101, test loss = 3591.906572\n",
            "Epoch 3244: train loss = 2345.668320, test loss = 3591.876563\n",
            "Epoch 3245: train loss = 2345.542707, test loss = 3591.846551\n",
            "Epoch 3246: train loss = 2345.417264, test loss = 3591.816537\n",
            "Epoch 3247: train loss = 2345.291988, test loss = 3591.786520\n",
            "Epoch 3248: train loss = 2345.166881, test loss = 3591.756501\n",
            "Epoch 3249: train loss = 2345.041941, test loss = 3591.726479\n",
            "Epoch 3250: train loss = 2344.917169, test loss = 3591.696455\n",
            "Epoch 3251: train loss = 2344.792564, test loss = 3591.666428\n",
            "Epoch 3252: train loss = 2344.668125, test loss = 3591.636398\n",
            "Epoch 3253: train loss = 2344.543852, test loss = 3591.606365\n",
            "Epoch 3254: train loss = 2344.419745, test loss = 3591.576329\n",
            "Epoch 3255: train loss = 2344.295803, test loss = 3591.546290\n",
            "Epoch 3256: train loss = 2344.172026, test loss = 3591.516248\n",
            "Epoch 3257: train loss = 2344.048414, test loss = 3591.486203\n",
            "Epoch 3258: train loss = 2343.924967, test loss = 3591.456154\n",
            "Epoch 3259: train loss = 2343.801683, test loss = 3591.426102\n",
            "Epoch 3260: train loss = 2343.678563, test loss = 3591.396047\n",
            "Epoch 3261: train loss = 2343.555606, test loss = 3591.365988\n",
            "Epoch 3262: train loss = 2343.432812, test loss = 3591.335925\n",
            "Epoch 3263: train loss = 2343.310181, test loss = 3591.305859\n",
            "Epoch 3264: train loss = 2343.187711, test loss = 3591.275789\n",
            "Epoch 3265: train loss = 2343.065404, test loss = 3591.245715\n",
            "Epoch 3266: train loss = 2342.943258, test loss = 3591.215637\n",
            "Epoch 3267: train loss = 2342.821272, test loss = 3591.185556\n",
            "Epoch 3268: train loss = 2342.699448, test loss = 3591.155470\n",
            "Epoch 3269: train loss = 2342.577784, test loss = 3591.125380\n",
            "Epoch 3270: train loss = 2342.456280, test loss = 3591.095287\n",
            "Epoch 3271: train loss = 2342.334936, test loss = 3591.065189\n",
            "Epoch 3272: train loss = 2342.213751, test loss = 3591.035086\n",
            "Epoch 3273: train loss = 2342.092725, test loss = 3591.004980\n",
            "Epoch 3274: train loss = 2341.971857, test loss = 3590.974869\n",
            "Epoch 3275: train loss = 2341.851148, test loss = 3590.944754\n",
            "Epoch 3276: train loss = 2341.730597, test loss = 3590.914634\n",
            "Epoch 3277: train loss = 2341.610203, test loss = 3590.884510\n",
            "Epoch 3278: train loss = 2341.489966, test loss = 3590.854381\n",
            "Epoch 3279: train loss = 2341.369887, test loss = 3590.824247\n",
            "Epoch 3280: train loss = 2341.249964, test loss = 3590.794109\n",
            "Epoch 3281: train loss = 2341.130196, test loss = 3590.763966\n",
            "Epoch 3282: train loss = 2341.010585, test loss = 3590.733819\n",
            "Epoch 3283: train loss = 2340.891129, test loss = 3590.703666\n",
            "Epoch 3284: train loss = 2340.771829, test loss = 3590.673509\n",
            "Epoch 3285: train loss = 2340.652683, test loss = 3590.643347\n",
            "Epoch 3286: train loss = 2340.533691, test loss = 3590.613180\n",
            "Epoch 3287: train loss = 2340.414854, test loss = 3590.583007\n",
            "Epoch 3288: train loss = 2340.296171, test loss = 3590.552830\n",
            "Epoch 3289: train loss = 2340.177640, test loss = 3590.522648\n",
            "Epoch 3290: train loss = 2340.059263, test loss = 3590.492460\n",
            "Epoch 3291: train loss = 2339.941039, test loss = 3590.462268\n",
            "Epoch 3292: train loss = 2339.822967, test loss = 3590.432070\n",
            "Epoch 3293: train loss = 2339.705047, test loss = 3590.401867\n",
            "Epoch 3294: train loss = 2339.587279, test loss = 3590.371658\n",
            "Epoch 3295: train loss = 2339.469662, test loss = 3590.341445\n",
            "Epoch 3296: train loss = 2339.352197, test loss = 3590.311225\n",
            "Epoch 3297: train loss = 2339.234882, test loss = 3590.281001\n",
            "Epoch 3298: train loss = 2339.117717, test loss = 3590.250771\n",
            "Epoch 3299: train loss = 2339.000702, test loss = 3590.220536\n",
            "Epoch 3300: train loss = 2338.883838, test loss = 3590.190295\n",
            "Epoch 3301: train loss = 2338.767122, test loss = 3590.160048\n",
            "Epoch 3302: train loss = 2338.650556, test loss = 3590.129796\n",
            "Epoch 3303: train loss = 2338.534138, test loss = 3590.099539\n",
            "Epoch 3304: train loss = 2338.417869, test loss = 3590.069276\n",
            "Epoch 3305: train loss = 2338.301748, test loss = 3590.039007\n",
            "Epoch 3306: train loss = 2338.185775, test loss = 3590.008732\n",
            "Epoch 3307: train loss = 2338.069949, test loss = 3589.978452\n",
            "Epoch 3308: train loss = 2337.954270, test loss = 3589.948166\n",
            "Epoch 3309: train loss = 2337.838738, test loss = 3589.917874\n",
            "Epoch 3310: train loss = 2337.723353, test loss = 3589.887576\n",
            "Epoch 3311: train loss = 2337.608113, test loss = 3589.857273\n",
            "Epoch 3312: train loss = 2337.493020, test loss = 3589.826964\n",
            "Epoch 3313: train loss = 2337.378071, test loss = 3589.796648\n",
            "Epoch 3314: train loss = 2337.263268, test loss = 3589.766327\n",
            "Epoch 3315: train loss = 2337.148610, test loss = 3589.736000\n",
            "Epoch 3316: train loss = 2337.034097, test loss = 3589.705668\n",
            "Epoch 3317: train loss = 2336.919727, test loss = 3589.675329\n",
            "Epoch 3318: train loss = 2336.805502, test loss = 3589.644984\n",
            "Epoch 3319: train loss = 2336.691420, test loss = 3589.614633\n",
            "Epoch 3320: train loss = 2336.577482, test loss = 3589.584276\n",
            "Epoch 3321: train loss = 2336.463686, test loss = 3589.553913\n",
            "Epoch 3322: train loss = 2336.350033, test loss = 3589.523544\n",
            "Epoch 3323: train loss = 2336.236522, test loss = 3589.493169\n",
            "Epoch 3324: train loss = 2336.123153, test loss = 3589.462788\n",
            "Epoch 3325: train loss = 2336.009926, test loss = 3589.432401\n",
            "Epoch 3326: train loss = 2335.896841, test loss = 3589.402007\n",
            "Epoch 3327: train loss = 2335.783896, test loss = 3589.371608\n",
            "Epoch 3328: train loss = 2335.671092, test loss = 3589.341202\n",
            "Epoch 3329: train loss = 2335.558429, test loss = 3589.310790\n",
            "Epoch 3330: train loss = 2335.445906, test loss = 3589.280372\n",
            "Epoch 3331: train loss = 2335.333522, test loss = 3589.249948\n",
            "Epoch 3332: train loss = 2335.221278, test loss = 3589.219517\n",
            "Epoch 3333: train loss = 2335.109174, test loss = 3589.189080\n",
            "Epoch 3334: train loss = 2334.997208, test loss = 3589.158637\n",
            "Epoch 3335: train loss = 2334.885381, test loss = 3589.128188\n",
            "Epoch 3336: train loss = 2334.773692, test loss = 3589.097733\n",
            "Epoch 3337: train loss = 2334.662141, test loss = 3589.067271\n",
            "Epoch 3338: train loss = 2334.550728, test loss = 3589.036803\n",
            "Epoch 3339: train loss = 2334.439452, test loss = 3589.006328\n",
            "Epoch 3340: train loss = 2334.328314, test loss = 3588.975848\n",
            "Epoch 3341: train loss = 2334.217312, test loss = 3588.945361\n",
            "Epoch 3342: train loss = 2334.106447, test loss = 3588.914867\n",
            "Epoch 3343: train loss = 2333.995717, test loss = 3588.884368\n",
            "Epoch 3344: train loss = 2333.885124, test loss = 3588.853862\n",
            "Epoch 3345: train loss = 2333.774666, test loss = 3588.823349\n",
            "Epoch 3346: train loss = 2333.664344, test loss = 3588.792831\n",
            "Epoch 3347: train loss = 2333.554157, test loss = 3588.762306\n",
            "Epoch 3348: train loss = 2333.444104, test loss = 3588.731774\n",
            "Epoch 3349: train loss = 2333.334186, test loss = 3588.701236\n",
            "Epoch 3350: train loss = 2333.224401, test loss = 3588.670692\n",
            "Epoch 3351: train loss = 2333.114751, test loss = 3588.640142\n",
            "Epoch 3352: train loss = 2333.005234, test loss = 3588.609585\n",
            "Epoch 3353: train loss = 2332.895851, test loss = 3588.579022\n",
            "Epoch 3354: train loss = 2332.786600, test loss = 3588.548452\n",
            "Epoch 3355: train loss = 2332.677482, test loss = 3588.517876\n",
            "Epoch 3356: train loss = 2332.568497, test loss = 3588.487293\n",
            "Epoch 3357: train loss = 2332.459643, test loss = 3588.456705\n",
            "Epoch 3358: train loss = 2332.350921, test loss = 3588.426109\n",
            "Epoch 3359: train loss = 2332.242331, test loss = 3588.395508\n",
            "Epoch 3360: train loss = 2332.133872, test loss = 3588.364900\n",
            "Epoch 3361: train loss = 2332.025544, test loss = 3588.334285\n",
            "Epoch 3362: train loss = 2331.917346, test loss = 3588.303664\n",
            "Epoch 3363: train loss = 2331.809279, test loss = 3588.273037\n",
            "Epoch 3364: train loss = 2331.701342, test loss = 3588.242403\n",
            "Epoch 3365: train loss = 2331.593535, test loss = 3588.211763\n",
            "Epoch 3366: train loss = 2331.485857, test loss = 3588.181117\n",
            "Epoch 3367: train loss = 2331.378309, test loss = 3588.150464\n",
            "Epoch 3368: train loss = 2331.270889, test loss = 3588.119805\n",
            "Epoch 3369: train loss = 2331.163598, test loss = 3588.089139\n",
            "Epoch 3370: train loss = 2331.056435, test loss = 3588.058467\n",
            "Epoch 3371: train loss = 2330.949401, test loss = 3588.027789\n",
            "Epoch 3372: train loss = 2330.842494, test loss = 3587.997104\n",
            "Epoch 3373: train loss = 2330.735715, test loss = 3587.966413\n",
            "Epoch 3374: train loss = 2330.629063, test loss = 3587.935715\n",
            "Epoch 3375: train loss = 2330.522538, test loss = 3587.905011\n",
            "Epoch 3376: train loss = 2330.416140, test loss = 3587.874301\n",
            "Epoch 3377: train loss = 2330.309868, test loss = 3587.843585\n",
            "Epoch 3378: train loss = 2330.203723, test loss = 3587.812862\n",
            "Epoch 3379: train loss = 2330.097703, test loss = 3587.782132\n",
            "Epoch 3380: train loss = 2329.991809, test loss = 3587.751397\n",
            "Epoch 3381: train loss = 2329.886040, test loss = 3587.720654\n",
            "Epoch 3382: train loss = 2329.780396, test loss = 3587.689906\n",
            "Epoch 3383: train loss = 2329.674877, test loss = 3587.659151\n",
            "Epoch 3384: train loss = 2329.569483, test loss = 3587.628390\n",
            "Epoch 3385: train loss = 2329.464213, test loss = 3587.597623\n",
            "Epoch 3386: train loss = 2329.359066, test loss = 3587.566849\n",
            "Epoch 3387: train loss = 2329.254044, test loss = 3587.536069\n",
            "Epoch 3388: train loss = 2329.149145, test loss = 3587.505283\n",
            "Epoch 3389: train loss = 2329.044369, test loss = 3587.474491\n",
            "Epoch 3390: train loss = 2328.939716, test loss = 3587.443692\n",
            "Epoch 3391: train loss = 2328.835186, test loss = 3587.412887\n",
            "Epoch 3392: train loss = 2328.730778, test loss = 3587.382076\n",
            "Epoch 3393: train loss = 2328.626492, test loss = 3587.351258\n",
            "Epoch 3394: train loss = 2328.522328, test loss = 3587.320434\n",
            "Epoch 3395: train loss = 2328.418285, test loss = 3587.289604\n",
            "Epoch 3396: train loss = 2328.314364, test loss = 3587.258768\n",
            "Epoch 3397: train loss = 2328.210564, test loss = 3587.227925\n",
            "Epoch 3398: train loss = 2328.106885, test loss = 3587.197077\n",
            "Epoch 3399: train loss = 2328.003326, test loss = 3587.166222\n",
            "Epoch 3400: train loss = 2327.899888, test loss = 3587.135361\n",
            "Epoch 3401: train loss = 2327.796569, test loss = 3587.104493\n",
            "Epoch 3402: train loss = 2327.693371, test loss = 3587.073620\n",
            "Epoch 3403: train loss = 2327.590292, test loss = 3587.042740\n",
            "Epoch 3404: train loss = 2327.487332, test loss = 3587.011855\n",
            "Epoch 3405: train loss = 2327.384491, test loss = 3586.980963\n",
            "Epoch 3406: train loss = 2327.281768, test loss = 3586.950065\n",
            "Epoch 3407: train loss = 2327.179164, test loss = 3586.919161\n",
            "Epoch 3408: train loss = 2327.076679, test loss = 3586.888250\n",
            "Epoch 3409: train loss = 2326.974311, test loss = 3586.857334\n",
            "Epoch 3410: train loss = 2326.872061, test loss = 3586.826412\n",
            "Epoch 3411: train loss = 2326.769929, test loss = 3586.795484\n",
            "Epoch 3412: train loss = 2326.667913, test loss = 3586.764549\n",
            "Epoch 3413: train loss = 2326.566015, test loss = 3586.733609\n",
            "Epoch 3414: train loss = 2326.464233, test loss = 3586.702662\n",
            "Epoch 3415: train loss = 2326.362568, test loss = 3586.671710\n",
            "Epoch 3416: train loss = 2326.261019, test loss = 3586.640751\n",
            "Epoch 3417: train loss = 2326.159586, test loss = 3586.609787\n",
            "Epoch 3418: train loss = 2326.058269, test loss = 3586.578816\n",
            "Epoch 3419: train loss = 2325.957067, test loss = 3586.547840\n",
            "Epoch 3420: train loss = 2325.855980, test loss = 3586.516858\n",
            "Epoch 3421: train loss = 2325.755008, test loss = 3586.485869\n",
            "Epoch 3422: train loss = 2325.654151, test loss = 3586.454875\n",
            "Epoch 3423: train loss = 2325.553408, test loss = 3586.423875\n",
            "Epoch 3424: train loss = 2325.452780, test loss = 3586.392869\n",
            "Epoch 3425: train loss = 2325.352265, test loss = 3586.361857\n",
            "Epoch 3426: train loss = 2325.251864, test loss = 3586.330840\n",
            "Epoch 3427: train loss = 2325.151577, test loss = 3586.299816\n",
            "Epoch 3428: train loss = 2325.051403, test loss = 3586.268787\n",
            "Epoch 3429: train loss = 2324.951342, test loss = 3586.237752\n",
            "Epoch 3430: train loss = 2324.851393, test loss = 3586.206711\n",
            "Epoch 3431: train loss = 2324.751557, test loss = 3586.175664\n",
            "Epoch 3432: train loss = 2324.651834, test loss = 3586.144611\n",
            "Epoch 3433: train loss = 2324.552222, test loss = 3586.113553\n",
            "Epoch 3434: train loss = 2324.452722, test loss = 3586.082489\n",
            "Epoch 3435: train loss = 2324.353334, test loss = 3586.051420\n",
            "Epoch 3436: train loss = 2324.254057, test loss = 3586.020344\n",
            "Epoch 3437: train loss = 2324.154891, test loss = 3585.989263\n",
            "Epoch 3438: train loss = 2324.055836, test loss = 3585.958176\n",
            "Epoch 3439: train loss = 2323.956892, test loss = 3585.927084\n",
            "Epoch 3440: train loss = 2323.858058, test loss = 3585.895986\n",
            "Epoch 3441: train loss = 2323.759334, test loss = 3585.864882\n",
            "Epoch 3442: train loss = 2323.660719, test loss = 3585.833773\n",
            "Epoch 3443: train loss = 2323.562215, test loss = 3585.802658\n",
            "Epoch 3444: train loss = 2323.463820, test loss = 3585.771538\n",
            "Epoch 3445: train loss = 2323.365534, test loss = 3585.740412\n",
            "Epoch 3446: train loss = 2323.267357, test loss = 3585.709280\n",
            "Epoch 3447: train loss = 2323.169289, test loss = 3585.678143\n",
            "Epoch 3448: train loss = 2323.071329, test loss = 3585.647001\n",
            "Epoch 3449: train loss = 2322.973478, test loss = 3585.615853\n",
            "Epoch 3450: train loss = 2322.875735, test loss = 3585.584700\n",
            "Epoch 3451: train loss = 2322.778099, test loss = 3585.553541\n",
            "Epoch 3452: train loss = 2322.680571, test loss = 3585.522376\n",
            "Epoch 3453: train loss = 2322.583150, test loss = 3585.491207\n",
            "Epoch 3454: train loss = 2322.485837, test loss = 3585.460031\n",
            "Epoch 3455: train loss = 2322.388630, test loss = 3585.428851\n",
            "Epoch 3456: train loss = 2322.291530, test loss = 3585.397665\n",
            "Epoch 3457: train loss = 2322.194536, test loss = 3585.366474\n",
            "Epoch 3458: train loss = 2322.097649, test loss = 3585.335277\n",
            "Epoch 3459: train loss = 2322.000868, test loss = 3585.304076\n",
            "Epoch 3460: train loss = 2321.904192, test loss = 3585.272868\n",
            "Epoch 3461: train loss = 2321.807622, test loss = 3585.241656\n",
            "Epoch 3462: train loss = 2321.711157, test loss = 3585.210438\n",
            "Epoch 3463: train loss = 2321.614797, test loss = 3585.179216\n",
            "Epoch 3464: train loss = 2321.518542, test loss = 3585.147987\n",
            "Epoch 3465: train loss = 2321.422392, test loss = 3585.116754\n",
            "Epoch 3466: train loss = 2321.326346, test loss = 3585.085516\n",
            "Epoch 3467: train loss = 2321.230405, test loss = 3585.054272\n",
            "Epoch 3468: train loss = 2321.134567, test loss = 3585.023023\n",
            "Epoch 3469: train loss = 2321.038833, test loss = 3584.991770\n",
            "Epoch 3470: train loss = 2320.943203, test loss = 3584.960511\n",
            "Epoch 3471: train loss = 2320.847676, test loss = 3584.929246\n",
            "Epoch 3472: train loss = 2320.752252, test loss = 3584.897977\n",
            "Epoch 3473: train loss = 2320.656931, test loss = 3584.866703\n",
            "Epoch 3474: train loss = 2320.561713, test loss = 3584.835424\n",
            "Epoch 3475: train loss = 2320.466597, test loss = 3584.804140\n",
            "Epoch 3476: train loss = 2320.371584, test loss = 3584.772851\n",
            "Epoch 3477: train loss = 2320.276672, test loss = 3584.741556\n",
            "Epoch 3478: train loss = 2320.181862, test loss = 3584.710257\n",
            "Epoch 3479: train loss = 2320.087154, test loss = 3584.678953\n",
            "Epoch 3480: train loss = 2319.992547, test loss = 3584.647644\n",
            "Epoch 3481: train loss = 2319.898042, test loss = 3584.616331\n",
            "Epoch 3482: train loss = 2319.803637, test loss = 3584.585012\n",
            "Epoch 3483: train loss = 2319.709333, test loss = 3584.553688\n",
            "Epoch 3484: train loss = 2319.615130, test loss = 3584.522360\n",
            "Epoch 3485: train loss = 2319.521027, test loss = 3584.491027\n",
            "Epoch 3486: train loss = 2319.427024, test loss = 3584.459689\n",
            "Epoch 3487: train loss = 2319.333121, test loss = 3584.428346\n",
            "Epoch 3488: train loss = 2319.239317, test loss = 3584.396999\n",
            "Epoch 3489: train loss = 2319.145613, test loss = 3584.365646\n",
            "Epoch 3490: train loss = 2319.052009, test loss = 3584.334289\n",
            "Epoch 3491: train loss = 2318.958503, test loss = 3584.302928\n",
            "Epoch 3492: train loss = 2318.865096, test loss = 3584.271561\n",
            "Epoch 3493: train loss = 2318.771788, test loss = 3584.240190\n",
            "Epoch 3494: train loss = 2318.678579, test loss = 3584.208815\n",
            "Epoch 3495: train loss = 2318.585467, test loss = 3584.177434\n",
            "Epoch 3496: train loss = 2318.492454, test loss = 3584.146050\n",
            "Epoch 3497: train loss = 2318.399539, test loss = 3584.114660\n",
            "Epoch 3498: train loss = 2318.306721, test loss = 3584.083266\n",
            "Epoch 3499: train loss = 2318.214000, test loss = 3584.051868\n",
            "Epoch 3500: train loss = 2318.121377, test loss = 3584.020465\n",
            "Epoch 3501: train loss = 2318.028851, test loss = 3583.989057\n",
            "Epoch 3502: train loss = 2317.936422, test loss = 3583.957645\n",
            "Epoch 3503: train loss = 2317.844089, test loss = 3583.926229\n",
            "Epoch 3504: train loss = 2317.751852, test loss = 3583.894808\n",
            "Epoch 3505: train loss = 2317.659712, test loss = 3583.863383\n",
            "Epoch 3506: train loss = 2317.567668, test loss = 3583.831953\n",
            "Epoch 3507: train loss = 2317.475720, test loss = 3583.800519\n",
            "Epoch 3508: train loss = 2317.383867, test loss = 3583.769080\n",
            "Epoch 3509: train loss = 2317.292110, test loss = 3583.737638\n",
            "Epoch 3510: train loss = 2317.200448, test loss = 3583.706191\n",
            "Epoch 3511: train loss = 2317.108881, test loss = 3583.674739\n",
            "Epoch 3512: train loss = 2317.017408, test loss = 3583.643284\n",
            "Epoch 3513: train loss = 2316.926031, test loss = 3583.611824\n",
            "Epoch 3514: train loss = 2316.834747, test loss = 3583.580360\n",
            "Epoch 3515: train loss = 2316.743558, test loss = 3583.548891\n",
            "Epoch 3516: train loss = 2316.652464, test loss = 3583.517419\n",
            "Epoch 3517: train loss = 2316.561462, test loss = 3583.485942\n",
            "Epoch 3518: train loss = 2316.470555, test loss = 3583.454461\n",
            "Epoch 3519: train loss = 2316.379741, test loss = 3583.422976\n",
            "Epoch 3520: train loss = 2316.289020, test loss = 3583.391487\n",
            "Epoch 3521: train loss = 2316.198392, test loss = 3583.359994\n",
            "Epoch 3522: train loss = 2316.107858, test loss = 3583.328497\n",
            "Epoch 3523: train loss = 2316.017415, test loss = 3583.296995\n",
            "Epoch 3524: train loss = 2315.927066, test loss = 3583.265490\n",
            "Epoch 3525: train loss = 2315.836808, test loss = 3583.233981\n",
            "Epoch 3526: train loss = 2315.746643, test loss = 3583.202467\n",
            "Epoch 3527: train loss = 2315.656570, test loss = 3583.170950\n",
            "Epoch 3528: train loss = 2315.566588, test loss = 3583.139429\n",
            "Epoch 3529: train loss = 2315.476698, test loss = 3583.107903\n",
            "Epoch 3530: train loss = 2315.386899, test loss = 3583.076374\n",
            "Epoch 3531: train loss = 2315.297192, test loss = 3583.044841\n",
            "Epoch 3532: train loss = 2315.207575, test loss = 3583.013304\n",
            "Epoch 3533: train loss = 2315.118050, test loss = 3582.981764\n",
            "Epoch 3534: train loss = 2315.028614, test loss = 3582.950219\n",
            "Epoch 3535: train loss = 2314.939270, test loss = 3582.918671\n",
            "Epoch 3536: train loss = 2314.850015, test loss = 3582.887119\n",
            "Epoch 3537: train loss = 2314.760851, test loss = 3582.855563\n",
            "Epoch 3538: train loss = 2314.671776, test loss = 3582.824003\n",
            "Epoch 3539: train loss = 2314.582791, test loss = 3582.792440\n",
            "Epoch 3540: train loss = 2314.493895, test loss = 3582.760873\n",
            "Epoch 3541: train loss = 2314.405089, test loss = 3582.729302\n",
            "Epoch 3542: train loss = 2314.316372, test loss = 3582.697727\n",
            "Epoch 3543: train loss = 2314.227744, test loss = 3582.666149\n",
            "Epoch 3544: train loss = 2314.139205, test loss = 3582.634568\n",
            "Epoch 3545: train loss = 2314.050754, test loss = 3582.602982\n",
            "Epoch 3546: train loss = 2313.962391, test loss = 3582.571393\n",
            "Epoch 3547: train loss = 2313.874117, test loss = 3582.539801\n",
            "Epoch 3548: train loss = 2313.785931, test loss = 3582.508205\n",
            "Epoch 3549: train loss = 2313.697833, test loss = 3582.476605\n",
            "Epoch 3550: train loss = 2313.609822, test loss = 3582.445002\n",
            "Epoch 3551: train loss = 2313.521899, test loss = 3582.413396\n",
            "Epoch 3552: train loss = 2313.434063, test loss = 3582.381786\n",
            "Epoch 3553: train loss = 2313.346315, test loss = 3582.350172\n",
            "Epoch 3554: train loss = 2313.258653, test loss = 3582.318555\n",
            "Epoch 3555: train loss = 2313.171078, test loss = 3582.286935\n",
            "Epoch 3556: train loss = 2313.083590, test loss = 3582.255312\n",
            "Epoch 3557: train loss = 2312.996188, test loss = 3582.223685\n",
            "Epoch 3558: train loss = 2312.908872, test loss = 3582.192054\n",
            "Epoch 3559: train loss = 2312.821643, test loss = 3582.160420\n",
            "Epoch 3560: train loss = 2312.734499, test loss = 3582.128783\n",
            "Epoch 3561: train loss = 2312.647441, test loss = 3582.097143\n",
            "Epoch 3562: train loss = 2312.560469, test loss = 3582.065500\n",
            "Epoch 3563: train loss = 2312.473582, test loss = 3582.033853\n",
            "Epoch 3564: train loss = 2312.386780, test loss = 3582.002203\n",
            "Epoch 3565: train loss = 2312.300063, test loss = 3581.970550\n",
            "Epoch 3566: train loss = 2312.213431, test loss = 3581.938893\n",
            "Epoch 3567: train loss = 2312.126884, test loss = 3581.907234\n",
            "Epoch 3568: train loss = 2312.040421, test loss = 3581.875571\n",
            "Epoch 3569: train loss = 2311.954043, test loss = 3581.843905\n",
            "Epoch 3570: train loss = 2311.867749, test loss = 3581.812236\n",
            "Epoch 3571: train loss = 2311.781539, test loss = 3581.780564\n",
            "Epoch 3572: train loss = 2311.695412, test loss = 3581.748889\n",
            "Epoch 3573: train loss = 2311.609370, test loss = 3581.717211\n",
            "Epoch 3574: train loss = 2311.523411, test loss = 3581.685530\n",
            "Epoch 3575: train loss = 2311.437535, test loss = 3581.653846\n",
            "Epoch 3576: train loss = 2311.351742, test loss = 3581.622158\n",
            "Epoch 3577: train loss = 2311.266032, test loss = 3581.590468\n",
            "Epoch 3578: train loss = 2311.180406, test loss = 3581.558775\n",
            "Epoch 3579: train loss = 2311.094861, test loss = 3581.527079\n",
            "Epoch 3580: train loss = 2311.009400, test loss = 3581.495380\n",
            "Epoch 3581: train loss = 2310.924020, test loss = 3581.463678\n",
            "Epoch 3582: train loss = 2310.838723, test loss = 3581.431974\n",
            "Epoch 3583: train loss = 2310.753508, test loss = 3581.400266\n",
            "Epoch 3584: train loss = 2310.668374, test loss = 3581.368556\n",
            "Epoch 3585: train loss = 2310.583323, test loss = 3581.336843\n",
            "Epoch 3586: train loss = 2310.498352, test loss = 3581.305127\n",
            "Epoch 3587: train loss = 2310.413464, test loss = 3581.273408\n",
            "Epoch 3588: train loss = 2310.328656, test loss = 3581.241686\n",
            "Epoch 3589: train loss = 2310.243929, test loss = 3581.209962\n",
            "Epoch 3590: train loss = 2310.159283, test loss = 3581.178235\n",
            "Epoch 3591: train loss = 2310.074718, test loss = 3581.146506\n",
            "Epoch 3592: train loss = 2309.990233, test loss = 3581.114773\n",
            "Epoch 3593: train loss = 2309.905829, test loss = 3581.083038\n",
            "Epoch 3594: train loss = 2309.821505, test loss = 3581.051301\n",
            "Epoch 3595: train loss = 2309.737261, test loss = 3581.019560\n",
            "Epoch 3596: train loss = 2309.653097, test loss = 3580.987818\n",
            "Epoch 3597: train loss = 2309.569012, test loss = 3580.956072\n",
            "Epoch 3598: train loss = 2309.485007, test loss = 3580.924324\n",
            "Epoch 3599: train loss = 2309.401082, test loss = 3580.892574\n",
            "Epoch 3600: train loss = 2309.317236, test loss = 3580.860821\n",
            "Epoch 3601: train loss = 2309.233468, test loss = 3580.829065\n",
            "Epoch 3602: train loss = 2309.149780, test loss = 3580.797307\n",
            "Epoch 3603: train loss = 2309.066171, test loss = 3580.765546\n",
            "Epoch 3604: train loss = 2308.982640, test loss = 3580.733784\n",
            "Epoch 3605: train loss = 2308.899187, test loss = 3580.702018\n",
            "Epoch 3606: train loss = 2308.815813, test loss = 3580.670250\n",
            "Epoch 3607: train loss = 2308.732517, test loss = 3580.638480\n",
            "Epoch 3608: train loss = 2308.649299, test loss = 3580.606708\n",
            "Epoch 3609: train loss = 2308.566159, test loss = 3580.574933\n",
            "Epoch 3610: train loss = 2308.483097, test loss = 3580.543155\n",
            "Epoch 3611: train loss = 2308.400112, test loss = 3580.511376\n",
            "Epoch 3612: train loss = 2308.317204, test loss = 3580.479594\n",
            "Epoch 3613: train loss = 2308.234374, test loss = 3580.447810\n",
            "Epoch 3614: train loss = 2308.151621, test loss = 3580.416023\n",
            "Epoch 3615: train loss = 2308.068944, test loss = 3580.384235\n",
            "Epoch 3616: train loss = 2307.986345, test loss = 3580.352444\n",
            "Epoch 3617: train loss = 2307.903822, test loss = 3580.320651\n",
            "Epoch 3618: train loss = 2307.821375, test loss = 3580.288855\n",
            "Epoch 3619: train loss = 2307.739005, test loss = 3580.257058\n",
            "Epoch 3620: train loss = 2307.656711, test loss = 3580.225258\n",
            "Epoch 3621: train loss = 2307.574492, test loss = 3580.193457\n",
            "Epoch 3622: train loss = 2307.492350, test loss = 3580.161653\n",
            "Epoch 3623: train loss = 2307.410284, test loss = 3580.129847\n",
            "Epoch 3624: train loss = 2307.328293, test loss = 3580.098039\n",
            "Epoch 3625: train loss = 2307.246377, test loss = 3580.066229\n",
            "Epoch 3626: train loss = 2307.164537, test loss = 3580.034416\n",
            "Epoch 3627: train loss = 2307.082771, test loss = 3580.002602\n",
            "Epoch 3628: train loss = 2307.001081, test loss = 3579.970786\n",
            "Epoch 3629: train loss = 2306.919465, test loss = 3579.938968\n",
            "Epoch 3630: train loss = 2306.837925, test loss = 3579.907148\n",
            "Epoch 3631: train loss = 2306.756458, test loss = 3579.875326\n",
            "Epoch 3632: train loss = 2306.675066, test loss = 3579.843502\n",
            "Epoch 3633: train loss = 2306.593749, test loss = 3579.811676\n",
            "Epoch 3634: train loss = 2306.512505, test loss = 3579.779848\n",
            "Epoch 3635: train loss = 2306.431335, test loss = 3579.748018\n",
            "Epoch 3636: train loss = 2306.350239, test loss = 3579.716187\n",
            "Epoch 3637: train loss = 2306.269217, test loss = 3579.684353\n",
            "Epoch 3638: train loss = 2306.188268, test loss = 3579.652518\n",
            "Epoch 3639: train loss = 2306.107393, test loss = 3579.620681\n",
            "Epoch 3640: train loss = 2306.026590, test loss = 3579.588842\n",
            "Epoch 3641: train loss = 2305.945861, test loss = 3579.557001\n",
            "Epoch 3642: train loss = 2305.865205, test loss = 3579.525159\n",
            "Epoch 3643: train loss = 2305.784621, test loss = 3579.493315\n",
            "Epoch 3644: train loss = 2305.704110, test loss = 3579.461469\n",
            "Epoch 3645: train loss = 2305.623672, test loss = 3579.429621\n",
            "Epoch 3646: train loss = 2305.543306, test loss = 3579.397772\n",
            "Epoch 3647: train loss = 2305.463012, test loss = 3579.365921\n",
            "Epoch 3648: train loss = 2305.382790, test loss = 3579.334069\n",
            "Epoch 3649: train loss = 2305.302641, test loss = 3579.302214\n",
            "Epoch 3650: train loss = 2305.222563, test loss = 3579.270359\n",
            "Epoch 3651: train loss = 2305.142556, test loss = 3579.238501\n",
            "Epoch 3652: train loss = 2305.062621, test loss = 3579.206642\n",
            "Epoch 3653: train loss = 2304.982758, test loss = 3579.174782\n",
            "Epoch 3654: train loss = 2304.902966, test loss = 3579.142919\n",
            "Epoch 3655: train loss = 2304.823244, test loss = 3579.111056\n",
            "Epoch 3656: train loss = 2304.743594, test loss = 3579.079191\n",
            "Epoch 3657: train loss = 2304.664015, test loss = 3579.047324\n",
            "Epoch 3658: train loss = 2304.584506, test loss = 3579.015456\n",
            "Epoch 3659: train loss = 2304.505068, test loss = 3578.983586\n",
            "Epoch 3660: train loss = 2304.425700, test loss = 3578.951715\n",
            "Epoch 3661: train loss = 2304.346402, test loss = 3578.919842\n",
            "Epoch 3662: train loss = 2304.267175, test loss = 3578.887969\n",
            "Epoch 3663: train loss = 2304.188018, test loss = 3578.856093\n",
            "Epoch 3664: train loss = 2304.108930, test loss = 3578.824216\n",
            "Epoch 3665: train loss = 2304.029912, test loss = 3578.792338\n",
            "Epoch 3666: train loss = 2303.950964, test loss = 3578.760459\n",
            "Epoch 3667: train loss = 2303.872085, test loss = 3578.728578\n",
            "Epoch 3668: train loss = 2303.793276, test loss = 3578.696696\n",
            "Epoch 3669: train loss = 2303.714535, test loss = 3578.664813\n",
            "Epoch 3670: train loss = 2303.635864, test loss = 3578.632928\n",
            "Epoch 3671: train loss = 2303.557262, test loss = 3578.601042\n",
            "Epoch 3672: train loss = 2303.478728, test loss = 3578.569155\n",
            "Epoch 3673: train loss = 2303.400263, test loss = 3578.537267\n",
            "Epoch 3674: train loss = 2303.321867, test loss = 3578.505377\n",
            "Epoch 3675: train loss = 2303.243539, test loss = 3578.473486\n",
            "Epoch 3676: train loss = 2303.165279, test loss = 3578.441594\n",
            "Epoch 3677: train loss = 2303.087088, test loss = 3578.409701\n",
            "Epoch 3678: train loss = 2303.008964, test loss = 3578.377806\n",
            "Epoch 3679: train loss = 2302.930908, test loss = 3578.345911\n",
            "Epoch 3680: train loss = 2302.852920, test loss = 3578.314014\n",
            "Epoch 3681: train loss = 2302.775000, test loss = 3578.282117\n",
            "Epoch 3682: train loss = 2302.697147, test loss = 3578.250218\n",
            "Epoch 3683: train loss = 2302.619361, test loss = 3578.218318\n",
            "Epoch 3684: train loss = 2302.541643, test loss = 3578.186417\n",
            "Epoch 3685: train loss = 2302.463992, test loss = 3578.154515\n",
            "Epoch 3686: train loss = 2302.386407, test loss = 3578.122612\n",
            "Epoch 3687: train loss = 2302.308890, test loss = 3578.090708\n",
            "Epoch 3688: train loss = 2302.231439, test loss = 3578.058803\n",
            "Epoch 3689: train loss = 2302.154055, test loss = 3578.026897\n",
            "Epoch 3690: train loss = 2302.076737, test loss = 3577.994990\n",
            "Epoch 3691: train loss = 2301.999485, test loss = 3577.963082\n",
            "Epoch 3692: train loss = 2301.922300, test loss = 3577.931174\n",
            "Epoch 3693: train loss = 2301.845181, test loss = 3577.899264\n",
            "Epoch 3694: train loss = 2301.768127, test loss = 3577.867353\n",
            "Epoch 3695: train loss = 2301.691140, test loss = 3577.835442\n",
            "Epoch 3696: train loss = 2301.614218, test loss = 3577.803529\n",
            "Epoch 3697: train loss = 2301.537362, test loss = 3577.771616\n",
            "Epoch 3698: train loss = 2301.460571, test loss = 3577.739702\n",
            "Epoch 3699: train loss = 2301.383845, test loss = 3577.707787\n",
            "Epoch 3700: train loss = 2301.307185, test loss = 3577.675871\n",
            "Epoch 3701: train loss = 2301.230590, test loss = 3577.643955\n",
            "Epoch 3702: train loss = 2301.154060, test loss = 3577.612038\n",
            "Epoch 3703: train loss = 2301.077594, test loss = 3577.580120\n",
            "Epoch 3704: train loss = 2301.001193, test loss = 3577.548201\n",
            "Epoch 3705: train loss = 2300.924857, test loss = 3577.516281\n",
            "Epoch 3706: train loss = 2300.848585, test loss = 3577.484361\n",
            "Epoch 3707: train loss = 2300.772378, test loss = 3577.452440\n",
            "Epoch 3708: train loss = 2300.696234, test loss = 3577.420518\n",
            "Epoch 3709: train loss = 2300.620155, test loss = 3577.388596\n",
            "Epoch 3710: train loss = 2300.544140, test loss = 3577.356673\n",
            "Epoch 3711: train loss = 2300.468189, test loss = 3577.324750\n",
            "Epoch 3712: train loss = 2300.392301, test loss = 3577.292825\n",
            "Epoch 3713: train loss = 2300.316477, test loss = 3577.260900\n",
            "Epoch 3714: train loss = 2300.240716, test loss = 3577.228975\n",
            "Epoch 3715: train loss = 2300.165019, test loss = 3577.197049\n",
            "Epoch 3716: train loss = 2300.089385, test loss = 3577.165122\n",
            "Epoch 3717: train loss = 2300.013814, test loss = 3577.133195\n",
            "Epoch 3718: train loss = 2299.938306, test loss = 3577.101268\n",
            "Epoch 3719: train loss = 2299.862861, test loss = 3577.069339\n",
            "Epoch 3720: train loss = 2299.787479, test loss = 3577.037411\n",
            "Epoch 3721: train loss = 2299.712159, test loss = 3577.005481\n",
            "Epoch 3722: train loss = 2299.636902, test loss = 3576.973552\n",
            "Epoch 3723: train loss = 2299.561707, test loss = 3576.941621\n",
            "Epoch 3724: train loss = 2299.486575, test loss = 3576.909691\n",
            "Epoch 3725: train loss = 2299.411505, test loss = 3576.877760\n",
            "Epoch 3726: train loss = 2299.336496, test loss = 3576.845828\n",
            "Epoch 3727: train loss = 2299.261550, test loss = 3576.813896\n",
            "Epoch 3728: train loss = 2299.186666, test loss = 3576.781964\n",
            "Epoch 3729: train loss = 2299.111843, test loss = 3576.750031\n",
            "Epoch 3730: train loss = 2299.037082, test loss = 3576.718098\n",
            "Epoch 3731: train loss = 2298.962382, test loss = 3576.686165\n",
            "Epoch 3732: train loss = 2298.887744, test loss = 3576.654231\n",
            "Epoch 3733: train loss = 2298.813167, test loss = 3576.622297\n",
            "Epoch 3734: train loss = 2298.738651, test loss = 3576.590363\n",
            "Epoch 3735: train loss = 2298.664196, test loss = 3576.558428\n",
            "Epoch 3736: train loss = 2298.589802, test loss = 3576.526493\n",
            "Epoch 3737: train loss = 2298.515469, test loss = 3576.494558\n",
            "Epoch 3738: train loss = 2298.441196, test loss = 3576.462622\n",
            "Epoch 3739: train loss = 2298.366984, test loss = 3576.430687\n",
            "Epoch 3740: train loss = 2298.292833, test loss = 3576.398751\n",
            "Epoch 3741: train loss = 2298.218741, test loss = 3576.366815\n",
            "Epoch 3742: train loss = 2298.144710, test loss = 3576.334878\n",
            "Epoch 3743: train loss = 2298.070740, test loss = 3576.302942\n",
            "Epoch 3744: train loss = 2297.996829, test loss = 3576.271005\n",
            "Epoch 3745: train loss = 2297.922978, test loss = 3576.239068\n",
            "Epoch 3746: train loss = 2297.849186, test loss = 3576.207131\n",
            "Epoch 3747: train loss = 2297.775455, test loss = 3576.175194\n",
            "Epoch 3748: train loss = 2297.701783, test loss = 3576.143256\n",
            "Epoch 3749: train loss = 2297.628170, test loss = 3576.111319\n",
            "Epoch 3750: train loss = 2297.554617, test loss = 3576.079381\n",
            "Epoch 3751: train loss = 2297.481123, test loss = 3576.047444\n",
            "Epoch 3752: train loss = 2297.407688, test loss = 3576.015506\n",
            "Epoch 3753: train loss = 2297.334312, test loss = 3575.983568\n",
            "Epoch 3754: train loss = 2297.260995, test loss = 3575.951631\n",
            "Epoch 3755: train loss = 2297.187737, test loss = 3575.919693\n",
            "Epoch 3756: train loss = 2297.114537, test loss = 3575.887755\n",
            "Epoch 3757: train loss = 2297.041396, test loss = 3575.855817\n",
            "Epoch 3758: train loss = 2296.968314, test loss = 3575.823880\n",
            "Epoch 3759: train loss = 2296.895290, test loss = 3575.791942\n",
            "Epoch 3760: train loss = 2296.822324, test loss = 3575.760004\n",
            "Epoch 3761: train loss = 2296.749416, test loss = 3575.728066\n",
            "Epoch 3762: train loss = 2296.676566, test loss = 3575.696129\n",
            "Epoch 3763: train loss = 2296.603774, test loss = 3575.664191\n",
            "Epoch 3764: train loss = 2296.531040, test loss = 3575.632254\n",
            "Epoch 3765: train loss = 2296.458364, test loss = 3575.600316\n",
            "Epoch 3766: train loss = 2296.385745, test loss = 3575.568379\n",
            "Epoch 3767: train loss = 2296.313184, test loss = 3575.536442\n",
            "Epoch 3768: train loss = 2296.240680, test loss = 3575.504505\n",
            "Epoch 3769: train loss = 2296.168233, test loss = 3575.472568\n",
            "Epoch 3770: train loss = 2296.095844, test loss = 3575.440632\n",
            "Epoch 3771: train loss = 2296.023512, test loss = 3575.408695\n",
            "Epoch 3772: train loss = 2295.951236, test loss = 3575.376759\n",
            "Epoch 3773: train loss = 2295.879018, test loss = 3575.344823\n",
            "Epoch 3774: train loss = 2295.806856, test loss = 3575.312887\n",
            "Epoch 3775: train loss = 2295.734751, test loss = 3575.280951\n",
            "Epoch 3776: train loss = 2295.662702, test loss = 3575.249016\n",
            "Epoch 3777: train loss = 2295.590710, test loss = 3575.217081\n",
            "Epoch 3778: train loss = 2295.518775, test loss = 3575.185146\n",
            "Epoch 3779: train loss = 2295.446895, test loss = 3575.153211\n",
            "Epoch 3780: train loss = 2295.375072, test loss = 3575.121277\n",
            "Epoch 3781: train loss = 2295.303304, test loss = 3575.089343\n",
            "Epoch 3782: train loss = 2295.231593, test loss = 3575.057409\n",
            "Epoch 3783: train loss = 2295.159937, test loss = 3575.025476\n",
            "Epoch 3784: train loss = 2295.088337, test loss = 3574.993543\n",
            "Epoch 3785: train loss = 2295.016793, test loss = 3574.961610\n",
            "Epoch 3786: train loss = 2294.945304, test loss = 3574.929678\n",
            "Epoch 3787: train loss = 2294.873871, test loss = 3574.897746\n",
            "Epoch 3788: train loss = 2294.802493, test loss = 3574.865814\n",
            "Epoch 3789: train loss = 2294.731170, test loss = 3574.833883\n",
            "Epoch 3790: train loss = 2294.659903, test loss = 3574.801952\n",
            "Epoch 3791: train loss = 2294.588690, test loss = 3574.770022\n",
            "Epoch 3792: train loss = 2294.517533, test loss = 3574.738092\n",
            "Epoch 3793: train loss = 2294.446430, test loss = 3574.706162\n",
            "Epoch 3794: train loss = 2294.375382, test loss = 3574.674233\n",
            "Epoch 3795: train loss = 2294.304388, test loss = 3574.642305\n",
            "Epoch 3796: train loss = 2294.233449, test loss = 3574.610377\n",
            "Epoch 3797: train loss = 2294.162565, test loss = 3574.578449\n",
            "Epoch 3798: train loss = 2294.091734, test loss = 3574.546522\n",
            "Epoch 3799: train loss = 2294.020958, test loss = 3574.514595\n",
            "Epoch 3800: train loss = 2293.950236, test loss = 3574.482669\n",
            "Epoch 3801: train loss = 2293.879569, test loss = 3574.450744\n",
            "Epoch 3802: train loss = 2293.808955, test loss = 3574.418819\n",
            "Epoch 3803: train loss = 2293.738395, test loss = 3574.386895\n",
            "Epoch 3804: train loss = 2293.667888, test loss = 3574.354971\n",
            "Epoch 3805: train loss = 2293.597435, test loss = 3574.323047\n",
            "Epoch 3806: train loss = 2293.527036, test loss = 3574.291125\n",
            "Epoch 3807: train loss = 2293.456691, test loss = 3574.259203\n",
            "Epoch 3808: train loss = 2293.386398, test loss = 3574.227281\n",
            "Epoch 3809: train loss = 2293.316159, test loss = 3574.195361\n",
            "Epoch 3810: train loss = 2293.245973, test loss = 3574.163440\n",
            "Epoch 3811: train loss = 2293.175840, test loss = 3574.131521\n",
            "Epoch 3812: train loss = 2293.105760, test loss = 3574.099602\n",
            "Epoch 3813: train loss = 2293.035733, test loss = 3574.067684\n",
            "Epoch 3814: train loss = 2292.965759, test loss = 3574.035766\n",
            "Epoch 3815: train loss = 2292.895838, test loss = 3574.003850\n",
            "Epoch 3816: train loss = 2292.825969, test loss = 3573.971934\n",
            "Epoch 3817: train loss = 2292.756152, test loss = 3573.940018\n",
            "Epoch 3818: train loss = 2292.686388, test loss = 3573.908104\n",
            "Epoch 3819: train loss = 2292.616676, test loss = 3573.876190\n",
            "Epoch 3820: train loss = 2292.547017, test loss = 3573.844276\n",
            "Epoch 3821: train loss = 2292.477409, test loss = 3573.812364\n",
            "Epoch 3822: train loss = 2292.407854, test loss = 3573.780452\n",
            "Epoch 3823: train loss = 2292.338350, test loss = 3573.748542\n",
            "Epoch 3824: train loss = 2292.268898, test loss = 3573.716632\n",
            "Epoch 3825: train loss = 2292.199499, test loss = 3573.684722\n",
            "Epoch 3826: train loss = 2292.130150, test loss = 3573.652814\n",
            "Epoch 3827: train loss = 2292.060854, test loss = 3573.620906\n",
            "Epoch 3828: train loss = 2291.991608, test loss = 3573.589000\n",
            "Epoch 3829: train loss = 2291.922415, test loss = 3573.557094\n",
            "Epoch 3830: train loss = 2291.853272, test loss = 3573.525189\n",
            "Epoch 3831: train loss = 2291.784181, test loss = 3573.493284\n",
            "Epoch 3832: train loss = 2291.715140, test loss = 3573.461381\n",
            "Epoch 3833: train loss = 2291.646151, test loss = 3573.429479\n",
            "Epoch 3834: train loss = 2291.577213, test loss = 3573.397577\n",
            "Epoch 3835: train loss = 2291.508325, test loss = 3573.365676\n",
            "Epoch 3836: train loss = 2291.439488, test loss = 3573.333777\n",
            "Epoch 3837: train loss = 2291.370702, test loss = 3573.301878\n",
            "Epoch 3838: train loss = 2291.301967, test loss = 3573.269980\n",
            "Epoch 3839: train loss = 2291.233282, test loss = 3573.238083\n",
            "Epoch 3840: train loss = 2291.164647, test loss = 3573.206187\n",
            "Epoch 3841: train loss = 2291.096063, test loss = 3573.174292\n",
            "Epoch 3842: train loss = 2291.027528, test loss = 3573.142398\n",
            "Epoch 3843: train loss = 2290.959044, test loss = 3573.110505\n",
            "Epoch 3844: train loss = 2290.890610, test loss = 3573.078613\n",
            "Epoch 3845: train loss = 2290.822226, test loss = 3573.046723\n",
            "Epoch 3846: train loss = 2290.753892, test loss = 3573.014833\n",
            "Epoch 3847: train loss = 2290.685607, test loss = 3572.982944\n",
            "Epoch 3848: train loss = 2290.617373, test loss = 3572.951056\n",
            "Epoch 3849: train loss = 2290.549187, test loss = 3572.919169\n",
            "Epoch 3850: train loss = 2290.481052, test loss = 3572.887283\n",
            "Epoch 3851: train loss = 2290.412965, test loss = 3572.855398\n",
            "Epoch 3852: train loss = 2290.344928, test loss = 3572.823515\n",
            "Epoch 3853: train loss = 2290.276940, test loss = 3572.791632\n",
            "Epoch 3854: train loss = 2290.209002, test loss = 3572.759751\n",
            "Epoch 3855: train loss = 2290.141112, test loss = 3572.727871\n",
            "Epoch 3856: train loss = 2290.073272, test loss = 3572.695991\n",
            "Epoch 3857: train loss = 2290.005480, test loss = 3572.664113\n",
            "Epoch 3858: train loss = 2289.937737, test loss = 3572.632236\n",
            "Epoch 3859: train loss = 2289.870043, test loss = 3572.600361\n",
            "Epoch 3860: train loss = 2289.802397, test loss = 3572.568486\n",
            "Epoch 3861: train loss = 2289.734800, test loss = 3572.536613\n",
            "Epoch 3862: train loss = 2289.667252, test loss = 3572.504740\n",
            "Epoch 3863: train loss = 2289.599752, test loss = 3572.472869\n",
            "Epoch 3864: train loss = 2289.532300, test loss = 3572.440999\n",
            "Epoch 3865: train loss = 2289.464896, test loss = 3572.409131\n",
            "Epoch 3866: train loss = 2289.397541, test loss = 3572.377263\n",
            "Epoch 3867: train loss = 2289.330233, test loss = 3572.345397\n",
            "Epoch 3868: train loss = 2289.262974, test loss = 3572.313532\n",
            "Epoch 3869: train loss = 2289.195762, test loss = 3572.281668\n",
            "Epoch 3870: train loss = 2289.128598, test loss = 3572.249806\n",
            "Epoch 3871: train loss = 2289.061482, test loss = 3572.217945\n",
            "Epoch 3872: train loss = 2288.994413, test loss = 3572.186085\n",
            "Epoch 3873: train loss = 2288.927392, test loss = 3572.154226\n",
            "Epoch 3874: train loss = 2288.860419, test loss = 3572.122369\n",
            "Epoch 3875: train loss = 2288.793493, test loss = 3572.090512\n",
            "Epoch 3876: train loss = 2288.726614, test loss = 3572.058658\n",
            "Epoch 3877: train loss = 2288.659782, test loss = 3572.026804\n",
            "Epoch 3878: train loss = 2288.592998, test loss = 3571.994952\n",
            "Epoch 3879: train loss = 2288.526260, test loss = 3571.963101\n",
            "Epoch 3880: train loss = 2288.459569, test loss = 3571.931252\n",
            "Epoch 3881: train loss = 2288.392926, test loss = 3571.899404\n",
            "Epoch 3882: train loss = 2288.326329, test loss = 3571.867557\n",
            "Epoch 3883: train loss = 2288.259779, test loss = 3571.835712\n",
            "Epoch 3884: train loss = 2288.193275, test loss = 3571.803868\n",
            "Epoch 3885: train loss = 2288.126818, test loss = 3571.772025\n",
            "Epoch 3886: train loss = 2288.060408, test loss = 3571.740184\n",
            "Epoch 3887: train loss = 2287.994044, test loss = 3571.708344\n",
            "Epoch 3888: train loss = 2287.927726, test loss = 3571.676506\n",
            "Epoch 3889: train loss = 2287.861455, test loss = 3571.644669\n",
            "Epoch 3890: train loss = 2287.795229, test loss = 3571.612833\n",
            "Epoch 3891: train loss = 2287.729050, test loss = 3571.580999\n",
            "Epoch 3892: train loss = 2287.662917, test loss = 3571.549166\n",
            "Epoch 3893: train loss = 2287.596829, test loss = 3571.517335\n",
            "Epoch 3894: train loss = 2287.530788, test loss = 3571.485506\n",
            "Epoch 3895: train loss = 2287.464792, test loss = 3571.453677\n",
            "Epoch 3896: train loss = 2287.398842, test loss = 3571.421851\n",
            "Epoch 3897: train loss = 2287.332938, test loss = 3571.390025\n",
            "Epoch 3898: train loss = 2287.267079, test loss = 3571.358202\n",
            "Epoch 3899: train loss = 2287.201265, test loss = 3571.326380\n",
            "Epoch 3900: train loss = 2287.135497, test loss = 3571.294559\n",
            "Epoch 3901: train loss = 2287.069774, test loss = 3571.262740\n",
            "Epoch 3902: train loss = 2287.004096, test loss = 3571.230922\n",
            "Epoch 3903: train loss = 2286.938464, test loss = 3571.199106\n",
            "Epoch 3904: train loss = 2286.872876, test loss = 3571.167291\n",
            "Epoch 3905: train loss = 2286.807334, test loss = 3571.135478\n",
            "Epoch 3906: train loss = 2286.741836, test loss = 3571.103667\n",
            "Epoch 3907: train loss = 2286.676384, test loss = 3571.071857\n",
            "Epoch 3908: train loss = 2286.610976, test loss = 3571.040049\n",
            "Epoch 3909: train loss = 2286.545612, test loss = 3571.008242\n",
            "Epoch 3910: train loss = 2286.480294, test loss = 3570.976437\n",
            "Epoch 3911: train loss = 2286.415019, test loss = 3570.944634\n",
            "Epoch 3912: train loss = 2286.349790, test loss = 3570.912832\n",
            "Epoch 3913: train loss = 2286.284604, test loss = 3570.881032\n",
            "Epoch 3914: train loss = 2286.219463, test loss = 3570.849234\n",
            "Epoch 3915: train loss = 2286.154366, test loss = 3570.817437\n",
            "Epoch 3916: train loss = 2286.089313, test loss = 3570.785641\n",
            "Epoch 3917: train loss = 2286.024305, test loss = 3570.753848\n",
            "Epoch 3918: train loss = 2285.959340, test loss = 3570.722056\n",
            "Epoch 3919: train loss = 2285.894419, test loss = 3570.690266\n",
            "Epoch 3920: train loss = 2285.829542, test loss = 3570.658477\n",
            "Epoch 3921: train loss = 2285.764709, test loss = 3570.626690\n",
            "Epoch 3922: train loss = 2285.699920, test loss = 3570.594905\n",
            "Epoch 3923: train loss = 2285.635174, test loss = 3570.563122\n",
            "Epoch 3924: train loss = 2285.570471, test loss = 3570.531340\n",
            "Epoch 3925: train loss = 2285.505813, test loss = 3570.499560\n",
            "Epoch 3926: train loss = 2285.441197, test loss = 3570.467782\n",
            "Epoch 3927: train loss = 2285.376625, test loss = 3570.436005\n",
            "Epoch 3928: train loss = 2285.312096, test loss = 3570.404231\n",
            "Epoch 3929: train loss = 2285.247610, test loss = 3570.372458\n",
            "Epoch 3930: train loss = 2285.183168, test loss = 3570.340686\n",
            "Epoch 3931: train loss = 2285.118768, test loss = 3570.308917\n",
            "Epoch 3932: train loss = 2285.054412, test loss = 3570.277149\n",
            "Epoch 3933: train loss = 2284.990098, test loss = 3570.245383\n",
            "Epoch 3934: train loss = 2284.925827, test loss = 3570.213619\n",
            "Epoch 3935: train loss = 2284.861599, test loss = 3570.181857\n",
            "Epoch 3936: train loss = 2284.797413, test loss = 3570.150097\n",
            "Epoch 3937: train loss = 2284.733270, test loss = 3570.118338\n",
            "Epoch 3938: train loss = 2284.669170, test loss = 3570.086581\n",
            "Epoch 3939: train loss = 2284.605112, test loss = 3570.054826\n",
            "Epoch 3940: train loss = 2284.541096, test loss = 3570.023073\n",
            "Epoch 3941: train loss = 2284.477123, test loss = 3569.991322\n",
            "Epoch 3942: train loss = 2284.413192, test loss = 3569.959572\n",
            "Epoch 3943: train loss = 2284.349303, test loss = 3569.927824\n",
            "Epoch 3944: train loss = 2284.285457, test loss = 3569.896079\n",
            "Epoch 3945: train loss = 2284.221652, test loss = 3569.864335\n",
            "Epoch 3946: train loss = 2284.157889, test loss = 3569.832593\n",
            "Epoch 3947: train loss = 2284.094168, test loss = 3569.800853\n",
            "Epoch 3948: train loss = 2284.030489, test loss = 3569.769115\n",
            "Epoch 3949: train loss = 2283.966852, test loss = 3569.737378\n",
            "Epoch 3950: train loss = 2283.903256, test loss = 3569.705644\n",
            "Epoch 3951: train loss = 2283.839702, test loss = 3569.673912\n",
            "Epoch 3952: train loss = 2283.776189, test loss = 3569.642181\n",
            "Epoch 3953: train loss = 2283.712718, test loss = 3569.610453\n",
            "Epoch 3954: train loss = 2283.649289, test loss = 3569.578726\n",
            "Epoch 3955: train loss = 2283.585900, test loss = 3569.547001\n",
            "Epoch 3956: train loss = 2283.522553, test loss = 3569.515279\n",
            "Epoch 3957: train loss = 2283.459248, test loss = 3569.483558\n",
            "Epoch 3958: train loss = 2283.395983, test loss = 3569.451839\n",
            "Epoch 3959: train loss = 2283.332759, test loss = 3569.420122\n",
            "Epoch 3960: train loss = 2283.269576, test loss = 3569.388408\n",
            "Epoch 3961: train loss = 2283.206435, test loss = 3569.356695\n",
            "Epoch 3962: train loss = 2283.143334, test loss = 3569.324984\n",
            "Epoch 3963: train loss = 2283.080273, test loss = 3569.293275\n",
            "Epoch 3964: train loss = 2283.017254, test loss = 3569.261568\n",
            "Epoch 3965: train loss = 2282.954275, test loss = 3569.229864\n",
            "Epoch 3966: train loss = 2282.891337, test loss = 3569.198161\n",
            "Epoch 3967: train loss = 2282.828439, test loss = 3569.166460\n",
            "Epoch 3968: train loss = 2282.765582, test loss = 3569.134762\n",
            "Epoch 3969: train loss = 2282.702765, test loss = 3569.103065\n",
            "Epoch 3970: train loss = 2282.639988, test loss = 3569.071370\n",
            "Epoch 3971: train loss = 2282.577252, test loss = 3569.039678\n",
            "Epoch 3972: train loss = 2282.514556, test loss = 3569.007988\n",
            "Epoch 3973: train loss = 2282.451900, test loss = 3568.976299\n",
            "Epoch 3974: train loss = 2282.389284, test loss = 3568.944613\n",
            "Epoch 3975: train loss = 2282.326708, test loss = 3568.912929\n",
            "Epoch 3976: train loss = 2282.264171, test loss = 3568.881247\n",
            "Epoch 3977: train loss = 2282.201675, test loss = 3568.849567\n",
            "Epoch 3978: train loss = 2282.139219, test loss = 3568.817889\n",
            "Epoch 3979: train loss = 2282.076802, test loss = 3568.786213\n",
            "Epoch 3980: train loss = 2282.014425, test loss = 3568.754540\n",
            "Epoch 3981: train loss = 2281.952087, test loss = 3568.722868\n",
            "Epoch 3982: train loss = 2281.889789, test loss = 3568.691199\n",
            "Epoch 3983: train loss = 2281.827530, test loss = 3568.659532\n",
            "Epoch 3984: train loss = 2281.765311, test loss = 3568.627867\n",
            "Epoch 3985: train loss = 2281.703131, test loss = 3568.596204\n",
            "Epoch 3986: train loss = 2281.640991, test loss = 3568.564543\n",
            "Epoch 3987: train loss = 2281.578889, test loss = 3568.532884\n",
            "Epoch 3988: train loss = 2281.516827, test loss = 3568.501228\n",
            "Epoch 3989: train loss = 2281.454803, test loss = 3568.469574\n",
            "Epoch 3990: train loss = 2281.392819, test loss = 3568.437922\n",
            "Epoch 3991: train loss = 2281.330874, test loss = 3568.406272\n",
            "Epoch 3992: train loss = 2281.268967, test loss = 3568.374624\n",
            "Epoch 3993: train loss = 2281.207100, test loss = 3568.342979\n",
            "Epoch 3994: train loss = 2281.145271, test loss = 3568.311336\n",
            "Epoch 3995: train loss = 2281.083481, test loss = 3568.279695\n",
            "Epoch 3996: train loss = 2281.021729, test loss = 3568.248056\n",
            "Epoch 3997: train loss = 2280.960016, test loss = 3568.216420\n",
            "Epoch 3998: train loss = 2280.898341, test loss = 3568.184785\n",
            "Epoch 3999: train loss = 2280.836705, test loss = 3568.153153\n",
            "Epoch 4000: train loss = 2280.775108, test loss = 3568.121524\n",
            "Epoch 4001: train loss = 2280.713548, test loss = 3568.089896\n",
            "Epoch 4002: train loss = 2280.652027, test loss = 3568.058271\n",
            "Epoch 4003: train loss = 2280.590544, test loss = 3568.026648\n",
            "Epoch 4004: train loss = 2280.529099, test loss = 3567.995027\n",
            "Epoch 4005: train loss = 2280.467692, test loss = 3567.963409\n",
            "Epoch 4006: train loss = 2280.406323, test loss = 3567.931793\n",
            "Epoch 4007: train loss = 2280.344992, test loss = 3567.900179\n",
            "Epoch 4008: train loss = 2280.283699, test loss = 3567.868567\n",
            "Epoch 4009: train loss = 2280.222444, test loss = 3567.836958\n",
            "Epoch 4010: train loss = 2280.161226, test loss = 3567.805351\n",
            "Epoch 4011: train loss = 2280.100047, test loss = 3567.773747\n",
            "Epoch 4012: train loss = 2280.038904, test loss = 3567.742145\n",
            "Epoch 4013: train loss = 2279.977800, test loss = 3567.710545\n",
            "Epoch 4014: train loss = 2279.916733, test loss = 3567.678947\n",
            "Epoch 4015: train loss = 2279.855703, test loss = 3567.647352\n",
            "Epoch 4016: train loss = 2279.794711, test loss = 3567.615759\n",
            "Epoch 4017: train loss = 2279.733756, test loss = 3567.584168\n",
            "Epoch 4018: train loss = 2279.672838, test loss = 3567.552580\n",
            "Epoch 4019: train loss = 2279.611958, test loss = 3567.520995\n",
            "Epoch 4020: train loss = 2279.551114, test loss = 3567.489411\n",
            "Epoch 4021: train loss = 2279.490308, test loss = 3567.457830\n",
            "Epoch 4022: train loss = 2279.429539, test loss = 3567.426252\n",
            "Epoch 4023: train loss = 2279.368806, test loss = 3567.394675\n",
            "Epoch 4024: train loss = 2279.308111, test loss = 3567.363101\n",
            "Epoch 4025: train loss = 2279.247452, test loss = 3567.331530\n",
            "Epoch 4026: train loss = 2279.186831, test loss = 3567.299961\n",
            "Epoch 4027: train loss = 2279.126246, test loss = 3567.268394\n",
            "Epoch 4028: train loss = 2279.065697, test loss = 3567.236830\n",
            "Epoch 4029: train loss = 2279.005185, test loss = 3567.205268\n",
            "Epoch 4030: train loss = 2278.944710, test loss = 3567.173709\n",
            "Epoch 4031: train loss = 2278.884271, test loss = 3567.142152\n",
            "Epoch 4032: train loss = 2278.823869, test loss = 3567.110598\n",
            "Epoch 4033: train loss = 2278.763503, test loss = 3567.079046\n",
            "Epoch 4034: train loss = 2278.703174, test loss = 3567.047496\n",
            "Epoch 4035: train loss = 2278.642880, test loss = 3567.015949\n",
            "Epoch 4036: train loss = 2278.582623, test loss = 3566.984404\n",
            "Epoch 4037: train loss = 2278.522402, test loss = 3566.952862\n",
            "Epoch 4038: train loss = 2278.462217, test loss = 3566.921323\n",
            "Epoch 4039: train loss = 2278.402068, test loss = 3566.889785\n",
            "Epoch 4040: train loss = 2278.341955, test loss = 3566.858251\n",
            "Epoch 4041: train loss = 2278.281878, test loss = 3566.826719\n",
            "Epoch 4042: train loss = 2278.221837, test loss = 3566.795189\n",
            "Epoch 4043: train loss = 2278.161832, test loss = 3566.763662\n",
            "Epoch 4044: train loss = 2278.101862, test loss = 3566.732137\n",
            "Epoch 4045: train loss = 2278.041928, test loss = 3566.700615\n",
            "Epoch 4046: train loss = 2277.982030, test loss = 3566.669095\n",
            "Epoch 4047: train loss = 2277.922167, test loss = 3566.637578\n",
            "Epoch 4048: train loss = 2277.862340, test loss = 3566.606064\n",
            "Epoch 4049: train loss = 2277.802548, test loss = 3566.574551\n",
            "Epoch 4050: train loss = 2277.742791, test loss = 3566.543042\n",
            "Epoch 4051: train loss = 2277.683070, test loss = 3566.511535\n",
            "Epoch 4052: train loss = 2277.623384, test loss = 3566.480031\n",
            "Epoch 4053: train loss = 2277.563734, test loss = 3566.448529\n",
            "Epoch 4054: train loss = 2277.504119, test loss = 3566.417030\n",
            "Epoch 4055: train loss = 2277.444538, test loss = 3566.385533\n",
            "Epoch 4056: train loss = 2277.384993, test loss = 3566.354039\n",
            "Epoch 4057: train loss = 2277.325483, test loss = 3566.322547\n",
            "Epoch 4058: train loss = 2277.266008, test loss = 3566.291058\n",
            "Epoch 4059: train loss = 2277.206567, test loss = 3566.259572\n",
            "Epoch 4060: train loss = 2277.147162, test loss = 3566.228088\n",
            "Epoch 4061: train loss = 2277.087791, test loss = 3566.196607\n",
            "Epoch 4062: train loss = 2277.028455, test loss = 3566.165128\n",
            "Epoch 4063: train loss = 2276.969154, test loss = 3566.133652\n",
            "Epoch 4064: train loss = 2276.909887, test loss = 3566.102179\n",
            "Epoch 4065: train loss = 2276.850655, test loss = 3566.070708\n",
            "Epoch 4066: train loss = 2276.791457, test loss = 3566.039240\n",
            "Epoch 4067: train loss = 2276.732294, test loss = 3566.007775\n",
            "Epoch 4068: train loss = 2276.673166, test loss = 3565.976312\n",
            "Epoch 4069: train loss = 2276.614071, test loss = 3565.944852\n",
            "Epoch 4070: train loss = 2276.555011, test loss = 3565.913394\n",
            "Epoch 4071: train loss = 2276.495985, test loss = 3565.881940\n",
            "Epoch 4072: train loss = 2276.436994, test loss = 3565.850487\n",
            "Epoch 4073: train loss = 2276.378036, test loss = 3565.819038\n",
            "Epoch 4074: train loss = 2276.319113, test loss = 3565.787591\n",
            "Epoch 4075: train loss = 2276.260223, test loss = 3565.756147\n",
            "Epoch 4076: train loss = 2276.201368, test loss = 3565.724705\n",
            "Epoch 4077: train loss = 2276.142546, test loss = 3565.693266\n",
            "Epoch 4078: train loss = 2276.083759, test loss = 3565.661830\n",
            "Epoch 4079: train loss = 2276.025005, test loss = 3565.630397\n",
            "Epoch 4080: train loss = 2275.966285, test loss = 3565.598966\n",
            "Epoch 4081: train loss = 2275.907598, test loss = 3565.567538\n",
            "Epoch 4082: train loss = 2275.848946, test loss = 3565.536113\n",
            "Epoch 4083: train loss = 2275.790326, test loss = 3565.504690\n",
            "Epoch 4084: train loss = 2275.731741, test loss = 3565.473270\n",
            "Epoch 4085: train loss = 2275.673189, test loss = 3565.441853\n",
            "Epoch 4086: train loss = 2275.614670, test loss = 3565.410439\n",
            "Epoch 4087: train loss = 2275.556185, test loss = 3565.379027\n",
            "Epoch 4088: train loss = 2275.497733, test loss = 3565.347618\n",
            "Epoch 4089: train loss = 2275.439315, test loss = 3565.316212\n",
            "Epoch 4090: train loss = 2275.380929, test loss = 3565.284808\n",
            "Epoch 4091: train loss = 2275.322577, test loss = 3565.253408\n",
            "Epoch 4092: train loss = 2275.264258, test loss = 3565.222010\n",
            "Epoch 4093: train loss = 2275.205972, test loss = 3565.190614\n",
            "Epoch 4094: train loss = 2275.147719, test loss = 3565.159222\n",
            "Epoch 4095: train loss = 2275.089499, test loss = 3565.127832\n",
            "Epoch 4096: train loss = 2275.031313, test loss = 3565.096445\n",
            "Epoch 4097: train loss = 2274.973158, test loss = 3565.065061\n",
            "Epoch 4098: train loss = 2274.915037, test loss = 3565.033680\n",
            "Epoch 4099: train loss = 2274.856949, test loss = 3565.002301\n",
            "Epoch 4100: train loss = 2274.798893, test loss = 3564.970926\n",
            "Epoch 4101: train loss = 2274.740870, test loss = 3564.939553\n",
            "Epoch 4102: train loss = 2274.682879, test loss = 3564.908183\n",
            "Epoch 4103: train loss = 2274.624922, test loss = 3564.876815\n",
            "Epoch 4104: train loss = 2274.566996, test loss = 3564.845451\n",
            "Epoch 4105: train loss = 2274.509103, test loss = 3564.814089\n",
            "Epoch 4106: train loss = 2274.451243, test loss = 3564.782730\n",
            "Epoch 4107: train loss = 2274.393415, test loss = 3564.751374\n",
            "Epoch 4108: train loss = 2274.335619, test loss = 3564.720021\n",
            "Epoch 4109: train loss = 2274.277856, test loss = 3564.688670\n",
            "Epoch 4110: train loss = 2274.220124, test loss = 3564.657323\n",
            "Epoch 4111: train loss = 2274.162425, test loss = 3564.625978\n",
            "Epoch 4112: train loss = 2274.104758, test loss = 3564.594636\n",
            "Epoch 4113: train loss = 2274.047123, test loss = 3564.563297\n",
            "Epoch 4114: train loss = 2273.989521, test loss = 3564.531961\n",
            "Epoch 4115: train loss = 2273.931950, test loss = 3564.500628\n",
            "Epoch 4116: train loss = 2273.874411, test loss = 3564.469298\n",
            "Epoch 4117: train loss = 2273.816904, test loss = 3564.437970\n",
            "Epoch 4118: train loss = 2273.759429, test loss = 3564.406645\n",
            "Epoch 4119: train loss = 2273.701985, test loss = 3564.375324\n",
            "Epoch 4120: train loss = 2273.644573, test loss = 3564.344005\n",
            "Epoch 4121: train loss = 2273.587193, test loss = 3564.312689\n",
            "Epoch 4122: train loss = 2273.529845, test loss = 3564.281376\n",
            "Epoch 4123: train loss = 2273.472528, test loss = 3564.250066\n",
            "Epoch 4124: train loss = 2273.415243, test loss = 3564.218758\n",
            "Epoch 4125: train loss = 2273.357989, test loss = 3564.187454\n",
            "Epoch 4126: train loss = 2273.300767, test loss = 3564.156152\n",
            "Epoch 4127: train loss = 2273.243576, test loss = 3564.124854\n",
            "Epoch 4128: train loss = 2273.186417, test loss = 3564.093558\n",
            "Epoch 4129: train loss = 2273.129288, test loss = 3564.062265\n",
            "Epoch 4130: train loss = 2273.072191, test loss = 3564.030976\n",
            "Epoch 4131: train loss = 2273.015126, test loss = 3563.999689\n",
            "Epoch 4132: train loss = 2272.958091, test loss = 3563.968405\n",
            "Epoch 4133: train loss = 2272.901087, test loss = 3563.937124\n",
            "Epoch 4134: train loss = 2272.844115, test loss = 3563.905846\n",
            "Epoch 4135: train loss = 2272.787174, test loss = 3563.874571\n",
            "Epoch 4136: train loss = 2272.730263, test loss = 3563.843299\n",
            "Epoch 4137: train loss = 2272.673384, test loss = 3563.812030\n",
            "Epoch 4138: train loss = 2272.616535, test loss = 3563.780763\n",
            "Epoch 4139: train loss = 2272.559717, test loss = 3563.749500\n",
            "Epoch 4140: train loss = 2272.502930, test loss = 3563.718240\n",
            "Epoch 4141: train loss = 2272.446174, test loss = 3563.686983\n",
            "Epoch 4142: train loss = 2272.389448, test loss = 3563.655728\n",
            "Epoch 4143: train loss = 2272.332753, test loss = 3563.624477\n",
            "Epoch 4144: train loss = 2272.276089, test loss = 3563.593229\n",
            "Epoch 4145: train loss = 2272.219455, test loss = 3563.561983\n",
            "Epoch 4146: train loss = 2272.162851, test loss = 3563.530741\n",
            "Epoch 4147: train loss = 2272.106279, test loss = 3563.499502\n",
            "Epoch 4148: train loss = 2272.049736, test loss = 3563.468265\n",
            "Epoch 4149: train loss = 2271.993224, test loss = 3563.437032\n",
            "Epoch 4150: train loss = 2271.936742, test loss = 3563.405802\n",
            "Epoch 4151: train loss = 2271.880291, test loss = 3563.374575\n",
            "Epoch 4152: train loss = 2271.823869, test loss = 3563.343350\n",
            "Epoch 4153: train loss = 2271.767478, test loss = 3563.312129\n",
            "Epoch 4154: train loss = 2271.711117, test loss = 3563.280911\n",
            "Epoch 4155: train loss = 2271.654786, test loss = 3563.249696\n",
            "Epoch 4156: train loss = 2271.598486, test loss = 3563.218483\n",
            "Epoch 4157: train loss = 2271.542215, test loss = 3563.187274\n",
            "Epoch 4158: train loss = 2271.485974, test loss = 3563.156068\n",
            "Epoch 4159: train loss = 2271.429763, test loss = 3563.124865\n",
            "Epoch 4160: train loss = 2271.373582, test loss = 3563.093665\n",
            "Epoch 4161: train loss = 2271.317431, test loss = 3563.062468\n",
            "Epoch 4162: train loss = 2271.261309, test loss = 3563.031275\n",
            "Epoch 4163: train loss = 2271.205217, test loss = 3563.000084\n",
            "Epoch 4164: train loss = 2271.149155, test loss = 3562.968896\n",
            "Epoch 4165: train loss = 2271.093123, test loss = 3562.937712\n",
            "Epoch 4166: train loss = 2271.037120, test loss = 3562.906530\n",
            "Epoch 4167: train loss = 2270.981147, test loss = 3562.875351\n",
            "Epoch 4168: train loss = 2270.925203, test loss = 3562.844176\n",
            "Epoch 4169: train loss = 2270.869289, test loss = 3562.813004\n",
            "Epoch 4170: train loss = 2270.813404, test loss = 3562.781835\n",
            "Epoch 4171: train loss = 2270.757548, test loss = 3562.750668\n",
            "Epoch 4172: train loss = 2270.701722, test loss = 3562.719505\n",
            "Epoch 4173: train loss = 2270.645926, test loss = 3562.688346\n",
            "Epoch 4174: train loss = 2270.590158, test loss = 3562.657189\n",
            "Epoch 4175: train loss = 2270.534420, test loss = 3562.626035\n",
            "Epoch 4176: train loss = 2270.478710, test loss = 3562.594885\n",
            "Epoch 4177: train loss = 2270.423030, test loss = 3562.563737\n",
            "Epoch 4178: train loss = 2270.367379, test loss = 3562.532593\n",
            "Epoch 4179: train loss = 2270.311757, test loss = 3562.501452\n",
            "Epoch 4180: train loss = 2270.256164, test loss = 3562.470313\n",
            "Epoch 4181: train loss = 2270.200600, test loss = 3562.439179\n",
            "Epoch 4182: train loss = 2270.145065, test loss = 3562.408047\n",
            "Epoch 4183: train loss = 2270.089559, test loss = 3562.376918\n",
            "Epoch 4184: train loss = 2270.034081, test loss = 3562.345793\n",
            "Epoch 4185: train loss = 2269.978632, test loss = 3562.314670\n",
            "Epoch 4186: train loss = 2269.923212, test loss = 3562.283551\n",
            "Epoch 4187: train loss = 2269.867821, test loss = 3562.252435\n",
            "Epoch 4188: train loss = 2269.812458, test loss = 3562.221322\n",
            "Epoch 4189: train loss = 2269.757124, test loss = 3562.190212\n",
            "Epoch 4190: train loss = 2269.701819, test loss = 3562.159106\n",
            "Epoch 4191: train loss = 2269.646542, test loss = 3562.128002\n",
            "Epoch 4192: train loss = 2269.591293, test loss = 3562.096902\n",
            "Epoch 4193: train loss = 2269.536073, test loss = 3562.065805\n",
            "Epoch 4194: train loss = 2269.480882, test loss = 3562.034711\n",
            "Epoch 4195: train loss = 2269.425718, test loss = 3562.003621\n",
            "Epoch 4196: train loss = 2269.370583, test loss = 3561.972533\n",
            "Epoch 4197: train loss = 2269.315477, test loss = 3561.941449\n",
            "Epoch 4198: train loss = 2269.260398, test loss = 3561.910368\n",
            "Epoch 4199: train loss = 2269.205348, test loss = 3561.879290\n",
            "Epoch 4200: train loss = 2269.150326, test loss = 3561.848215\n",
            "Epoch 4201: train loss = 2269.095332, test loss = 3561.817144\n",
            "Epoch 4202: train loss = 2269.040366, test loss = 3561.786075\n",
            "Epoch 4203: train loss = 2268.985428, test loss = 3561.755010\n",
            "Epoch 4204: train loss = 2268.930518, test loss = 3561.723948\n",
            "Epoch 4205: train loss = 2268.875636, test loss = 3561.692890\n",
            "Epoch 4206: train loss = 2268.820781, test loss = 3561.661834\n",
            "Epoch 4207: train loss = 2268.765955, test loss = 3561.630782\n",
            "Epoch 4208: train loss = 2268.711157, test loss = 3561.599733\n",
            "Epoch 4209: train loss = 2268.656386, test loss = 3561.568688\n",
            "Epoch 4210: train loss = 2268.601643, test loss = 3561.537645\n",
            "Epoch 4211: train loss = 2268.546928, test loss = 3561.506606\n",
            "Epoch 4212: train loss = 2268.492240, test loss = 3561.475570\n",
            "Epoch 4213: train loss = 2268.437580, test loss = 3561.444537\n",
            "Epoch 4214: train loss = 2268.382948, test loss = 3561.413508\n",
            "Epoch 4215: train loss = 2268.328343, test loss = 3561.382482\n",
            "Epoch 4216: train loss = 2268.273765, test loss = 3561.351459\n",
            "Epoch 4217: train loss = 2268.219215, test loss = 3561.320439\n",
            "Epoch 4218: train loss = 2268.164693, test loss = 3561.289423\n",
            "Epoch 4219: train loss = 2268.110197, test loss = 3561.258409\n",
            "Epoch 4220: train loss = 2268.055730, test loss = 3561.227400\n",
            "Epoch 4221: train loss = 2268.001289, test loss = 3561.196393\n",
            "Epoch 4222: train loss = 2267.946876, test loss = 3561.165390\n",
            "Epoch 4223: train loss = 2267.892490, test loss = 3561.134390\n",
            "Epoch 4224: train loss = 2267.838131, test loss = 3561.103393\n",
            "Epoch 4225: train loss = 2267.783799, test loss = 3561.072399\n",
            "Epoch 4226: train loss = 2267.729494, test loss = 3561.041409\n",
            "Epoch 4227: train loss = 2267.675217, test loss = 3561.010422\n",
            "Epoch 4228: train loss = 2267.620966, test loss = 3560.979439\n",
            "Epoch 4229: train loss = 2267.566742, test loss = 3560.948459\n",
            "Epoch 4230: train loss = 2267.512546, test loss = 3560.917482\n",
            "Epoch 4231: train loss = 2267.458376, test loss = 3560.886508\n",
            "Epoch 4232: train loss = 2267.404233, test loss = 3560.855538\n",
            "Epoch 4233: train loss = 2267.350117, test loss = 3560.824571\n",
            "Epoch 4234: train loss = 2267.296028, test loss = 3560.793607\n",
            "Epoch 4235: train loss = 2267.241965, test loss = 3560.762647\n",
            "Epoch 4236: train loss = 2267.187929, test loss = 3560.731689\n",
            "Epoch 4237: train loss = 2267.133920, test loss = 3560.700736\n",
            "Epoch 4238: train loss = 2267.079937, test loss = 3560.669785\n",
            "Epoch 4239: train loss = 2267.025981, test loss = 3560.638838\n",
            "Epoch 4240: train loss = 2266.972052, test loss = 3560.607895\n",
            "Epoch 4241: train loss = 2266.918149, test loss = 3560.576954\n",
            "Epoch 4242: train loss = 2266.864273, test loss = 3560.546017\n",
            "Epoch 4243: train loss = 2266.810423, test loss = 3560.515084\n",
            "Epoch 4244: train loss = 2266.756599, test loss = 3560.484153\n",
            "Epoch 4245: train loss = 2266.702802, test loss = 3560.453226\n",
            "Epoch 4246: train loss = 2266.649031, test loss = 3560.422303\n",
            "Epoch 4247: train loss = 2266.595286, test loss = 3560.391383\n",
            "Epoch 4248: train loss = 2266.541568, test loss = 3560.360466\n",
            "Epoch 4249: train loss = 2266.487876, test loss = 3560.329552\n",
            "Epoch 4250: train loss = 2266.434210, test loss = 3560.298642\n",
            "Epoch 4251: train loss = 2266.380570, test loss = 3560.267735\n",
            "Epoch 4252: train loss = 2266.326956, test loss = 3560.236832\n",
            "Epoch 4253: train loss = 2266.273369, test loss = 3560.205932\n",
            "Epoch 4254: train loss = 2266.219807, test loss = 3560.175035\n",
            "Epoch 4255: train loss = 2266.166271, test loss = 3560.144142\n",
            "Epoch 4256: train loss = 2266.112762, test loss = 3560.113252\n",
            "Epoch 4257: train loss = 2266.059278, test loss = 3560.082366\n",
            "Epoch 4258: train loss = 2266.005820, test loss = 3560.051483\n",
            "Epoch 4259: train loss = 2265.952388, test loss = 3560.020603\n",
            "Epoch 4260: train loss = 2265.898982, test loss = 3559.989727\n",
            "Epoch 4261: train loss = 2265.845601, test loss = 3559.958854\n",
            "Epoch 4262: train loss = 2265.792246, test loss = 3559.927985\n",
            "Epoch 4263: train loss = 2265.738917, test loss = 3559.897119\n",
            "Epoch 4264: train loss = 2265.685614, test loss = 3559.866256\n",
            "Epoch 4265: train loss = 2265.632336, test loss = 3559.835397\n",
            "Epoch 4266: train loss = 2265.579084, test loss = 3559.804541\n",
            "Epoch 4267: train loss = 2265.525857, test loss = 3559.773689\n",
            "Epoch 4268: train loss = 2265.472656, test loss = 3559.742840\n",
            "Epoch 4269: train loss = 2265.419480, test loss = 3559.711995\n",
            "Epoch 4270: train loss = 2265.366330, test loss = 3559.681152\n",
            "Epoch 4271: train loss = 2265.313205, test loss = 3559.650314\n",
            "Epoch 4272: train loss = 2265.260106, test loss = 3559.619479\n",
            "Epoch 4273: train loss = 2265.207031, test loss = 3559.588647\n",
            "Epoch 4274: train loss = 2265.153983, test loss = 3559.557819\n",
            "Epoch 4275: train loss = 2265.100959, test loss = 3559.526994\n",
            "Epoch 4276: train loss = 2265.047961, test loss = 3559.496173\n",
            "Epoch 4277: train loss = 2264.994987, test loss = 3559.465355\n",
            "Epoch 4278: train loss = 2264.942039, test loss = 3559.434540\n",
            "Epoch 4279: train loss = 2264.889116, test loss = 3559.403729\n",
            "Epoch 4280: train loss = 2264.836219, test loss = 3559.372922\n",
            "Epoch 4281: train loss = 2264.783346, test loss = 3559.342118\n",
            "Epoch 4282: train loss = 2264.730498, test loss = 3559.311317\n",
            "Epoch 4283: train loss = 2264.677675, test loss = 3559.280520\n",
            "Epoch 4284: train loss = 2264.624877, test loss = 3559.249726\n",
            "Epoch 4285: train loss = 2264.572104, test loss = 3559.218936\n",
            "Epoch 4286: train loss = 2264.519356, test loss = 3559.188150\n",
            "Epoch 4287: train loss = 2264.466633, test loss = 3559.157366\n",
            "Epoch 4288: train loss = 2264.413935, test loss = 3559.126587\n",
            "Epoch 4289: train loss = 2264.361261, test loss = 3559.095810\n",
            "Epoch 4290: train loss = 2264.308612, test loss = 3559.065038\n",
            "Epoch 4291: train loss = 2264.255988, test loss = 3559.034269\n",
            "Epoch 4292: train loss = 2264.203388, test loss = 3559.003503\n",
            "Epoch 4293: train loss = 2264.150813, test loss = 3558.972741\n",
            "Epoch 4294: train loss = 2264.098263, test loss = 3558.941982\n",
            "Epoch 4295: train loss = 2264.045737, test loss = 3558.911227\n",
            "Epoch 4296: train loss = 2263.993236, test loss = 3558.880475\n",
            "Epoch 4297: train loss = 2263.940759, test loss = 3558.849727\n",
            "Epoch 4298: train loss = 2263.888306, test loss = 3558.818982\n",
            "Epoch 4299: train loss = 2263.835879, test loss = 3558.788241\n",
            "Epoch 4300: train loss = 2263.783475, test loss = 3558.757503\n",
            "Epoch 4301: train loss = 2263.731096, test loss = 3558.726769\n",
            "Epoch 4302: train loss = 2263.678741, test loss = 3558.696039\n",
            "Epoch 4303: train loss = 2263.626410, test loss = 3558.665312\n",
            "Epoch 4304: train loss = 2263.574104, test loss = 3558.634588\n",
            "Epoch 4305: train loss = 2263.521822, test loss = 3558.603868\n",
            "Epoch 4306: train loss = 2263.469564, test loss = 3558.573152\n",
            "Epoch 4307: train loss = 2263.417330, test loss = 3558.542439\n",
            "Epoch 4308: train loss = 2263.365120, test loss = 3558.511730\n",
            "Epoch 4309: train loss = 2263.312935, test loss = 3558.481024\n",
            "Epoch 4310: train loss = 2263.260773, test loss = 3558.450322\n",
            "Epoch 4311: train loss = 2263.208636, test loss = 3558.419623\n",
            "Epoch 4312: train loss = 2263.156522, test loss = 3558.388928\n",
            "Epoch 4313: train loss = 2263.104433, test loss = 3558.358236\n",
            "Epoch 4314: train loss = 2263.052367, test loss = 3558.327548\n",
            "Epoch 4315: train loss = 2263.000325, test loss = 3558.296864\n",
            "Epoch 4316: train loss = 2262.948307, test loss = 3558.266183\n",
            "Epoch 4317: train loss = 2262.896313, test loss = 3558.235506\n",
            "Epoch 4318: train loss = 2262.844343, test loss = 3558.204832\n",
            "Epoch 4319: train loss = 2262.792396, test loss = 3558.174162\n",
            "Epoch 4320: train loss = 2262.740473, test loss = 3558.143495\n",
            "Epoch 4321: train loss = 2262.688574, test loss = 3558.112832\n",
            "Epoch 4322: train loss = 2262.636699, test loss = 3558.082173\n",
            "Epoch 4323: train loss = 2262.584847, test loss = 3558.051517\n",
            "Epoch 4324: train loss = 2262.533018, test loss = 3558.020865\n",
            "Epoch 4325: train loss = 2262.481214, test loss = 3557.990216\n",
            "Epoch 4326: train loss = 2262.429432, test loss = 3557.959571\n",
            "Epoch 4327: train loss = 2262.377675, test loss = 3557.928929\n",
            "Epoch 4328: train loss = 2262.325940, test loss = 3557.898292\n",
            "Epoch 4329: train loss = 2262.274229, test loss = 3557.867657\n",
            "Epoch 4330: train loss = 2262.222542, test loss = 3557.837027\n",
            "Epoch 4331: train loss = 2262.170878, test loss = 3557.806400\n",
            "Epoch 4332: train loss = 2262.119237, test loss = 3557.775776\n",
            "Epoch 4333: train loss = 2262.067619, test loss = 3557.745156\n",
            "Epoch 4334: train loss = 2262.016025, test loss = 3557.714540\n",
            "Epoch 4335: train loss = 2261.964454, test loss = 3557.683928\n",
            "Epoch 4336: train loss = 2261.912906, test loss = 3557.653319\n",
            "Epoch 4337: train loss = 2261.861382, test loss = 3557.622713\n",
            "Epoch 4338: train loss = 2261.809880, test loss = 3557.592112\n",
            "Epoch 4339: train loss = 2261.758402, test loss = 3557.561514\n",
            "Epoch 4340: train loss = 2261.706947, test loss = 3557.530919\n",
            "Epoch 4341: train loss = 2261.655514, test loss = 3557.500328\n",
            "Epoch 4342: train loss = 2261.604105, test loss = 3557.469741\n",
            "Epoch 4343: train loss = 2261.552719, test loss = 3557.439158\n",
            "Epoch 4344: train loss = 2261.501355, test loss = 3557.408578\n",
            "Epoch 4345: train loss = 2261.450015, test loss = 3557.378002\n",
            "Epoch 4346: train loss = 2261.398697, test loss = 3557.347429\n",
            "Epoch 4347: train loss = 2261.347403, test loss = 3557.316860\n",
            "Epoch 4348: train loss = 2261.296131, test loss = 3557.286295\n",
            "Epoch 4349: train loss = 2261.244882, test loss = 3557.255733\n",
            "Epoch 4350: train loss = 2261.193656, test loss = 3557.225175\n",
            "Epoch 4351: train loss = 2261.142452, test loss = 3557.194621\n",
            "Epoch 4352: train loss = 2261.091271, test loss = 3557.164070\n",
            "Epoch 4353: train loss = 2261.040113, test loss = 3557.133523\n",
            "Epoch 4354: train loss = 2260.988977, test loss = 3557.102980\n",
            "Epoch 4355: train loss = 2260.937864, test loss = 3557.072440\n",
            "Epoch 4356: train loss = 2260.886774, test loss = 3557.041904\n",
            "Epoch 4357: train loss = 2260.835706, test loss = 3557.011372\n",
            "Epoch 4358: train loss = 2260.784661, test loss = 3556.980843\n",
            "Epoch 4359: train loss = 2260.733638, test loss = 3556.950318\n",
            "Epoch 4360: train loss = 2260.682638, test loss = 3556.919797\n",
            "Epoch 4361: train loss = 2260.631660, test loss = 3556.889279\n",
            "Epoch 4362: train loss = 2260.580704, test loss = 3556.858766\n",
            "Epoch 4363: train loss = 2260.529771, test loss = 3556.828255\n",
            "Epoch 4364: train loss = 2260.478860, test loss = 3556.797749\n",
            "Epoch 4365: train loss = 2260.427971, test loss = 3556.767246\n",
            "Epoch 4366: train loss = 2260.377105, test loss = 3556.736747\n",
            "Epoch 4367: train loss = 2260.326261, test loss = 3556.706252\n",
            "Epoch 4368: train loss = 2260.275439, test loss = 3556.675760\n",
            "Epoch 4369: train loss = 2260.224639, test loss = 3556.645272\n",
            "Epoch 4370: train loss = 2260.173862, test loss = 3556.614788\n",
            "Epoch 4371: train loss = 2260.123107, test loss = 3556.584307\n",
            "Epoch 4372: train loss = 2260.072373, test loss = 3556.553830\n",
            "Epoch 4373: train loss = 2260.021662, test loss = 3556.523357\n",
            "Epoch 4374: train loss = 2259.970973, test loss = 3556.492888\n",
            "Epoch 4375: train loss = 2259.920306, test loss = 3556.462422\n",
            "Epoch 4376: train loss = 2259.869660, test loss = 3556.431960\n",
            "Epoch 4377: train loss = 2259.819037, test loss = 3556.401502\n",
            "Epoch 4378: train loss = 2259.768436, test loss = 3556.371047\n",
            "Epoch 4379: train loss = 2259.717856, test loss = 3556.340597\n",
            "Epoch 4380: train loss = 2259.667298, test loss = 3556.310150\n",
            "Epoch 4381: train loss = 2259.616763, test loss = 3556.279706\n",
            "Epoch 4382: train loss = 2259.566249, test loss = 3556.249267\n",
            "Epoch 4383: train loss = 2259.515756, test loss = 3556.218831\n",
            "Epoch 4384: train loss = 2259.465286, test loss = 3556.188399\n",
            "Epoch 4385: train loss = 2259.414837, test loss = 3556.157970\n",
            "Epoch 4386: train loss = 2259.364410, test loss = 3556.127546\n",
            "Epoch 4387: train loss = 2259.314004, test loss = 3556.097125\n",
            "Epoch 4388: train loss = 2259.263621, test loss = 3556.066708\n",
            "Epoch 4389: train loss = 2259.213258, test loss = 3556.036295\n",
            "Epoch 4390: train loss = 2259.162918, test loss = 3556.005885\n",
            "Epoch 4391: train loss = 2259.112598, test loss = 3555.975479\n",
            "Epoch 4392: train loss = 2259.062301, test loss = 3555.945077\n",
            "Epoch 4393: train loss = 2259.012025, test loss = 3555.914679\n",
            "Epoch 4394: train loss = 2258.961770, test loss = 3555.884284\n",
            "Epoch 4395: train loss = 2258.911537, test loss = 3555.853894\n",
            "Epoch 4396: train loss = 2258.861325, test loss = 3555.823507\n",
            "Epoch 4397: train loss = 2258.811134, test loss = 3555.793123\n",
            "Epoch 4398: train loss = 2258.760965, test loss = 3555.762744\n",
            "Epoch 4399: train loss = 2258.710817, test loss = 3555.732368\n",
            "Epoch 4400: train loss = 2258.660690, test loss = 3555.701997\n",
            "Epoch 4401: train loss = 2258.610585, test loss = 3555.671629\n",
            "Epoch 4402: train loss = 2258.560501, test loss = 3555.641264\n",
            "Epoch 4403: train loss = 2258.510438, test loss = 3555.610904\n",
            "Epoch 4404: train loss = 2258.460396, test loss = 3555.580547\n",
            "Epoch 4405: train loss = 2258.410375, test loss = 3555.550194\n",
            "Epoch 4406: train loss = 2258.360376, test loss = 3555.519845\n",
            "Epoch 4407: train loss = 2258.310397, test loss = 3555.489500\n",
            "Epoch 4408: train loss = 2258.260440, test loss = 3555.459158\n",
            "Epoch 4409: train loss = 2258.210504, test loss = 3555.428821\n",
            "Epoch 4410: train loss = 2258.160588, test loss = 3555.398487\n",
            "Epoch 4411: train loss = 2258.110694, test loss = 3555.368157\n",
            "Epoch 4412: train loss = 2258.060820, test loss = 3555.337830\n",
            "Epoch 4413: train loss = 2258.010968, test loss = 3555.307508\n",
            "Epoch 4414: train loss = 2257.961136, test loss = 3555.277189\n",
            "Epoch 4415: train loss = 2257.911325, test loss = 3555.246875\n",
            "Epoch 4416: train loss = 2257.861535, test loss = 3555.216564\n",
            "Epoch 4417: train loss = 2257.811766, test loss = 3555.186256\n",
            "Epoch 4418: train loss = 2257.762018, test loss = 3555.155953\n",
            "Epoch 4419: train loss = 2257.712290, test loss = 3555.125653\n",
            "Epoch 4420: train loss = 2257.662583, test loss = 3555.095358\n",
            "Epoch 4421: train loss = 2257.612897, test loss = 3555.065066\n",
            "Epoch 4422: train loss = 2257.563231, test loss = 3555.034778\n",
            "Epoch 4423: train loss = 2257.513586, test loss = 3555.004494\n",
            "Epoch 4424: train loss = 2257.463962, test loss = 3554.974213\n",
            "Epoch 4425: train loss = 2257.414358, test loss = 3554.943937\n",
            "Epoch 4426: train loss = 2257.364775, test loss = 3554.913664\n",
            "Epoch 4427: train loss = 2257.315212, test loss = 3554.883395\n",
            "Epoch 4428: train loss = 2257.265670, test loss = 3554.853130\n",
            "Epoch 4429: train loss = 2257.216149, test loss = 3554.822869\n",
            "Epoch 4430: train loss = 2257.166647, test loss = 3554.792612\n",
            "Epoch 4431: train loss = 2257.117167, test loss = 3554.762359\n",
            "Epoch 4432: train loss = 2257.067706, test loss = 3554.732109\n",
            "Epoch 4433: train loss = 2257.018266, test loss = 3554.701863\n",
            "Epoch 4434: train loss = 2256.968847, test loss = 3554.671622\n",
            "Epoch 4435: train loss = 2256.919447, test loss = 3554.641384\n",
            "Epoch 4436: train loss = 2256.870068, test loss = 3554.611150\n",
            "Epoch 4437: train loss = 2256.820709, test loss = 3554.580919\n",
            "Epoch 4438: train loss = 2256.771371, test loss = 3554.550693\n",
            "Epoch 4439: train loss = 2256.722052, test loss = 3554.520471\n",
            "Epoch 4440: train loss = 2256.672754, test loss = 3554.490252\n",
            "Epoch 4441: train loss = 2256.623476, test loss = 3554.460037\n",
            "Epoch 4442: train loss = 2256.574218, test loss = 3554.429826\n",
            "Epoch 4443: train loss = 2256.524981, test loss = 3554.399619\n",
            "Epoch 4444: train loss = 2256.475763, test loss = 3554.369416\n",
            "Epoch 4445: train loss = 2256.426565, test loss = 3554.339217\n",
            "Epoch 4446: train loss = 2256.377388, test loss = 3554.309022\n",
            "Epoch 4447: train loss = 2256.328230, test loss = 3554.278830\n",
            "Epoch 4448: train loss = 2256.279092, test loss = 3554.248643\n",
            "Epoch 4449: train loss = 2256.229975, test loss = 3554.218459\n",
            "Epoch 4450: train loss = 2256.180877, test loss = 3554.188280\n",
            "Epoch 4451: train loss = 2256.131799, test loss = 3554.158104\n",
            "Epoch 4452: train loss = 2256.082741, test loss = 3554.127932\n",
            "Epoch 4453: train loss = 2256.033703, test loss = 3554.097764\n",
            "Epoch 4454: train loss = 2255.984685, test loss = 3554.067600\n",
            "Epoch 4455: train loss = 2255.935686, test loss = 3554.037440\n",
            "Epoch 4456: train loss = 2255.886707, test loss = 3554.007283\n",
            "Epoch 4457: train loss = 2255.837748, test loss = 3553.977131\n",
            "Epoch 4458: train loss = 2255.788809, test loss = 3553.946983\n",
            "Epoch 4459: train loss = 2255.739890, test loss = 3553.916838\n",
            "Epoch 4460: train loss = 2255.690990, test loss = 3553.886697\n",
            "Epoch 4461: train loss = 2255.642109, test loss = 3553.856561\n",
            "Epoch 4462: train loss = 2255.593249, test loss = 3553.826428\n",
            "Epoch 4463: train loss = 2255.544408, test loss = 3553.796299\n",
            "Epoch 4464: train loss = 2255.495586, test loss = 3553.766174\n",
            "Epoch 4465: train loss = 2255.446784, test loss = 3553.736053\n",
            "Epoch 4466: train loss = 2255.398002, test loss = 3553.705936\n",
            "Epoch 4467: train loss = 2255.349239, test loss = 3553.675823\n",
            "Epoch 4468: train loss = 2255.300495, test loss = 3553.645714\n",
            "Epoch 4469: train loss = 2255.251771, test loss = 3553.615609\n",
            "Epoch 4470: train loss = 2255.203066, test loss = 3553.585508\n",
            "Epoch 4471: train loss = 2255.154381, test loss = 3553.555410\n",
            "Epoch 4472: train loss = 2255.105715, test loss = 3553.525317\n",
            "Epoch 4473: train loss = 2255.057069, test loss = 3553.495228\n",
            "Epoch 4474: train loss = 2255.008442, test loss = 3553.465142\n",
            "Epoch 4475: train loss = 2254.959834, test loss = 3553.435061\n",
            "Epoch 4476: train loss = 2254.911245, test loss = 3553.404983\n",
            "Epoch 4477: train loss = 2254.862675, test loss = 3553.374909\n",
            "Epoch 4478: train loss = 2254.814125, test loss = 3553.344840\n",
            "Epoch 4479: train loss = 2254.765594, test loss = 3553.314774\n",
            "Epoch 4480: train loss = 2254.717082, test loss = 3553.284712\n",
            "Epoch 4481: train loss = 2254.668590, test loss = 3553.254655\n",
            "Epoch 4482: train loss = 2254.620116, test loss = 3553.224601\n",
            "Epoch 4483: train loss = 2254.571662, test loss = 3553.194551\n",
            "Epoch 4484: train loss = 2254.523226, test loss = 3553.164505\n",
            "Epoch 4485: train loss = 2254.474810, test loss = 3553.134463\n",
            "Epoch 4486: train loss = 2254.426412, test loss = 3553.104426\n",
            "Epoch 4487: train loss = 2254.378034, test loss = 3553.074392\n",
            "Epoch 4488: train loss = 2254.329675, test loss = 3553.044362\n",
            "Epoch 4489: train loss = 2254.281334, test loss = 3553.014336\n",
            "Epoch 4490: train loss = 2254.233013, test loss = 3552.984314\n",
            "Epoch 4491: train loss = 2254.184710, test loss = 3552.954296\n",
            "Epoch 4492: train loss = 2254.136426, test loss = 3552.924282\n",
            "Epoch 4493: train loss = 2254.088161, test loss = 3552.894272\n",
            "Epoch 4494: train loss = 2254.039915, test loss = 3552.864266\n",
            "Epoch 4495: train loss = 2253.991688, test loss = 3552.834264\n",
            "Epoch 4496: train loss = 2253.943479, test loss = 3552.804266\n",
            "Epoch 4497: train loss = 2253.895290, test loss = 3552.774272\n",
            "Epoch 4498: train loss = 2253.847119, test loss = 3552.744282\n",
            "Epoch 4499: train loss = 2253.798966, test loss = 3552.714296\n",
            "Epoch 4500: train loss = 2253.750833, test loss = 3552.684314\n",
            "Epoch 4501: train loss = 2253.702718, test loss = 3552.654336\n",
            "Epoch 4502: train loss = 2253.654621, test loss = 3552.624362\n",
            "Epoch 4503: train loss = 2253.606544, test loss = 3552.594393\n",
            "Epoch 4504: train loss = 2253.558484, test loss = 3552.564427\n",
            "Epoch 4505: train loss = 2253.510444, test loss = 3552.534465\n",
            "Epoch 4506: train loss = 2253.462422, test loss = 3552.504507\n",
            "Epoch 4507: train loss = 2253.414418, test loss = 3552.474553\n",
            "Epoch 4508: train loss = 2253.366433, test loss = 3552.444603\n",
            "Epoch 4509: train loss = 2253.318467, test loss = 3552.414658\n",
            "Epoch 4510: train loss = 2253.270519, test loss = 3552.384716\n",
            "Epoch 4511: train loss = 2253.222589, test loss = 3552.354778\n",
            "Epoch 4512: train loss = 2253.174678, test loss = 3552.324844\n",
            "Epoch 4513: train loss = 2253.126785, test loss = 3552.294915\n",
            "Epoch 4514: train loss = 2253.078910, test loss = 3552.264989\n",
            "Epoch 4515: train loss = 2253.031054, test loss = 3552.235068\n",
            "Epoch 4516: train loss = 2252.983216, test loss = 3552.205150\n",
            "Epoch 4517: train loss = 2252.935397, test loss = 3552.175237\n",
            "Epoch 4518: train loss = 2252.887595, test loss = 3552.145327\n",
            "Epoch 4519: train loss = 2252.839812, test loss = 3552.115422\n",
            "Epoch 4520: train loss = 2252.792047, test loss = 3552.085520\n",
            "Epoch 4521: train loss = 2252.744301, test loss = 3552.055623\n",
            "Epoch 4522: train loss = 2252.696572, test loss = 3552.025730\n",
            "Epoch 4523: train loss = 2252.648862, test loss = 3551.995841\n",
            "Epoch 4524: train loss = 2252.601170, test loss = 3551.965956\n",
            "Epoch 4525: train loss = 2252.553495, test loss = 3551.936075\n",
            "Epoch 4526: train loss = 2252.505839, test loss = 3551.906198\n",
            "Epoch 4527: train loss = 2252.458201, test loss = 3551.876325\n",
            "Epoch 4528: train loss = 2252.410581, test loss = 3551.846456\n",
            "Epoch 4529: train loss = 2252.362979, test loss = 3551.816591\n",
            "Epoch 4530: train loss = 2252.315395, test loss = 3551.786731\n",
            "Epoch 4531: train loss = 2252.267829, test loss = 3551.756874\n",
            "Epoch 4532: train loss = 2252.220281, test loss = 3551.727021\n",
            "Epoch 4533: train loss = 2252.172751, test loss = 3551.697173\n",
            "Epoch 4534: train loss = 2252.125239, test loss = 3551.667328\n",
            "Epoch 4535: train loss = 2252.077745, test loss = 3551.637488\n",
            "Epoch 4536: train loss = 2252.030268, test loss = 3551.607652\n",
            "Epoch 4537: train loss = 2251.982809, test loss = 3551.577820\n",
            "Epoch 4538: train loss = 2251.935368, test loss = 3551.547992\n",
            "Epoch 4539: train loss = 2251.887945, test loss = 3551.518168\n",
            "Epoch 4540: train loss = 2251.840540, test loss = 3551.488348\n",
            "Epoch 4541: train loss = 2251.793153, test loss = 3551.458532\n",
            "Epoch 4542: train loss = 2251.745783, test loss = 3551.428721\n",
            "Epoch 4543: train loss = 2251.698430, test loss = 3551.398913\n",
            "Epoch 4544: train loss = 2251.651096, test loss = 3551.369110\n",
            "Epoch 4545: train loss = 2251.603779, test loss = 3551.339310\n",
            "Epoch 4546: train loss = 2251.556480, test loss = 3551.309515\n",
            "Epoch 4547: train loss = 2251.509198, test loss = 3551.279724\n",
            "Epoch 4548: train loss = 2251.461934, test loss = 3551.249937\n",
            "Epoch 4549: train loss = 2251.414688, test loss = 3551.220154\n",
            "Epoch 4550: train loss = 2251.367459, test loss = 3551.190375\n",
            "Epoch 4551: train loss = 2251.320248, test loss = 3551.160600\n",
            "Epoch 4552: train loss = 2251.273054, test loss = 3551.130830\n",
            "Epoch 4553: train loss = 2251.225878, test loss = 3551.101063\n",
            "Epoch 4554: train loss = 2251.178719, test loss = 3551.071301\n",
            "Epoch 4555: train loss = 2251.131577, test loss = 3551.041543\n",
            "Epoch 4556: train loss = 2251.084453, test loss = 3551.011789\n",
            "Epoch 4557: train loss = 2251.037346, test loss = 3550.982039\n",
            "Epoch 4558: train loss = 2250.990257, test loss = 3550.952293\n",
            "Epoch 4559: train loss = 2250.943185, test loss = 3550.922551\n",
            "Epoch 4560: train loss = 2250.896131, test loss = 3550.892814\n",
            "Epoch 4561: train loss = 2250.849093, test loss = 3550.863080\n",
            "Epoch 4562: train loss = 2250.802073, test loss = 3550.833351\n",
            "Epoch 4563: train loss = 2250.755070, test loss = 3550.803626\n",
            "Epoch 4564: train loss = 2250.708085, test loss = 3550.773905\n",
            "Epoch 4565: train loss = 2250.661117, test loss = 3550.744188\n",
            "Epoch 4566: train loss = 2250.614166, test loss = 3550.714475\n",
            "Epoch 4567: train loss = 2250.567232, test loss = 3550.684767\n",
            "Epoch 4568: train loss = 2250.520315, test loss = 3550.655062\n",
            "Epoch 4569: train loss = 2250.473415, test loss = 3550.625362\n",
            "Epoch 4570: train loss = 2250.426533, test loss = 3550.595666\n",
            "Epoch 4571: train loss = 2250.379667, test loss = 3550.565974\n",
            "Epoch 4572: train loss = 2250.332819, test loss = 3550.536286\n",
            "Epoch 4573: train loss = 2250.285988, test loss = 3550.506603\n",
            "Epoch 4574: train loss = 2250.239173, test loss = 3550.476923\n",
            "Epoch 4575: train loss = 2250.192376, test loss = 3550.447248\n",
            "Epoch 4576: train loss = 2250.145596, test loss = 3550.417577\n",
            "Epoch 4577: train loss = 2250.098833, test loss = 3550.387910\n",
            "Epoch 4578: train loss = 2250.052086, test loss = 3550.358247\n",
            "Epoch 4579: train loss = 2250.005357, test loss = 3550.328589\n",
            "Epoch 4580: train loss = 2249.958644, test loss = 3550.298934\n",
            "Epoch 4581: train loss = 2249.911949, test loss = 3550.269284\n",
            "Epoch 4582: train loss = 2249.865270, test loss = 3550.239638\n",
            "Epoch 4583: train loss = 2249.818608, test loss = 3550.209996\n",
            "Epoch 4584: train loss = 2249.771963, test loss = 3550.180358\n",
            "Epoch 4585: train loss = 2249.725335, test loss = 3550.150725\n",
            "Epoch 4586: train loss = 2249.678723, test loss = 3550.121095\n",
            "Epoch 4587: train loss = 2249.632128, test loss = 3550.091470\n",
            "Epoch 4588: train loss = 2249.585550, test loss = 3550.061849\n",
            "Epoch 4589: train loss = 2249.538989, test loss = 3550.032233\n",
            "Epoch 4590: train loss = 2249.492445, test loss = 3550.002620\n",
            "Epoch 4591: train loss = 2249.445917, test loss = 3549.973012\n",
            "Epoch 4592: train loss = 2249.399405, test loss = 3549.943408\n",
            "Epoch 4593: train loss = 2249.352911, test loss = 3549.913808\n",
            "Epoch 4594: train loss = 2249.306433, test loss = 3549.884212\n",
            "Epoch 4595: train loss = 2249.259971, test loss = 3549.854620\n",
            "Epoch 4596: train loss = 2249.213527, test loss = 3549.825033\n",
            "Epoch 4597: train loss = 2249.167098, test loss = 3549.795450\n",
            "Epoch 4598: train loss = 2249.120687, test loss = 3549.765871\n",
            "Epoch 4599: train loss = 2249.074292, test loss = 3549.736296\n",
            "Epoch 4600: train loss = 2249.027913, test loss = 3549.706726\n",
            "Epoch 4601: train loss = 2248.981551, test loss = 3549.677160\n",
            "Epoch 4602: train loss = 2248.935205, test loss = 3549.647598\n",
            "Epoch 4603: train loss = 2248.888876, test loss = 3549.618040\n",
            "Epoch 4604: train loss = 2248.842563, test loss = 3549.588486\n",
            "Epoch 4605: train loss = 2248.796266, test loss = 3549.558937\n",
            "Epoch 4606: train loss = 2248.749986, test loss = 3549.529392\n",
            "Epoch 4607: train loss = 2248.703722, test loss = 3549.499851\n",
            "Epoch 4608: train loss = 2248.657475, test loss = 3549.470314\n",
            "Epoch 4609: train loss = 2248.611244, test loss = 3549.440782\n",
            "Epoch 4610: train loss = 2248.565029, test loss = 3549.411254\n",
            "Epoch 4611: train loss = 2248.518830, test loss = 3549.381730\n",
            "Epoch 4612: train loss = 2248.472648, test loss = 3549.352210\n",
            "Epoch 4613: train loss = 2248.426482, test loss = 3549.322695\n",
            "Epoch 4614: train loss = 2248.380332, test loss = 3549.293183\n",
            "Epoch 4615: train loss = 2248.334199, test loss = 3549.263677\n",
            "Epoch 4616: train loss = 2248.288081, test loss = 3549.234174\n",
            "Epoch 4617: train loss = 2248.241980, test loss = 3549.204675\n",
            "Epoch 4618: train loss = 2248.195895, test loss = 3549.175181\n",
            "Epoch 4619: train loss = 2248.149826, test loss = 3549.145691\n",
            "Epoch 4620: train loss = 2248.103773, test loss = 3549.116206\n",
            "Epoch 4621: train loss = 2248.057736, test loss = 3549.086724\n",
            "Epoch 4622: train loss = 2248.011715, test loss = 3549.057247\n",
            "Epoch 4623: train loss = 2247.965710, test loss = 3549.027774\n",
            "Epoch 4624: train loss = 2247.919722, test loss = 3548.998305\n",
            "Epoch 4625: train loss = 2247.873749, test loss = 3548.968841\n",
            "Epoch 4626: train loss = 2247.827792, test loss = 3548.939381\n",
            "Epoch 4627: train loss = 2247.781851, test loss = 3548.909925\n",
            "Epoch 4628: train loss = 2247.735927, test loss = 3548.880474\n",
            "Epoch 4629: train loss = 2247.690018, test loss = 3548.851026\n",
            "Epoch 4630: train loss = 2247.644125, test loss = 3548.821583\n",
            "Epoch 4631: train loss = 2247.598248, test loss = 3548.792145\n",
            "Epoch 4632: train loss = 2247.552387, test loss = 3548.762710\n",
            "Epoch 4633: train loss = 2247.506541, test loss = 3548.733280\n",
            "Epoch 4634: train loss = 2247.460712, test loss = 3548.703854\n",
            "Epoch 4635: train loss = 2247.414898, test loss = 3548.674433\n",
            "Epoch 4636: train loss = 2247.369100, test loss = 3548.645015\n",
            "Epoch 4637: train loss = 2247.323318, test loss = 3548.615602\n",
            "Epoch 4638: train loss = 2247.277552, test loss = 3548.586194\n",
            "Epoch 4639: train loss = 2247.231801, test loss = 3548.556789\n",
            "Epoch 4640: train loss = 2247.186066, test loss = 3548.527389\n",
            "Epoch 4641: train loss = 2247.140347, test loss = 3548.497994\n",
            "Epoch 4642: train loss = 2247.094643, test loss = 3548.468602\n",
            "Epoch 4643: train loss = 2247.048955, test loss = 3548.439215\n",
            "Epoch 4644: train loss = 2247.003283, test loss = 3548.409832\n",
            "Epoch 4645: train loss = 2246.957626, test loss = 3548.380453\n",
            "Epoch 4646: train loss = 2246.911985, test loss = 3548.351079\n",
            "Epoch 4647: train loss = 2246.866360, test loss = 3548.321709\n",
            "Epoch 4648: train loss = 2246.820750, test loss = 3548.292344\n",
            "Epoch 4649: train loss = 2246.775156, test loss = 3548.262982\n",
            "Epoch 4650: train loss = 2246.729577, test loss = 3548.233625\n",
            "Epoch 4651: train loss = 2246.684014, test loss = 3548.204273\n",
            "Epoch 4652: train loss = 2246.638466, test loss = 3548.174924\n",
            "Epoch 4653: train loss = 2246.592934, test loss = 3548.145580\n",
            "Epoch 4654: train loss = 2246.547417, test loss = 3548.116241\n",
            "Epoch 4655: train loss = 2246.501915, test loss = 3548.086905\n",
            "Epoch 4656: train loss = 2246.456429, test loss = 3548.057574\n",
            "Epoch 4657: train loss = 2246.410959, test loss = 3548.028248\n",
            "Epoch 4658: train loss = 2246.365504, test loss = 3547.998925\n",
            "Epoch 4659: train loss = 2246.320064, test loss = 3547.969607\n",
            "Epoch 4660: train loss = 2246.274639, test loss = 3547.940294\n",
            "Epoch 4661: train loss = 2246.229230, test loss = 3547.910984\n",
            "Epoch 4662: train loss = 2246.183836, test loss = 3547.881679\n",
            "Epoch 4663: train loss = 2246.138458, test loss = 3547.852379\n",
            "Epoch 4664: train loss = 2246.093095, test loss = 3547.823082\n",
            "Epoch 4665: train loss = 2246.047747, test loss = 3547.793791\n",
            "Epoch 4666: train loss = 2246.002414, test loss = 3547.764503\n",
            "Epoch 4667: train loss = 2245.957096, test loss = 3547.735220\n",
            "Epoch 4668: train loss = 2245.911794, test loss = 3547.705941\n",
            "Epoch 4669: train loss = 2245.866507, test loss = 3547.676666\n",
            "Epoch 4670: train loss = 2245.821235, test loss = 3547.647396\n",
            "Epoch 4671: train loss = 2245.775978, test loss = 3547.618130\n",
            "Epoch 4672: train loss = 2245.730736, test loss = 3547.588869\n",
            "Epoch 4673: train loss = 2245.685509, test loss = 3547.559612\n",
            "Epoch 4674: train loss = 2245.640298, test loss = 3547.530359\n",
            "Epoch 4675: train loss = 2245.595101, test loss = 3547.501111\n",
            "Epoch 4676: train loss = 2245.549920, test loss = 3547.471867\n",
            "Epoch 4677: train loss = 2245.504754, test loss = 3547.442627\n",
            "Epoch 4678: train loss = 2245.459602, test loss = 3547.413392\n",
            "Epoch 4679: train loss = 2245.414466, test loss = 3547.384162\n",
            "Epoch 4680: train loss = 2245.369345, test loss = 3547.354935\n",
            "Epoch 4681: train loss = 2245.324238, test loss = 3547.325713\n",
            "Epoch 4682: train loss = 2245.279147, test loss = 3547.296495\n",
            "Epoch 4683: train loss = 2245.234070, test loss = 3547.267282\n",
            "Epoch 4684: train loss = 2245.189009, test loss = 3547.238073\n",
            "Epoch 4685: train loss = 2245.143962, test loss = 3547.208869\n",
            "Epoch 4686: train loss = 2245.098930, test loss = 3547.179669\n",
            "Epoch 4687: train loss = 2245.053914, test loss = 3547.150473\n",
            "Epoch 4688: train loss = 2245.008911, test loss = 3547.121282\n",
            "Epoch 4689: train loss = 2244.963924, test loss = 3547.092095\n",
            "Epoch 4690: train loss = 2244.918952, test loss = 3547.062912\n",
            "Epoch 4691: train loss = 2244.873994, test loss = 3547.033734\n",
            "Epoch 4692: train loss = 2244.829051, test loss = 3547.004561\n",
            "Epoch 4693: train loss = 2244.784123, test loss = 3546.975391\n",
            "Epoch 4694: train loss = 2244.739210, test loss = 3546.946226\n",
            "Epoch 4695: train loss = 2244.694311, test loss = 3546.917066\n",
            "Epoch 4696: train loss = 2244.649427, test loss = 3546.887910\n",
            "Epoch 4697: train loss = 2244.604558, test loss = 3546.858758\n",
            "Epoch 4698: train loss = 2244.559703, test loss = 3546.829611\n",
            "Epoch 4699: train loss = 2244.514863, test loss = 3546.800468\n",
            "Epoch 4700: train loss = 2244.470038, test loss = 3546.771330\n",
            "Epoch 4701: train loss = 2244.425228, test loss = 3546.742196\n",
            "Epoch 4702: train loss = 2244.380431, test loss = 3546.713067\n",
            "Epoch 4703: train loss = 2244.335650, test loss = 3546.683942\n",
            "Epoch 4704: train loss = 2244.290883, test loss = 3546.654821\n",
            "Epoch 4705: train loss = 2244.246131, test loss = 3546.625705\n",
            "Epoch 4706: train loss = 2244.201393, test loss = 3546.596593\n",
            "Epoch 4707: train loss = 2244.156670, test loss = 3546.567486\n",
            "Epoch 4708: train loss = 2244.111961, test loss = 3546.538383\n",
            "Epoch 4709: train loss = 2244.067267, test loss = 3546.509284\n",
            "Epoch 4710: train loss = 2244.022587, test loss = 3546.480190\n",
            "Epoch 4711: train loss = 2243.977921, test loss = 3546.451101\n",
            "Epoch 4712: train loss = 2243.933270, test loss = 3546.422016\n",
            "Epoch 4713: train loss = 2243.888634, test loss = 3546.392935\n",
            "Epoch 4714: train loss = 2243.844012, test loss = 3546.363859\n",
            "Epoch 4715: train loss = 2243.799404, test loss = 3546.334787\n",
            "Epoch 4716: train loss = 2243.754811, test loss = 3546.305720\n",
            "Epoch 4717: train loss = 2243.710231, test loss = 3546.276657\n",
            "Epoch 4718: train loss = 2243.665667, test loss = 3546.247598\n",
            "Epoch 4719: train loss = 2243.621116, test loss = 3546.218545\n",
            "Epoch 4720: train loss = 2243.576580, test loss = 3546.189495\n",
            "Epoch 4721: train loss = 2243.532058, test loss = 3546.160450\n",
            "Epoch 4722: train loss = 2243.487551, test loss = 3546.131410\n",
            "Epoch 4723: train loss = 2243.443058, test loss = 3546.102374\n",
            "Epoch 4724: train loss = 2243.398579, test loss = 3546.073342\n",
            "Epoch 4725: train loss = 2243.354114, test loss = 3546.044315\n",
            "Epoch 4726: train loss = 2243.309663, test loss = 3546.015292\n",
            "Epoch 4727: train loss = 2243.265227, test loss = 3545.986274\n",
            "Epoch 4728: train loss = 2243.220804, test loss = 3545.957261\n",
            "Epoch 4729: train loss = 2243.176396, test loss = 3545.928251\n",
            "Epoch 4730: train loss = 2243.132002, test loss = 3545.899247\n",
            "Epoch 4731: train loss = 2243.087622, test loss = 3545.870247\n",
            "Epoch 4732: train loss = 2243.043256, test loss = 3545.841251\n",
            "Epoch 4733: train loss = 2242.998905, test loss = 3545.812260\n",
            "Epoch 4734: train loss = 2242.954567, test loss = 3545.783273\n",
            "Epoch 4735: train loss = 2242.910243, test loss = 3545.754291\n",
            "Epoch 4736: train loss = 2242.865934, test loss = 3545.725313\n",
            "Epoch 4737: train loss = 2242.821638, test loss = 3545.696340\n",
            "Epoch 4738: train loss = 2242.777357, test loss = 3545.667371\n",
            "Epoch 4739: train loss = 2242.733089, test loss = 3545.638407\n",
            "Epoch 4740: train loss = 2242.688836, test loss = 3545.609447\n",
            "Epoch 4741: train loss = 2242.644596, test loss = 3545.580492\n",
            "Epoch 4742: train loss = 2242.600370, test loss = 3545.551541\n",
            "Epoch 4743: train loss = 2242.556159, test loss = 3545.522595\n",
            "Epoch 4744: train loss = 2242.511961, test loss = 3545.493654\n",
            "Epoch 4745: train loss = 2242.467777, test loss = 3545.464716\n",
            "Epoch 4746: train loss = 2242.423607, test loss = 3545.435784\n",
            "Epoch 4747: train loss = 2242.379450, test loss = 3545.406856\n",
            "Epoch 4748: train loss = 2242.335308, test loss = 3545.377932\n",
            "Epoch 4749: train loss = 2242.291179, test loss = 3545.349013\n",
            "Epoch 4750: train loss = 2242.247065, test loss = 3545.320099\n",
            "Epoch 4751: train loss = 2242.202964, test loss = 3545.291189\n",
            "Epoch 4752: train loss = 2242.158876, test loss = 3545.262283\n",
            "Epoch 4753: train loss = 2242.114803, test loss = 3545.233382\n",
            "Epoch 4754: train loss = 2242.070743, test loss = 3545.204486\n",
            "Epoch 4755: train loss = 2242.026697, test loss = 3545.175594\n",
            "Epoch 4756: train loss = 2241.982665, test loss = 3545.146707\n",
            "Epoch 4757: train loss = 2241.938647, test loss = 3545.117824\n",
            "Epoch 4758: train loss = 2241.894642, test loss = 3545.088946\n",
            "Epoch 4759: train loss = 2241.850651, test loss = 3545.060072\n",
            "Epoch 4760: train loss = 2241.806673, test loss = 3545.031203\n",
            "Epoch 4761: train loss = 2241.762709, test loss = 3545.002339\n",
            "Epoch 4762: train loss = 2241.718759, test loss = 3544.973479\n",
            "Epoch 4763: train loss = 2241.674822, test loss = 3544.944623\n",
            "Epoch 4764: train loss = 2241.630899, test loss = 3544.915772\n",
            "Epoch 4765: train loss = 2241.586989, test loss = 3544.886926\n",
            "Epoch 4766: train loss = 2241.543094, test loss = 3544.858084\n",
            "Epoch 4767: train loss = 2241.499211, test loss = 3544.829247\n",
            "Epoch 4768: train loss = 2241.455342, test loss = 3544.800414\n",
            "Epoch 4769: train loss = 2241.411487, test loss = 3544.771586\n",
            "Epoch 4770: train loss = 2241.367645, test loss = 3544.742763\n",
            "Epoch 4771: train loss = 2241.323817, test loss = 3544.713944\n",
            "Epoch 4772: train loss = 2241.280002, test loss = 3544.685130\n",
            "Epoch 4773: train loss = 2241.236200, test loss = 3544.656320\n",
            "Epoch 4774: train loss = 2241.192412, test loss = 3544.627515\n",
            "Epoch 4775: train loss = 2241.148637, test loss = 3544.598714\n",
            "Epoch 4776: train loss = 2241.104876, test loss = 3544.569918\n",
            "Epoch 4777: train loss = 2241.061128, test loss = 3544.541127\n",
            "Epoch 4778: train loss = 2241.017394, test loss = 3544.512340\n",
            "Epoch 4779: train loss = 2240.973673, test loss = 3544.483558\n",
            "Epoch 4780: train loss = 2240.929965, test loss = 3544.454780\n",
            "Epoch 4781: train loss = 2240.886270, test loss = 3544.426007\n",
            "Epoch 4782: train loss = 2240.842589, test loss = 3544.397239\n",
            "Epoch 4783: train loss = 2240.798921, test loss = 3544.368475\n",
            "Epoch 4784: train loss = 2240.755267, test loss = 3544.339716\n",
            "Epoch 4785: train loss = 2240.711625, test loss = 3544.310961\n",
            "Epoch 4786: train loss = 2240.667997, test loss = 3544.282211\n",
            "Epoch 4787: train loss = 2240.624383, test loss = 3544.253466\n",
            "Epoch 4788: train loss = 2240.580781, test loss = 3544.224725\n",
            "Epoch 4789: train loss = 2240.537193, test loss = 3544.195989\n",
            "Epoch 4790: train loss = 2240.493617, test loss = 3544.167257\n",
            "Epoch 4791: train loss = 2240.450055, test loss = 3544.138530\n",
            "Epoch 4792: train loss = 2240.406506, test loss = 3544.109808\n",
            "Epoch 4793: train loss = 2240.362971, test loss = 3544.081090\n",
            "Epoch 4794: train loss = 2240.319448, test loss = 3544.052377\n",
            "Epoch 4795: train loss = 2240.275939, test loss = 3544.023669\n",
            "Epoch 4796: train loss = 2240.232442, test loss = 3543.994965\n",
            "Epoch 4797: train loss = 2240.188959, test loss = 3543.966266\n",
            "Epoch 4798: train loss = 2240.145489, test loss = 3543.937571\n",
            "Epoch 4799: train loss = 2240.102031, test loss = 3543.908881\n",
            "Epoch 4800: train loss = 2240.058587, test loss = 3543.880196\n",
            "Epoch 4801: train loss = 2240.015156, test loss = 3543.851515\n",
            "Epoch 4802: train loss = 2239.971738, test loss = 3543.822839\n",
            "Epoch 4803: train loss = 2239.928333, test loss = 3543.794168\n",
            "Epoch 4804: train loss = 2239.884941, test loss = 3543.765501\n",
            "Epoch 4805: train loss = 2239.841562, test loss = 3543.736839\n",
            "Epoch 4806: train loss = 2239.798195, test loss = 3543.708182\n",
            "Epoch 4807: train loss = 2239.754842, test loss = 3543.679529\n",
            "Epoch 4808: train loss = 2239.711502, test loss = 3543.650881\n",
            "Epoch 4809: train loss = 2239.668174, test loss = 3543.622237\n",
            "Epoch 4810: train loss = 2239.624860, test loss = 3543.593598\n",
            "Epoch 4811: train loss = 2239.581558, test loss = 3543.564964\n",
            "Epoch 4812: train loss = 2239.538269, test loss = 3543.536335\n",
            "Epoch 4813: train loss = 2239.494993, test loss = 3543.507710\n",
            "Epoch 4814: train loss = 2239.451730, test loss = 3543.479090\n",
            "Epoch 4815: train loss = 2239.408480, test loss = 3543.450475\n",
            "Epoch 4816: train loss = 2239.365242, test loss = 3543.421864\n",
            "Epoch 4817: train loss = 2239.322017, test loss = 3543.393258\n",
            "Epoch 4818: train loss = 2239.278805, test loss = 3543.364656\n",
            "Epoch 4819: train loss = 2239.235606, test loss = 3543.336060\n",
            "Epoch 4820: train loss = 2239.192420, test loss = 3543.307468\n",
            "Epoch 4821: train loss = 2239.149246, test loss = 3543.278880\n",
            "Epoch 4822: train loss = 2239.106085, test loss = 3543.250298\n",
            "Epoch 4823: train loss = 2239.062937, test loss = 3543.221720\n",
            "Epoch 4824: train loss = 2239.019801, test loss = 3543.193146\n",
            "Epoch 4825: train loss = 2238.976678, test loss = 3543.164578\n",
            "Epoch 4826: train loss = 2238.933568, test loss = 3543.136014\n",
            "Epoch 4827: train loss = 2238.890470, test loss = 3543.107455\n",
            "Epoch 4828: train loss = 2238.847385, test loss = 3543.078900\n",
            "Epoch 4829: train loss = 2238.804313, test loss = 3543.050350\n",
            "Epoch 4830: train loss = 2238.761253, test loss = 3543.021805\n",
            "Epoch 4831: train loss = 2238.718206, test loss = 3542.993265\n",
            "Epoch 4832: train loss = 2238.675171, test loss = 3542.964729\n",
            "Epoch 4833: train loss = 2238.632149, test loss = 3542.936198\n",
            "Epoch 4834: train loss = 2238.589140, test loss = 3542.907672\n",
            "Epoch 4835: train loss = 2238.546143, test loss = 3542.879150\n",
            "Epoch 4836: train loss = 2238.503159, test loss = 3542.850634\n",
            "Epoch 4837: train loss = 2238.460187, test loss = 3542.822122\n",
            "Epoch 4838: train loss = 2238.417227, test loss = 3542.793614\n",
            "Epoch 4839: train loss = 2238.374280, test loss = 3542.765112\n",
            "Epoch 4840: train loss = 2238.331346, test loss = 3542.736614\n",
            "Epoch 4841: train loss = 2238.288424, test loss = 3542.708120\n",
            "Epoch 4842: train loss = 2238.245514, test loss = 3542.679632\n",
            "Epoch 4843: train loss = 2238.202617, test loss = 3542.651148\n",
            "Epoch 4844: train loss = 2238.159732, test loss = 3542.622669\n",
            "Epoch 4845: train loss = 2238.116859, test loss = 3542.594195\n",
            "Epoch 4846: train loss = 2238.073999, test loss = 3542.565726\n",
            "Epoch 4847: train loss = 2238.031152, test loss = 3542.537261\n",
            "Epoch 4848: train loss = 2237.988316, test loss = 3542.508801\n",
            "Epoch 4849: train loss = 2237.945493, test loss = 3542.480346\n",
            "Epoch 4850: train loss = 2237.902683, test loss = 3542.451895\n",
            "Epoch 4851: train loss = 2237.859884, test loss = 3542.423450\n",
            "Epoch 4852: train loss = 2237.817098, test loss = 3542.395009\n",
            "Epoch 4853: train loss = 2237.774324, test loss = 3542.366572\n",
            "Epoch 4854: train loss = 2237.731563, test loss = 3542.338141\n",
            "Epoch 4855: train loss = 2237.688814, test loss = 3542.309714\n",
            "Epoch 4856: train loss = 2237.646077, test loss = 3542.281292\n",
            "Epoch 4857: train loss = 2237.603352, test loss = 3542.252875\n",
            "Epoch 4858: train loss = 2237.560639, test loss = 3542.224463\n",
            "Epoch 4859: train loss = 2237.517939, test loss = 3542.196055\n",
            "Epoch 4860: train loss = 2237.475251, test loss = 3542.167652\n",
            "Epoch 4861: train loss = 2237.432575, test loss = 3542.139254\n",
            "Epoch 4862: train loss = 2237.389911, test loss = 3542.110861\n",
            "Epoch 4863: train loss = 2237.347259, test loss = 3542.082473\n",
            "Epoch 4864: train loss = 2237.304619, test loss = 3542.054089\n",
            "Epoch 4865: train loss = 2237.261992, test loss = 3542.025710\n",
            "Epoch 4866: train loss = 2237.219376, test loss = 3541.997336\n",
            "Epoch 4867: train loss = 2237.176773, test loss = 3541.968967\n",
            "Epoch 4868: train loss = 2237.134182, test loss = 3541.940602\n",
            "Epoch 4869: train loss = 2237.091603, test loss = 3541.912242\n",
            "Epoch 4870: train loss = 2237.049036, test loss = 3541.883887\n",
            "Epoch 4871: train loss = 2237.006480, test loss = 3541.855537\n",
            "Epoch 4872: train loss = 2236.963937, test loss = 3541.827192\n",
            "Epoch 4873: train loss = 2236.921406, test loss = 3541.798851\n",
            "Epoch 4874: train loss = 2236.878887, test loss = 3541.770516\n",
            "Epoch 4875: train loss = 2236.836380, test loss = 3541.742185\n",
            "Epoch 4876: train loss = 2236.793885, test loss = 3541.713859\n",
            "Epoch 4877: train loss = 2236.751402, test loss = 3541.685537\n",
            "Epoch 4878: train loss = 2236.708931, test loss = 3541.657221\n",
            "Epoch 4879: train loss = 2236.666471, test loss = 3541.628909\n",
            "Epoch 4880: train loss = 2236.624024, test loss = 3541.600603\n",
            "Epoch 4881: train loss = 2236.581589, test loss = 3541.572301\n",
            "Epoch 4882: train loss = 2236.539165, test loss = 3541.544004\n",
            "Epoch 4883: train loss = 2236.496753, test loss = 3541.515711\n",
            "Epoch 4884: train loss = 2236.454353, test loss = 3541.487424\n",
            "Epoch 4885: train loss = 2236.411965, test loss = 3541.459141\n",
            "Epoch 4886: train loss = 2236.369589, test loss = 3541.430863\n",
            "Epoch 4887: train loss = 2236.327225, test loss = 3541.402590\n",
            "Epoch 4888: train loss = 2236.284872, test loss = 3541.374322\n",
            "Epoch 4889: train loss = 2236.242531, test loss = 3541.346059\n",
            "Epoch 4890: train loss = 2236.200203, test loss = 3541.317801\n",
            "Epoch 4891: train loss = 2236.157885, test loss = 3541.289547\n",
            "Epoch 4892: train loss = 2236.115580, test loss = 3541.261298\n",
            "Epoch 4893: train loss = 2236.073286, test loss = 3541.233054\n",
            "Epoch 4894: train loss = 2236.031004, test loss = 3541.204815\n",
            "Epoch 4895: train loss = 2235.988734, test loss = 3541.176581\n",
            "Epoch 4896: train loss = 2235.946475, test loss = 3541.148352\n",
            "Epoch 4897: train loss = 2235.904229, test loss = 3541.120128\n",
            "Epoch 4898: train loss = 2235.861993, test loss = 3541.091908\n",
            "Epoch 4899: train loss = 2235.819770, test loss = 3541.063693\n",
            "Epoch 4900: train loss = 2235.777558, test loss = 3541.035484\n",
            "Epoch 4901: train loss = 2235.735358, test loss = 3541.007279\n",
            "Epoch 4902: train loss = 2235.693169, test loss = 3540.979079\n",
            "Epoch 4903: train loss = 2235.650992, test loss = 3540.950883\n",
            "Epoch 4904: train loss = 2235.608827, test loss = 3540.922693\n",
            "Epoch 4905: train loss = 2235.566673, test loss = 3540.894508\n",
            "Epoch 4906: train loss = 2235.524531, test loss = 3540.866327\n",
            "Epoch 4907: train loss = 2235.482400, test loss = 3540.838152\n",
            "Epoch 4908: train loss = 2235.440281, test loss = 3540.809981\n",
            "Epoch 4909: train loss = 2235.398173, test loss = 3540.781815\n",
            "Epoch 4910: train loss = 2235.356077, test loss = 3540.753654\n",
            "Epoch 4911: train loss = 2235.313993, test loss = 3540.725498\n",
            "Epoch 4912: train loss = 2235.271919, test loss = 3540.697347\n",
            "Epoch 4913: train loss = 2235.229858, test loss = 3540.669201\n",
            "Epoch 4914: train loss = 2235.187808, test loss = 3540.641060\n",
            "Epoch 4915: train loss = 2235.145769, test loss = 3540.612923\n",
            "Epoch 4916: train loss = 2235.103742, test loss = 3540.584792\n",
            "Epoch 4917: train loss = 2235.061726, test loss = 3540.556665\n",
            "Epoch 4918: train loss = 2235.019722, test loss = 3540.528543\n",
            "Epoch 4919: train loss = 2234.977729, test loss = 3540.500427\n",
            "Epoch 4920: train loss = 2234.935747, test loss = 3540.472315\n",
            "Epoch 4921: train loss = 2234.893777, test loss = 3540.444208\n",
            "Epoch 4922: train loss = 2234.851818, test loss = 3540.416106\n",
            "Epoch 4923: train loss = 2234.809871, test loss = 3540.388009\n",
            "Epoch 4924: train loss = 2234.767935, test loss = 3540.359917\n",
            "Epoch 4925: train loss = 2234.726010, test loss = 3540.331830\n",
            "Epoch 4926: train loss = 2234.684096, test loss = 3540.303748\n",
            "Epoch 4927: train loss = 2234.642194, test loss = 3540.275670\n",
            "Epoch 4928: train loss = 2234.600303, test loss = 3540.247598\n",
            "Epoch 4929: train loss = 2234.558424, test loss = 3540.219531\n",
            "Epoch 4930: train loss = 2234.516555, test loss = 3540.191468\n",
            "Epoch 4931: train loss = 2234.474698, test loss = 3540.163411\n",
            "Epoch 4932: train loss = 2234.432853, test loss = 3540.135358\n",
            "Epoch 4933: train loss = 2234.391018, test loss = 3540.107311\n",
            "Epoch 4934: train loss = 2234.349195, test loss = 3540.079268\n",
            "Epoch 4935: train loss = 2234.307382, test loss = 3540.051230\n",
            "Epoch 4936: train loss = 2234.265582, test loss = 3540.023198\n",
            "Epoch 4937: train loss = 2234.223792, test loss = 3539.995170\n",
            "Epoch 4938: train loss = 2234.182013, test loss = 3539.967147\n",
            "Epoch 4939: train loss = 2234.140246, test loss = 3539.939130\n",
            "Epoch 4940: train loss = 2234.098489, test loss = 3539.911117\n",
            "Epoch 4941: train loss = 2234.056744, test loss = 3539.883109\n",
            "Epoch 4942: train loss = 2234.015010, test loss = 3539.855106\n",
            "Epoch 4943: train loss = 2233.973287, test loss = 3539.827108\n",
            "Epoch 4944: train loss = 2233.931575, test loss = 3539.799115\n",
            "Epoch 4945: train loss = 2233.889875, test loss = 3539.771127\n",
            "Epoch 4946: train loss = 2233.848185, test loss = 3539.743145\n",
            "Epoch 4947: train loss = 2233.806506, test loss = 3539.715167\n",
            "Epoch 4948: train loss = 2233.764839, test loss = 3539.687194\n",
            "Epoch 4949: train loss = 2233.723182, test loss = 3539.659226\n",
            "Epoch 4950: train loss = 2233.681536, test loss = 3539.631263\n",
            "Epoch 4951: train loss = 2233.639902, test loss = 3539.603305\n",
            "Epoch 4952: train loss = 2233.598278, test loss = 3539.575352\n",
            "Epoch 4953: train loss = 2233.556666, test loss = 3539.547404\n",
            "Epoch 4954: train loss = 2233.515064, test loss = 3539.519462\n",
            "Epoch 4955: train loss = 2233.473474, test loss = 3539.491524\n",
            "Epoch 4956: train loss = 2233.431894, test loss = 3539.463591\n",
            "Epoch 4957: train loss = 2233.390325, test loss = 3539.435663\n",
            "Epoch 4958: train loss = 2233.348767, test loss = 3539.407740\n",
            "Epoch 4959: train loss = 2233.307220, test loss = 3539.379823\n",
            "Epoch 4960: train loss = 2233.265684, test loss = 3539.351910\n",
            "Epoch 4961: train loss = 2233.224159, test loss = 3539.324002\n",
            "Epoch 4962: train loss = 2233.182645, test loss = 3539.296099\n",
            "Epoch 4963: train loss = 2233.141142, test loss = 3539.268202\n",
            "Epoch 4964: train loss = 2233.099649, test loss = 3539.240309\n",
            "Epoch 4965: train loss = 2233.058167, test loss = 3539.212422\n",
            "Epoch 4966: train loss = 2233.016697, test loss = 3539.184539\n",
            "Epoch 4967: train loss = 2232.975237, test loss = 3539.156662\n",
            "Epoch 4968: train loss = 2232.933787, test loss = 3539.128789\n",
            "Epoch 4969: train loss = 2232.892349, test loss = 3539.100922\n",
            "Epoch 4970: train loss = 2232.850921, test loss = 3539.073060\n",
            "Epoch 4971: train loss = 2232.809504, test loss = 3539.045202\n",
            "Epoch 4972: train loss = 2232.768098, test loss = 3539.017350\n",
            "Epoch 4973: train loss = 2232.726703, test loss = 3538.989503\n",
            "Epoch 4974: train loss = 2232.685318, test loss = 3538.961661\n",
            "Epoch 4975: train loss = 2232.643944, test loss = 3538.933824\n",
            "Epoch 4976: train loss = 2232.602581, test loss = 3538.905992\n",
            "Epoch 4977: train loss = 2232.561228, test loss = 3538.878165\n",
            "Epoch 4978: train loss = 2232.519887, test loss = 3538.850344\n",
            "Epoch 4979: train loss = 2232.478555, test loss = 3538.822527\n",
            "Epoch 4980: train loss = 2232.437235, test loss = 3538.794715\n",
            "Epoch 4981: train loss = 2232.395925, test loss = 3538.766909\n",
            "Epoch 4982: train loss = 2232.354626, test loss = 3538.739107\n",
            "Epoch 4983: train loss = 2232.313337, test loss = 3538.711311\n",
            "Epoch 4984: train loss = 2232.272059, test loss = 3538.683520\n",
            "Epoch 4985: train loss = 2232.230792, test loss = 3538.655734\n",
            "Epoch 4986: train loss = 2232.189535, test loss = 3538.627953\n",
            "Epoch 4987: train loss = 2232.148289, test loss = 3538.600177\n",
            "Epoch 4988: train loss = 2232.107053, test loss = 3538.572406\n",
            "Epoch 4989: train loss = 2232.065828, test loss = 3538.544640\n",
            "Epoch 4990: train loss = 2232.024614, test loss = 3538.516880\n",
            "Epoch 4991: train loss = 2231.983410, test loss = 3538.489124\n",
            "Epoch 4992: train loss = 2231.942216, test loss = 3538.461374\n",
            "Epoch 4993: train loss = 2231.901033, test loss = 3538.433629\n",
            "Epoch 4994: train loss = 2231.859861, test loss = 3538.405888\n",
            "Epoch 4995: train loss = 2231.818699, test loss = 3538.378153\n",
            "Epoch 4996: train loss = 2231.777547, test loss = 3538.350424\n",
            "Epoch 4997: train loss = 2231.736406, test loss = 3538.322699\n",
            "Epoch 4998: train loss = 2231.695276, test loss = 3538.294979\n",
            "Epoch 4999: train loss = 2231.654155, test loss = 3538.267265\n",
            "Epoch 5000: train loss = 2231.613046, test loss = 3538.239555\n",
            "Epoch 5001: train loss = 2231.571946, test loss = 3538.211851\n",
            "Epoch 5002: train loss = 2231.530857, test loss = 3538.184152\n",
            "Epoch 5003: train loss = 2231.489779, test loss = 3538.156458\n",
            "Epoch 5004: train loss = 2231.448711, test loss = 3538.128769\n",
            "Epoch 5005: train loss = 2231.407653, test loss = 3538.101086\n",
            "Epoch 5006: train loss = 2231.366606, test loss = 3538.073407\n",
            "Epoch 5007: train loss = 2231.325569, test loss = 3538.045734\n",
            "Epoch 5008: train loss = 2231.284542, test loss = 3538.018066\n",
            "Epoch 5009: train loss = 2231.243525, test loss = 3537.990403\n",
            "Epoch 5010: train loss = 2231.202519, test loss = 3537.962745\n",
            "Epoch 5011: train loss = 2231.161524, test loss = 3537.935093\n",
            "Epoch 5012: train loss = 2231.120538, test loss = 3537.907445\n",
            "Epoch 5013: train loss = 2231.079563, test loss = 3537.879803\n",
            "Epoch 5014: train loss = 2231.038598, test loss = 3537.852166\n",
            "Epoch 5015: train loss = 2230.997643, test loss = 3537.824534\n",
            "Epoch 5016: train loss = 2230.956699, test loss = 3537.796907\n",
            "Epoch 5017: train loss = 2230.915765, test loss = 3537.769286\n",
            "Epoch 5018: train loss = 2230.874841, test loss = 3537.741669\n",
            "Epoch 5019: train loss = 2230.833927, test loss = 3537.714058\n",
            "Epoch 5020: train loss = 2230.793024, test loss = 3537.686452\n",
            "Epoch 5021: train loss = 2230.752130, test loss = 3537.658852\n",
            "Epoch 5022: train loss = 2230.711247, test loss = 3537.631256\n",
            "Epoch 5023: train loss = 2230.670374, test loss = 3537.603666\n",
            "Epoch 5024: train loss = 2230.629511, test loss = 3537.576081\n",
            "Epoch 5025: train loss = 2230.588658, test loss = 3537.548501\n",
            "Epoch 5026: train loss = 2230.547816, test loss = 3537.520926\n",
            "Epoch 5027: train loss = 2230.506983, test loss = 3537.493356\n",
            "Epoch 5028: train loss = 2230.466161, test loss = 3537.465792\n",
            "Epoch 5029: train loss = 2230.425349, test loss = 3537.438233\n",
            "Epoch 5030: train loss = 2230.384547, test loss = 3537.410679\n",
            "Epoch 5031: train loss = 2230.343755, test loss = 3537.383131\n",
            "Epoch 5032: train loss = 2230.302973, test loss = 3537.355587\n",
            "Epoch 5033: train loss = 2230.262201, test loss = 3537.328049\n",
            "Epoch 5034: train loss = 2230.221439, test loss = 3537.300516\n",
            "Epoch 5035: train loss = 2230.180687, test loss = 3537.272989\n",
            "Epoch 5036: train loss = 2230.139945, test loss = 3537.245466\n",
            "Epoch 5037: train loss = 2230.099213, test loss = 3537.217949\n",
            "Epoch 5038: train loss = 2230.058491, test loss = 3537.190437\n",
            "Epoch 5039: train loss = 2230.017780, test loss = 3537.162931\n",
            "Epoch 5040: train loss = 2229.977078, test loss = 3537.135429\n",
            "Epoch 5041: train loss = 2229.936386, test loss = 3537.107933\n",
            "Epoch 5042: train loss = 2229.895704, test loss = 3537.080442\n",
            "Epoch 5043: train loss = 2229.855032, test loss = 3537.052957\n",
            "Epoch 5044: train loss = 2229.814370, test loss = 3537.025476\n",
            "Epoch 5045: train loss = 2229.773718, test loss = 3536.998001\n",
            "Epoch 5046: train loss = 2229.733075, test loss = 3536.970532\n",
            "Epoch 5047: train loss = 2229.692443, test loss = 3536.943067\n",
            "Epoch 5048: train loss = 2229.651821, test loss = 3536.915608\n",
            "Epoch 5049: train loss = 2229.611208, test loss = 3536.888154\n",
            "Epoch 5050: train loss = 2229.570606, test loss = 3536.860705\n",
            "Epoch 5051: train loss = 2229.530013, test loss = 3536.833262\n",
            "Epoch 5052: train loss = 2229.489430, test loss = 3536.805824\n",
            "Epoch 5053: train loss = 2229.448857, test loss = 3536.778391\n",
            "Epoch 5054: train loss = 2229.408293, test loss = 3536.750964\n",
            "Epoch 5055: train loss = 2229.367740, test loss = 3536.723541\n",
            "Epoch 5056: train loss = 2229.327196, test loss = 3536.696125\n",
            "Epoch 5057: train loss = 2229.286662, test loss = 3536.668713\n",
            "Epoch 5058: train loss = 2229.246138, test loss = 3536.641307\n",
            "Epoch 5059: train loss = 2229.205624, test loss = 3536.613906\n",
            "Epoch 5060: train loss = 2229.165119, test loss = 3536.586510\n",
            "Epoch 5061: train loss = 2229.124624, test loss = 3536.559120\n",
            "Epoch 5062: train loss = 2229.084139, test loss = 3536.531735\n",
            "Epoch 5063: train loss = 2229.043664, test loss = 3536.504355\n",
            "Epoch 5064: train loss = 2229.003198, test loss = 3536.476981\n",
            "Epoch 5065: train loss = 2228.962742, test loss = 3536.449612\n",
            "Epoch 5066: train loss = 2228.922296, test loss = 3536.422249\n",
            "Epoch 5067: train loss = 2228.881860, test loss = 3536.394890\n",
            "Epoch 5068: train loss = 2228.841433, test loss = 3536.367537\n",
            "Epoch 5069: train loss = 2228.801015, test loss = 3536.340190\n",
            "Epoch 5070: train loss = 2228.760608, test loss = 3536.312847\n",
            "Epoch 5071: train loss = 2228.720210, test loss = 3536.285511\n",
            "Epoch 5072: train loss = 2228.679822, test loss = 3536.258179\n",
            "Epoch 5073: train loss = 2228.639443, test loss = 3536.230853\n",
            "Epoch 5074: train loss = 2228.599074, test loss = 3536.203532\n",
            "Epoch 5075: train loss = 2228.558715, test loss = 3536.176217\n",
            "Epoch 5076: train loss = 2228.518365, test loss = 3536.148907\n",
            "Epoch 5077: train loss = 2228.478024, test loss = 3536.121602\n",
            "Epoch 5078: train loss = 2228.437694, test loss = 3536.094302\n",
            "Epoch 5079: train loss = 2228.397373, test loss = 3536.067009\n",
            "Epoch 5080: train loss = 2228.357061, test loss = 3536.039720\n",
            "Epoch 5081: train loss = 2228.316759, test loss = 3536.012437\n",
            "Epoch 5082: train loss = 2228.276466, test loss = 3535.985159\n",
            "Epoch 5083: train loss = 2228.236183, test loss = 3535.957887\n",
            "Epoch 5084: train loss = 2228.195910, test loss = 3535.930620\n",
            "Epoch 5085: train loss = 2228.155646, test loss = 3535.903358\n",
            "Epoch 5086: train loss = 2228.115391, test loss = 3535.876102\n",
            "Epoch 5087: train loss = 2228.075146, test loss = 3535.848851\n",
            "Epoch 5088: train loss = 2228.034911, test loss = 3535.821606\n",
            "Epoch 5089: train loss = 2227.994684, test loss = 3535.794366\n",
            "Epoch 5090: train loss = 2227.954468, test loss = 3535.767131\n",
            "Epoch 5091: train loss = 2227.914260, test loss = 3535.739902\n",
            "Epoch 5092: train loss = 2227.874063, test loss = 3535.712678\n",
            "Epoch 5093: train loss = 2227.833874, test loss = 3535.685460\n",
            "Epoch 5094: train loss = 2227.793695, test loss = 3535.658247\n",
            "Epoch 5095: train loss = 2227.753526, test loss = 3535.631040\n",
            "Epoch 5096: train loss = 2227.713365, test loss = 3535.603837\n",
            "Epoch 5097: train loss = 2227.673214, test loss = 3535.576641\n",
            "Epoch 5098: train loss = 2227.633073, test loss = 3535.549450\n",
            "Epoch 5099: train loss = 2227.592941, test loss = 3535.522264\n",
            "Epoch 5100: train loss = 2227.552818, test loss = 3535.495084\n",
            "Epoch 5101: train loss = 2227.512704, test loss = 3535.467909\n",
            "Epoch 5102: train loss = 2227.472600, test loss = 3535.440740\n",
            "Epoch 5103: train loss = 2227.432505, test loss = 3535.413576\n",
            "Epoch 5104: train loss = 2227.392420, test loss = 3535.386417\n",
            "Epoch 5105: train loss = 2227.352343, test loss = 3535.359264\n",
            "Epoch 5106: train loss = 2227.312276, test loss = 3535.332117\n",
            "Epoch 5107: train loss = 2227.272218, test loss = 3535.304975\n",
            "Epoch 5108: train loss = 2227.232170, test loss = 3535.277838\n",
            "Epoch 5109: train loss = 2227.192131, test loss = 3535.250707\n",
            "Epoch 5110: train loss = 2227.152101, test loss = 3535.223582\n",
            "Epoch 5111: train loss = 2227.112080, test loss = 3535.196462\n",
            "Epoch 5112: train loss = 2227.072068, test loss = 3535.169347\n",
            "Epoch 5113: train loss = 2227.032066, test loss = 3535.142238\n",
            "Epoch 5114: train loss = 2226.992073, test loss = 3535.115134\n",
            "Epoch 5115: train loss = 2226.952089, test loss = 3535.088036\n",
            "Epoch 5116: train loss = 2226.912114, test loss = 3535.060944\n",
            "Epoch 5117: train loss = 2226.872148, test loss = 3535.033857\n",
            "Epoch 5118: train loss = 2226.832192, test loss = 3535.006775\n",
            "Epoch 5119: train loss = 2226.792244, test loss = 3534.979699\n",
            "Epoch 5120: train loss = 2226.752306, test loss = 3534.952628\n",
            "Epoch 5121: train loss = 2226.712377, test loss = 3534.925563\n",
            "Epoch 5122: train loss = 2226.672457, test loss = 3534.898504\n",
            "Epoch 5123: train loss = 2226.632546, test loss = 3534.871450\n",
            "Epoch 5124: train loss = 2226.592644, test loss = 3534.844401\n",
            "Epoch 5125: train loss = 2226.552752, test loss = 3534.817358\n",
            "Epoch 5126: train loss = 2226.512868, test loss = 3534.790321\n",
            "Epoch 5127: train loss = 2226.472993, test loss = 3534.763289\n",
            "Epoch 5128: train loss = 2226.433128, test loss = 3534.736263\n",
            "Epoch 5129: train loss = 2226.393271, test loss = 3534.709242\n",
            "Epoch 5130: train loss = 2226.353424, test loss = 3534.682227\n",
            "Epoch 5131: train loss = 2226.313585, test loss = 3534.655217\n",
            "Epoch 5132: train loss = 2226.273756, test loss = 3534.628213\n",
            "Epoch 5133: train loss = 2226.233936, test loss = 3534.601214\n",
            "Epoch 5134: train loss = 2226.194124, test loss = 3534.574221\n",
            "Epoch 5135: train loss = 2226.154322, test loss = 3534.547234\n",
            "Epoch 5136: train loss = 2226.114528, test loss = 3534.520252\n",
            "Epoch 5137: train loss = 2226.074744, test loss = 3534.493276\n",
            "Epoch 5138: train loss = 2226.034968, test loss = 3534.466305\n",
            "Epoch 5139: train loss = 2225.995202, test loss = 3534.439340\n",
            "Epoch 5140: train loss = 2225.955444, test loss = 3534.412380\n",
            "Epoch 5141: train loss = 2225.915695, test loss = 3534.385426\n",
            "Epoch 5142: train loss = 2225.875955, test loss = 3534.358478\n",
            "Epoch 5143: train loss = 2225.836224, test loss = 3534.331535\n",
            "Epoch 5144: train loss = 2225.796502, test loss = 3534.304598\n",
            "Epoch 5145: train loss = 2225.756789, test loss = 3534.277667\n",
            "Epoch 5146: train loss = 2225.717085, test loss = 3534.250741\n",
            "Epoch 5147: train loss = 2225.677390, test loss = 3534.223820\n",
            "Epoch 5148: train loss = 2225.637703, test loss = 3534.196906\n",
            "Epoch 5149: train loss = 2225.598025, test loss = 3534.169997\n",
            "Epoch 5150: train loss = 2225.558356, test loss = 3534.143093\n",
            "Epoch 5151: train loss = 2225.518696, test loss = 3534.116195\n",
            "Epoch 5152: train loss = 2225.479045, test loss = 3534.089303\n",
            "Epoch 5153: train loss = 2225.439403, test loss = 3534.062416\n",
            "Epoch 5154: train loss = 2225.399769, test loss = 3534.035535\n",
            "Epoch 5155: train loss = 2225.360144, test loss = 3534.008660\n",
            "Epoch 5156: train loss = 2225.320528, test loss = 3533.981790\n",
            "Epoch 5157: train loss = 2225.280921, test loss = 3533.954926\n",
            "Epoch 5158: train loss = 2225.241322, test loss = 3533.928068\n",
            "Epoch 5159: train loss = 2225.201732, test loss = 3533.901215\n",
            "Epoch 5160: train loss = 2225.162151, test loss = 3533.874368\n",
            "Epoch 5161: train loss = 2225.122579, test loss = 3533.847527\n",
            "Epoch 5162: train loss = 2225.083015, test loss = 3533.820691\n",
            "Epoch 5163: train loss = 2225.043460, test loss = 3533.793861\n",
            "Epoch 5164: train loss = 2225.003914, test loss = 3533.767037\n",
            "Epoch 5165: train loss = 2224.964376, test loss = 3533.740218\n",
            "Epoch 5166: train loss = 2224.924847, test loss = 3533.713405\n",
            "Epoch 5167: train loss = 2224.885327, test loss = 3533.686598\n",
            "Epoch 5168: train loss = 2224.845815, test loss = 3533.659796\n",
            "Epoch 5169: train loss = 2224.806313, test loss = 3533.633000\n",
            "Epoch 5170: train loss = 2224.766818, test loss = 3533.606210\n",
            "Epoch 5171: train loss = 2224.727333, test loss = 3533.579425\n",
            "Epoch 5172: train loss = 2224.687855, test loss = 3533.552646\n",
            "Epoch 5173: train loss = 2224.648387, test loss = 3533.525873\n",
            "Epoch 5174: train loss = 2224.608927, test loss = 3533.499106\n",
            "Epoch 5175: train loss = 2224.569476, test loss = 3533.472344\n",
            "Epoch 5176: train loss = 2224.530033, test loss = 3533.445588\n",
            "Epoch 5177: train loss = 2224.490599, test loss = 3533.418838\n",
            "Epoch 5178: train loss = 2224.451174, test loss = 3533.392093\n",
            "Epoch 5179: train loss = 2224.411757, test loss = 3533.365354\n",
            "Epoch 5180: train loss = 2224.372348, test loss = 3533.338621\n",
            "Epoch 5181: train loss = 2224.332948, test loss = 3533.311894\n",
            "Epoch 5182: train loss = 2224.293557, test loss = 3533.285172\n",
            "Epoch 5183: train loss = 2224.254174, test loss = 3533.258456\n",
            "Epoch 5184: train loss = 2224.214800, test loss = 3533.231746\n",
            "Epoch 5185: train loss = 2224.175434, test loss = 3533.205042\n",
            "Epoch 5186: train loss = 2224.136076, test loss = 3533.178343\n",
            "Epoch 5187: train loss = 2224.096727, test loss = 3533.151650\n",
            "Epoch 5188: train loss = 2224.057387, test loss = 3533.124963\n",
            "Epoch 5189: train loss = 2224.018055, test loss = 3533.098282\n",
            "Epoch 5190: train loss = 2223.978731, test loss = 3533.071606\n",
            "Epoch 5191: train loss = 2223.939416, test loss = 3533.044937\n",
            "Epoch 5192: train loss = 2223.900109, test loss = 3533.018273\n",
            "Epoch 5193: train loss = 2223.860811, test loss = 3532.991615\n",
            "Epoch 5194: train loss = 2223.821521, test loss = 3532.964962\n",
            "Epoch 5195: train loss = 2223.782240, test loss = 3532.938315\n",
            "Epoch 5196: train loss = 2223.742967, test loss = 3532.911675\n",
            "Epoch 5197: train loss = 2223.703702, test loss = 3532.885040\n",
            "Epoch 5198: train loss = 2223.664445, test loss = 3532.858410\n",
            "Epoch 5199: train loss = 2223.625197, test loss = 3532.831787\n",
            "Epoch 5200: train loss = 2223.585958, test loss = 3532.805169\n",
            "Epoch 5201: train loss = 2223.546727, test loss = 3532.778558\n",
            "Epoch 5202: train loss = 2223.507504, test loss = 3532.751952\n",
            "Epoch 5203: train loss = 2223.468289, test loss = 3532.725351\n",
            "Epoch 5204: train loss = 2223.429083, test loss = 3532.698757\n",
            "Epoch 5205: train loss = 2223.389884, test loss = 3532.672169\n",
            "Epoch 5206: train loss = 2223.350695, test loss = 3532.645586\n",
            "Epoch 5207: train loss = 2223.311513, test loss = 3532.619009\n",
            "Epoch 5208: train loss = 2223.272340, test loss = 3532.592438\n",
            "Epoch 5209: train loss = 2223.233175, test loss = 3532.565873\n",
            "Epoch 5210: train loss = 2223.194018, test loss = 3532.539314\n",
            "Epoch 5211: train loss = 2223.154870, test loss = 3532.512760\n",
            "Epoch 5212: train loss = 2223.115730, test loss = 3532.486213\n",
            "Epoch 5213: train loss = 2223.076598, test loss = 3532.459671\n",
            "Epoch 5214: train loss = 2223.037474, test loss = 3532.433135\n",
            "Epoch 5215: train loss = 2222.998358, test loss = 3532.406605\n",
            "Epoch 5216: train loss = 2222.959251, test loss = 3532.380081\n",
            "Epoch 5217: train loss = 2222.920152, test loss = 3532.353563\n",
            "Epoch 5218: train loss = 2222.881061, test loss = 3532.327051\n",
            "Epoch 5219: train loss = 2222.841978, test loss = 3532.300544\n",
            "Epoch 5220: train loss = 2222.802904, test loss = 3532.274044\n",
            "Epoch 5221: train loss = 2222.763837, test loss = 3532.247549\n",
            "Epoch 5222: train loss = 2222.724779, test loss = 3532.221061\n",
            "Epoch 5223: train loss = 2222.685728, test loss = 3532.194578\n",
            "Epoch 5224: train loss = 2222.646686, test loss = 3532.168101\n",
            "Epoch 5225: train loss = 2222.607652, test loss = 3532.141630\n",
            "Epoch 5226: train loss = 2222.568627, test loss = 3532.115165\n",
            "Epoch 5227: train loss = 2222.529609, test loss = 3532.088706\n",
            "Epoch 5228: train loss = 2222.490599, test loss = 3532.062252\n",
            "Epoch 5229: train loss = 2222.451597, test loss = 3532.035805\n",
            "Epoch 5230: train loss = 2222.412604, test loss = 3532.009364\n",
            "Epoch 5231: train loss = 2222.373618, test loss = 3531.982928\n",
            "Epoch 5232: train loss = 2222.334641, test loss = 3531.956499\n",
            "Epoch 5233: train loss = 2222.295672, test loss = 3531.930075\n",
            "Epoch 5234: train loss = 2222.256710, test loss = 3531.903658\n",
            "Epoch 5235: train loss = 2222.217757, test loss = 3531.877246\n",
            "Epoch 5236: train loss = 2222.178812, test loss = 3531.850840\n",
            "Epoch 5237: train loss = 2222.139874, test loss = 3531.824441\n",
            "Epoch 5238: train loss = 2222.100945, test loss = 3531.798047\n",
            "Epoch 5239: train loss = 2222.062023, test loss = 3531.771659\n",
            "Epoch 5240: train loss = 2222.023110, test loss = 3531.745277\n",
            "Epoch 5241: train loss = 2221.984205, test loss = 3531.718902\n",
            "Epoch 5242: train loss = 2221.945307, test loss = 3531.692532\n",
            "Epoch 5243: train loss = 2221.906418, test loss = 3531.666168\n",
            "Epoch 5244: train loss = 2221.867536, test loss = 3531.639810\n",
            "Epoch 5245: train loss = 2221.828663, test loss = 3531.613458\n",
            "Epoch 5246: train loss = 2221.789797, test loss = 3531.587112\n",
            "Epoch 5247: train loss = 2221.750939, test loss = 3531.560773\n",
            "Epoch 5248: train loss = 2221.712089, test loss = 3531.534439\n",
            "Epoch 5249: train loss = 2221.673247, test loss = 3531.508111\n",
            "Epoch 5250: train loss = 2221.634413, test loss = 3531.481789\n",
            "Epoch 5251: train loss = 2221.595587, test loss = 3531.455473\n",
            "Epoch 5252: train loss = 2221.556768, test loss = 3531.429164\n",
            "Epoch 5253: train loss = 2221.517958, test loss = 3531.402860\n",
            "Epoch 5254: train loss = 2221.479155, test loss = 3531.376562\n",
            "Epoch 5255: train loss = 2221.440360, test loss = 3531.350271\n",
            "Epoch 5256: train loss = 2221.401573, test loss = 3531.323985\n",
            "Epoch 5257: train loss = 2221.362794, test loss = 3531.297705\n",
            "Epoch 5258: train loss = 2221.324023, test loss = 3531.271432\n",
            "Epoch 5259: train loss = 2221.285259, test loss = 3531.245165\n",
            "Epoch 5260: train loss = 2221.246503, test loss = 3531.218903\n",
            "Epoch 5261: train loss = 2221.207755, test loss = 3531.192648\n",
            "Epoch 5262: train loss = 2221.169015, test loss = 3531.166399\n",
            "Epoch 5263: train loss = 2221.130283, test loss = 3531.140156\n",
            "Epoch 5264: train loss = 2221.091558, test loss = 3531.113918\n",
            "Epoch 5265: train loss = 2221.052841, test loss = 3531.087688\n",
            "Epoch 5266: train loss = 2221.014132, test loss = 3531.061463\n",
            "Epoch 5267: train loss = 2220.975430, test loss = 3531.035244\n",
            "Epoch 5268: train loss = 2220.936736, test loss = 3531.009031\n",
            "Epoch 5269: train loss = 2220.898050, test loss = 3530.982825\n",
            "Epoch 5270: train loss = 2220.859372, test loss = 3530.956624\n",
            "Epoch 5271: train loss = 2220.820701, test loss = 3530.930430\n",
            "Epoch 5272: train loss = 2220.782038, test loss = 3530.904241\n",
            "Epoch 5273: train loss = 2220.743383, test loss = 3530.878059\n",
            "Epoch 5274: train loss = 2220.704735, test loss = 3530.851883\n",
            "Epoch 5275: train loss = 2220.666095, test loss = 3530.825713\n",
            "Epoch 5276: train loss = 2220.627463, test loss = 3530.799550\n",
            "Epoch 5277: train loss = 2220.588838, test loss = 3530.773392\n",
            "Epoch 5278: train loss = 2220.550221, test loss = 3530.747241\n",
            "Epoch 5279: train loss = 2220.511612, test loss = 3530.721095\n",
            "Epoch 5280: train loss = 2220.473010, test loss = 3530.694956\n",
            "Epoch 5281: train loss = 2220.434416, test loss = 3530.668823\n",
            "Epoch 5282: train loss = 2220.395829, test loss = 3530.642696\n",
            "Epoch 5283: train loss = 2220.357250, test loss = 3530.616576\n",
            "Epoch 5284: train loss = 2220.318679, test loss = 3530.590461\n",
            "Epoch 5285: train loss = 2220.280115, test loss = 3530.564353\n",
            "Epoch 5286: train loss = 2220.241558, test loss = 3530.538251\n",
            "Epoch 5287: train loss = 2220.203010, test loss = 3530.512155\n",
            "Epoch 5288: train loss = 2220.164468, test loss = 3530.486065\n",
            "Epoch 5289: train loss = 2220.125935, test loss = 3530.459982\n",
            "Epoch 5290: train loss = 2220.087408, test loss = 3530.433904\n",
            "Epoch 5291: train loss = 2220.048890, test loss = 3530.407833\n",
            "Epoch 5292: train loss = 2220.010378, test loss = 3530.381768\n",
            "Epoch 5293: train loss = 2219.971875, test loss = 3530.355710\n",
            "Epoch 5294: train loss = 2219.933379, test loss = 3530.329657\n",
            "Epoch 5295: train loss = 2219.894890, test loss = 3530.303611\n",
            "Epoch 5296: train loss = 2219.856409, test loss = 3530.277571\n",
            "Epoch 5297: train loss = 2219.817935, test loss = 3530.251537\n",
            "Epoch 5298: train loss = 2219.779468, test loss = 3530.225509\n",
            "Epoch 5299: train loss = 2219.741009, test loss = 3530.199488\n",
            "Epoch 5300: train loss = 2219.702558, test loss = 3530.173473\n",
            "Epoch 5301: train loss = 2219.664114, test loss = 3530.147464\n",
            "Epoch 5302: train loss = 2219.625677, test loss = 3530.121461\n",
            "Epoch 5303: train loss = 2219.587248, test loss = 3530.095465\n",
            "Epoch 5304: train loss = 2219.548826, test loss = 3530.069475\n",
            "Epoch 5305: train loss = 2219.510412, test loss = 3530.043491\n",
            "Epoch 5306: train loss = 2219.472005, test loss = 3530.017514\n",
            "Epoch 5307: train loss = 2219.433605, test loss = 3529.991543\n",
            "Epoch 5308: train loss = 2219.395213, test loss = 3529.965578\n",
            "Epoch 5309: train loss = 2219.356828, test loss = 3529.939619\n",
            "Epoch 5310: train loss = 2219.318450, test loss = 3529.913667\n",
            "Epoch 5311: train loss = 2219.280080, test loss = 3529.887721\n",
            "Epoch 5312: train loss = 2219.241717, test loss = 3529.861781\n",
            "Epoch 5313: train loss = 2219.203361, test loss = 3529.835847\n",
            "Epoch 5314: train loss = 2219.165013, test loss = 3529.809920\n",
            "Epoch 5315: train loss = 2219.126672, test loss = 3529.783999\n",
            "Epoch 5316: train loss = 2219.088338, test loss = 3529.758085\n",
            "Epoch 5317: train loss = 2219.050011, test loss = 3529.732177\n",
            "Epoch 5318: train loss = 2219.011692, test loss = 3529.706275\n",
            "Epoch 5319: train loss = 2218.973380, test loss = 3529.680380\n",
            "Epoch 5320: train loss = 2218.935076, test loss = 3529.654490\n",
            "Epoch 5321: train loss = 2218.896778, test loss = 3529.628608\n",
            "Epoch 5322: train loss = 2218.858488, test loss = 3529.602731\n",
            "Epoch 5323: train loss = 2218.820205, test loss = 3529.576861\n",
            "Epoch 5324: train loss = 2218.781929, test loss = 3529.550997\n",
            "Epoch 5325: train loss = 2218.743661, test loss = 3529.525140\n",
            "Epoch 5326: train loss = 2218.705400, test loss = 3529.499289\n",
            "Epoch 5327: train loss = 2218.667145, test loss = 3529.473445\n",
            "Epoch 5328: train loss = 2218.628898, test loss = 3529.447606\n",
            "Epoch 5329: train loss = 2218.590659, test loss = 3529.421775\n",
            "Epoch 5330: train loss = 2218.552426, test loss = 3529.395949\n",
            "Epoch 5331: train loss = 2218.514201, test loss = 3529.370130\n",
            "Epoch 5332: train loss = 2218.475982, test loss = 3529.344318\n",
            "Epoch 5333: train loss = 2218.437771, test loss = 3529.318511\n",
            "Epoch 5334: train loss = 2218.399567, test loss = 3529.292712\n",
            "Epoch 5335: train loss = 2218.361370, test loss = 3529.266918\n",
            "Epoch 5336: train loss = 2218.323181, test loss = 3529.241131\n",
            "Epoch 5337: train loss = 2218.284998, test loss = 3529.215351\n",
            "Epoch 5338: train loss = 2218.246822, test loss = 3529.189577\n",
            "Epoch 5339: train loss = 2218.208654, test loss = 3529.163809\n",
            "Epoch 5340: train loss = 2218.170492, test loss = 3529.138048\n",
            "Epoch 5341: train loss = 2218.132338, test loss = 3529.112293\n",
            "Epoch 5342: train loss = 2218.094191, test loss = 3529.086545\n",
            "Epoch 5343: train loss = 2218.056051, test loss = 3529.060803\n",
            "Epoch 5344: train loss = 2218.017918, test loss = 3529.035068\n",
            "Epoch 5345: train loss = 2217.979791, test loss = 3529.009339\n",
            "Epoch 5346: train loss = 2217.941672, test loss = 3528.983617\n",
            "Epoch 5347: train loss = 2217.903560, test loss = 3528.957901\n",
            "Epoch 5348: train loss = 2217.865455, test loss = 3528.932191\n",
            "Epoch 5349: train loss = 2217.827357, test loss = 3528.906488\n",
            "Epoch 5350: train loss = 2217.789266, test loss = 3528.880792\n",
            "Epoch 5351: train loss = 2217.751182, test loss = 3528.855102\n",
            "Epoch 5352: train loss = 2217.713105, test loss = 3528.829418\n",
            "Epoch 5353: train loss = 2217.675035, test loss = 3528.803742\n",
            "Epoch 5354: train loss = 2217.636972, test loss = 3528.778071\n",
            "Epoch 5355: train loss = 2217.598915, test loss = 3528.752407\n",
            "Epoch 5356: train loss = 2217.560866, test loss = 3528.726750\n",
            "Epoch 5357: train loss = 2217.522824, test loss = 3528.701099\n",
            "Epoch 5358: train loss = 2217.484788, test loss = 3528.675455\n",
            "Epoch 5359: train loss = 2217.446760, test loss = 3528.649817\n",
            "Epoch 5360: train loss = 2217.408738, test loss = 3528.624186\n",
            "Epoch 5361: train loss = 2217.370724, test loss = 3528.598561\n",
            "Epoch 5362: train loss = 2217.332716, test loss = 3528.572943\n",
            "Epoch 5363: train loss = 2217.294715, test loss = 3528.547332\n",
            "Epoch 5364: train loss = 2217.256721, test loss = 3528.521727\n",
            "Epoch 5365: train loss = 2217.218734, test loss = 3528.496129\n",
            "Epoch 5366: train loss = 2217.180753, test loss = 3528.470537\n",
            "Epoch 5367: train loss = 2217.142780, test loss = 3528.444952\n",
            "Epoch 5368: train loss = 2217.104813, test loss = 3528.419373\n",
            "Epoch 5369: train loss = 2217.066853, test loss = 3528.393801\n",
            "Epoch 5370: train loss = 2217.028900, test loss = 3528.368236\n",
            "Epoch 5371: train loss = 2216.990954, test loss = 3528.342677\n",
            "Epoch 5372: train loss = 2216.953015, test loss = 3528.317125\n",
            "Epoch 5373: train loss = 2216.915082, test loss = 3528.291579\n",
            "Epoch 5374: train loss = 2216.877156, test loss = 3528.266041\n",
            "Epoch 5375: train loss = 2216.839237, test loss = 3528.240508\n",
            "Epoch 5376: train loss = 2216.801325, test loss = 3528.214983\n",
            "Epoch 5377: train loss = 2216.763420, test loss = 3528.189464\n",
            "Epoch 5378: train loss = 2216.725521, test loss = 3528.163951\n",
            "Epoch 5379: train loss = 2216.687629, test loss = 3528.138446\n",
            "Epoch 5380: train loss = 2216.649744, test loss = 3528.112947\n",
            "Epoch 5381: train loss = 2216.611865, test loss = 3528.087454\n",
            "Epoch 5382: train loss = 2216.573993, test loss = 3528.061969\n",
            "Epoch 5383: train loss = 2216.536128, test loss = 3528.036490\n",
            "Epoch 5384: train loss = 2216.498270, test loss = 3528.011017\n",
            "Epoch 5385: train loss = 2216.460418, test loss = 3527.985552\n",
            "Epoch 5386: train loss = 2216.422573, test loss = 3527.960093\n",
            "Epoch 5387: train loss = 2216.384735, test loss = 3527.934641\n",
            "Epoch 5388: train loss = 2216.346903, test loss = 3527.909195\n",
            "Epoch 5389: train loss = 2216.309078, test loss = 3527.883756\n",
            "Epoch 5390: train loss = 2216.271259, test loss = 3527.858324\n",
            "Epoch 5391: train loss = 2216.233448, test loss = 3527.832899\n",
            "Epoch 5392: train loss = 2216.195643, test loss = 3527.807480\n",
            "Epoch 5393: train loss = 2216.157844, test loss = 3527.782068\n",
            "Epoch 5394: train loss = 2216.120052, test loss = 3527.756663\n",
            "Epoch 5395: train loss = 2216.082267, test loss = 3527.731265\n",
            "Epoch 5396: train loss = 2216.044488, test loss = 3527.705873\n",
            "Epoch 5397: train loss = 2216.006716, test loss = 3527.680488\n",
            "Epoch 5398: train loss = 2215.968951, test loss = 3527.655110\n",
            "Epoch 5399: train loss = 2215.931192, test loss = 3527.629739\n",
            "Epoch 5400: train loss = 2215.893439, test loss = 3527.604374\n",
            "Epoch 5401: train loss = 2215.855693, test loss = 3527.579016\n",
            "Epoch 5402: train loss = 2215.817954, test loss = 3527.553665\n",
            "Epoch 5403: train loss = 2215.780221, test loss = 3527.528321\n",
            "Epoch 5404: train loss = 2215.742495, test loss = 3527.502983\n",
            "Epoch 5405: train loss = 2215.704776, test loss = 3527.477653\n",
            "Epoch 5406: train loss = 2215.667062, test loss = 3527.452329\n",
            "Epoch 5407: train loss = 2215.629356, test loss = 3527.427012\n",
            "Epoch 5408: train loss = 2215.591656, test loss = 3527.401702\n",
            "Epoch 5409: train loss = 2215.553962, test loss = 3527.376398\n",
            "Epoch 5410: train loss = 2215.516275, test loss = 3527.351102\n",
            "Epoch 5411: train loss = 2215.478594, test loss = 3527.325812\n",
            "Epoch 5412: train loss = 2215.440920, test loss = 3527.300529\n",
            "Epoch 5413: train loss = 2215.403252, test loss = 3527.275253\n",
            "Epoch 5414: train loss = 2215.365590, test loss = 3527.249984\n",
            "Epoch 5415: train loss = 2215.327935, test loss = 3527.224722\n",
            "Epoch 5416: train loss = 2215.290287, test loss = 3527.199466\n",
            "Epoch 5417: train loss = 2215.252645, test loss = 3527.174218\n",
            "Epoch 5418: train loss = 2215.215009, test loss = 3527.148976\n",
            "Epoch 5419: train loss = 2215.177380, test loss = 3527.123741\n",
            "Epoch 5420: train loss = 2215.139757, test loss = 3527.098513\n",
            "Epoch 5421: train loss = 2215.102140, test loss = 3527.073292\n",
            "Epoch 5422: train loss = 2215.064530, test loss = 3527.048078\n",
            "Epoch 5423: train loss = 2215.026926, test loss = 3527.022871\n",
            "Epoch 5424: train loss = 2214.989329, test loss = 3526.997671\n",
            "Epoch 5425: train loss = 2214.951738, test loss = 3526.972477\n",
            "Epoch 5426: train loss = 2214.914153, test loss = 3526.947291\n",
            "Epoch 5427: train loss = 2214.876575, test loss = 3526.922111\n",
            "Epoch 5428: train loss = 2214.839003, test loss = 3526.896939\n",
            "Epoch 5429: train loss = 2214.801437, test loss = 3526.871773\n",
            "Epoch 5430: train loss = 2214.763877, test loss = 3526.846615\n",
            "Epoch 5431: train loss = 2214.726324, test loss = 3526.821463\n",
            "Epoch 5432: train loss = 2214.688777, test loss = 3526.796318\n",
            "Epoch 5433: train loss = 2214.651237, test loss = 3526.771180\n",
            "Epoch 5434: train loss = 2214.613702, test loss = 3526.746050\n",
            "Epoch 5435: train loss = 2214.576174, test loss = 3526.720926\n",
            "Epoch 5436: train loss = 2214.538653, test loss = 3526.695809\n",
            "Epoch 5437: train loss = 2214.501137, test loss = 3526.670699\n",
            "Epoch 5438: train loss = 2214.463628, test loss = 3526.645596\n",
            "Epoch 5439: train loss = 2214.426125, test loss = 3526.620501\n",
            "Epoch 5440: train loss = 2214.388628, test loss = 3526.595412\n",
            "Epoch 5441: train loss = 2214.351137, test loss = 3526.570330\n",
            "Epoch 5442: train loss = 2214.313653, test loss = 3526.545255\n",
            "Epoch 5443: train loss = 2214.276175, test loss = 3526.520188\n",
            "Epoch 5444: train loss = 2214.238702, test loss = 3526.495127\n",
            "Epoch 5445: train loss = 2214.201237, test loss = 3526.470073\n",
            "Epoch 5446: train loss = 2214.163777, test loss = 3526.445027\n",
            "Epoch 5447: train loss = 2214.126323, test loss = 3526.419987\n",
            "Epoch 5448: train loss = 2214.088876, test loss = 3526.394955\n",
            "Epoch 5449: train loss = 2214.051435, test loss = 3526.369930\n",
            "Epoch 5450: train loss = 2214.014000, test loss = 3526.344911\n",
            "Epoch 5451: train loss = 2213.976571, test loss = 3526.319900\n",
            "Epoch 5452: train loss = 2213.939148, test loss = 3526.294896\n",
            "Epoch 5453: train loss = 2213.901731, test loss = 3526.269899\n",
            "Epoch 5454: train loss = 2213.864320, test loss = 3526.244909\n",
            "Epoch 5455: train loss = 2213.826916, test loss = 3526.219926\n",
            "Epoch 5456: train loss = 2213.789517, test loss = 3526.194951\n",
            "Epoch 5457: train loss = 2213.752125, test loss = 3526.169982\n",
            "Epoch 5458: train loss = 2213.714739, test loss = 3526.145021\n",
            "Epoch 5459: train loss = 2213.677358, test loss = 3526.120067\n",
            "Epoch 5460: train loss = 2213.639984, test loss = 3526.095119\n",
            "Epoch 5461: train loss = 2213.602616, test loss = 3526.070179\n",
            "Epoch 5462: train loss = 2213.565254, test loss = 3526.045247\n",
            "Epoch 5463: train loss = 2213.527898, test loss = 3526.020321\n",
            "Epoch 5464: train loss = 2213.490548, test loss = 3525.995402\n",
            "Epoch 5465: train loss = 2213.453204, test loss = 3525.970491\n",
            "Epoch 5466: train loss = 2213.415865, test loss = 3525.945587\n",
            "Epoch 5467: train loss = 2213.378533, test loss = 3525.920690\n",
            "Epoch 5468: train loss = 2213.341207, test loss = 3525.895800\n",
            "Epoch 5469: train loss = 2213.303887, test loss = 3525.870918\n",
            "Epoch 5470: train loss = 2213.266573, test loss = 3525.846042\n",
            "Epoch 5471: train loss = 2213.229265, test loss = 3525.821174\n",
            "Epoch 5472: train loss = 2213.191963, test loss = 3525.796313\n",
            "Epoch 5473: train loss = 2213.154666, test loss = 3525.771460\n",
            "Epoch 5474: train loss = 2213.117376, test loss = 3525.746613\n",
            "Epoch 5475: train loss = 2213.080091, test loss = 3525.721774\n",
            "Epoch 5476: train loss = 2213.042813, test loss = 3525.696942\n",
            "Epoch 5477: train loss = 2213.005540, test loss = 3525.672118\n",
            "Epoch 5478: train loss = 2212.968273, test loss = 3525.647300\n",
            "Epoch 5479: train loss = 2212.931013, test loss = 3525.622490\n",
            "Epoch 5480: train loss = 2212.893758, test loss = 3525.597687\n",
            "Epoch 5481: train loss = 2212.856508, test loss = 3525.572892\n",
            "Epoch 5482: train loss = 2212.819265, test loss = 3525.548103\n",
            "Epoch 5483: train loss = 2212.782028, test loss = 3525.523322\n",
            "Epoch 5484: train loss = 2212.744796, test loss = 3525.498549\n",
            "Epoch 5485: train loss = 2212.707571, test loss = 3525.473782\n",
            "Epoch 5486: train loss = 2212.670351, test loss = 3525.449023\n",
            "Epoch 5487: train loss = 2212.633137, test loss = 3525.424271\n",
            "Epoch 5488: train loss = 2212.595929, test loss = 3525.399527\n",
            "Epoch 5489: train loss = 2212.558726, test loss = 3525.374790\n",
            "Epoch 5490: train loss = 2212.521529, test loss = 3525.350060\n",
            "Epoch 5491: train loss = 2212.484339, test loss = 3525.325338\n",
            "Epoch 5492: train loss = 2212.447154, test loss = 3525.300623\n",
            "Epoch 5493: train loss = 2212.409974, test loss = 3525.275915\n",
            "Epoch 5494: train loss = 2212.372801, test loss = 3525.251215\n",
            "Epoch 5495: train loss = 2212.335633, test loss = 3525.226522\n",
            "Epoch 5496: train loss = 2212.298471, test loss = 3525.201837\n",
            "Epoch 5497: train loss = 2212.261315, test loss = 3525.177159\n",
            "Epoch 5498: train loss = 2212.224164, test loss = 3525.152488\n",
            "Epoch 5499: train loss = 2212.187019, test loss = 3525.127825\n",
            "Epoch 5500: train loss = 2212.149880, test loss = 3525.103169\n",
            "Epoch 5501: train loss = 2212.112747, test loss = 3525.078520\n",
            "Epoch 5502: train loss = 2212.075619, test loss = 3525.053879\n",
            "Epoch 5503: train loss = 2212.038497, test loss = 3525.029246\n",
            "Epoch 5504: train loss = 2212.001380, test loss = 3525.004620\n",
            "Epoch 5505: train loss = 2211.964270, test loss = 3524.980001\n",
            "Epoch 5506: train loss = 2211.927165, test loss = 3524.955390\n",
            "Epoch 5507: train loss = 2211.890065, test loss = 3524.930786\n",
            "Epoch 5508: train loss = 2211.852972, test loss = 3524.906190\n",
            "Epoch 5509: train loss = 2211.815883, test loss = 3524.881601\n",
            "Epoch 5510: train loss = 2211.778801, test loss = 3524.857020\n",
            "Epoch 5511: train loss = 2211.741724, test loss = 3524.832446\n",
            "Epoch 5512: train loss = 2211.704653, test loss = 3524.807879\n",
            "Epoch 5513: train loss = 2211.667587, test loss = 3524.783321\n",
            "Epoch 5514: train loss = 2211.630527, test loss = 3524.758769\n",
            "Epoch 5515: train loss = 2211.593472, test loss = 3524.734226\n",
            "Epoch 5516: train loss = 2211.556424, test loss = 3524.709689\n",
            "Epoch 5517: train loss = 2211.519380, test loss = 3524.685161\n",
            "Epoch 5518: train loss = 2211.482342, test loss = 3524.660639\n",
            "Epoch 5519: train loss = 2211.445310, test loss = 3524.636126\n",
            "Epoch 5520: train loss = 2211.408283, test loss = 3524.611620\n",
            "Epoch 5521: train loss = 2211.371262, test loss = 3524.587121\n",
            "Epoch 5522: train loss = 2211.334247, test loss = 3524.562631\n",
            "Epoch 5523: train loss = 2211.297236, test loss = 3524.538147\n",
            "Epoch 5524: train loss = 2211.260232, test loss = 3524.513672\n",
            "Epoch 5525: train loss = 2211.223233, test loss = 3524.489203\n",
            "Epoch 5526: train loss = 2211.186239, test loss = 3524.464743\n",
            "Epoch 5527: train loss = 2211.149251, test loss = 3524.440290\n",
            "Epoch 5528: train loss = 2211.112268, test loss = 3524.415845\n",
            "Epoch 5529: train loss = 2211.075291, test loss = 3524.391407\n",
            "Epoch 5530: train loss = 2211.038319, test loss = 3524.366977\n",
            "Epoch 5531: train loss = 2211.001353, test loss = 3524.342555\n",
            "Epoch 5532: train loss = 2210.964392, test loss = 3524.318140\n",
            "Epoch 5533: train loss = 2210.927436, test loss = 3524.293733\n",
            "Epoch 5534: train loss = 2210.890486, test loss = 3524.269334\n",
            "Epoch 5535: train loss = 2210.853541, test loss = 3524.244942\n",
            "Epoch 5536: train loss = 2210.816602, test loss = 3524.220558\n",
            "Epoch 5537: train loss = 2210.779668, test loss = 3524.196182\n",
            "Epoch 5538: train loss = 2210.742740, test loss = 3524.171813\n",
            "Epoch 5539: train loss = 2210.705817, test loss = 3524.147452\n",
            "Epoch 5540: train loss = 2210.668899, test loss = 3524.123099\n",
            "Epoch 5541: train loss = 2210.631987, test loss = 3524.098753\n",
            "Epoch 5542: train loss = 2210.595080, test loss = 3524.074416\n",
            "Epoch 5543: train loss = 2210.558178, test loss = 3524.050086\n",
            "Epoch 5544: train loss = 2210.521282, test loss = 3524.025763\n",
            "Epoch 5545: train loss = 2210.484391, test loss = 3524.001449\n",
            "Epoch 5546: train loss = 2210.447505, test loss = 3523.977142\n",
            "Epoch 5547: train loss = 2210.410624, test loss = 3523.952843\n",
            "Epoch 5548: train loss = 2210.373749, test loss = 3523.928552\n",
            "Epoch 5549: train loss = 2210.336880, test loss = 3523.904268\n",
            "Epoch 5550: train loss = 2210.300015, test loss = 3523.879993\n",
            "Epoch 5551: train loss = 2210.263156, test loss = 3523.855725\n",
            "Epoch 5552: train loss = 2210.226302, test loss = 3523.831465\n",
            "Epoch 5553: train loss = 2210.189453, test loss = 3523.807213\n",
            "Epoch 5554: train loss = 2210.152610, test loss = 3523.782968\n",
            "Epoch 5555: train loss = 2210.115771, test loss = 3523.758732\n",
            "Epoch 5556: train loss = 2210.078938, test loss = 3523.734503\n",
            "Epoch 5557: train loss = 2210.042110, test loss = 3523.710282\n",
            "Epoch 5558: train loss = 2210.005288, test loss = 3523.686069\n",
            "Epoch 5559: train loss = 2209.968470, test loss = 3523.661864\n",
            "Epoch 5560: train loss = 2209.931658, test loss = 3523.637667\n",
            "Epoch 5561: train loss = 2209.894851, test loss = 3523.613477\n",
            "Epoch 5562: train loss = 2209.858049, test loss = 3523.589296\n",
            "Epoch 5563: train loss = 2209.821253, test loss = 3523.565122\n",
            "Epoch 5564: train loss = 2209.784461, test loss = 3523.540956\n",
            "Epoch 5565: train loss = 2209.747675, test loss = 3523.516798\n",
            "Epoch 5566: train loss = 2209.710894, test loss = 3523.492649\n",
            "Epoch 5567: train loss = 2209.674118, test loss = 3523.468507\n",
            "Epoch 5568: train loss = 2209.637347, test loss = 3523.444373\n",
            "Epoch 5569: train loss = 2209.600581, test loss = 3523.420246\n",
            "Epoch 5570: train loss = 2209.563820, test loss = 3523.396128\n",
            "Epoch 5571: train loss = 2209.527064, test loss = 3523.372018\n",
            "Epoch 5572: train loss = 2209.490314, test loss = 3523.347916\n",
            "Epoch 5573: train loss = 2209.453568, test loss = 3523.323822\n",
            "Epoch 5574: train loss = 2209.416828, test loss = 3523.299735\n",
            "Epoch 5575: train loss = 2209.380093, test loss = 3523.275657\n",
            "Epoch 5576: train loss = 2209.343362, test loss = 3523.251587\n",
            "Epoch 5577: train loss = 2209.306637, test loss = 3523.227525\n",
            "Epoch 5578: train loss = 2209.269917, test loss = 3523.203471\n",
            "Epoch 5579: train loss = 2209.233202, test loss = 3523.179424\n",
            "Epoch 5580: train loss = 2209.196492, test loss = 3523.155386\n",
            "Epoch 5581: train loss = 2209.159786, test loss = 3523.131356\n",
            "Epoch 5582: train loss = 2209.123086, test loss = 3523.107334\n",
            "Epoch 5583: train loss = 2209.086391, test loss = 3523.083320\n",
            "Epoch 5584: train loss = 2209.049701, test loss = 3523.059314\n",
            "Epoch 5585: train loss = 2209.013016, test loss = 3523.035316\n",
            "Epoch 5586: train loss = 2208.976336, test loss = 3523.011327\n",
            "Epoch 5587: train loss = 2208.939660, test loss = 3522.987345\n",
            "Epoch 5588: train loss = 2208.902990, test loss = 3522.963371\n",
            "Epoch 5589: train loss = 2208.866325, test loss = 3522.939406\n",
            "Epoch 5590: train loss = 2208.829664, test loss = 3522.915449\n",
            "Epoch 5591: train loss = 2208.793009, test loss = 3522.891499\n",
            "Epoch 5592: train loss = 2208.756358, test loss = 3522.867558\n",
            "Epoch 5593: train loss = 2208.719712, test loss = 3522.843625\n",
            "Epoch 5594: train loss = 2208.683072, test loss = 3522.819701\n",
            "Epoch 5595: train loss = 2208.646436, test loss = 3522.795784\n",
            "Epoch 5596: train loss = 2208.609805, test loss = 3522.771876\n",
            "Epoch 5597: train loss = 2208.573179, test loss = 3522.747975\n",
            "Epoch 5598: train loss = 2208.536557, test loss = 3522.724083\n",
            "Epoch 5599: train loss = 2208.499941, test loss = 3522.700200\n",
            "Epoch 5600: train loss = 2208.463329, test loss = 3522.676324\n",
            "Epoch 5601: train loss = 2208.426722, test loss = 3522.652457\n",
            "Epoch 5602: train loss = 2208.390121, test loss = 3522.628597\n",
            "Epoch 5603: train loss = 2208.353523, test loss = 3522.604746\n",
            "Epoch 5604: train loss = 2208.316931, test loss = 3522.580904\n",
            "Epoch 5605: train loss = 2208.280344, test loss = 3522.557069\n",
            "Epoch 5606: train loss = 2208.243761, test loss = 3522.533243\n",
            "Epoch 5607: train loss = 2208.207183, test loss = 3522.509425\n",
            "Epoch 5608: train loss = 2208.170610, test loss = 3522.485616\n",
            "Epoch 5609: train loss = 2208.134041, test loss = 3522.461814\n",
            "Epoch 5610: train loss = 2208.097478, test loss = 3522.438021\n",
            "Epoch 5611: train loss = 2208.060919, test loss = 3522.414236\n",
            "Epoch 5612: train loss = 2208.024365, test loss = 3522.390460\n",
            "Epoch 5613: train loss = 2207.987815, test loss = 3522.366692\n",
            "Epoch 5614: train loss = 2207.951270, test loss = 3522.342932\n",
            "Epoch 5615: train loss = 2207.914730, test loss = 3522.319181\n",
            "Epoch 5616: train loss = 2207.878195, test loss = 3522.295438\n",
            "Epoch 5617: train loss = 2207.841664, test loss = 3522.271703\n",
            "Epoch 5618: train loss = 2207.805138, test loss = 3522.247977\n",
            "Epoch 5619: train loss = 2207.768617, test loss = 3522.224259\n",
            "Epoch 5620: train loss = 2207.732100, test loss = 3522.200549\n",
            "Epoch 5621: train loss = 2207.695589, test loss = 3522.176848\n",
            "Epoch 5622: train loss = 2207.659081, test loss = 3522.153155\n",
            "Epoch 5623: train loss = 2207.622579, test loss = 3522.129471\n",
            "Epoch 5624: train loss = 2207.586081, test loss = 3522.105795\n",
            "Epoch 5625: train loss = 2207.549587, test loss = 3522.082127\n",
            "Epoch 5626: train loss = 2207.513098, test loss = 3522.058468\n",
            "Epoch 5627: train loss = 2207.476614, test loss = 3522.034818\n",
            "Epoch 5628: train loss = 2207.440134, test loss = 3522.011176\n",
            "Epoch 5629: train loss = 2207.403659, test loss = 3521.987542\n",
            "Epoch 5630: train loss = 2207.367189, test loss = 3521.963917\n",
            "Epoch 5631: train loss = 2207.330723, test loss = 3521.940300\n",
            "Epoch 5632: train loss = 2207.294262, test loss = 3521.916692\n",
            "Epoch 5633: train loss = 2207.257805, test loss = 3521.893092\n",
            "Epoch 5634: train loss = 2207.221353, test loss = 3521.869501\n",
            "Epoch 5635: train loss = 2207.184905, test loss = 3521.845919\n",
            "Epoch 5636: train loss = 2207.148462, test loss = 3521.822345\n",
            "Epoch 5637: train loss = 2207.112023, test loss = 3521.798779\n",
            "Epoch 5638: train loss = 2207.075589, test loss = 3521.775222\n",
            "Epoch 5639: train loss = 2207.039159, test loss = 3521.751674\n",
            "Epoch 5640: train loss = 2207.002734, test loss = 3521.728134\n",
            "Epoch 5641: train loss = 2206.966313, test loss = 3521.704603\n",
            "Epoch 5642: train loss = 2206.929897, test loss = 3521.681080\n",
            "Epoch 5643: train loss = 2206.893485, test loss = 3521.657566\n",
            "Epoch 5644: train loss = 2206.857078, test loss = 3521.634061\n",
            "Epoch 5645: train loss = 2206.820675, test loss = 3521.610564\n",
            "Epoch 5646: train loss = 2206.784276, test loss = 3521.587076\n",
            "Epoch 5647: train loss = 2206.747882, test loss = 3521.563596\n",
            "Epoch 5648: train loss = 2206.711492, test loss = 3521.540125\n",
            "Epoch 5649: train loss = 2206.675107, test loss = 3521.516663\n",
            "Epoch 5650: train loss = 2206.638726, test loss = 3521.493210\n",
            "Epoch 5651: train loss = 2206.602350, test loss = 3521.469765\n",
            "Epoch 5652: train loss = 2206.565977, test loss = 3521.446329\n",
            "Epoch 5653: train loss = 2206.529610, test loss = 3521.422901\n",
            "Epoch 5654: train loss = 2206.493246, test loss = 3521.399482\n",
            "Epoch 5655: train loss = 2206.456887, test loss = 3521.376073\n",
            "Epoch 5656: train loss = 2206.420532, test loss = 3521.352671\n",
            "Epoch 5657: train loss = 2206.384182, test loss = 3521.329279\n",
            "Epoch 5658: train loss = 2206.347836, test loss = 3521.305895\n",
            "Epoch 5659: train loss = 2206.311494, test loss = 3521.282520\n",
            "Epoch 5660: train loss = 2206.275157, test loss = 3521.259154\n",
            "Epoch 5661: train loss = 2206.238823, test loss = 3521.235796\n",
            "Epoch 5662: train loss = 2206.202494, test loss = 3521.212448\n",
            "Epoch 5663: train loss = 2206.166170, test loss = 3521.189108\n",
            "Epoch 5664: train loss = 2206.129849, test loss = 3521.165777\n",
            "Epoch 5665: train loss = 2206.093533, test loss = 3521.142455\n",
            "Epoch 5666: train loss = 2206.057221, test loss = 3521.119141\n",
            "Epoch 5667: train loss = 2206.020914, test loss = 3521.095837\n",
            "Epoch 5668: train loss = 2205.984610, test loss = 3521.072541\n",
            "Epoch 5669: train loss = 2205.948311, test loss = 3521.049254\n",
            "Epoch 5670: train loss = 2205.912016, test loss = 3521.025977\n",
            "Epoch 5671: train loss = 2205.875725, test loss = 3521.002708\n",
            "Epoch 5672: train loss = 2205.839439, test loss = 3520.979447\n",
            "Epoch 5673: train loss = 2205.803156, test loss = 3520.956196\n",
            "Epoch 5674: train loss = 2205.766878, test loss = 3520.932954\n",
            "Epoch 5675: train loss = 2205.730604, test loss = 3520.909721\n",
            "Epoch 5676: train loss = 2205.694334, test loss = 3520.886496\n",
            "Epoch 5677: train loss = 2205.658068, test loss = 3520.863281\n",
            "Epoch 5678: train loss = 2205.621806, test loss = 3520.840074\n",
            "Epoch 5679: train loss = 2205.585548, test loss = 3520.816877\n",
            "Epoch 5680: train loss = 2205.549295, test loss = 3520.793688\n",
            "Epoch 5681: train loss = 2205.513046, test loss = 3520.770508\n",
            "Epoch 5682: train loss = 2205.476800, test loss = 3520.747338\n",
            "Epoch 5683: train loss = 2205.440559, test loss = 3520.724176\n",
            "Epoch 5684: train loss = 2205.404322, test loss = 3520.701024\n",
            "Epoch 5685: train loss = 2205.368089, test loss = 3520.677880\n",
            "Epoch 5686: train loss = 2205.331860, test loss = 3520.654746\n",
            "Epoch 5687: train loss = 2205.295635, test loss = 3520.631621\n",
            "Epoch 5688: train loss = 2205.259414, test loss = 3520.608504\n",
            "Epoch 5689: train loss = 2205.223197, test loss = 3520.585397\n",
            "Epoch 5690: train loss = 2205.186984, test loss = 3520.562299\n",
            "Epoch 5691: train loss = 2205.150775, test loss = 3520.539210\n",
            "Epoch 5692: train loss = 2205.114570, test loss = 3520.516130\n",
            "Epoch 5693: train loss = 2205.078369, test loss = 3520.493059\n",
            "Epoch 5694: train loss = 2205.042172, test loss = 3520.469997\n",
            "Epoch 5695: train loss = 2205.005979, test loss = 3520.446945\n",
            "Epoch 5696: train loss = 2204.969790, test loss = 3520.423901\n",
            "Epoch 5697: train loss = 2204.933605, test loss = 3520.400867\n",
            "Epoch 5698: train loss = 2204.897424, test loss = 3520.377842\n",
            "Epoch 5699: train loss = 2204.861247, test loss = 3520.354826\n",
            "Epoch 5700: train loss = 2204.825074, test loss = 3520.331819\n",
            "Epoch 5701: train loss = 2204.788904, test loss = 3520.308822\n",
            "Epoch 5702: train loss = 2204.752739, test loss = 3520.285834\n",
            "Epoch 5703: train loss = 2204.716577, test loss = 3520.262855\n",
            "Epoch 5704: train loss = 2204.680420, test loss = 3520.239885\n",
            "Epoch 5705: train loss = 2204.644266, test loss = 3520.216924\n",
            "Epoch 5706: train loss = 2204.608116, test loss = 3520.193973\n",
            "Epoch 5707: train loss = 2204.571970, test loss = 3520.171031\n",
            "Epoch 5708: train loss = 2204.535827, test loss = 3520.148098\n",
            "Epoch 5709: train loss = 2204.499689, test loss = 3520.125175\n",
            "Epoch 5710: train loss = 2204.463554, test loss = 3520.102261\n",
            "Epoch 5711: train loss = 2204.427424, test loss = 3520.079356\n",
            "Epoch 5712: train loss = 2204.391297, test loss = 3520.056461\n",
            "Epoch 5713: train loss = 2204.355173, test loss = 3520.033575\n",
            "Epoch 5714: train loss = 2204.319054, test loss = 3520.010698\n",
            "Epoch 5715: train loss = 2204.282938, test loss = 3519.987831\n",
            "Epoch 5716: train loss = 2204.246826, test loss = 3519.964973\n",
            "Epoch 5717: train loss = 2204.210718, test loss = 3519.942124\n",
            "Epoch 5718: train loss = 2204.174614, test loss = 3519.919285\n",
            "Epoch 5719: train loss = 2204.138513, test loss = 3519.896455\n",
            "Epoch 5720: train loss = 2204.102416, test loss = 3519.873635\n",
            "Epoch 5721: train loss = 2204.066323, test loss = 3519.850824\n",
            "Epoch 5722: train loss = 2204.030233, test loss = 3519.828022\n",
            "Epoch 5723: train loss = 2203.994147, test loss = 3519.805230\n",
            "Epoch 5724: train loss = 2203.958065, test loss = 3519.782448\n",
            "Epoch 5725: train loss = 2203.921987, test loss = 3519.759675\n",
            "Epoch 5726: train loss = 2203.885912, test loss = 3519.736911\n",
            "Epoch 5727: train loss = 2203.849841, test loss = 3519.714157\n",
            "Epoch 5728: train loss = 2203.813773, test loss = 3519.691413\n",
            "Epoch 5729: train loss = 2203.777709, test loss = 3519.668678\n",
            "Epoch 5730: train loss = 2203.741649, test loss = 3519.645952\n",
            "Epoch 5731: train loss = 2203.705592, test loss = 3519.623236\n",
            "Epoch 5732: train loss = 2203.669539, test loss = 3519.600530\n",
            "Epoch 5733: train loss = 2203.633490, test loss = 3519.577833\n",
            "Epoch 5734: train loss = 2203.597444, test loss = 3519.555146\n",
            "Epoch 5735: train loss = 2203.561402, test loss = 3519.532468\n",
            "Epoch 5736: train loss = 2203.525363, test loss = 3519.509800\n",
            "Epoch 5737: train loss = 2203.489328, test loss = 3519.487142\n",
            "Epoch 5738: train loss = 2203.453296, test loss = 3519.464493\n",
            "Epoch 5739: train loss = 2203.417268, test loss = 3519.441854\n",
            "Epoch 5740: train loss = 2203.381243, test loss = 3519.419225\n",
            "Epoch 5741: train loss = 2203.345222, test loss = 3519.396605\n",
            "Epoch 5742: train loss = 2203.309205, test loss = 3519.373995\n",
            "Epoch 5743: train loss = 2203.273191, test loss = 3519.351395\n",
            "Epoch 5744: train loss = 2203.237180, test loss = 3519.328804\n",
            "Epoch 5745: train loss = 2203.201173, test loss = 3519.306223\n",
            "Epoch 5746: train loss = 2203.165169, test loss = 3519.283652\n",
            "Epoch 5747: train loss = 2203.129169, test loss = 3519.261091\n",
            "Epoch 5748: train loss = 2203.093172, test loss = 3519.238539\n",
            "Epoch 5749: train loss = 2203.057179, test loss = 3519.215997\n",
            "Epoch 5750: train loss = 2203.021189, test loss = 3519.193465\n",
            "Epoch 5751: train loss = 2202.985202, test loss = 3519.170943\n",
            "Epoch 5752: train loss = 2202.949219, test loss = 3519.148430\n",
            "Epoch 5753: train loss = 2202.913239, test loss = 3519.125928\n",
            "Epoch 5754: train loss = 2202.877263, test loss = 3519.103435\n",
            "Epoch 5755: train loss = 2202.841290, test loss = 3519.080952\n",
            "Epoch 5756: train loss = 2202.805320, test loss = 3519.058479\n",
            "Epoch 5757: train loss = 2202.769354, test loss = 3519.036016\n",
            "Epoch 5758: train loss = 2202.733391, test loss = 3519.013562\n",
            "Epoch 5759: train loss = 2202.697431, test loss = 3518.991119\n",
            "Epoch 5760: train loss = 2202.661475, test loss = 3518.968685\n",
            "Epoch 5761: train loss = 2202.625522, test loss = 3518.946262\n",
            "Epoch 5762: train loss = 2202.589572, test loss = 3518.923848\n",
            "Epoch 5763: train loss = 2202.553626, test loss = 3518.901444\n",
            "Epoch 5764: train loss = 2202.517683, test loss = 3518.879051\n",
            "Epoch 5765: train loss = 2202.481743, test loss = 3518.856667\n",
            "Epoch 5766: train loss = 2202.445806, test loss = 3518.834293\n",
            "Epoch 5767: train loss = 2202.409873, test loss = 3518.811929\n",
            "Epoch 5768: train loss = 2202.373942, test loss = 3518.789576\n",
            "Epoch 5769: train loss = 2202.338015, test loss = 3518.767232\n",
            "Epoch 5770: train loss = 2202.302092, test loss = 3518.744898\n",
            "Epoch 5771: train loss = 2202.266171, test loss = 3518.722575\n",
            "Epoch 5772: train loss = 2202.230254, test loss = 3518.700261\n",
            "Epoch 5773: train loss = 2202.194339, test loss = 3518.677957\n",
            "Epoch 5774: train loss = 2202.158428, test loss = 3518.655664\n",
            "Epoch 5775: train loss = 2202.122521, test loss = 3518.633381\n",
            "Epoch 5776: train loss = 2202.086616, test loss = 3518.611108\n",
            "Epoch 5777: train loss = 2202.050714, test loss = 3518.588845\n",
            "Epoch 5778: train loss = 2202.014816, test loss = 3518.566592\n",
            "Epoch 5779: train loss = 2201.978920, test loss = 3518.544349\n",
            "Epoch 5780: train loss = 2201.943028, test loss = 3518.522116\n",
            "Epoch 5781: train loss = 2201.907139, test loss = 3518.499894\n",
            "Epoch 5782: train loss = 2201.871253, test loss = 3518.477682\n",
            "Epoch 5783: train loss = 2201.835370, test loss = 3518.455480\n",
            "Epoch 5784: train loss = 2201.799490, test loss = 3518.433288\n",
            "Epoch 5785: train loss = 2201.763613, test loss = 3518.411107\n",
            "Epoch 5786: train loss = 2201.727739, test loss = 3518.388936\n",
            "Epoch 5787: train loss = 2201.691868, test loss = 3518.366775\n",
            "Epoch 5788: train loss = 2201.656000, test loss = 3518.344624\n",
            "Epoch 5789: train loss = 2201.620135, test loss = 3518.322484\n",
            "Epoch 5790: train loss = 2201.584273, test loss = 3518.300354\n",
            "Epoch 5791: train loss = 2201.548415, test loss = 3518.278234\n",
            "Epoch 5792: train loss = 2201.512559, test loss = 3518.256125\n",
            "Epoch 5793: train loss = 2201.476706, test loss = 3518.234026\n",
            "Epoch 5794: train loss = 2201.440856, test loss = 3518.211937\n",
            "Epoch 5795: train loss = 2201.405009, test loss = 3518.189859\n",
            "Epoch 5796: train loss = 2201.369165, test loss = 3518.167791\n",
            "Epoch 5797: train loss = 2201.333323, test loss = 3518.145733\n",
            "Epoch 5798: train loss = 2201.297485, test loss = 3518.123686\n",
            "Epoch 5799: train loss = 2201.261650, test loss = 3518.101650\n",
            "Epoch 5800: train loss = 2201.225817, test loss = 3518.079623\n",
            "Epoch 5801: train loss = 2201.189987, test loss = 3518.057608\n",
            "Epoch 5802: train loss = 2201.154161, test loss = 3518.035602\n",
            "Epoch 5803: train loss = 2201.118337, test loss = 3518.013608\n",
            "Epoch 5804: train loss = 2201.082515, test loss = 3517.991624\n",
            "Epoch 5805: train loss = 2201.046697, test loss = 3517.969650\n",
            "Epoch 5806: train loss = 2201.010882, test loss = 3517.947687\n",
            "Epoch 5807: train loss = 2200.975069, test loss = 3517.925734\n",
            "Epoch 5808: train loss = 2200.939259, test loss = 3517.903792\n",
            "Epoch 5809: train loss = 2200.903452, test loss = 3517.881861\n",
            "Epoch 5810: train loss = 2200.867648, test loss = 3517.859940\n",
            "Epoch 5811: train loss = 2200.831846, test loss = 3517.838029\n",
            "Epoch 5812: train loss = 2200.796047, test loss = 3517.816130\n",
            "Epoch 5813: train loss = 2200.760251, test loss = 3517.794241\n",
            "Epoch 5814: train loss = 2200.724458, test loss = 3517.772362\n",
            "Epoch 5815: train loss = 2200.688667, test loss = 3517.750495\n",
            "Epoch 5816: train loss = 2200.652879, test loss = 3517.728638\n",
            "Epoch 5817: train loss = 2200.617094, test loss = 3517.706791\n",
            "Epoch 5818: train loss = 2200.581311, test loss = 3517.684956\n",
            "Epoch 5819: train loss = 2200.545531, test loss = 3517.663131\n",
            "Epoch 5820: train loss = 2200.509754, test loss = 3517.641317\n",
            "Epoch 5821: train loss = 2200.473979, test loss = 3517.619514\n",
            "Epoch 5822: train loss = 2200.438207, test loss = 3517.597721\n",
            "Epoch 5823: train loss = 2200.402438, test loss = 3517.575939\n",
            "Epoch 5824: train loss = 2200.366671, test loss = 3517.554168\n",
            "Epoch 5825: train loss = 2200.330907, test loss = 3517.532408\n",
            "Epoch 5826: train loss = 2200.295145, test loss = 3517.510659\n",
            "Epoch 5827: train loss = 2200.259386, test loss = 3517.488920\n",
            "Epoch 5828: train loss = 2200.223629, test loss = 3517.467192\n",
            "Epoch 5829: train loss = 2200.187875, test loss = 3517.445476\n",
            "Epoch 5830: train loss = 2200.152124, test loss = 3517.423770\n",
            "Epoch 5831: train loss = 2200.116375, test loss = 3517.402075\n",
            "Epoch 5832: train loss = 2200.080629, test loss = 3517.380391\n",
            "Epoch 5833: train loss = 2200.044885, test loss = 3517.358718\n",
            "Epoch 5834: train loss = 2200.009143, test loss = 3517.337056\n",
            "Epoch 5835: train loss = 2199.973405, test loss = 3517.315404\n",
            "Epoch 5836: train loss = 2199.937668, test loss = 3517.293764\n",
            "Epoch 5837: train loss = 2199.901934, test loss = 3517.272135\n",
            "Epoch 5838: train loss = 2199.866203, test loss = 3517.250517\n",
            "Epoch 5839: train loss = 2199.830473, test loss = 3517.228910\n",
            "Epoch 5840: train loss = 2199.794747, test loss = 3517.207314\n",
            "Epoch 5841: train loss = 2199.759022, test loss = 3517.185729\n",
            "Epoch 5842: train loss = 2199.723301, test loss = 3517.164155\n",
            "Epoch 5843: train loss = 2199.687581, test loss = 3517.142592\n",
            "Epoch 5844: train loss = 2199.651864, test loss = 3517.121040\n",
            "Epoch 5845: train loss = 2199.616149, test loss = 3517.099499\n",
            "Epoch 5846: train loss = 2199.580437, test loss = 3517.077970\n",
            "Epoch 5847: train loss = 2199.544726, test loss = 3517.056452\n",
            "Epoch 5848: train loss = 2199.509019, test loss = 3517.034944\n",
            "Epoch 5849: train loss = 2199.473313, test loss = 3517.013449\n",
            "Epoch 5850: train loss = 2199.437610, test loss = 3516.991964\n",
            "Epoch 5851: train loss = 2199.401909, test loss = 3516.970490\n",
            "Epoch 5852: train loss = 2199.366210, test loss = 3516.949028\n",
            "Epoch 5853: train loss = 2199.330514, test loss = 3516.927577\n",
            "Epoch 5854: train loss = 2199.294820, test loss = 3516.906137\n",
            "Epoch 5855: train loss = 2199.259128, test loss = 3516.884709\n",
            "Epoch 5856: train loss = 2199.223438, test loss = 3516.863292\n",
            "Epoch 5857: train loss = 2199.187751, test loss = 3516.841886\n",
            "Epoch 5858: train loss = 2199.152065, test loss = 3516.820491\n",
            "Epoch 5859: train loss = 2199.116382, test loss = 3516.799108\n",
            "Epoch 5860: train loss = 2199.080701, test loss = 3516.777736\n",
            "Epoch 5861: train loss = 2199.045022, test loss = 3516.756376\n",
            "Epoch 5862: train loss = 2199.009346, test loss = 3516.735027\n",
            "Epoch 5863: train loss = 2198.973671, test loss = 3516.713689\n",
            "Epoch 5864: train loss = 2198.937999, test loss = 3516.692363\n",
            "Epoch 5865: train loss = 2198.902329, test loss = 3516.671049\n",
            "Epoch 5866: train loss = 2198.866660, test loss = 3516.649745\n",
            "Epoch 5867: train loss = 2198.830994, test loss = 3516.628454\n",
            "Epoch 5868: train loss = 2198.795330, test loss = 3516.607173\n",
            "Epoch 5869: train loss = 2198.759668, test loss = 3516.585905\n",
            "Epoch 5870: train loss = 2198.724008, test loss = 3516.564648\n",
            "Epoch 5871: train loss = 2198.688350, test loss = 3516.543402\n",
            "Epoch 5872: train loss = 2198.652695, test loss = 3516.522168\n",
            "Epoch 5873: train loss = 2198.617041, test loss = 3516.500946\n",
            "Epoch 5874: train loss = 2198.581389, test loss = 3516.479735\n",
            "Epoch 5875: train loss = 2198.545739, test loss = 3516.458535\n",
            "Epoch 5876: train loss = 2198.510091, test loss = 3516.437348\n",
            "Epoch 5877: train loss = 2198.474445, test loss = 3516.416172\n",
            "Epoch 5878: train loss = 2198.438801, test loss = 3516.395008\n",
            "Epoch 5879: train loss = 2198.403159, test loss = 3516.373855\n",
            "Epoch 5880: train loss = 2198.367519, test loss = 3516.352714\n",
            "Epoch 5881: train loss = 2198.331880, test loss = 3516.331585\n",
            "Epoch 5882: train loss = 2198.296244, test loss = 3516.310468\n",
            "Epoch 5883: train loss = 2198.260609, test loss = 3516.289362\n",
            "Epoch 5884: train loss = 2198.224977, test loss = 3516.268269\n",
            "Epoch 5885: train loss = 2198.189346, test loss = 3516.247187\n",
            "Epoch 5886: train loss = 2198.153717, test loss = 3516.226116\n",
            "Epoch 5887: train loss = 2198.118090, test loss = 3516.205058\n",
            "Epoch 5888: train loss = 2198.082464, test loss = 3516.184012\n",
            "Epoch 5889: train loss = 2198.046841, test loss = 3516.162977\n",
            "Epoch 5890: train loss = 2198.011219, test loss = 3516.141954\n",
            "Epoch 5891: train loss = 2197.975599, test loss = 3516.120943\n",
            "Epoch 5892: train loss = 2197.939980, test loss = 3516.099945\n",
            "Epoch 5893: train loss = 2197.904364, test loss = 3516.078958\n",
            "Epoch 5894: train loss = 2197.868749, test loss = 3516.057983\n",
            "Epoch 5895: train loss = 2197.833136, test loss = 3516.037020\n",
            "Epoch 5896: train loss = 2197.797525, test loss = 3516.016069\n",
            "Epoch 5897: train loss = 2197.761915, test loss = 3515.995130\n",
            "Epoch 5898: train loss = 2197.726307, test loss = 3515.974203\n",
            "Epoch 5899: train loss = 2197.690700, test loss = 3515.953288\n",
            "Epoch 5900: train loss = 2197.655096, test loss = 3515.932385\n",
            "Epoch 5901: train loss = 2197.619493, test loss = 3515.911494\n",
            "Epoch 5902: train loss = 2197.583891, test loss = 3515.890615\n",
            "Epoch 5903: train loss = 2197.548291, test loss = 3515.869749\n",
            "Epoch 5904: train loss = 2197.512693, test loss = 3515.848895\n",
            "Epoch 5905: train loss = 2197.477096, test loss = 3515.828052\n",
            "Epoch 5906: train loss = 2197.441501, test loss = 3515.807222\n",
            "Epoch 5907: train loss = 2197.405907, test loss = 3515.786405\n",
            "Epoch 5908: train loss = 2197.370315, test loss = 3515.765599\n",
            "Epoch 5909: train loss = 2197.334725, test loss = 3515.744806\n",
            "Epoch 5910: train loss = 2197.299136, test loss = 3515.724025\n",
            "Epoch 5911: train loss = 2197.263548, test loss = 3515.703256\n",
            "Epoch 5912: train loss = 2197.227962, test loss = 3515.682499\n",
            "Epoch 5913: train loss = 2197.192378, test loss = 3515.661755\n",
            "Epoch 5914: train loss = 2197.156794, test loss = 3515.641023\n",
            "Epoch 5915: train loss = 2197.121213, test loss = 3515.620304\n",
            "Epoch 5916: train loss = 2197.085632, test loss = 3515.599597\n",
            "Epoch 5917: train loss = 2197.050053, test loss = 3515.578902\n",
            "Epoch 5918: train loss = 2197.014476, test loss = 3515.558219\n",
            "Epoch 5919: train loss = 2196.978900, test loss = 3515.537550\n",
            "Epoch 5920: train loss = 2196.943325, test loss = 3515.516892\n",
            "Epoch 5921: train loss = 2196.907752, test loss = 3515.496247\n",
            "Epoch 5922: train loss = 2196.872180, test loss = 3515.475615\n",
            "Epoch 5923: train loss = 2196.836609, test loss = 3515.454995\n",
            "Epoch 5924: train loss = 2196.801039, test loss = 3515.434387\n",
            "Epoch 5925: train loss = 2196.765471, test loss = 3515.413792\n",
            "Epoch 5926: train loss = 2196.729904, test loss = 3515.393210\n",
            "Epoch 5927: train loss = 2196.694339, test loss = 3515.372640\n",
            "Epoch 5928: train loss = 2196.658774, test loss = 3515.352083\n",
            "Epoch 5929: train loss = 2196.623211, test loss = 3515.331539\n",
            "Epoch 5930: train loss = 2196.587649, test loss = 3515.311007\n",
            "Epoch 5931: train loss = 2196.552088, test loss = 3515.290488\n",
            "Epoch 5932: train loss = 2196.516529, test loss = 3515.269981\n",
            "Epoch 5933: train loss = 2196.480971, test loss = 3515.249488\n",
            "Epoch 5934: train loss = 2196.445413, test loss = 3515.229007\n",
            "Epoch 5935: train loss = 2196.409857, test loss = 3515.208538\n",
            "Epoch 5936: train loss = 2196.374302, test loss = 3515.188083\n",
            "Epoch 5937: train loss = 2196.338749, test loss = 3515.167640\n",
            "Epoch 5938: train loss = 2196.303196, test loss = 3515.147210\n",
            "Epoch 5939: train loss = 2196.267644, test loss = 3515.126793\n",
            "Epoch 5940: train loss = 2196.232094, test loss = 3515.106389\n",
            "Epoch 5941: train loss = 2196.196544, test loss = 3515.085998\n",
            "Epoch 5942: train loss = 2196.160996, test loss = 3515.065619\n",
            "Epoch 5943: train loss = 2196.125448, test loss = 3515.045254\n",
            "Epoch 5944: train loss = 2196.089902, test loss = 3515.024901\n",
            "Epoch 5945: train loss = 2196.054356, test loss = 3515.004562\n",
            "Epoch 5946: train loss = 2196.018812, test loss = 3514.984235\n",
            "Epoch 5947: train loss = 2195.983268, test loss = 3514.963922\n",
            "Epoch 5948: train loss = 2195.947726, test loss = 3514.943621\n",
            "Epoch 5949: train loss = 2195.912184, test loss = 3514.923333\n",
            "Epoch 5950: train loss = 2195.876643, test loss = 3514.903059\n",
            "Epoch 5951: train loss = 2195.841104, test loss = 3514.882798\n",
            "Epoch 5952: train loss = 2195.805565, test loss = 3514.862549\n",
            "Epoch 5953: train loss = 2195.770027, test loss = 3514.842314\n",
            "Epoch 5954: train loss = 2195.734489, test loss = 3514.822092\n",
            "Epoch 5955: train loss = 2195.698953, test loss = 3514.801883\n",
            "Epoch 5956: train loss = 2195.663417, test loss = 3514.781688\n",
            "Epoch 5957: train loss = 2195.627883, test loss = 3514.761505\n",
            "Epoch 5958: train loss = 2195.592349, test loss = 3514.741336\n",
            "Epoch 5959: train loss = 2195.556815, test loss = 3514.721180\n",
            "Epoch 5960: train loss = 2195.521283, test loss = 3514.701038\n",
            "Epoch 5961: train loss = 2195.485751, test loss = 3514.680909\n",
            "Epoch 5962: train loss = 2195.450220, test loss = 3514.660793\n",
            "Epoch 5963: train loss = 2195.414690, test loss = 3514.640690\n",
            "Epoch 5964: train loss = 2195.379160, test loss = 3514.620601\n",
            "Epoch 5965: train loss = 2195.343632, test loss = 3514.600525\n",
            "Epoch 5966: train loss = 2195.308103, test loss = 3514.580462\n",
            "Epoch 5967: train loss = 2195.272576, test loss = 3514.560413\n",
            "Epoch 5968: train loss = 2195.237049, test loss = 3514.540378\n",
            "Epoch 5969: train loss = 2195.201522, test loss = 3514.520356\n",
            "Epoch 5970: train loss = 2195.165997, test loss = 3514.500347\n",
            "Epoch 5971: train loss = 2195.130471, test loss = 3514.480352\n",
            "Epoch 5972: train loss = 2195.094947, test loss = 3514.460371\n",
            "Epoch 5973: train loss = 2195.059423, test loss = 3514.440403\n",
            "Epoch 5974: train loss = 2195.023899, test loss = 3514.420449\n",
            "Epoch 5975: train loss = 2194.988376, test loss = 3514.400508\n",
            "Epoch 5976: train loss = 2194.952854, test loss = 3514.380581\n",
            "Epoch 5977: train loss = 2194.917331, test loss = 3514.360668\n",
            "Epoch 5978: train loss = 2194.881810, test loss = 3514.340768\n",
            "Epoch 5979: train loss = 2194.846289, test loss = 3514.320882\n",
            "Epoch 5980: train loss = 2194.810768, test loss = 3514.301010\n",
            "Epoch 5981: train loss = 2194.775248, test loss = 3514.281152\n",
            "Epoch 5982: train loss = 2194.739728, test loss = 3514.261307\n",
            "Epoch 5983: train loss = 2194.704208, test loss = 3514.241476\n",
            "Epoch 5984: train loss = 2194.668689, test loss = 3514.221659\n",
            "Epoch 5985: train loss = 2194.633170, test loss = 3514.201856\n",
            "Epoch 5986: train loss = 2194.597652, test loss = 3514.182067\n",
            "Epoch 5987: train loss = 2194.562133, test loss = 3514.162292\n",
            "Epoch 5988: train loss = 2194.526615, test loss = 3514.142530\n",
            "Epoch 5989: train loss = 2194.491098, test loss = 3514.122783\n",
            "Epoch 5990: train loss = 2194.455580, test loss = 3514.103050\n",
            "Epoch 5991: train loss = 2194.420063, test loss = 3514.083330\n",
            "Epoch 5992: train loss = 2194.384546, test loss = 3514.063625\n",
            "Epoch 5993: train loss = 2194.349030, test loss = 3514.043933\n",
            "Epoch 5994: train loss = 2194.313513, test loss = 3514.024256\n",
            "Epoch 5995: train loss = 2194.277997, test loss = 3514.004593\n",
            "Epoch 5996: train loss = 2194.242480, test loss = 3513.984944\n",
            "Epoch 5997: train loss = 2194.206964, test loss = 3513.965309\n",
            "Epoch 5998: train loss = 2194.171448, test loss = 3513.945688\n",
            "Epoch 5999: train loss = 2194.135932, test loss = 3513.926081\n",
            "Epoch 6000: train loss = 2194.100417, test loss = 3513.906489\n",
            "Epoch 6001: train loss = 2194.064901, test loss = 3513.886911\n",
            "Epoch 6002: train loss = 2194.029385, test loss = 3513.867347\n",
            "Epoch 6003: train loss = 2193.993870, test loss = 3513.847798\n",
            "Epoch 6004: train loss = 2193.958354, test loss = 3513.828263\n",
            "Epoch 6005: train loss = 2193.922838, test loss = 3513.808742\n",
            "Epoch 6006: train loss = 2193.887322, test loss = 3513.789235\n",
            "Epoch 6007: train loss = 2193.851807, test loss = 3513.769743\n",
            "Epoch 6008: train loss = 2193.816291, test loss = 3513.750266\n",
            "Epoch 6009: train loss = 2193.780775, test loss = 3513.730802\n",
            "Epoch 6010: train loss = 2193.745259, test loss = 3513.711354\n",
            "Epoch 6011: train loss = 2193.709743, test loss = 3513.691920\n",
            "Epoch 6012: train loss = 2193.674227, test loss = 3513.672500\n",
            "Epoch 6013: train loss = 2193.638710, test loss = 3513.653095\n",
            "Epoch 6014: train loss = 2193.603194, test loss = 3513.633704\n",
            "Epoch 6015: train loss = 2193.567677, test loss = 3513.614328\n",
            "Epoch 6016: train loss = 2193.532160, test loss = 3513.594967\n",
            "Epoch 6017: train loss = 2193.496642, test loss = 3513.575620\n",
            "Epoch 6018: train loss = 2193.461125, test loss = 3513.556289\n",
            "Epoch 6019: train loss = 2193.425607, test loss = 3513.536971\n",
            "Epoch 6020: train loss = 2193.390089, test loss = 3513.517669\n",
            "Epoch 6021: train loss = 2193.354571, test loss = 3513.498381\n",
            "Epoch 6022: train loss = 2193.319052, test loss = 3513.479108\n",
            "Epoch 6023: train loss = 2193.283533, test loss = 3513.459850\n",
            "Epoch 6024: train loss = 2193.248014, test loss = 3513.440607\n",
            "Epoch 6025: train loss = 2193.212494, test loss = 3513.421379\n",
            "Epoch 6026: train loss = 2193.176974, test loss = 3513.402165\n",
            "Epoch 6027: train loss = 2193.141453, test loss = 3513.382967\n",
            "Epoch 6028: train loss = 2193.105932, test loss = 3513.363783\n",
            "Epoch 6029: train loss = 2193.070410, test loss = 3513.344614\n",
            "Epoch 6030: train loss = 2193.034888, test loss = 3513.325461\n",
            "Epoch 6031: train loss = 2192.999366, test loss = 3513.306322\n",
            "Epoch 6032: train loss = 2192.963843, test loss = 3513.287199\n",
            "Epoch 6033: train loss = 2192.928319, test loss = 3513.268090\n",
            "Epoch 6034: train loss = 2192.892795, test loss = 3513.248997\n",
            "Epoch 6035: train loss = 2192.857270, test loss = 3513.229919\n",
            "Epoch 6036: train loss = 2192.821745, test loss = 3513.210856\n",
            "Epoch 6037: train loss = 2192.786219, test loss = 3513.191808\n",
            "Epoch 6038: train loss = 2192.750692, test loss = 3513.172776\n",
            "Epoch 6039: train loss = 2192.715165, test loss = 3513.153759\n",
            "Epoch 6040: train loss = 2192.679637, test loss = 3513.134757\n",
            "Epoch 6041: train loss = 2192.644108, test loss = 3513.115770\n",
            "Epoch 6042: train loss = 2192.608579, test loss = 3513.096799\n",
            "Epoch 6043: train loss = 2192.573049, test loss = 3513.077843\n",
            "Epoch 6044: train loss = 2192.537518, test loss = 3513.058902\n",
            "Epoch 6045: train loss = 2192.501986, test loss = 3513.039977\n",
            "Epoch 6046: train loss = 2192.466454, test loss = 3513.021068\n",
            "Epoch 6047: train loss = 2192.430920, test loss = 3513.002174\n",
            "Epoch 6048: train loss = 2192.395386, test loss = 3512.983295\n",
            "Epoch 6049: train loss = 2192.359851, test loss = 3512.964432\n",
            "Epoch 6050: train loss = 2192.324315, test loss = 3512.945584\n",
            "Epoch 6051: train loss = 2192.288778, test loss = 3512.926753\n",
            "Epoch 6052: train loss = 2192.253241, test loss = 3512.907936\n",
            "Epoch 6053: train loss = 2192.217702, test loss = 3512.889136\n",
            "Epoch 6054: train loss = 2192.182162, test loss = 3512.870351\n",
            "Epoch 6055: train loss = 2192.146621, test loss = 3512.851582\n",
            "Epoch 6056: train loss = 2192.111080, test loss = 3512.832829\n",
            "Epoch 6057: train loss = 2192.075537, test loss = 3512.814091\n",
            "Epoch 6058: train loss = 2192.039993, test loss = 3512.795369\n",
            "Epoch 6059: train loss = 2192.004448, test loss = 3512.776664\n",
            "Epoch 6060: train loss = 2191.968902, test loss = 3512.757974\n",
            "Epoch 6061: train loss = 2191.933355, test loss = 3512.739299\n",
            "Epoch 6062: train loss = 2191.897807, test loss = 3512.720641\n",
            "Epoch 6063: train loss = 2191.862257, test loss = 3512.701999\n",
            "Epoch 6064: train loss = 2191.826707, test loss = 3512.683373\n",
            "Epoch 6065: train loss = 2191.791155, test loss = 3512.664763\n",
            "Epoch 6066: train loss = 2191.755602, test loss = 3512.646169\n",
            "Epoch 6067: train loss = 2191.720047, test loss = 3512.627591\n",
            "Epoch 6068: train loss = 2191.684492, test loss = 3512.609029\n",
            "Epoch 6069: train loss = 2191.648935, test loss = 3512.590483\n",
            "Epoch 6070: train loss = 2191.613377, test loss = 3512.571954\n",
            "Epoch 6071: train loss = 2191.577817, test loss = 3512.553441\n",
            "Epoch 6072: train loss = 2191.542256, test loss = 3512.534944\n",
            "Epoch 6073: train loss = 2191.506694, test loss = 3512.516463\n",
            "Epoch 6074: train loss = 2191.471130, test loss = 3512.497998\n",
            "Epoch 6075: train loss = 2191.435565, test loss = 3512.479550\n",
            "Epoch 6076: train loss = 2191.399998, test loss = 3512.461119\n",
            "Epoch 6077: train loss = 2191.364430, test loss = 3512.442703\n",
            "Epoch 6078: train loss = 2191.328860, test loss = 3512.424305\n",
            "Epoch 6079: train loss = 2191.293289, test loss = 3512.405922\n",
            "Epoch 6080: train loss = 2191.257716, test loss = 3512.387556\n",
            "Epoch 6081: train loss = 2191.222142, test loss = 3512.369207\n",
            "Epoch 6082: train loss = 2191.186566, test loss = 3512.350874\n",
            "Epoch 6083: train loss = 2191.150988, test loss = 3512.332558\n",
            "Epoch 6084: train loss = 2191.115409, test loss = 3512.314259\n",
            "Epoch 6085: train loss = 2191.079828, test loss = 3512.295976\n",
            "Epoch 6086: train loss = 2191.044246, test loss = 3512.277710\n",
            "Epoch 6087: train loss = 2191.008661, test loss = 3512.259461\n",
            "Epoch 6088: train loss = 2190.973075, test loss = 3512.241228\n",
            "Epoch 6089: train loss = 2190.937488, test loss = 3512.223012\n",
            "Epoch 6090: train loss = 2190.901898, test loss = 3512.204814\n",
            "Epoch 6091: train loss = 2190.866307, test loss = 3512.186632\n",
            "Epoch 6092: train loss = 2190.830713, test loss = 3512.168466\n",
            "Epoch 6093: train loss = 2190.795118, test loss = 3512.150318\n",
            "Epoch 6094: train loss = 2190.759521, test loss = 3512.132187\n",
            "Epoch 6095: train loss = 2190.723922, test loss = 3512.114073\n",
            "Epoch 6096: train loss = 2190.688322, test loss = 3512.095976\n",
            "Epoch 6097: train loss = 2190.652719, test loss = 3512.077896\n",
            "Epoch 6098: train loss = 2190.617114, test loss = 3512.059833\n",
            "Epoch 6099: train loss = 2190.581507, test loss = 3512.041787\n",
            "Epoch 6100: train loss = 2190.545898, test loss = 3512.023759\n",
            "Epoch 6101: train loss = 2190.510288, test loss = 3512.005747\n",
            "Epoch 6102: train loss = 2190.474675, test loss = 3511.987753\n",
            "Epoch 6103: train loss = 2190.439060, test loss = 3511.969777\n",
            "Epoch 6104: train loss = 2190.403442, test loss = 3511.951817\n",
            "Epoch 6105: train loss = 2190.367823, test loss = 3511.933875\n",
            "Epoch 6106: train loss = 2190.332201, test loss = 3511.915950\n",
            "Epoch 6107: train loss = 2190.296578, test loss = 3511.898043\n",
            "Epoch 6108: train loss = 2190.260952, test loss = 3511.880153\n",
            "Epoch 6109: train loss = 2190.225323, test loss = 3511.862281\n",
            "Epoch 6110: train loss = 2190.189693, test loss = 3511.844426\n",
            "Epoch 6111: train loss = 2190.154060, test loss = 3511.826589\n",
            "Epoch 6112: train loss = 2190.118425, test loss = 3511.808770\n",
            "Epoch 6113: train loss = 2190.082787, test loss = 3511.790968\n",
            "Epoch 6114: train loss = 2190.047147, test loss = 3511.773184\n",
            "Epoch 6115: train loss = 2190.011505, test loss = 3511.755418\n",
            "Epoch 6116: train loss = 2189.975860, test loss = 3511.737669\n",
            "Epoch 6117: train loss = 2189.940213, test loss = 3511.719938\n",
            "Epoch 6118: train loss = 2189.904563, test loss = 3511.702225\n",
            "Epoch 6119: train loss = 2189.868911, test loss = 3511.684530\n",
            "Epoch 6120: train loss = 2189.833256, test loss = 3511.666853\n",
            "Epoch 6121: train loss = 2189.797598, test loss = 3511.649194\n",
            "Epoch 6122: train loss = 2189.761938, test loss = 3511.631552\n",
            "Epoch 6123: train loss = 2189.726275, test loss = 3511.613929\n",
            "Epoch 6124: train loss = 2189.690610, test loss = 3511.596324\n",
            "Epoch 6125: train loss = 2189.654942, test loss = 3511.578737\n",
            "Epoch 6126: train loss = 2189.619271, test loss = 3511.561168\n",
            "Epoch 6127: train loss = 2189.583598, test loss = 3511.543617\n",
            "Epoch 6128: train loss = 2189.547921, test loss = 3511.526085\n",
            "Epoch 6129: train loss = 2189.512242, test loss = 3511.508571\n",
            "Epoch 6130: train loss = 2189.476560, test loss = 3511.491075\n",
            "Epoch 6131: train loss = 2189.440876, test loss = 3511.473597\n",
            "Epoch 6132: train loss = 2189.405188, test loss = 3511.456138\n",
            "Epoch 6133: train loss = 2189.369497, test loss = 3511.438698\n",
            "Epoch 6134: train loss = 2189.333804, test loss = 3511.421275\n",
            "Epoch 6135: train loss = 2189.298107, test loss = 3511.403872\n",
            "Epoch 6136: train loss = 2189.262408, test loss = 3511.386486\n",
            "Epoch 6137: train loss = 2189.226706, test loss = 3511.369120\n",
            "Epoch 6138: train loss = 2189.191000, test loss = 3511.351772\n",
            "Epoch 6139: train loss = 2189.155291, test loss = 3511.334442\n",
            "Epoch 6140: train loss = 2189.119580, test loss = 3511.317132\n",
            "Epoch 6141: train loss = 2189.083865, test loss = 3511.299840\n",
            "Epoch 6142: train loss = 2189.048147, test loss = 3511.282567\n",
            "Epoch 6143: train loss = 2189.012426, test loss = 3511.265313\n",
            "Epoch 6144: train loss = 2188.976701, test loss = 3511.248077\n",
            "Epoch 6145: train loss = 2188.940973, test loss = 3511.230861\n",
            "Epoch 6146: train loss = 2188.905242, test loss = 3511.213663\n",
            "Epoch 6147: train loss = 2188.869508, test loss = 3511.196485\n",
            "Epoch 6148: train loss = 2188.833770, test loss = 3511.179325\n",
            "Epoch 6149: train loss = 2188.798029, test loss = 3511.162185\n",
            "Epoch 6150: train loss = 2188.762285, test loss = 3511.145063\n",
            "Epoch 6151: train loss = 2188.726537, test loss = 3511.127961\n",
            "Epoch 6152: train loss = 2188.690786, test loss = 3511.110878\n",
            "Epoch 6153: train loss = 2188.655031, test loss = 3511.093815\n",
            "Epoch 6154: train loss = 2188.619272, test loss = 3511.076770\n",
            "Epoch 6155: train loss = 2188.583511, test loss = 3511.059745\n",
            "Epoch 6156: train loss = 2188.547745, test loss = 3511.042740\n",
            "Epoch 6157: train loss = 2188.511976, test loss = 3511.025753\n",
            "Epoch 6158: train loss = 2188.476203, test loss = 3511.008787\n",
            "Epoch 6159: train loss = 2188.440427, test loss = 3510.991839\n",
            "Epoch 6160: train loss = 2188.404646, test loss = 3510.974912\n",
            "Epoch 6161: train loss = 2188.368862, test loss = 3510.958004\n",
            "Epoch 6162: train loss = 2188.333075, test loss = 3510.941115\n",
            "Epoch 6163: train loss = 2188.297283, test loss = 3510.924247\n",
            "Epoch 6164: train loss = 2188.261488, test loss = 3510.907398\n",
            "Epoch 6165: train loss = 2188.225689, test loss = 3510.890568\n",
            "Epoch 6166: train loss = 2188.189885, test loss = 3510.873759\n",
            "Epoch 6167: train loss = 2188.154078, test loss = 3510.856969\n",
            "Epoch 6168: train loss = 2188.118267, test loss = 3510.840200\n",
            "Epoch 6169: train loss = 2188.082452, test loss = 3510.823450\n",
            "Epoch 6170: train loss = 2188.046633, test loss = 3510.806720\n",
            "Epoch 6171: train loss = 2188.010810, test loss = 3510.790011\n",
            "Epoch 6172: train loss = 2187.974983, test loss = 3510.773321\n",
            "Epoch 6173: train loss = 2187.939152, test loss = 3510.756652\n",
            "Epoch 6174: train loss = 2187.903316, test loss = 3510.740003\n",
            "Epoch 6175: train loss = 2187.867476, test loss = 3510.723374\n",
            "Epoch 6176: train loss = 2187.831632, test loss = 3510.706765\n",
            "Epoch 6177: train loss = 2187.795784, test loss = 3510.690177\n",
            "Epoch 6178: train loss = 2187.759932, test loss = 3510.673609\n",
            "Epoch 6179: train loss = 2187.724075, test loss = 3510.657062\n",
            "Epoch 6180: train loss = 2187.688213, test loss = 3510.640535\n",
            "Epoch 6181: train loss = 2187.652348, test loss = 3510.624028\n",
            "Epoch 6182: train loss = 2187.616478, test loss = 3510.607542\n",
            "Epoch 6183: train loss = 2187.580603, test loss = 3510.591077\n",
            "Epoch 6184: train loss = 2187.544724, test loss = 3510.574632\n",
            "Epoch 6185: train loss = 2187.508841, test loss = 3510.558209\n",
            "Epoch 6186: train loss = 2187.472953, test loss = 3510.541805\n",
            "Epoch 6187: train loss = 2187.437060, test loss = 3510.525423\n",
            "Epoch 6188: train loss = 2187.401162, test loss = 3510.509062\n",
            "Epoch 6189: train loss = 2187.365260, test loss = 3510.492721\n",
            "Epoch 6190: train loss = 2187.329353, test loss = 3510.476402\n",
            "Epoch 6191: train loss = 2187.293442, test loss = 3510.460103\n",
            "Epoch 6192: train loss = 2187.257526, test loss = 3510.443826\n",
            "Epoch 6193: train loss = 2187.221604, test loss = 3510.427569\n",
            "Epoch 6194: train loss = 2187.185678, test loss = 3510.411334\n",
            "Epoch 6195: train loss = 2187.149748, test loss = 3510.395120\n",
            "Epoch 6196: train loss = 2187.113812, test loss = 3510.378928\n",
            "Epoch 6197: train loss = 2187.077871, test loss = 3510.362756\n",
            "Epoch 6198: train loss = 2187.041925, test loss = 3510.346606\n",
            "Epoch 6199: train loss = 2187.005974, test loss = 3510.330478\n",
            "Epoch 6200: train loss = 2186.970018, test loss = 3510.314371\n",
            "Epoch 6201: train loss = 2186.934057, test loss = 3510.298285\n",
            "Epoch 6202: train loss = 2186.898091, test loss = 3510.282221\n",
            "Epoch 6203: train loss = 2186.862120, test loss = 3510.266179\n",
            "Epoch 6204: train loss = 2186.826143, test loss = 3510.250159\n",
            "Epoch 6205: train loss = 2186.790162, test loss = 3510.234160\n",
            "Epoch 6206: train loss = 2186.754174, test loss = 3510.218183\n",
            "Epoch 6207: train loss = 2186.718182, test loss = 3510.202228\n",
            "Epoch 6208: train loss = 2186.682184, test loss = 3510.186294\n",
            "Epoch 6209: train loss = 2186.646181, test loss = 3510.170383\n",
            "Epoch 6210: train loss = 2186.610172, test loss = 3510.154494\n",
            "Epoch 6211: train loss = 2186.574158, test loss = 3510.138626\n",
            "Epoch 6212: train loss = 2186.538138, test loss = 3510.122781\n",
            "Epoch 6213: train loss = 2186.502113, test loss = 3510.106958\n",
            "Epoch 6214: train loss = 2186.466082, test loss = 3510.091158\n",
            "Epoch 6215: train loss = 2186.430045, test loss = 3510.075379\n",
            "Epoch 6216: train loss = 2186.394003, test loss = 3510.059623\n",
            "Epoch 6217: train loss = 2186.357955, test loss = 3510.043889\n",
            "Epoch 6218: train loss = 2186.321901, test loss = 3510.028178\n",
            "Epoch 6219: train loss = 2186.285842, test loss = 3510.012489\n",
            "Epoch 6220: train loss = 2186.249776, test loss = 3509.996823\n",
            "Epoch 6221: train loss = 2186.213705, test loss = 3509.981180\n",
            "Epoch 6222: train loss = 2186.177628, test loss = 3509.965559\n",
            "Epoch 6223: train loss = 2186.141545, test loss = 3509.949961\n",
            "Epoch 6224: train loss = 2186.105456, test loss = 3509.934386\n",
            "Epoch 6225: train loss = 2186.069360, test loss = 3509.918833\n",
            "Epoch 6226: train loss = 2186.033259, test loss = 3509.903304\n",
            "Epoch 6227: train loss = 2185.997151, test loss = 3509.887797\n",
            "Epoch 6228: train loss = 2185.961038, test loss = 3509.872314\n",
            "Epoch 6229: train loss = 2185.924918, test loss = 3509.856853\n",
            "Epoch 6230: train loss = 2185.888792, test loss = 3509.841416\n",
            "Epoch 6231: train loss = 2185.852659, test loss = 3509.826002\n",
            "Epoch 6232: train loss = 2185.816521, test loss = 3509.810612\n",
            "Epoch 6233: train loss = 2185.780375, test loss = 3509.795244\n",
            "Epoch 6234: train loss = 2185.744224, test loss = 3509.779900\n",
            "Epoch 6235: train loss = 2185.708066, test loss = 3509.764580\n",
            "Epoch 6236: train loss = 2185.671901, test loss = 3509.749283\n",
            "Epoch 6237: train loss = 2185.635730, test loss = 3509.734009\n",
            "Epoch 6238: train loss = 2185.599552, test loss = 3509.718759\n",
            "Epoch 6239: train loss = 2185.563368, test loss = 3509.703533\n",
            "Epoch 6240: train loss = 2185.527177, test loss = 3509.688331\n",
            "Epoch 6241: train loss = 2185.490979, test loss = 3509.673152\n",
            "Epoch 6242: train loss = 2185.454775, test loss = 3509.657998\n",
            "Epoch 6243: train loss = 2185.418563, test loss = 3509.642867\n",
            "Epoch 6244: train loss = 2185.382345, test loss = 3509.627761\n",
            "Epoch 6245: train loss = 2185.346120, test loss = 3509.612678\n",
            "Epoch 6246: train loss = 2185.309888, test loss = 3509.597620\n",
            "Epoch 6247: train loss = 2185.273648, test loss = 3509.582585\n",
            "Epoch 6248: train loss = 2185.237402, test loss = 3509.567575\n",
            "Epoch 6249: train loss = 2185.201149, test loss = 3509.552590\n",
            "Epoch 6250: train loss = 2185.164889, test loss = 3509.537629\n",
            "Epoch 6251: train loss = 2185.128621, test loss = 3509.522692\n",
            "Epoch 6252: train loss = 2185.092346, test loss = 3509.507780\n",
            "Epoch 6253: train loss = 2185.056064, test loss = 3509.492892\n",
            "Epoch 6254: train loss = 2185.019775, test loss = 3509.478029\n",
            "Epoch 6255: train loss = 2184.983478, test loss = 3509.463191\n",
            "Epoch 6256: train loss = 2184.947174, test loss = 3509.448378\n",
            "Epoch 6257: train loss = 2184.910862, test loss = 3509.433589\n",
            "Epoch 6258: train loss = 2184.874543, test loss = 3509.418826\n",
            "Epoch 6259: train loss = 2184.838216, test loss = 3509.404087\n",
            "Epoch 6260: train loss = 2184.801881, test loss = 3509.389374\n",
            "Epoch 6261: train loss = 2184.765539, test loss = 3509.374685\n",
            "Epoch 6262: train loss = 2184.729190, test loss = 3509.360022\n",
            "Epoch 6263: train loss = 2184.692832, test loss = 3509.345384\n",
            "Epoch 6264: train loss = 2184.656467, test loss = 3509.330772\n",
            "Epoch 6265: train loss = 2184.620094, test loss = 3509.316185\n",
            "Epoch 6266: train loss = 2184.583713, test loss = 3509.301623\n",
            "Epoch 6267: train loss = 2184.547324, test loss = 3509.287087\n",
            "Epoch 6268: train loss = 2184.510927, test loss = 3509.272577\n",
            "Epoch 6269: train loss = 2184.474521, test loss = 3509.258092\n",
            "Epoch 6270: train loss = 2184.438108, test loss = 3509.243633\n",
            "Epoch 6271: train loss = 2184.401687, test loss = 3509.229200\n",
            "Epoch 6272: train loss = 2184.365257, test loss = 3509.214793\n",
            "Epoch 6273: train loss = 2184.328820, test loss = 3509.200412\n",
            "Epoch 6274: train loss = 2184.292373, test loss = 3509.186057\n",
            "Epoch 6275: train loss = 2184.255919, test loss = 3509.171728\n",
            "Epoch 6276: train loss = 2184.219456, test loss = 3509.157425\n",
            "Epoch 6277: train loss = 2184.182985, test loss = 3509.143148\n",
            "Epoch 6278: train loss = 2184.146505, test loss = 3509.128898\n",
            "Epoch 6279: train loss = 2184.110016, test loss = 3509.114675\n",
            "Epoch 6280: train loss = 2184.073519, test loss = 3509.100477\n",
            "Epoch 6281: train loss = 2184.037014, test loss = 3509.086307\n",
            "Epoch 6282: train loss = 2184.000499, test loss = 3509.072163\n",
            "Epoch 6283: train loss = 2183.963976, test loss = 3509.058046\n",
            "Epoch 6284: train loss = 2183.927444, test loss = 3509.043956\n",
            "Epoch 6285: train loss = 2183.890903, test loss = 3509.029892\n",
            "Epoch 6286: train loss = 2183.854353, test loss = 3509.015856\n",
            "Epoch 6287: train loss = 2183.817794, test loss = 3509.001846\n",
            "Epoch 6288: train loss = 2183.781226, test loss = 3508.987864\n",
            "Epoch 6289: train loss = 2183.744649, test loss = 3508.973909\n",
            "Epoch 6290: train loss = 2183.708063, test loss = 3508.959981\n",
            "Epoch 6291: train loss = 2183.671467, test loss = 3508.946081\n",
            "Epoch 6292: train loss = 2183.634863, test loss = 3508.932208\n",
            "Epoch 6293: train loss = 2183.598249, test loss = 3508.918362\n",
            "Epoch 6294: train loss = 2183.561625, test loss = 3508.904544\n",
            "Epoch 6295: train loss = 2183.524992, test loss = 3508.890754\n",
            "Epoch 6296: train loss = 2183.488350, test loss = 3508.876992\n",
            "Epoch 6297: train loss = 2183.451698, test loss = 3508.863257\n",
            "Epoch 6298: train loss = 2183.415036, test loss = 3508.849551\n",
            "Epoch 6299: train loss = 2183.378365, test loss = 3508.835872\n",
            "Epoch 6300: train loss = 2183.341684, test loss = 3508.822222\n",
            "Epoch 6301: train loss = 2183.304993, test loss = 3508.808599\n",
            "Epoch 6302: train loss = 2183.268293, test loss = 3508.795005\n",
            "Epoch 6303: train loss = 2183.231582, test loss = 3508.781439\n",
            "Epoch 6304: train loss = 2183.194862, test loss = 3508.767902\n",
            "Epoch 6305: train loss = 2183.158131, test loss = 3508.754394\n",
            "Epoch 6306: train loss = 2183.121391, test loss = 3508.740913\n",
            "Epoch 6307: train loss = 2183.084640, test loss = 3508.727462\n",
            "Epoch 6308: train loss = 2183.047879, test loss = 3508.714039\n",
            "Epoch 6309: train loss = 2183.011108, test loss = 3508.700646\n",
            "Epoch 6310: train loss = 2182.974326, test loss = 3508.687281\n",
            "Epoch 6311: train loss = 2182.937534, test loss = 3508.673945\n",
            "Epoch 6312: train loss = 2182.900732, test loss = 3508.660638\n",
            "Epoch 6313: train loss = 2182.863919, test loss = 3508.647361\n",
            "Epoch 6314: train loss = 2182.827095, test loss = 3508.634113\n",
            "Epoch 6315: train loss = 2182.790261, test loss = 3508.620894\n",
            "Epoch 6316: train loss = 2182.753417, test loss = 3508.607705\n",
            "Epoch 6317: train loss = 2182.716561, test loss = 3508.594545\n",
            "Epoch 6318: train loss = 2182.679695, test loss = 3508.581415\n",
            "Epoch 6319: train loss = 2182.642817, test loss = 3508.568315\n",
            "Epoch 6320: train loss = 2182.605929, test loss = 3508.555245\n",
            "Epoch 6321: train loss = 2182.569030, test loss = 3508.542204\n",
            "Epoch 6322: train loss = 2182.532119, test loss = 3508.529194\n",
            "Epoch 6323: train loss = 2182.495198, test loss = 3508.516214\n",
            "Epoch 6324: train loss = 2182.458265, test loss = 3508.503264\n",
            "Epoch 6325: train loss = 2182.421321, test loss = 3508.490344\n",
            "Epoch 6326: train loss = 2182.384366, test loss = 3508.477455\n",
            "Epoch 6327: train loss = 2182.347399, test loss = 3508.464596\n",
            "Epoch 6328: train loss = 2182.310421, test loss = 3508.451768\n",
            "Epoch 6329: train loss = 2182.273431, test loss = 3508.438970\n",
            "Epoch 6330: train loss = 2182.236429, test loss = 3508.426204\n",
            "Epoch 6331: train loss = 2182.199416, test loss = 3508.413468\n",
            "Epoch 6332: train loss = 2182.162391, test loss = 3508.400764\n",
            "Epoch 6333: train loss = 2182.125355, test loss = 3508.388090\n",
            "Epoch 6334: train loss = 2182.088306, test loss = 3508.375448\n",
            "Epoch 6335: train loss = 2182.051246, test loss = 3508.362836\n",
            "Epoch 6336: train loss = 2182.014173, test loss = 3508.350257\n",
            "Epoch 6337: train loss = 2181.977088, test loss = 3508.337709\n",
            "Epoch 6338: train loss = 2181.939992, test loss = 3508.325192\n",
            "Epoch 6339: train loss = 2181.902882, test loss = 3508.312707\n",
            "Epoch 6340: train loss = 2181.865761, test loss = 3508.300254\n",
            "Epoch 6341: train loss = 2181.828627, test loss = 3508.287833\n",
            "Epoch 6342: train loss = 2181.791481, test loss = 3508.275444\n",
            "Epoch 6343: train loss = 2181.754322, test loss = 3508.263087\n",
            "Epoch 6344: train loss = 2181.717151, test loss = 3508.250762\n",
            "Epoch 6345: train loss = 2181.679967, test loss = 3508.238470\n",
            "Epoch 6346: train loss = 2181.642770, test loss = 3508.226210\n",
            "Epoch 6347: train loss = 2181.605561, test loss = 3508.213982\n",
            "Epoch 6348: train loss = 2181.568338, test loss = 3508.201787\n",
            "Epoch 6349: train loss = 2181.531103, test loss = 3508.189625\n",
            "Epoch 6350: train loss = 2181.493854, test loss = 3508.177496\n",
            "Epoch 6351: train loss = 2181.456593, test loss = 3508.165400\n",
            "Epoch 6352: train loss = 2181.419318, test loss = 3508.153337\n",
            "Epoch 6353: train loss = 2181.382030, test loss = 3508.141307\n",
            "Epoch 6354: train loss = 2181.344729, test loss = 3508.129310\n",
            "Epoch 6355: train loss = 2181.307414, test loss = 3508.117347\n",
            "Epoch 6356: train loss = 2181.270086, test loss = 3508.105417\n",
            "Epoch 6357: train loss = 2181.232744, test loss = 3508.093521\n",
            "Epoch 6358: train loss = 2181.195388, test loss = 3508.081659\n",
            "Epoch 6359: train loss = 2181.158019, test loss = 3508.069831\n",
            "Epoch 6360: train loss = 2181.120636, test loss = 3508.058036\n",
            "Epoch 6361: train loss = 2181.083239, test loss = 3508.046276\n",
            "Epoch 6362: train loss = 2181.045828, test loss = 3508.034550\n",
            "Epoch 6363: train loss = 2181.008403, test loss = 3508.022858\n",
            "Epoch 6364: train loss = 2180.970964, test loss = 3508.011201\n",
            "Epoch 6365: train loss = 2180.933511, test loss = 3507.999578\n",
            "Epoch 6366: train loss = 2180.896043, test loss = 3507.987990\n",
            "Epoch 6367: train loss = 2180.858561, test loss = 3507.976437\n",
            "Epoch 6368: train loss = 2180.821065, test loss = 3507.964919\n",
            "Epoch 6369: train loss = 2180.783554, test loss = 3507.953435\n",
            "Epoch 6370: train loss = 2180.746029, test loss = 3507.941987\n",
            "Epoch 6371: train loss = 2180.708488, test loss = 3507.930575\n",
            "Epoch 6372: train loss = 2180.670933, test loss = 3507.919197\n",
            "Epoch 6373: train loss = 2180.633363, test loss = 3507.907855\n",
            "Epoch 6374: train loss = 2180.595779, test loss = 3507.896549\n",
            "Epoch 6375: train loss = 2180.558179, test loss = 3507.885279\n",
            "Epoch 6376: train loss = 2180.520564, test loss = 3507.874044\n",
            "Epoch 6377: train loss = 2180.482934, test loss = 3507.862846\n",
            "Epoch 6378: train loss = 2180.445288, test loss = 3507.851684\n",
            "Epoch 6379: train loss = 2180.407627, test loss = 3507.840558\n",
            "Epoch 6380: train loss = 2180.369951, test loss = 3507.829468\n",
            "Epoch 6381: train loss = 2180.332259, test loss = 3507.818415\n",
            "Epoch 6382: train loss = 2180.294551, test loss = 3507.807399\n",
            "Epoch 6383: train loss = 2180.256828, test loss = 3507.796419\n",
            "Epoch 6384: train loss = 2180.219089, test loss = 3507.785477\n",
            "Epoch 6385: train loss = 2180.181334, test loss = 3507.774571\n",
            "Epoch 6386: train loss = 2180.143563, test loss = 3507.763703\n",
            "Epoch 6387: train loss = 2180.105775, test loss = 3507.752872\n",
            "Epoch 6388: train loss = 2180.067972, test loss = 3507.742079\n",
            "Epoch 6389: train loss = 2180.030152, test loss = 3507.731323\n",
            "Epoch 6390: train loss = 2179.992316, test loss = 3507.720605\n",
            "Epoch 6391: train loss = 2179.954464, test loss = 3507.709925\n",
            "Epoch 6392: train loss = 2179.916595, test loss = 3507.699283\n",
            "Epoch 6393: train loss = 2179.878709, test loss = 3507.688679\n",
            "Epoch 6394: train loss = 2179.840807, test loss = 3507.678113\n",
            "Epoch 6395: train loss = 2179.802887, test loss = 3507.667586\n",
            "Epoch 6396: train loss = 2179.764951, test loss = 3507.657097\n",
            "Epoch 6397: train loss = 2179.726998, test loss = 3507.646647\n",
            "Epoch 6398: train loss = 2179.689027, test loss = 3507.636236\n",
            "Epoch 6399: train loss = 2179.651040, test loss = 3507.625864\n",
            "Epoch 6400: train loss = 2179.613035, test loss = 3507.615531\n",
            "Epoch 6401: train loss = 2179.575013, test loss = 3507.605238\n",
            "Epoch 6402: train loss = 2179.536973, test loss = 3507.594984\n",
            "Epoch 6403: train loss = 2179.498915, test loss = 3507.584769\n",
            "Epoch 6404: train loss = 2179.460840, test loss = 3507.574594\n",
            "Epoch 6405: train loss = 2179.422747, test loss = 3507.564459\n",
            "Epoch 6406: train loss = 2179.384636, test loss = 3507.554365\n",
            "Epoch 6407: train loss = 2179.346507, test loss = 3507.544310\n",
            "Epoch 6408: train loss = 2179.308360, test loss = 3507.534296\n",
            "Epoch 6409: train loss = 2179.270194, test loss = 3507.524322\n",
            "Epoch 6410: train loss = 2179.232011, test loss = 3507.514389\n",
            "Epoch 6411: train loss = 2179.193809, test loss = 3507.504497\n",
            "Epoch 6412: train loss = 2179.155588, test loss = 3507.494645\n",
            "Epoch 6413: train loss = 2179.117349, test loss = 3507.484835\n",
            "Epoch 6414: train loss = 2179.079091, test loss = 3507.475066\n",
            "Epoch 6415: train loss = 2179.040814, test loss = 3507.465339\n",
            "Epoch 6416: train loss = 2179.002518, test loss = 3507.455653\n",
            "Epoch 6417: train loss = 2178.964203, test loss = 3507.446008\n",
            "Epoch 6418: train loss = 2178.925869, test loss = 3507.436406\n",
            "Epoch 6419: train loss = 2178.887516, test loss = 3507.426846\n",
            "Epoch 6420: train loss = 2178.849143, test loss = 3507.417328\n",
            "Epoch 6421: train loss = 2178.810751, test loss = 3507.407853\n",
            "Epoch 6422: train loss = 2178.772339, test loss = 3507.398420\n",
            "Epoch 6423: train loss = 2178.733907, test loss = 3507.389030\n",
            "Epoch 6424: train loss = 2178.695456, test loss = 3507.379682\n",
            "Epoch 6425: train loss = 2178.656984, test loss = 3507.370378\n",
            "Epoch 6426: train loss = 2178.618493, test loss = 3507.361117\n",
            "Epoch 6427: train loss = 2178.579981, test loss = 3507.351900\n",
            "Epoch 6428: train loss = 2178.541449, test loss = 3507.342726\n",
            "Epoch 6429: train loss = 2178.502897, test loss = 3507.333595\n",
            "Epoch 6430: train loss = 2178.464324, test loss = 3507.324509\n",
            "Epoch 6431: train loss = 2178.425731, test loss = 3507.315467\n",
            "Epoch 6432: train loss = 2178.387116, test loss = 3507.306469\n",
            "Epoch 6433: train loss = 2178.348481, test loss = 3507.297516\n",
            "Epoch 6434: train loss = 2178.309825, test loss = 3507.288607\n",
            "Epoch 6435: train loss = 2178.271148, test loss = 3507.279743\n",
            "Epoch 6436: train loss = 2178.232450, test loss = 3507.270924\n",
            "Epoch 6437: train loss = 2178.193730, test loss = 3507.262150\n",
            "Epoch 6438: train loss = 2178.154989, test loss = 3507.253422\n",
            "Epoch 6439: train loss = 2178.116226, test loss = 3507.244739\n",
            "Epoch 6440: train loss = 2178.077442, test loss = 3507.236102\n",
            "Epoch 6441: train loss = 2178.038635, test loss = 3507.227511\n",
            "Epoch 6442: train loss = 2177.999807, test loss = 3507.218966\n",
            "Epoch 6443: train loss = 2177.960957, test loss = 3507.210467\n",
            "Epoch 6444: train loss = 2177.922084, test loss = 3507.202015\n",
            "Epoch 6445: train loss = 2177.883189, test loss = 3507.193609\n",
            "Epoch 6446: train loss = 2177.844272, test loss = 3507.185250\n",
            "Epoch 6447: train loss = 2177.805332, test loss = 3507.176939\n",
            "Epoch 6448: train loss = 2177.766369, test loss = 3507.168674\n",
            "Epoch 6449: train loss = 2177.727384, test loss = 3507.160457\n",
            "Epoch 6450: train loss = 2177.688376, test loss = 3507.152288\n",
            "Epoch 6451: train loss = 2177.649344, test loss = 3507.144167\n",
            "Epoch 6452: train loss = 2177.610289, test loss = 3507.136093\n",
            "Epoch 6453: train loss = 2177.571211, test loss = 3507.128068\n",
            "Epoch 6454: train loss = 2177.532110, test loss = 3507.120091\n",
            "Epoch 6455: train loss = 2177.492984, test loss = 3507.112163\n",
            "Epoch 6456: train loss = 2177.453835, test loss = 3507.104284\n",
            "Epoch 6457: train loss = 2177.414663, test loss = 3507.096454\n",
            "Epoch 6458: train loss = 2177.375466, test loss = 3507.088673\n",
            "Epoch 6459: train loss = 2177.336245, test loss = 3507.080942\n",
            "Epoch 6460: train loss = 2177.296999, test loss = 3507.073260\n",
            "Epoch 6461: train loss = 2177.257730, test loss = 3507.065628\n",
            "Epoch 6462: train loss = 2177.218435, test loss = 3507.058047\n",
            "Epoch 6463: train loss = 2177.179116, test loss = 3507.050515\n",
            "Epoch 6464: train loss = 2177.139772, test loss = 3507.043034\n",
            "Epoch 6465: train loss = 2177.100403, test loss = 3507.035604\n",
            "Epoch 6466: train loss = 2177.061009, test loss = 3507.028225\n",
            "Epoch 6467: train loss = 2177.021590, test loss = 3507.020898\n",
            "Epoch 6468: train loss = 2176.982145, test loss = 3507.013621\n",
            "Epoch 6469: train loss = 2176.942675, test loss = 3507.006397\n",
            "Epoch 6470: train loss = 2176.903179, test loss = 3506.999224\n",
            "Epoch 6471: train loss = 2176.863657, test loss = 3506.992103\n",
            "Epoch 6472: train loss = 2176.824109, test loss = 3506.985035\n",
            "Epoch 6473: train loss = 2176.784535, test loss = 3506.978019\n",
            "Epoch 6474: train loss = 2176.744934, test loss = 3506.971056\n",
            "Epoch 6475: train loss = 2176.705307, test loss = 3506.964146\n",
            "Epoch 6476: train loss = 2176.665654, test loss = 3506.957289\n",
            "Epoch 6477: train loss = 2176.625974, test loss = 3506.950486\n",
            "Epoch 6478: train loss = 2176.586267, test loss = 3506.943736\n",
            "Epoch 6479: train loss = 2176.546532, test loss = 3506.937040\n",
            "Epoch 6480: train loss = 2176.506771, test loss = 3506.930399\n",
            "Epoch 6481: train loss = 2176.466982, test loss = 3506.923812\n",
            "Epoch 6482: train loss = 2176.427166, test loss = 3506.917280\n",
            "Epoch 6483: train loss = 2176.387322, test loss = 3506.910803\n",
            "Epoch 6484: train loss = 2176.347450, test loss = 3506.904380\n",
            "Epoch 6485: train loss = 2176.307550, test loss = 3506.898014\n",
            "Epoch 6486: train loss = 2176.267622, test loss = 3506.891703\n",
            "Epoch 6487: train loss = 2176.227666, test loss = 3506.885448\n",
            "Epoch 6488: train loss = 2176.187681, test loss = 3506.879249\n",
            "Epoch 6489: train loss = 2176.147667, test loss = 3506.873106\n",
            "Epoch 6490: train loss = 2176.107625, test loss = 3506.867021\n",
            "Epoch 6491: train loss = 2176.067554, test loss = 3506.860992\n",
            "Epoch 6492: train loss = 2176.027454, test loss = 3506.855020\n",
            "Epoch 6493: train loss = 2175.987324, test loss = 3506.849106\n",
            "Epoch 6494: train loss = 2175.947165, test loss = 3506.843250\n",
            "Epoch 6495: train loss = 2175.906976, test loss = 3506.837451\n",
            "Epoch 6496: train loss = 2175.866757, test loss = 3506.831711\n",
            "Epoch 6497: train loss = 2175.826508, test loss = 3506.826030\n",
            "Epoch 6498: train loss = 2175.786230, test loss = 3506.820407\n",
            "Epoch 6499: train loss = 2175.745920, test loss = 3506.814843\n",
            "Epoch 6500: train loss = 2175.705581, test loss = 3506.809339\n",
            "Epoch 6501: train loss = 2175.665211, test loss = 3506.803894\n",
            "Epoch 6502: train loss = 2175.624809, test loss = 3506.798510\n",
            "Epoch 6503: train loss = 2175.584377, test loss = 3506.793185\n",
            "Epoch 6504: train loss = 2175.543914, test loss = 3506.787921\n",
            "Epoch 6505: train loss = 2175.503419, test loss = 3506.782718\n",
            "Epoch 6506: train loss = 2175.462892, test loss = 3506.777576\n",
            "Epoch 6507: train loss = 2175.422334, test loss = 3506.772495\n",
            "Epoch 6508: train loss = 2175.381744, test loss = 3506.767475\n",
            "Epoch 6509: train loss = 2175.341122, test loss = 3506.762518\n",
            "Epoch 6510: train loss = 2175.300467, test loss = 3506.757623\n",
            "Epoch 6511: train loss = 2175.259780, test loss = 3506.752790\n",
            "Epoch 6512: train loss = 2175.219061, test loss = 3506.748021\n",
            "Epoch 6513: train loss = 2175.178308, test loss = 3506.743314\n",
            "Epoch 6514: train loss = 2175.137523, test loss = 3506.738671\n",
            "Epoch 6515: train loss = 2175.096704, test loss = 3506.734091\n",
            "Epoch 6516: train loss = 2175.055851, test loss = 3506.729575\n",
            "Epoch 6517: train loss = 2175.014965, test loss = 3506.725124\n",
            "Epoch 6518: train loss = 2174.974046, test loss = 3506.720737\n",
            "Epoch 6519: train loss = 2174.933092, test loss = 3506.716416\n",
            "Epoch 6520: train loss = 2174.892104, test loss = 3506.712159\n",
            "Epoch 6521: train loss = 2174.851081, test loss = 3506.707969\n",
            "Epoch 6522: train loss = 2174.810024, test loss = 3506.703844\n",
            "Epoch 6523: train loss = 2174.768933, test loss = 3506.699785\n",
            "Epoch 6524: train loss = 2174.727806, test loss = 3506.695793\n",
            "Epoch 6525: train loss = 2174.686644, test loss = 3506.691868\n",
            "Epoch 6526: train loss = 2174.645446, test loss = 3506.688010\n",
            "Epoch 6527: train loss = 2174.604213, test loss = 3506.684219\n",
            "Epoch 6528: train loss = 2174.562944, test loss = 3506.680496\n",
            "Epoch 6529: train loss = 2174.521639, test loss = 3506.676842\n",
            "Epoch 6530: train loss = 2174.480297, test loss = 3506.673256\n",
            "Epoch 6531: train loss = 2174.438920, test loss = 3506.669739\n",
            "Epoch 6532: train loss = 2174.397505, test loss = 3506.666291\n",
            "Epoch 6533: train loss = 2174.356054, test loss = 3506.662913\n",
            "Epoch 6534: train loss = 2174.314565, test loss = 3506.659604\n",
            "Epoch 6535: train loss = 2174.273040, test loss = 3506.656366\n",
            "Epoch 6536: train loss = 2174.231476, test loss = 3506.653199\n",
            "Epoch 6537: train loss = 2174.189875, test loss = 3506.650102\n",
            "Epoch 6538: train loss = 2174.148236, test loss = 3506.647077\n",
            "Epoch 6539: train loss = 2174.106559, test loss = 3506.644124\n",
            "Epoch 6540: train loss = 2174.064843, test loss = 3506.641242\n",
            "Epoch 6541: train loss = 2174.023089, test loss = 3506.638433\n",
            "Epoch 6542: train loss = 2173.981295, test loss = 3506.635697\n",
            "Epoch 6543: train loss = 2173.939463, test loss = 3506.633034\n",
            "Epoch 6544: train loss = 2173.897591, test loss = 3506.630444\n",
            "Epoch 6545: train loss = 2173.855680, test loss = 3506.627929\n",
            "Epoch 6546: train loss = 2173.813729, test loss = 3506.625487\n",
            "Epoch 6547: train loss = 2173.771738, test loss = 3506.623120\n",
            "Epoch 6548: train loss = 2173.729707, test loss = 3506.620829\n",
            "Epoch 6549: train loss = 2173.687635, test loss = 3506.618612\n",
            "Epoch 6550: train loss = 2173.645522, test loss = 3506.616472\n",
            "Epoch 6551: train loss = 2173.603369, test loss = 3506.614407\n",
            "Epoch 6552: train loss = 2173.561174, test loss = 3506.612420\n",
            "Epoch 6553: train loss = 2173.518938, test loss = 3506.610509\n",
            "Epoch 6554: train loss = 2173.476660, test loss = 3506.608676\n",
            "Epoch 6555: train loss = 2173.434340, test loss = 3506.606920\n",
            "Epoch 6556: train loss = 2173.391978, test loss = 3506.605243\n",
            "Epoch 6557: train loss = 2173.349574, test loss = 3506.603644\n",
            "Epoch 6558: train loss = 2173.307126, test loss = 3506.602124\n",
            "Epoch 6559: train loss = 2173.264636, test loss = 3506.600684\n",
            "Epoch 6560: train loss = 2173.222103, test loss = 3506.599323\n",
            "Epoch 6561: train loss = 2173.179526, test loss = 3506.598043\n",
            "Epoch 6562: train loss = 2173.136906, test loss = 3506.596844\n",
            "Epoch 6563: train loss = 2173.094241, test loss = 3506.595725\n",
            "Epoch 6564: train loss = 2173.051532, test loss = 3506.594688\n",
            "Epoch 6565: train loss = 2173.008779, test loss = 3506.593733\n",
            "Epoch 6566: train loss = 2172.965981, test loss = 3506.592861\n",
            "Epoch 6567: train loss = 2172.923138, test loss = 3506.592071\n",
            "Epoch 6568: train loss = 2172.880250, test loss = 3506.591365\n",
            "Epoch 6569: train loss = 2172.837316, test loss = 3506.590742\n",
            "Epoch 6570: train loss = 2172.794337, test loss = 3506.590204\n",
            "Epoch 6571: train loss = 2172.751311, test loss = 3506.589750\n",
            "Epoch 6572: train loss = 2172.708239, test loss = 3506.589381\n",
            "Epoch 6573: train loss = 2172.665120, test loss = 3506.589098\n",
            "Epoch 6574: train loss = 2172.621955, test loss = 3506.588901\n",
            "Epoch 6575: train loss = 2172.578742, test loss = 3506.588790\n",
            "Epoch 6576: train loss = 2172.535481, test loss = 3506.588766\n",
            "Epoch 6577: train loss = 2172.492173, test loss = 3506.588829\n",
            "Epoch 6578: train loss = 2172.448817, test loss = 3506.588981\n",
            "Epoch 6579: train loss = 2172.405413, test loss = 3506.589220\n",
            "Epoch 6580: train loss = 2172.361960, test loss = 3506.589549\n",
            "Epoch 6581: train loss = 2172.318458, test loss = 3506.589967\n",
            "Epoch 6582: train loss = 2172.274906, test loss = 3506.590474\n",
            "Epoch 6583: train loss = 2172.231306, test loss = 3506.591072\n",
            "Epoch 6584: train loss = 2172.187655, test loss = 3506.591761\n",
            "Epoch 6585: train loss = 2172.143955, test loss = 3506.592541\n",
            "Epoch 6586: train loss = 2172.100204, test loss = 3506.593413\n",
            "Epoch 6587: train loss = 2172.056402, test loss = 3506.594377\n",
            "Epoch 6588: train loss = 2172.012549, test loss = 3506.595434\n",
            "Epoch 6589: train loss = 2171.968645, test loss = 3506.596584\n",
            "Epoch 6590: train loss = 2171.924690, test loss = 3506.597827\n",
            "Epoch 6591: train loss = 2171.880682, test loss = 3506.599166\n",
            "Epoch 6592: train loss = 2171.836622, test loss = 3506.600599\n",
            "Epoch 6593: train loss = 2171.792510, test loss = 3506.602127\n",
            "Epoch 6594: train loss = 2171.748345, test loss = 3506.603751\n",
            "Epoch 6595: train loss = 2171.704126, test loss = 3506.605472\n",
            "Epoch 6596: train loss = 2171.659855, test loss = 3506.607290\n",
            "Epoch 6597: train loss = 2171.615529, test loss = 3506.609205\n",
            "Epoch 6598: train loss = 2171.571149, test loss = 3506.611218\n",
            "Epoch 6599: train loss = 2171.526715, test loss = 3506.613330\n",
            "Epoch 6600: train loss = 2171.482226, test loss = 3506.615541\n",
            "Epoch 6601: train loss = 2171.437681, test loss = 3506.617852\n",
            "Epoch 6602: train loss = 2171.393082, test loss = 3506.620263\n",
            "Epoch 6603: train loss = 2171.348426, test loss = 3506.622775\n",
            "Epoch 6604: train loss = 2171.303715, test loss = 3506.625388\n",
            "Epoch 6605: train loss = 2171.258947, test loss = 3506.628103\n",
            "Epoch 6606: train loss = 2171.214122, test loss = 3506.630921\n",
            "Epoch 6607: train loss = 2171.169240, test loss = 3506.633842\n",
            "Epoch 6608: train loss = 2171.124300, test loss = 3506.636867\n",
            "Epoch 6609: train loss = 2171.079303, test loss = 3506.639996\n",
            "Epoch 6610: train loss = 2171.034247, test loss = 3506.643229\n",
            "Epoch 6611: train loss = 2170.989133, test loss = 3506.646569\n",
            "Epoch 6612: train loss = 2170.943960, test loss = 3506.650014\n",
            "Epoch 6613: train loss = 2170.898727, test loss = 3506.653566\n",
            "Epoch 6614: train loss = 2170.853435, test loss = 3506.657225\n",
            "Epoch 6615: train loss = 2170.808083, test loss = 3506.660993\n",
            "Epoch 6616: train loss = 2170.762671, test loss = 3506.664869\n",
            "Epoch 6617: train loss = 2170.717198, test loss = 3506.668853\n",
            "Epoch 6618: train loss = 2170.671663, test loss = 3506.672948\n",
            "Epoch 6619: train loss = 2170.626068, test loss = 3506.677153\n",
            "Epoch 6620: train loss = 2170.580410, test loss = 3506.681469\n",
            "Epoch 6621: train loss = 2170.534690, test loss = 3506.685897\n",
            "Epoch 6622: train loss = 2170.488908, test loss = 3506.690437\n",
            "Epoch 6623: train loss = 2170.443063, test loss = 3506.695091\n",
            "Epoch 6624: train loss = 2170.397154, test loss = 3506.699857\n",
            "Epoch 6625: train loss = 2170.351182, test loss = 3506.704738\n",
            "Epoch 6626: train loss = 2170.305145, test loss = 3506.709734\n",
            "Epoch 6627: train loss = 2170.259044, test loss = 3506.714846\n",
            "Epoch 6628: train loss = 2170.212878, test loss = 3506.720073\n",
            "Epoch 6629: train loss = 2170.166646, test loss = 3506.725418\n",
            "Epoch 6630: train loss = 2170.120349, test loss = 3506.730881\n",
            "Epoch 6631: train loss = 2170.073986, test loss = 3506.736461\n",
            "Epoch 6632: train loss = 2170.027556, test loss = 3506.742161\n",
            "Epoch 6633: train loss = 2169.981059, test loss = 3506.747980\n",
            "Epoch 6634: train loss = 2169.934495, test loss = 3506.753920\n",
            "Epoch 6635: train loss = 2169.887863, test loss = 3506.759981\n",
            "Epoch 6636: train loss = 2169.841163, test loss = 3506.766163\n",
            "Epoch 6637: train loss = 2169.794395, test loss = 3506.772468\n",
            "Epoch 6638: train loss = 2169.747557, test loss = 3506.778896\n",
            "Epoch 6639: train loss = 2169.700650, test loss = 3506.785449\n",
            "Epoch 6640: train loss = 2169.653673, test loss = 3506.792125\n",
            "Epoch 6641: train loss = 2169.606625, test loss = 3506.798928\n",
            "Epoch 6642: train loss = 2169.559507, test loss = 3506.805856\n",
            "Epoch 6643: train loss = 2169.512318, test loss = 3506.812911\n",
            "Epoch 6644: train loss = 2169.465057, test loss = 3506.820094\n",
            "Epoch 6645: train loss = 2169.417724, test loss = 3506.827405\n",
            "Epoch 6646: train loss = 2169.370319, test loss = 3506.834846\n",
            "Epoch 6647: train loss = 2169.322840, test loss = 3506.842416\n",
            "Epoch 6648: train loss = 2169.275288, test loss = 3506.850117\n",
            "Epoch 6649: train loss = 2169.227663, test loss = 3506.857949\n",
            "Epoch 6650: train loss = 2169.179963, test loss = 3506.865913\n",
            "Epoch 6651: train loss = 2169.132188, test loss = 3506.874011\n",
            "Epoch 6652: train loss = 2169.084338, test loss = 3506.882242\n",
            "Epoch 6653: train loss = 2169.036412, test loss = 3506.890607\n",
            "Epoch 6654: train loss = 2168.988410, test loss = 3506.899108\n",
            "Epoch 6655: train loss = 2168.940331, test loss = 3506.907745\n",
            "Epoch 6656: train loss = 2168.892176, test loss = 3506.916519\n",
            "Epoch 6657: train loss = 2168.843942, test loss = 3506.925431\n",
            "Epoch 6658: train loss = 2168.795631, test loss = 3506.934481\n",
            "Epoch 6659: train loss = 2168.747241, test loss = 3506.943671\n",
            "Epoch 6660: train loss = 2168.698772, test loss = 3506.953001\n",
            "Epoch 6661: train loss = 2168.650224, test loss = 3506.962472\n",
            "Epoch 6662: train loss = 2168.601595, test loss = 3506.972084\n",
            "Epoch 6663: train loss = 2168.552886, test loss = 3506.981840\n",
            "Epoch 6664: train loss = 2168.504096, test loss = 3506.991739\n",
            "Epoch 6665: train loss = 2168.455224, test loss = 3507.001782\n",
            "Epoch 6666: train loss = 2168.406270, test loss = 3507.011971\n",
            "Epoch 6667: train loss = 2168.357234, test loss = 3507.022306\n",
            "Epoch 6668: train loss = 2168.308115, test loss = 3507.032788\n",
            "Epoch 6669: train loss = 2168.258912, test loss = 3507.043417\n",
            "Epoch 6670: train loss = 2168.209625, test loss = 3507.054196\n",
            "Epoch 6671: train loss = 2168.160253, test loss = 3507.065124\n",
            "Epoch 6672: train loss = 2168.110796, test loss = 3507.076202\n",
            "Epoch 6673: train loss = 2168.061254, test loss = 3507.087432\n",
            "Epoch 6674: train loss = 2168.011625, test loss = 3507.098814\n",
            "Epoch 6675: train loss = 2167.961909, test loss = 3507.110349\n",
            "Epoch 6676: train loss = 2167.912106, test loss = 3507.122039\n",
            "Epoch 6677: train loss = 2167.862216, test loss = 3507.133883\n",
            "Epoch 6678: train loss = 2167.812237, test loss = 3507.145884\n",
            "Epoch 6679: train loss = 2167.762168, test loss = 3507.158041\n",
            "Epoch 6680: train loss = 2167.712011, test loss = 3507.170356\n",
            "Epoch 6681: train loss = 2167.661763, test loss = 3507.182830\n",
            "Epoch 6682: train loss = 2167.611425, test loss = 3507.195464\n",
            "Epoch 6683: train loss = 2167.560996, test loss = 3507.208258\n",
            "Epoch 6684: train loss = 2167.510474, test loss = 3507.221213\n",
            "Epoch 6685: train loss = 2167.459861, test loss = 3507.234332\n",
            "Epoch 6686: train loss = 2167.409154, test loss = 3507.247613\n",
            "Epoch 6687: train loss = 2167.358354, test loss = 3507.261059\n",
            "Epoch 6688: train loss = 2167.307460, test loss = 3507.274671\n",
            "Epoch 6689: train loss = 2167.256472, test loss = 3507.288449\n",
            "Epoch 6690: train loss = 2167.205388, test loss = 3507.302394\n",
            "Epoch 6691: train loss = 2167.154208, test loss = 3507.316508\n",
            "Epoch 6692: train loss = 2167.102932, test loss = 3507.330791\n",
            "Epoch 6693: train loss = 2167.051558, test loss = 3507.345244\n",
            "Epoch 6694: train loss = 2167.000087, test loss = 3507.359869\n",
            "Epoch 6695: train loss = 2166.948518, test loss = 3507.374666\n",
            "Epoch 6696: train loss = 2166.896850, test loss = 3507.389637\n",
            "Epoch 6697: train loss = 2166.845082, test loss = 3507.404782\n",
            "Epoch 6698: train loss = 2166.793214, test loss = 3507.420102\n",
            "Epoch 6699: train loss = 2166.741246, test loss = 3507.435599\n",
            "Epoch 6700: train loss = 2166.689176, test loss = 3507.451273\n",
            "Epoch 6701: train loss = 2166.637004, test loss = 3507.467126\n",
            "Epoch 6702: train loss = 2166.584729, test loss = 3507.483159\n",
            "Epoch 6703: train loss = 2166.532351, test loss = 3507.499372\n",
            "Epoch 6704: train loss = 2166.479870, test loss = 3507.515766\n",
            "Epoch 6705: train loss = 2166.427283, test loss = 3507.532344\n",
            "Epoch 6706: train loss = 2166.374591, test loss = 3507.549105\n",
            "Epoch 6707: train loss = 2166.321794, test loss = 3507.566051\n",
            "Epoch 6708: train loss = 2166.268890, test loss = 3507.583183\n",
            "Epoch 6709: train loss = 2166.215878, test loss = 3507.600503\n",
            "Epoch 6710: train loss = 2166.162759, test loss = 3507.618010\n",
            "Epoch 6711: train loss = 2166.109531, test loss = 3507.635706\n",
            "Epoch 6712: train loss = 2166.056194, test loss = 3507.653593\n",
            "Epoch 6713: train loss = 2166.002747, test loss = 3507.671671\n",
            "Epoch 6714: train loss = 2165.949189, test loss = 3507.689942\n",
            "Epoch 6715: train loss = 2165.895520, test loss = 3507.708406\n",
            "Epoch 6716: train loss = 2165.841739, test loss = 3507.727065\n",
            "Epoch 6717: train loss = 2165.787846, test loss = 3507.745919\n",
            "Epoch 6718: train loss = 2165.733839, test loss = 3507.764971\n",
            "Epoch 6719: train loss = 2165.679718, test loss = 3507.784221\n",
            "Epoch 6720: train loss = 2165.625482, test loss = 3507.803670\n",
            "Epoch 6721: train loss = 2165.571130, test loss = 3507.823319\n",
            "Epoch 6722: train loss = 2165.516662, test loss = 3507.843169\n",
            "Epoch 6723: train loss = 2165.462078, test loss = 3507.863223\n",
            "Epoch 6724: train loss = 2165.407375, test loss = 3507.883480\n",
            "Epoch 6725: train loss = 2165.352555, test loss = 3507.903942\n",
            "Epoch 6726: train loss = 2165.297615, test loss = 3507.924610\n",
            "Epoch 6727: train loss = 2165.242555, test loss = 3507.945485\n",
            "Epoch 6728: train loss = 2165.187374, test loss = 3507.966568\n",
            "Epoch 6729: train loss = 2165.132072, test loss = 3507.987861\n",
            "Epoch 6730: train loss = 2165.076648, test loss = 3508.009365\n",
            "Epoch 6731: train loss = 2165.021102, test loss = 3508.031081\n",
            "Epoch 6732: train loss = 2164.965431, test loss = 3508.053009\n",
            "Epoch 6733: train loss = 2164.909636, test loss = 3508.075152\n",
            "Epoch 6734: train loss = 2164.853716, test loss = 3508.097511\n",
            "Epoch 6735: train loss = 2164.797670, test loss = 3508.120085\n",
            "Epoch 6736: train loss = 2164.741497, test loss = 3508.142878\n",
            "Epoch 6737: train loss = 2164.685197, test loss = 3508.165890\n",
            "Epoch 6738: train loss = 2164.628769, test loss = 3508.189121\n",
            "Epoch 6739: train loss = 2164.572211, test loss = 3508.212574\n",
            "Epoch 6740: train loss = 2164.515524, test loss = 3508.236250\n",
            "Epoch 6741: train loss = 2164.458706, test loss = 3508.260149\n",
            "Epoch 6742: train loss = 2164.401756, test loss = 3508.284274\n",
            "Epoch 6743: train loss = 2164.344674, test loss = 3508.308624\n",
            "Epoch 6744: train loss = 2164.287460, test loss = 3508.333202\n",
            "Epoch 6745: train loss = 2164.230111, test loss = 3508.358008\n",
            "Epoch 6746: train loss = 2164.172628, test loss = 3508.383045\n",
            "Epoch 6747: train loss = 2164.115009, test loss = 3508.408312\n",
            "Epoch 6748: train loss = 2164.057254, test loss = 3508.433811\n",
            "Epoch 6749: train loss = 2163.999362, test loss = 3508.459544\n",
            "Epoch 6750: train loss = 2163.941331, test loss = 3508.485511\n",
            "Epoch 6751: train loss = 2163.883163, test loss = 3508.511714\n",
            "Epoch 6752: train loss = 2163.824854, test loss = 3508.538155\n",
            "Epoch 6753: train loss = 2163.766405, test loss = 3508.564833\n",
            "Epoch 6754: train loss = 2163.707815, test loss = 3508.591752\n",
            "Epoch 6755: train loss = 2163.649082, test loss = 3508.618910\n",
            "Epoch 6756: train loss = 2163.590207, test loss = 3508.646311\n",
            "Epoch 6757: train loss = 2163.531187, test loss = 3508.673955\n",
            "Epoch 6758: train loss = 2163.472023, test loss = 3508.701843\n",
            "Epoch 6759: train loss = 2163.412713, test loss = 3508.729977\n",
            "Epoch 6760: train loss = 2163.353257, test loss = 3508.758358\n",
            "Epoch 6761: train loss = 2163.293654, test loss = 3508.786987\n",
            "Epoch 6762: train loss = 2163.233902, test loss = 3508.815865\n",
            "Epoch 6763: train loss = 2163.174002, test loss = 3508.844994\n",
            "Epoch 6764: train loss = 2163.113951, test loss = 3508.874374\n",
            "Epoch 6765: train loss = 2163.053750, test loss = 3508.904007\n",
            "Epoch 6766: train loss = 2162.993397, test loss = 3508.933895\n",
            "Epoch 6767: train loss = 2162.932892, test loss = 3508.964038\n",
            "Epoch 6768: train loss = 2162.872233, test loss = 3508.994437\n",
            "Epoch 6769: train loss = 2162.811419, test loss = 3509.025094\n",
            "Epoch 6770: train loss = 2162.750451, test loss = 3509.056010\n",
            "Epoch 6771: train loss = 2162.689326, test loss = 3509.087186\n",
            "Epoch 6772: train loss = 2162.628044, test loss = 3509.118624\n",
            "Epoch 6773: train loss = 2162.566605, test loss = 3509.150324\n",
            "Epoch 6774: train loss = 2162.505006, test loss = 3509.182288\n",
            "Epoch 6775: train loss = 2162.443248, test loss = 3509.214516\n",
            "Epoch 6776: train loss = 2162.381329, test loss = 3509.247011\n",
            "Epoch 6777: train loss = 2162.319249, test loss = 3509.279774\n",
            "Epoch 6778: train loss = 2162.257006, test loss = 3509.312804\n",
            "Epoch 6779: train loss = 2162.194600, test loss = 3509.346105\n",
            "Epoch 6780: train loss = 2162.132030, test loss = 3509.379676\n",
            "Epoch 6781: train loss = 2162.069294, test loss = 3509.413519\n",
            "Epoch 6782: train loss = 2162.006392, test loss = 3509.447636\n",
            "Epoch 6783: train loss = 2161.943324, test loss = 3509.482026\n",
            "Epoch 6784: train loss = 2161.880087, test loss = 3509.516693\n",
            "Epoch 6785: train loss = 2161.816681, test loss = 3509.551635\n",
            "Epoch 6786: train loss = 2161.753106, test loss = 3509.586856\n",
            "Epoch 6787: train loss = 2161.689360, test loss = 3509.622355\n",
            "Epoch 6788: train loss = 2161.625442, test loss = 3509.658134\n",
            "Epoch 6789: train loss = 2161.561352, test loss = 3509.694194\n",
            "Epoch 6790: train loss = 2161.497088, test loss = 3509.730537\n",
            "Epoch 6791: train loss = 2161.432650, test loss = 3509.767162\n",
            "Epoch 6792: train loss = 2161.368037, test loss = 3509.804072\n",
            "Epoch 6793: train loss = 2161.303247, test loss = 3509.841268\n",
            "Epoch 6794: train loss = 2161.238280, test loss = 3509.878749\n",
            "Epoch 6795: train loss = 2161.173135, test loss = 3509.916519\n",
            "Epoch 6796: train loss = 2161.107811, test loss = 3509.954576\n",
            "Epoch 6797: train loss = 2161.042307, test loss = 3509.992924\n",
            "Epoch 6798: train loss = 2160.976623, test loss = 3510.031562\n",
            "Epoch 6799: train loss = 2160.910756, test loss = 3510.070492\n",
            "Epoch 6800: train loss = 2160.844707, test loss = 3510.109714\n",
            "Epoch 6801: train loss = 2160.778474, test loss = 3510.149230\n",
            "Epoch 6802: train loss = 2160.712056, test loss = 3510.189040\n",
            "Epoch 6803: train loss = 2160.645454, test loss = 3510.229146\n",
            "Epoch 6804: train loss = 2160.578664, test loss = 3510.269549\n",
            "Epoch 6805: train loss = 2160.511688, test loss = 3510.310249\n",
            "Epoch 6806: train loss = 2160.444523, test loss = 3510.351247\n",
            "Epoch 6807: train loss = 2160.377170, test loss = 3510.392545\n",
            "Epoch 6808: train loss = 2160.309626, test loss = 3510.434142\n",
            "Epoch 6809: train loss = 2160.241891, test loss = 3510.476041\n",
            "Epoch 6810: train loss = 2160.173965, test loss = 3510.518241\n",
            "Epoch 6811: train loss = 2160.105846, test loss = 3510.560745\n",
            "Epoch 6812: train loss = 2160.037534, test loss = 3510.603551\n",
            "Epoch 6813: train loss = 2159.969027, test loss = 3510.646663\n",
            "Epoch 6814: train loss = 2159.900324, test loss = 3510.690079\n",
            "Epoch 6815: train loss = 2159.831426, test loss = 3510.733801\n",
            "Epoch 6816: train loss = 2159.762331, test loss = 3510.777830\n",
            "Epoch 6817: train loss = 2159.693038, test loss = 3510.822166\n",
            "Epoch 6818: train loss = 2159.623546, test loss = 3510.866811\n",
            "Epoch 6819: train loss = 2159.553854, test loss = 3510.911764\n",
            "Epoch 6820: train loss = 2159.483962, test loss = 3510.957027\n",
            "Epoch 6821: train loss = 2159.413869, test loss = 3511.002599\n",
            "Epoch 6822: train loss = 2159.343574, test loss = 3511.048483\n",
            "Epoch 6823: train loss = 2159.273075, test loss = 3511.094678\n",
            "Epoch 6824: train loss = 2159.202374, test loss = 3511.141185\n",
            "Epoch 6825: train loss = 2159.131467, test loss = 3511.188005\n",
            "Epoch 6826: train loss = 2159.060355, test loss = 3511.235137\n",
            "Epoch 6827: train loss = 2158.989038, test loss = 3511.282584\n",
            "Epoch 6828: train loss = 2158.917513, test loss = 3511.330344\n",
            "Epoch 6829: train loss = 2158.845781, test loss = 3511.378419\n",
            "Epoch 6830: train loss = 2158.773840, test loss = 3511.426809\n",
            "Epoch 6831: train loss = 2158.701690, test loss = 3511.475514\n",
            "Epoch 6832: train loss = 2158.629331, test loss = 3511.524535\n",
            "Epoch 6833: train loss = 2158.556761, test loss = 3511.573873\n",
            "Epoch 6834: train loss = 2158.483979, test loss = 3511.623526\n",
            "Epoch 6835: train loss = 2158.410986, test loss = 3511.673497\n",
            "Epoch 6836: train loss = 2158.337779, test loss = 3511.723785\n",
            "Epoch 6837: train loss = 2158.264360, test loss = 3511.774390\n",
            "Epoch 6838: train loss = 2158.190727, test loss = 3511.825313\n",
            "Epoch 6839: train loss = 2158.116878, test loss = 3511.876553\n",
            "Epoch 6840: train loss = 2158.042815, test loss = 3511.928112\n",
            "Epoch 6841: train loss = 2157.968536, test loss = 3511.979988\n",
            "Epoch 6842: train loss = 2157.894040, test loss = 3512.032183\n",
            "Epoch 6843: train loss = 2157.819327, test loss = 3512.084695\n",
            "Epoch 6844: train loss = 2157.744396, test loss = 3512.137526\n",
            "Epoch 6845: train loss = 2157.669248, test loss = 3512.190676\n",
            "Epoch 6846: train loss = 2157.593880, test loss = 3512.244143\n",
            "Epoch 6847: train loss = 2157.518294, test loss = 3512.297929\n",
            "Epoch 6848: train loss = 2157.442487, test loss = 3512.352032\n",
            "Epoch 6849: train loss = 2157.366461, test loss = 3512.406454\n",
            "Epoch 6850: train loss = 2157.290213, test loss = 3512.461193\n",
            "Epoch 6851: train loss = 2157.213745, test loss = 3512.516249\n",
            "Epoch 6852: train loss = 2157.137055, test loss = 3512.571623\n",
            "Epoch 6853: train loss = 2157.060143, test loss = 3512.627314\n",
            "Epoch 6854: train loss = 2156.983009, test loss = 3512.683321\n",
            "Epoch 6855: train loss = 2156.905652, test loss = 3512.739645\n",
            "Epoch 6856: train loss = 2156.828072, test loss = 3512.796284\n",
            "Epoch 6857: train loss = 2156.750269, test loss = 3512.853239\n",
            "Epoch 6858: train loss = 2156.672242, test loss = 3512.910509\n",
            "Epoch 6859: train loss = 2156.593992, test loss = 3512.968093\n",
            "Epoch 6860: train loss = 2156.515517, test loss = 3513.025991\n",
            "Epoch 6861: train loss = 2156.436818, test loss = 3513.084202\n",
            "Epoch 6862: train loss = 2156.357895, test loss = 3513.142726\n",
            "Epoch 6863: train loss = 2156.278747, test loss = 3513.201561\n",
            "Epoch 6864: train loss = 2156.199374, test loss = 3513.260708\n",
            "Epoch 6865: train loss = 2156.119777, test loss = 3513.320165\n",
            "Epoch 6866: train loss = 2156.039954, test loss = 3513.379931\n",
            "Epoch 6867: train loss = 2155.959906, test loss = 3513.440006\n",
            "Epoch 6868: train loss = 2155.879634, test loss = 3513.500389\n",
            "Epoch 6869: train loss = 2155.799136, test loss = 3513.561078\n",
            "Epoch 6870: train loss = 2155.718414, test loss = 3513.622073\n",
            "Epoch 6871: train loss = 2155.637467, test loss = 3513.683373\n",
            "Epoch 6872: train loss = 2155.556295, test loss = 3513.744977\n",
            "Epoch 6873: train loss = 2155.474898, test loss = 3513.806883\n",
            "Epoch 6874: train loss = 2155.393277, test loss = 3513.869090\n",
            "Epoch 6875: train loss = 2155.311432, test loss = 3513.931597\n",
            "Epoch 6876: train loss = 2155.229362, test loss = 3513.994403\n",
            "Epoch 6877: train loss = 2155.147069, test loss = 3514.057506\n",
            "Epoch 6878: train loss = 2155.064553, test loss = 3514.120905\n",
            "Epoch 6879: train loss = 2154.981813, test loss = 3514.184599\n",
            "Epoch 6880: train loss = 2154.898851, test loss = 3514.248586\n",
            "Epoch 6881: train loss = 2154.815667, test loss = 3514.312865\n",
            "Epoch 6882: train loss = 2154.732260, test loss = 3514.377433\n",
            "Epoch 6883: train loss = 2154.648633, test loss = 3514.442290\n",
            "Epoch 6884: train loss = 2154.564784, test loss = 3514.507434\n",
            "Epoch 6885: train loss = 2154.480716, test loss = 3514.572862\n",
            "Epoch 6886: train loss = 2154.396427, test loss = 3514.638574\n",
            "Epoch 6887: train loss = 2154.311920, test loss = 3514.704567\n",
            "Epoch 6888: train loss = 2154.227195, test loss = 3514.770839\n",
            "Epoch 6889: train loss = 2154.142252, test loss = 3514.837389\n",
            "Epoch 6890: train loss = 2154.057092, test loss = 3514.904214\n",
            "Epoch 6891: train loss = 2153.971716, test loss = 3514.971313\n",
            "Epoch 6892: train loss = 2153.886125, test loss = 3515.038684\n",
            "Epoch 6893: train loss = 2153.800320, test loss = 3515.106323\n",
            "Epoch 6894: train loss = 2153.714301, test loss = 3515.174230\n",
            "Epoch 6895: train loss = 2153.628070, test loss = 3515.242402\n",
            "Epoch 6896: train loss = 2153.541628, test loss = 3515.310836\n",
            "Epoch 6897: train loss = 2153.454975, test loss = 3515.379530\n",
            "Epoch 6898: train loss = 2153.368114, test loss = 3515.448483\n",
            "Epoch 6899: train loss = 2153.281044, test loss = 3515.517690\n",
            "Epoch 6900: train loss = 2153.193768, test loss = 3515.587151\n",
            "Epoch 6901: train loss = 2153.106285, test loss = 3515.656862\n",
            "Epoch 6902: train loss = 2153.018599, test loss = 3515.726821\n",
            "Epoch 6903: train loss = 2152.930710, test loss = 3515.797025\n",
            "Epoch 6904: train loss = 2152.842619, test loss = 3515.867471\n",
            "Epoch 6905: train loss = 2152.754327, test loss = 3515.938158\n",
            "Epoch 6906: train loss = 2152.665837, test loss = 3516.009081\n",
            "Epoch 6907: train loss = 2152.577150, test loss = 3516.080238\n",
            "Epoch 6908: train loss = 2152.488267, test loss = 3516.151627\n",
            "Epoch 6909: train loss = 2152.399190, test loss = 3516.223244\n",
            "Epoch 6910: train loss = 2152.309921, test loss = 3516.295086\n",
            "Epoch 6911: train loss = 2152.220460, test loss = 3516.367150\n",
            "Epoch 6912: train loss = 2152.130811, test loss = 3516.439434\n",
            "Epoch 6913: train loss = 2152.040974, test loss = 3516.511934\n",
            "Epoch 6914: train loss = 2151.950952, test loss = 3516.584646\n",
            "Epoch 6915: train loss = 2151.860746, test loss = 3516.657569\n",
            "Epoch 6916: train loss = 2151.770359, test loss = 3516.730698\n",
            "Epoch 6917: train loss = 2151.679791, test loss = 3516.804030\n",
            "Epoch 6918: train loss = 2151.589046, test loss = 3516.877562\n",
            "Epoch 6919: train loss = 2151.498125, test loss = 3516.951291\n",
            "Epoch 6920: train loss = 2151.407030, test loss = 3517.025212\n",
            "Epoch 6921: train loss = 2151.315764, test loss = 3517.099323\n",
            "Epoch 6922: train loss = 2151.224328, test loss = 3517.173620\n",
            "Epoch 6923: train loss = 2151.132726, test loss = 3517.248100\n",
            "Epoch 6924: train loss = 2151.040958, test loss = 3517.322758\n",
            "Epoch 6925: train loss = 2150.949027, test loss = 3517.397592\n",
            "Epoch 6926: train loss = 2150.856936, test loss = 3517.472597\n",
            "Epoch 6927: train loss = 2150.764687, test loss = 3517.547771\n",
            "Epoch 6928: train loss = 2150.672282, test loss = 3517.623108\n",
            "Epoch 6929: train loss = 2150.579724, test loss = 3517.698606\n",
            "Epoch 6930: train loss = 2150.487015, test loss = 3517.774261\n",
            "Epoch 6931: train loss = 2150.394158, test loss = 3517.850068\n",
            "Epoch 6932: train loss = 2150.301155, test loss = 3517.926024\n",
            "Epoch 6933: train loss = 2150.208010, test loss = 3518.002125\n",
            "Epoch 6934: train loss = 2150.114723, test loss = 3518.078367\n",
            "Epoch 6935: train loss = 2150.021299, test loss = 3518.154746\n",
            "Epoch 6936: train loss = 2149.927740, test loss = 3518.231258\n",
            "Epoch 6937: train loss = 2149.834048, test loss = 3518.307899\n",
            "Epoch 6938: train loss = 2149.740226, test loss = 3518.384666\n",
            "Epoch 6939: train loss = 2149.646278, test loss = 3518.461553\n",
            "Epoch 6940: train loss = 2149.552205, test loss = 3518.538557\n",
            "Epoch 6941: train loss = 2149.458011, test loss = 3518.615673\n",
            "Epoch 6942: train loss = 2149.363698, test loss = 3518.692898\n",
            "Epoch 6943: train loss = 2149.269270, test loss = 3518.770228\n",
            "Epoch 6944: train loss = 2149.174729, test loss = 3518.847657\n",
            "Epoch 6945: train loss = 2149.080078, test loss = 3518.925183\n",
            "Epoch 6946: train loss = 2148.985321, test loss = 3519.002800\n",
            "Epoch 6947: train loss = 2148.890459, test loss = 3519.080504\n",
            "Epoch 6948: train loss = 2148.795497, test loss = 3519.158292\n",
            "Epoch 6949: train loss = 2148.700437, test loss = 3519.236159\n",
            "Epoch 6950: train loss = 2148.605282, test loss = 3519.314101\n",
            "Epoch 6951: train loss = 2148.510035, test loss = 3519.392112\n",
            "Epoch 6952: train loss = 2148.414700, test loss = 3519.470190\n",
            "Epoch 6953: train loss = 2148.319279, test loss = 3519.548330\n",
            "Epoch 6954: train loss = 2148.223775, test loss = 3519.626527\n",
            "Epoch 6955: train loss = 2148.128192, test loss = 3519.704777\n",
            "Epoch 6956: train loss = 2148.032532, test loss = 3519.783076\n",
            "Epoch 6957: train loss = 2147.936799, test loss = 3519.861419\n",
            "Epoch 6958: train loss = 2147.840996, test loss = 3519.939802\n",
            "Epoch 6959: train loss = 2147.745126, test loss = 3520.018220\n",
            "Epoch 6960: train loss = 2147.649192, test loss = 3520.096670\n",
            "Epoch 6961: train loss = 2147.553198, test loss = 3520.175147\n",
            "Epoch 6962: train loss = 2147.457146, test loss = 3520.253646\n",
            "Epoch 6963: train loss = 2147.361039, test loss = 3520.332164\n",
            "Epoch 6964: train loss = 2147.264881, test loss = 3520.410696\n",
            "Epoch 6965: train loss = 2147.168675, test loss = 3520.489236\n",
            "Epoch 6966: train loss = 2147.072424, test loss = 3520.567783\n",
            "Epoch 6967: train loss = 2146.976131, test loss = 3520.646330\n",
            "Epoch 6968: train loss = 2146.879799, test loss = 3520.724873\n",
            "Epoch 6969: train loss = 2146.783432, test loss = 3520.803409\n",
            "Epoch 6970: train loss = 2146.687032, test loss = 3520.881932\n",
            "Epoch 6971: train loss = 2146.590602, test loss = 3520.960439\n",
            "Epoch 6972: train loss = 2146.494147, test loss = 3521.038925\n",
            "Epoch 6973: train loss = 2146.397668, test loss = 3521.117387\n",
            "Epoch 6974: train loss = 2146.301168, test loss = 3521.195819\n",
            "Epoch 6975: train loss = 2146.204652, test loss = 3521.274217\n",
            "Epoch 6976: train loss = 2146.108122, test loss = 3521.352578\n",
            "Epoch 6977: train loss = 2146.011581, test loss = 3521.430897\n",
            "Epoch 6978: train loss = 2145.915031, test loss = 3521.509170\n",
            "Epoch 6979: train loss = 2145.818477, test loss = 3521.587392\n",
            "Epoch 6980: train loss = 2145.721920, test loss = 3521.665560\n",
            "Epoch 6981: train loss = 2145.625365, test loss = 3521.743670\n",
            "Epoch 6982: train loss = 2145.528813, test loss = 3521.821717\n",
            "Epoch 6983: train loss = 2145.432268, test loss = 3521.899697\n",
            "Epoch 6984: train loss = 2145.335732, test loss = 3521.977606\n",
            "Epoch 6985: train loss = 2145.239209, test loss = 3522.055440\n",
            "Epoch 6986: train loss = 2145.142701, test loss = 3522.133196\n",
            "Epoch 6987: train loss = 2145.046211, test loss = 3522.210869\n",
            "Epoch 6988: train loss = 2144.949742, test loss = 3522.288454\n",
            "Epoch 6989: train loss = 2144.853297, test loss = 3522.365950\n",
            "Epoch 6990: train loss = 2144.756878, test loss = 3522.443350\n",
            "Epoch 6991: train loss = 2144.660488, test loss = 3522.520652\n",
            "Epoch 6992: train loss = 2144.564130, test loss = 3522.597852\n",
            "Epoch 6993: train loss = 2144.467806, test loss = 3522.674945\n",
            "Epoch 6994: train loss = 2144.371519, test loss = 3522.751929\n",
            "Epoch 6995: train loss = 2144.275271, test loss = 3522.828799\n",
            "Epoch 6996: train loss = 2144.179066, test loss = 3522.905552\n",
            "Epoch 6997: train loss = 2144.082905, test loss = 3522.982184\n",
            "Epoch 6998: train loss = 2143.986791, test loss = 3523.058691\n",
            "Epoch 6999: train loss = 2143.890726, test loss = 3523.135070\n",
            "Epoch 7000: train loss = 2143.794713, test loss = 3523.211317\n",
            "Epoch 7001: train loss = 2143.698754, test loss = 3523.287429\n",
            "Epoch 7002: train loss = 2143.602852, test loss = 3523.363402\n",
            "Epoch 7003: train loss = 2143.507009, test loss = 3523.439232\n",
            "Epoch 7004: train loss = 2143.411226, test loss = 3523.514917\n",
            "Epoch 7005: train loss = 2143.315507, test loss = 3523.590454\n",
            "Epoch 7006: train loss = 2143.219853, test loss = 3523.665837\n",
            "Epoch 7007: train loss = 2143.124266, test loss = 3523.741065\n",
            "Epoch 7008: train loss = 2143.028750, test loss = 3523.816134\n",
            "Epoch 7009: train loss = 2142.933304, test loss = 3523.891041\n",
            "Epoch 7010: train loss = 2142.837933, test loss = 3523.965783\n",
            "Epoch 7011: train loss = 2142.742637, test loss = 3524.040356\n",
            "Epoch 7012: train loss = 2142.647419, test loss = 3524.114758\n",
            "Epoch 7013: train loss = 2142.552280, test loss = 3524.188985\n",
            "Epoch 7014: train loss = 2142.457222, test loss = 3524.263035\n",
            "Epoch 7015: train loss = 2142.362248, test loss = 3524.336904\n",
            "Epoch 7016: train loss = 2142.267358, test loss = 3524.410589\n",
            "Epoch 7017: train loss = 2142.172555, test loss = 3524.484089\n",
            "Epoch 7018: train loss = 2142.077841, test loss = 3524.557399\n",
            "Epoch 7019: train loss = 2141.983216, test loss = 3524.630517\n",
            "Epoch 7020: train loss = 2141.888684, test loss = 3524.703440\n",
            "Epoch 7021: train loss = 2141.794244, test loss = 3524.776167\n",
            "Epoch 7022: train loss = 2141.699899, test loss = 3524.848693\n",
            "Epoch 7023: train loss = 2141.605650, test loss = 3524.921016\n",
            "Epoch 7024: train loss = 2141.511499, test loss = 3524.993134\n",
            "Epoch 7025: train loss = 2141.417447, test loss = 3525.065045\n",
            "Epoch 7026: train loss = 2141.323496, test loss = 3525.136745\n",
            "Epoch 7027: train loss = 2141.229646, test loss = 3525.208233\n",
            "Epoch 7028: train loss = 2141.135900, test loss = 3525.279505\n",
            "Epoch 7029: train loss = 2141.042258, test loss = 3525.350560\n",
            "Epoch 7030: train loss = 2140.948721, test loss = 3525.421396\n",
            "Epoch 7031: train loss = 2140.855291, test loss = 3525.492009\n",
            "Epoch 7032: train loss = 2140.761969, test loss = 3525.562399\n",
            "Epoch 7033: train loss = 2140.668757, test loss = 3525.632562\n",
            "Epoch 7034: train loss = 2140.575654, test loss = 3525.702497\n",
            "Epoch 7035: train loss = 2140.482662, test loss = 3525.772201\n",
            "Epoch 7036: train loss = 2140.389783, test loss = 3525.841673\n",
            "Epoch 7037: train loss = 2140.297017, test loss = 3525.910910\n",
            "Epoch 7038: train loss = 2140.204364, test loss = 3525.979911\n",
            "Epoch 7039: train loss = 2140.111827, test loss = 3526.048673\n",
            "Epoch 7040: train loss = 2140.019405, test loss = 3526.117196\n",
            "Epoch 7041: train loss = 2139.927100, test loss = 3526.185476\n",
            "Epoch 7042: train loss = 2139.834913, test loss = 3526.253513\n",
            "Epoch 7043: train loss = 2139.742843, test loss = 3526.321304\n",
            "Epoch 7044: train loss = 2139.650892, test loss = 3526.388848\n",
            "Epoch 7045: train loss = 2139.559061, test loss = 3526.456143\n",
            "Epoch 7046: train loss = 2139.467350, test loss = 3526.523188\n",
            "Epoch 7047: train loss = 2139.375760, test loss = 3526.589982\n",
            "Epoch 7048: train loss = 2139.284291, test loss = 3526.656522\n",
            "Epoch 7049: train loss = 2139.192944, test loss = 3526.722807\n",
            "Epoch 7050: train loss = 2139.101719, test loss = 3526.788836\n",
            "Epoch 7051: train loss = 2139.010617, test loss = 3526.854607\n",
            "Epoch 7052: train loss = 2138.919639, test loss = 3526.920120\n",
            "Epoch 7053: train loss = 2138.828784, test loss = 3526.985372\n",
            "Epoch 7054: train loss = 2138.738054, test loss = 3527.050363\n",
            "Epoch 7055: train loss = 2138.647448, test loss = 3527.115092\n",
            "Epoch 7056: train loss = 2138.556967, test loss = 3527.179557\n",
            "Epoch 7057: train loss = 2138.466612, test loss = 3527.243757\n",
            "Epoch 7058: train loss = 2138.376382, test loss = 3527.307691\n",
            "Epoch 7059: train loss = 2138.286277, test loss = 3527.371358\n",
            "Epoch 7060: train loss = 2138.196299, test loss = 3527.434757\n",
            "Epoch 7061: train loss = 2138.106447, test loss = 3527.497888\n",
            "Epoch 7062: train loss = 2138.016721, test loss = 3527.560748\n",
            "Epoch 7063: train loss = 2137.927122, test loss = 3527.623339\n",
            "Epoch 7064: train loss = 2137.837649, test loss = 3527.685657\n",
            "Epoch 7065: train loss = 2137.748303, test loss = 3527.747704\n",
            "Epoch 7066: train loss = 2137.659084, test loss = 3527.809477\n",
            "Epoch 7067: train loss = 2137.569992, test loss = 3527.870977\n",
            "Epoch 7068: train loss = 2137.481027, test loss = 3527.932203\n",
            "Epoch 7069: train loss = 2137.392189, test loss = 3527.993154\n",
            "Epoch 7070: train loss = 2137.303478, test loss = 3528.053829\n",
            "Epoch 7071: train loss = 2137.214893, test loss = 3528.114228\n",
            "Epoch 7072: train loss = 2137.126436, test loss = 3528.174350\n",
            "Epoch 7073: train loss = 2137.038105, test loss = 3528.234196\n",
            "Epoch 7074: train loss = 2136.949902, test loss = 3528.293764\n",
            "Epoch 7075: train loss = 2136.861824, test loss = 3528.353054\n",
            "Epoch 7076: train loss = 2136.773874, test loss = 3528.412065\n",
            "Epoch 7077: train loss = 2136.686049, test loss = 3528.470798\n",
            "Epoch 7078: train loss = 2136.598351, test loss = 3528.529252\n",
            "Epoch 7079: train loss = 2136.510779, test loss = 3528.587427\n",
            "Epoch 7080: train loss = 2136.423332, test loss = 3528.645322\n",
            "Epoch 7081: train loss = 2136.336011, test loss = 3528.702938\n",
            "Epoch 7082: train loss = 2136.248816, test loss = 3528.760274\n",
            "Epoch 7083: train loss = 2136.161745, test loss = 3528.817330\n",
            "Epoch 7084: train loss = 2136.074800, test loss = 3528.874106\n",
            "Epoch 7085: train loss = 2135.987979, test loss = 3528.930601\n",
            "Epoch 7086: train loss = 2135.901282, test loss = 3528.986817\n",
            "Epoch 7087: train loss = 2135.814709, test loss = 3529.042752\n",
            "Epoch 7088: train loss = 2135.728260, test loss = 3529.098407\n",
            "Epoch 7089: train loss = 2135.641934, test loss = 3529.153782\n",
            "Epoch 7090: train loss = 2135.555731, test loss = 3529.208877\n",
            "Epoch 7091: train loss = 2135.469651, test loss = 3529.263692\n",
            "Epoch 7092: train loss = 2135.383693, test loss = 3529.318227\n",
            "Epoch 7093: train loss = 2135.297857, test loss = 3529.372483\n",
            "Epoch 7094: train loss = 2135.212142, test loss = 3529.426459\n",
            "Epoch 7095: train loss = 2135.126549, test loss = 3529.480156\n",
            "Epoch 7096: train loss = 2135.041076, test loss = 3529.533573\n",
            "Epoch 7097: train loss = 2134.955723, test loss = 3529.586712\n",
            "Epoch 7098: train loss = 2134.870490, test loss = 3529.639573\n",
            "Epoch 7099: train loss = 2134.785377, test loss = 3529.692155\n",
            "Epoch 7100: train loss = 2134.700382, test loss = 3529.744460\n",
            "Epoch 7101: train loss = 2134.615506, test loss = 3529.796487\n",
            "Epoch 7102: train loss = 2134.530748, test loss = 3529.848237\n",
            "Epoch 7103: train loss = 2134.446108, test loss = 3529.899711\n",
            "Epoch 7104: train loss = 2134.361585, test loss = 3529.950908\n",
            "Epoch 7105: train loss = 2134.277178, test loss = 3530.001830\n",
            "Epoch 7106: train loss = 2134.192887, test loss = 3530.052476\n",
            "Epoch 7107: train loss = 2134.108712, test loss = 3530.102848\n",
            "Epoch 7108: train loss = 2134.024652, test loss = 3530.152945\n",
            "Epoch 7109: train loss = 2133.940707, test loss = 3530.202769\n",
            "Epoch 7110: train loss = 2133.856875, test loss = 3530.252320\n",
            "Epoch 7111: train loss = 2133.773158, test loss = 3530.301598\n",
            "Epoch 7112: train loss = 2133.689553, test loss = 3530.350604\n",
            "Epoch 7113: train loss = 2133.606061, test loss = 3530.399338\n",
            "Epoch 7114: train loss = 2133.522681, test loss = 3530.447802\n",
            "Epoch 7115: train loss = 2133.439412, test loss = 3530.495996\n",
            "Epoch 7116: train loss = 2133.356254, test loss = 3530.543921\n",
            "Epoch 7117: train loss = 2133.273207, test loss = 3530.591577\n",
            "Epoch 7118: train loss = 2133.190269, test loss = 3530.638965\n",
            "Epoch 7119: train loss = 2133.107441, test loss = 3530.686085\n",
            "Epoch 7120: train loss = 2133.024721, test loss = 3530.732939\n",
            "Epoch 7121: train loss = 2132.942110, test loss = 3530.779528\n",
            "Epoch 7122: train loss = 2132.859606, test loss = 3530.825851\n",
            "Epoch 7123: train loss = 2132.777210, test loss = 3530.871910\n",
            "Epoch 7124: train loss = 2132.694920, test loss = 3530.917705\n",
            "Epoch 7125: train loss = 2132.612736, test loss = 3530.963238\n",
            "Epoch 7126: train loss = 2132.530657, test loss = 3531.008509\n",
            "Epoch 7127: train loss = 2132.448684, test loss = 3531.053518\n",
            "Epoch 7128: train loss = 2132.366814, test loss = 3531.098268\n",
            "Epoch 7129: train loss = 2132.285049, test loss = 3531.142758\n",
            "Epoch 7130: train loss = 2132.203387, test loss = 3531.186990\n",
            "Epoch 7131: train loss = 2132.121827, test loss = 3531.230964\n",
            "Epoch 7132: train loss = 2132.040369, test loss = 3531.274681\n",
            "Epoch 7133: train loss = 2131.959014, test loss = 3531.318142\n",
            "Epoch 7134: train loss = 2131.877759, test loss = 3531.361348\n",
            "Epoch 7135: train loss = 2131.796604, test loss = 3531.404300\n",
            "Epoch 7136: train loss = 2131.715550, test loss = 3531.446999\n",
            "Epoch 7137: train loss = 2131.634594, test loss = 3531.489446\n",
            "Epoch 7138: train loss = 2131.553738, test loss = 3531.531641\n",
            "Epoch 7139: train loss = 2131.472980, test loss = 3531.573586\n",
            "Epoch 7140: train loss = 2131.392320, test loss = 3531.615282\n",
            "Epoch 7141: train loss = 2131.311757, test loss = 3531.656729\n",
            "Epoch 7142: train loss = 2131.231290, test loss = 3531.697928\n",
            "Epoch 7143: train loss = 2131.150920, test loss = 3531.738880\n",
            "Epoch 7144: train loss = 2131.070645, test loss = 3531.779587\n",
            "Epoch 7145: train loss = 2130.990465, test loss = 3531.820050\n",
            "Epoch 7146: train loss = 2130.910380, test loss = 3531.860268\n",
            "Epoch 7147: train loss = 2130.830389, test loss = 3531.900244\n",
            "Epoch 7148: train loss = 2130.750491, test loss = 3531.939978\n",
            "Epoch 7149: train loss = 2130.670686, test loss = 3531.979472\n",
            "Epoch 7150: train loss = 2130.590973, test loss = 3532.018725\n",
            "Epoch 7151: train loss = 2130.511353, test loss = 3532.057740\n",
            "Epoch 7152: train loss = 2130.431824, test loss = 3532.096517\n",
            "Epoch 7153: train loss = 2130.352386, test loss = 3532.135057\n",
            "Epoch 7154: train loss = 2130.273038, test loss = 3532.173361\n",
            "Epoch 7155: train loss = 2130.193780, test loss = 3532.211431\n",
            "Epoch 7156: train loss = 2130.114611, test loss = 3532.249266\n",
            "Epoch 7157: train loss = 2130.035531, test loss = 3532.286869\n",
            "Epoch 7158: train loss = 2129.956540, test loss = 3532.324240\n",
            "Epoch 7159: train loss = 2129.877637, test loss = 3532.361380\n",
            "Epoch 7160: train loss = 2129.798821, test loss = 3532.398291\n",
            "Epoch 7161: train loss = 2129.720092, test loss = 3532.434973\n",
            "Epoch 7162: train loss = 2129.641449, test loss = 3532.471427\n",
            "Epoch 7163: train loss = 2129.562893, test loss = 3532.507654\n",
            "Epoch 7164: train loss = 2129.484422, test loss = 3532.543655\n",
            "Epoch 7165: train loss = 2129.406036, test loss = 3532.579432\n",
            "Epoch 7166: train loss = 2129.327735, test loss = 3532.614985\n",
            "Epoch 7167: train loss = 2129.249518, test loss = 3532.650315\n",
            "Epoch 7168: train loss = 2129.171384, test loss = 3532.685423\n",
            "Epoch 7169: train loss = 2129.093334, test loss = 3532.720311\n",
            "Epoch 7170: train loss = 2129.015367, test loss = 3532.754979\n",
            "Epoch 7171: train loss = 2128.937482, test loss = 3532.789428\n",
            "Epoch 7172: train loss = 2128.859679, test loss = 3532.823659\n",
            "Epoch 7173: train loss = 2128.781958, test loss = 3532.857674\n",
            "Epoch 7174: train loss = 2128.704318, test loss = 3532.891473\n",
            "Epoch 7175: train loss = 2128.626758, test loss = 3532.925057\n",
            "Epoch 7176: train loss = 2128.549279, test loss = 3532.958427\n",
            "Epoch 7177: train loss = 2128.471879, test loss = 3532.991585\n",
            "Epoch 7178: train loss = 2128.394559, test loss = 3533.024531\n",
            "Epoch 7179: train loss = 2128.317318, test loss = 3533.057266\n",
            "Epoch 7180: train loss = 2128.240156, test loss = 3533.089791\n",
            "Epoch 7181: train loss = 2128.163072, test loss = 3533.122107\n",
            "Epoch 7182: train loss = 2128.086066, test loss = 3533.154215\n",
            "Epoch 7183: train loss = 2128.009137, test loss = 3533.186116\n",
            "Epoch 7184: train loss = 2127.932285, test loss = 3533.217812\n",
            "Epoch 7185: train loss = 2127.855511, test loss = 3533.249302\n",
            "Epoch 7186: train loss = 2127.778812, test loss = 3533.280588\n",
            "Epoch 7187: train loss = 2127.702190, test loss = 3533.311671\n",
            "Epoch 7188: train loss = 2127.625643, test loss = 3533.342552\n",
            "Epoch 7189: train loss = 2127.549171, test loss = 3533.373232\n",
            "Epoch 7190: train loss = 2127.472775, test loss = 3533.403711\n",
            "Epoch 7191: train loss = 2127.396452, test loss = 3533.433992\n",
            "Epoch 7192: train loss = 2127.320205, test loss = 3533.464073\n",
            "Epoch 7193: train loss = 2127.244031, test loss = 3533.493957\n",
            "Epoch 7194: train loss = 2127.167930, test loss = 3533.523645\n",
            "Epoch 7195: train loss = 2127.091903, test loss = 3533.553137\n",
            "Epoch 7196: train loss = 2127.015949, test loss = 3533.582434\n",
            "Epoch 7197: train loss = 2126.940067, test loss = 3533.611537\n",
            "Epoch 7198: train loss = 2126.864258, test loss = 3533.640447\n",
            "Epoch 7199: train loss = 2126.788520, test loss = 3533.669165\n",
            "Epoch 7200: train loss = 2126.712854, test loss = 3533.697692\n",
            "Epoch 7201: train loss = 2126.637259, test loss = 3533.726029\n",
            "Epoch 7202: train loss = 2126.561736, test loss = 3533.754177\n",
            "Epoch 7203: train loss = 2126.486283, test loss = 3533.782135\n",
            "Epoch 7204: train loss = 2126.410901, test loss = 3533.809907\n",
            "Epoch 7205: train loss = 2126.335588, test loss = 3533.837491\n",
            "Epoch 7206: train loss = 2126.260346, test loss = 3533.864890\n",
            "Epoch 7207: train loss = 2126.185173, test loss = 3533.892103\n",
            "Epoch 7208: train loss = 2126.110069, test loss = 3533.919132\n",
            "Epoch 7209: train loss = 2126.035035, test loss = 3533.945978\n",
            "Epoch 7210: train loss = 2125.960069, test loss = 3533.972641\n",
            "Epoch 7211: train loss = 2125.885171, test loss = 3533.999123\n",
            "Epoch 7212: train loss = 2125.810342, test loss = 3534.025424\n",
            "Epoch 7213: train loss = 2125.735581, test loss = 3534.051544\n",
            "Epoch 7214: train loss = 2125.660888, test loss = 3534.077486\n",
            "Epoch 7215: train loss = 2125.586261, test loss = 3534.103249\n",
            "Epoch 7216: train loss = 2125.511703, test loss = 3534.128834\n",
            "Epoch 7217: train loss = 2125.437211, test loss = 3534.154243\n",
            "Epoch 7218: train loss = 2125.362785, test loss = 3534.179476\n",
            "Epoch 7219: train loss = 2125.288427, test loss = 3534.204533\n",
            "Epoch 7220: train loss = 2125.214134, test loss = 3534.229416\n",
            "Epoch 7221: train loss = 2125.139908, test loss = 3534.254125\n",
            "Epoch 7222: train loss = 2125.065747, test loss = 3534.278662\n",
            "Epoch 7223: train loss = 2124.991652, test loss = 3534.303026\n",
            "Epoch 7224: train loss = 2124.917622, test loss = 3534.327219\n",
            "Epoch 7225: train loss = 2124.843657, test loss = 3534.351241\n",
            "Epoch 7226: train loss = 2124.769758, test loss = 3534.375093\n",
            "Epoch 7227: train loss = 2124.695923, test loss = 3534.398777\n",
            "Epoch 7228: train loss = 2124.622152, test loss = 3534.422292\n",
            "Epoch 7229: train loss = 2124.548446, test loss = 3534.445639\n",
            "Epoch 7230: train loss = 2124.474804, test loss = 3534.468820\n",
            "Epoch 7231: train loss = 2124.401225, test loss = 3534.491834\n",
            "Epoch 7232: train loss = 2124.327711, test loss = 3534.514683\n",
            "Epoch 7233: train loss = 2124.254260, test loss = 3534.537367\n",
            "Epoch 7234: train loss = 2124.180872, test loss = 3534.559887\n",
            "Epoch 7235: train loss = 2124.107547, test loss = 3534.582244\n",
            "Epoch 7236: train loss = 2124.034286, test loss = 3534.604438\n",
            "Epoch 7237: train loss = 2123.961087, test loss = 3534.626470\n",
            "Epoch 7238: train loss = 2123.887951, test loss = 3534.648341\n",
            "Epoch 7239: train loss = 2123.814877, test loss = 3534.670052\n",
            "Epoch 7240: train loss = 2123.741865, test loss = 3534.691602\n",
            "Epoch 7241: train loss = 2123.668916, test loss = 3534.712994\n",
            "Epoch 7242: train loss = 2123.596028, test loss = 3534.734227\n",
            "Epoch 7243: train loss = 2123.523202, test loss = 3534.755301\n",
            "Epoch 7244: train loss = 2123.450438, test loss = 3534.776219\n",
            "Epoch 7245: train loss = 2123.377735, test loss = 3534.796980\n",
            "Epoch 7246: train loss = 2123.305093, test loss = 3534.817585\n",
            "Epoch 7247: train loss = 2123.232513, test loss = 3534.838035\n",
            "Epoch 7248: train loss = 2123.159994, test loss = 3534.858331\n",
            "Epoch 7249: train loss = 2123.087535, test loss = 3534.878472\n",
            "Epoch 7250: train loss = 2123.015137, test loss = 3534.898460\n",
            "Epoch 7251: train loss = 2122.942800, test loss = 3534.918295\n",
            "Epoch 7252: train loss = 2122.870523, test loss = 3534.937978\n",
            "Epoch 7253: train loss = 2122.798306, test loss = 3534.957509\n",
            "Epoch 7254: train loss = 2122.726149, test loss = 3534.976890\n",
            "Epoch 7255: train loss = 2122.654053, test loss = 3534.996120\n",
            "Epoch 7256: train loss = 2122.582016, test loss = 3535.015200\n",
            "Epoch 7257: train loss = 2122.510039, test loss = 3535.034132\n",
            "Epoch 7258: train loss = 2122.438121, test loss = 3535.052914\n",
            "Epoch 7259: train loss = 2122.366263, test loss = 3535.071549\n",
            "Epoch 7260: train loss = 2122.294465, test loss = 3535.090036\n",
            "Epoch 7261: train loss = 2122.222725, test loss = 3535.108377\n",
            "Epoch 7262: train loss = 2122.151045, test loss = 3535.126571\n",
            "Epoch 7263: train loss = 2122.079424, test loss = 3535.144620\n",
            "Epoch 7264: train loss = 2122.007861, test loss = 3535.162524\n",
            "Epoch 7265: train loss = 2121.936357, test loss = 3535.180283\n",
            "Epoch 7266: train loss = 2121.864912, test loss = 3535.197898\n",
            "Epoch 7267: train loss = 2121.793526, test loss = 3535.215369\n",
            "Epoch 7268: train loss = 2121.722198, test loss = 3535.232698\n",
            "Epoch 7269: train loss = 2121.650928, test loss = 3535.249884\n",
            "Epoch 7270: train loss = 2121.579716, test loss = 3535.266929\n",
            "Epoch 7271: train loss = 2121.508563, test loss = 3535.283832\n",
            "Epoch 7272: train loss = 2121.437467, test loss = 3535.300595\n",
            "Epoch 7273: train loss = 2121.366430, test loss = 3535.317217\n",
            "Epoch 7274: train loss = 2121.295450, test loss = 3535.333699\n",
            "Epoch 7275: train loss = 2121.224528, test loss = 3535.350043\n",
            "Epoch 7276: train loss = 2121.153664, test loss = 3535.366248\n",
            "Epoch 7277: train loss = 2121.082857, test loss = 3535.382314\n",
            "Epoch 7278: train loss = 2121.012107, test loss = 3535.398243\n",
            "Epoch 7279: train loss = 2120.941415, test loss = 3535.414034\n",
            "Epoch 7280: train loss = 2120.870780, test loss = 3535.429689\n",
            "Epoch 7281: train loss = 2120.800202, test loss = 3535.445208\n",
            "Epoch 7282: train loss = 2120.729682, test loss = 3535.460591\n",
            "Epoch 7283: train loss = 2120.659218, test loss = 3535.475839\n",
            "Epoch 7284: train loss = 2120.588811, test loss = 3535.490953\n",
            "Epoch 7285: train loss = 2120.518461, test loss = 3535.505931\n",
            "Epoch 7286: train loss = 2120.448168, test loss = 3535.520777\n",
            "Epoch 7287: train loss = 2120.377931, test loss = 3535.535489\n",
            "Epoch 7288: train loss = 2120.307751, test loss = 3535.550068\n",
            "Epoch 7289: train loss = 2120.237628, test loss = 3535.564515\n",
            "Epoch 7290: train loss = 2120.167561, test loss = 3535.578829\n",
            "Epoch 7291: train loss = 2120.097550, test loss = 3535.593013\n",
            "Epoch 7292: train loss = 2120.027595, test loss = 3535.607065\n",
            "Epoch 7293: train loss = 2119.957697, test loss = 3535.620987\n",
            "Epoch 7294: train loss = 2119.887855, test loss = 3535.634779\n",
            "Epoch 7295: train loss = 2119.818068, test loss = 3535.648441\n",
            "Epoch 7296: train loss = 2119.748338, test loss = 3535.661975\n",
            "Epoch 7297: train loss = 2119.678664, test loss = 3535.675379\n",
            "Epoch 7298: train loss = 2119.609045, test loss = 3535.688655\n",
            "Epoch 7299: train loss = 2119.539483, test loss = 3535.701803\n",
            "Epoch 7300: train loss = 2119.469976, test loss = 3535.714824\n",
            "Epoch 7301: train loss = 2119.400524, test loss = 3535.727718\n",
            "Epoch 7302: train loss = 2119.331128, test loss = 3535.740485\n",
            "Epoch 7303: train loss = 2119.261788, test loss = 3535.753126\n",
            "Epoch 7304: train loss = 2119.192503, test loss = 3535.765641\n",
            "Epoch 7305: train loss = 2119.123274, test loss = 3535.778031\n",
            "Epoch 7306: train loss = 2119.054100, test loss = 3535.790296\n",
            "Epoch 7307: train loss = 2118.984981, test loss = 3535.802436\n",
            "Epoch 7308: train loss = 2118.915917, test loss = 3535.814453\n",
            "Epoch 7309: train loss = 2118.846908, test loss = 3535.826345\n",
            "Epoch 7310: train loss = 2118.777955, test loss = 3535.838115\n",
            "Epoch 7311: train loss = 2118.709056, test loss = 3535.849761\n",
            "Epoch 7312: train loss = 2118.640213, test loss = 3535.861285\n",
            "Epoch 7313: train loss = 2118.571424, test loss = 3535.872687\n",
            "Epoch 7314: train loss = 2118.502690, test loss = 3535.883968\n",
            "Epoch 7315: train loss = 2118.434011, test loss = 3535.895127\n",
            "Epoch 7316: train loss = 2118.365387, test loss = 3535.906165\n",
            "Epoch 7317: train loss = 2118.296817, test loss = 3535.917082\n",
            "Epoch 7318: train loss = 2118.228302, test loss = 3535.927880\n",
            "Epoch 7319: train loss = 2118.159842, test loss = 3535.938557\n",
            "Epoch 7320: train loss = 2118.091436, test loss = 3535.949115\n",
            "Epoch 7321: train loss = 2118.023085, test loss = 3535.959555\n",
            "Epoch 7322: train loss = 2117.954788, test loss = 3535.969875\n",
            "Epoch 7323: train loss = 2117.886545, test loss = 3535.980077\n",
            "Epoch 7324: train loss = 2117.818357, test loss = 3535.990162\n",
            "Epoch 7325: train loss = 2117.750223, test loss = 3536.000128\n",
            "Epoch 7326: train loss = 2117.682143, test loss = 3536.009978\n",
            "Epoch 7327: train loss = 2117.614118, test loss = 3536.019711\n",
            "Epoch 7328: train loss = 2117.546146, test loss = 3536.029327\n",
            "Epoch 7329: train loss = 2117.478229, test loss = 3536.038828\n",
            "Epoch 7330: train loss = 2117.410365, test loss = 3536.048212\n",
            "Epoch 7331: train loss = 2117.342556, test loss = 3536.057481\n",
            "Epoch 7332: train loss = 2117.274800, test loss = 3536.066636\n",
            "Epoch 7333: train loss = 2117.207099, test loss = 3536.075675\n",
            "Epoch 7334: train loss = 2117.139451, test loss = 3536.084600\n",
            "Epoch 7335: train loss = 2117.071857, test loss = 3536.093411\n",
            "Epoch 7336: train loss = 2117.004317, test loss = 3536.102109\n",
            "Epoch 7337: train loss = 2116.936830, test loss = 3536.110693\n",
            "Epoch 7338: train loss = 2116.869398, test loss = 3536.119164\n",
            "Epoch 7339: train loss = 2116.802018, test loss = 3536.127523\n",
            "Epoch 7340: train loss = 2116.734693, test loss = 3536.135769\n",
            "Epoch 7341: train loss = 2116.667421, test loss = 3536.143904\n",
            "Epoch 7342: train loss = 2116.600202, test loss = 3536.151927\n",
            "Epoch 7343: train loss = 2116.533037, test loss = 3536.159838\n",
            "Epoch 7344: train loss = 2116.465925, test loss = 3536.167639\n",
            "Epoch 7345: train loss = 2116.398867, test loss = 3536.175329\n",
            "Epoch 7346: train loss = 2116.331862, test loss = 3536.182909\n",
            "Epoch 7347: train loss = 2116.264910, test loss = 3536.190378\n",
            "Epoch 7348: train loss = 2116.198011, test loss = 3536.197738\n",
            "Epoch 7349: train loss = 2116.131166, test loss = 3536.204989\n",
            "Epoch 7350: train loss = 2116.064373, test loss = 3536.212131\n",
            "Epoch 7351: train loss = 2115.997634, test loss = 3536.219164\n",
            "Epoch 7352: train loss = 2115.930948, test loss = 3536.226089\n",
            "Epoch 7353: train loss = 2115.864315, test loss = 3536.232905\n",
            "Epoch 7354: train loss = 2115.797735, test loss = 3536.239614\n",
            "Epoch 7355: train loss = 2115.731208, test loss = 3536.246216\n",
            "Epoch 7356: train loss = 2115.664734, test loss = 3536.252710\n",
            "Epoch 7357: train loss = 2115.598312, test loss = 3536.259098\n",
            "Epoch 7358: train loss = 2115.531944, test loss = 3536.265379\n",
            "Epoch 7359: train loss = 2115.465628, test loss = 3536.271553\n",
            "Epoch 7360: train loss = 2115.399365, test loss = 3536.277622\n",
            "Epoch 7361: train loss = 2115.333155, test loss = 3536.283586\n",
            "Epoch 7362: train loss = 2115.266997, test loss = 3536.289444\n",
            "Epoch 7363: train loss = 2115.200892, test loss = 3536.295197\n",
            "Epoch 7364: train loss = 2115.134840, test loss = 3536.300845\n",
            "Epoch 7365: train loss = 2115.068840, test loss = 3536.306389\n",
            "Epoch 7366: train loss = 2115.002893, test loss = 3536.311829\n",
            "Epoch 7367: train loss = 2114.936998, test loss = 3536.317165\n",
            "Epoch 7368: train loss = 2114.871155, test loss = 3536.322398\n",
            "Epoch 7369: train loss = 2114.805365, test loss = 3536.327528\n",
            "Epoch 7370: train loss = 2114.739628, test loss = 3536.332554\n",
            "Epoch 7371: train loss = 2114.673942, test loss = 3536.337478\n",
            "Epoch 7372: train loss = 2114.608309, test loss = 3536.342299\n",
            "Epoch 7373: train loss = 2114.542728, test loss = 3536.347019\n",
            "Epoch 7374: train loss = 2114.477200, test loss = 3536.351637\n",
            "Epoch 7375: train loss = 2114.411723, test loss = 3536.356153\n",
            "Epoch 7376: train loss = 2114.346299, test loss = 3536.360568\n",
            "Epoch 7377: train loss = 2114.280926, test loss = 3536.364882\n",
            "Epoch 7378: train loss = 2114.215606, test loss = 3536.369096\n",
            "Epoch 7379: train loss = 2114.150338, test loss = 3536.373209\n",
            "Epoch 7380: train loss = 2114.085122, test loss = 3536.377222\n",
            "Epoch 7381: train loss = 2114.019957, test loss = 3536.381135\n",
            "Epoch 7382: train loss = 2113.954845, test loss = 3536.384949\n",
            "Epoch 7383: train loss = 2113.889784, test loss = 3536.388664\n",
            "Epoch 7384: train loss = 2113.824776, test loss = 3536.392279\n",
            "Epoch 7385: train loss = 2113.759819, test loss = 3536.395796\n",
            "Epoch 7386: train loss = 2113.694913, test loss = 3536.399215\n",
            "Epoch 7387: train loss = 2113.630060, test loss = 3536.402535\n",
            "Epoch 7388: train loss = 2113.565258, test loss = 3536.405758\n",
            "Epoch 7389: train loss = 2113.500508, test loss = 3536.408883\n",
            "Epoch 7390: train loss = 2113.435809, test loss = 3536.411910\n",
            "Epoch 7391: train loss = 2113.371162, test loss = 3536.414841\n",
            "Epoch 7392: train loss = 2113.306566, test loss = 3536.417675\n",
            "Epoch 7393: train loss = 2113.242022, test loss = 3536.420412\n",
            "Epoch 7394: train loss = 2113.177530, test loss = 3536.423053\n",
            "Epoch 7395: train loss = 2113.113088, test loss = 3536.425598\n",
            "Epoch 7396: train loss = 2113.048698, test loss = 3536.428047\n",
            "Epoch 7397: train loss = 2112.984360, test loss = 3536.430401\n",
            "Epoch 7398: train loss = 2112.920072, test loss = 3536.432659\n",
            "Epoch 7399: train loss = 2112.855836, test loss = 3536.434823\n",
            "Epoch 7400: train loss = 2112.791651, test loss = 3536.436891\n",
            "Epoch 7401: train loss = 2112.727518, test loss = 3536.438866\n",
            "Epoch 7402: train loss = 2112.663435, test loss = 3536.440746\n",
            "Epoch 7403: train loss = 2112.599403, test loss = 3536.442532\n",
            "Epoch 7404: train loss = 2112.535423, test loss = 3536.444225\n",
            "Epoch 7405: train loss = 2112.471493, test loss = 3536.445824\n",
            "Epoch 7406: train loss = 2112.407615, test loss = 3536.447330\n",
            "Epoch 7407: train loss = 2112.343787, test loss = 3536.448744\n",
            "Epoch 7408: train loss = 2112.280011, test loss = 3536.450064\n",
            "Epoch 7409: train loss = 2112.216285, test loss = 3536.451292\n",
            "Epoch 7410: train loss = 2112.152610, test loss = 3536.452428\n",
            "Epoch 7411: train loss = 2112.088985, test loss = 3536.453472\n",
            "Epoch 7412: train loss = 2112.025412, test loss = 3536.454425\n",
            "Epoch 7413: train loss = 2111.961889, test loss = 3536.455286\n",
            "Epoch 7414: train loss = 2111.898417, test loss = 3536.456056\n",
            "Epoch 7415: train loss = 2111.834995, test loss = 3536.456735\n",
            "Epoch 7416: train loss = 2111.771624, test loss = 3536.457323\n",
            "Epoch 7417: train loss = 2111.708304, test loss = 3536.457821\n",
            "Epoch 7418: train loss = 2111.645034, test loss = 3536.458229\n",
            "Epoch 7419: train loss = 2111.581814, test loss = 3536.458546\n",
            "Epoch 7420: train loss = 2111.518645, test loss = 3536.458775\n",
            "Epoch 7421: train loss = 2111.455526, test loss = 3536.458913\n",
            "Epoch 7422: train loss = 2111.392458, test loss = 3536.458963\n",
            "Epoch 7423: train loss = 2111.329440, test loss = 3536.458923\n",
            "Epoch 7424: train loss = 2111.266472, test loss = 3536.458795\n",
            "Epoch 7425: train loss = 2111.203554, test loss = 3536.458578\n",
            "Epoch 7426: train loss = 2111.140686, test loss = 3536.458274\n",
            "Epoch 7427: train loss = 2111.077869, test loss = 3536.457881\n",
            "Epoch 7428: train loss = 2111.015102, test loss = 3536.457400\n",
            "Epoch 7429: train loss = 2110.952384, test loss = 3536.456832\n",
            "Epoch 7430: train loss = 2110.889717, test loss = 3536.456176\n",
            "Epoch 7431: train loss = 2110.827100, test loss = 3536.455434\n",
            "Epoch 7432: train loss = 2110.764532, test loss = 3536.454604\n",
            "Epoch 7433: train loss = 2110.702015, test loss = 3536.453688\n",
            "Epoch 7434: train loss = 2110.639547, test loss = 3536.452686\n",
            "Epoch 7435: train loss = 2110.577129, test loss = 3536.451598\n",
            "Epoch 7436: train loss = 2110.514761, test loss = 3536.450423\n",
            "Epoch 7437: train loss = 2110.452442, test loss = 3536.449163\n",
            "Epoch 7438: train loss = 2110.390173, test loss = 3536.447818\n",
            "Epoch 7439: train loss = 2110.327954, test loss = 3536.446387\n",
            "Epoch 7440: train loss = 2110.265784, test loss = 3536.444872\n",
            "Epoch 7441: train loss = 2110.203664, test loss = 3536.443271\n",
            "Epoch 7442: train loss = 2110.141594, test loss = 3536.441586\n",
            "Epoch 7443: train loss = 2110.079573, test loss = 3536.439817\n",
            "Epoch 7444: train loss = 2110.017601, test loss = 3536.437964\n",
            "Epoch 7445: train loss = 2109.955678, test loss = 3536.436027\n",
            "Epoch 7446: train loss = 2109.893805, test loss = 3536.434006\n",
            "Epoch 7447: train loss = 2109.831982, test loss = 3536.431902\n",
            "Epoch 7448: train loss = 2109.770207, test loss = 3536.429715\n",
            "Epoch 7449: train loss = 2109.708482, test loss = 3536.427445\n",
            "Epoch 7450: train loss = 2109.646806, test loss = 3536.425092\n",
            "Epoch 7451: train loss = 2109.585178, test loss = 3536.422657\n",
            "Epoch 7452: train loss = 2109.523600, test loss = 3536.420139\n",
            "Epoch 7453: train loss = 2109.462071, test loss = 3536.417540\n",
            "Epoch 7454: train loss = 2109.400591, test loss = 3536.414858\n",
            "Epoch 7455: train loss = 2109.339160, test loss = 3536.412095\n",
            "Epoch 7456: train loss = 2109.277778, test loss = 3536.409250\n",
            "Epoch 7457: train loss = 2109.216444, test loss = 3536.406325\n",
            "Epoch 7458: train loss = 2109.155160, test loss = 3536.403318\n",
            "Epoch 7459: train loss = 2109.093924, test loss = 3536.400231\n",
            "Epoch 7460: train loss = 2109.032737, test loss = 3536.397063\n",
            "Epoch 7461: train loss = 2108.971598, test loss = 3536.393815\n",
            "Epoch 7462: train loss = 2108.910508, test loss = 3536.390487\n",
            "Epoch 7463: train loss = 2108.849467, test loss = 3536.387078\n",
            "Epoch 7464: train loss = 2108.788474, test loss = 3536.383591\n",
            "Epoch 7465: train loss = 2108.727530, test loss = 3536.380023\n",
            "Epoch 7466: train loss = 2108.666634, test loss = 3536.376377\n",
            "Epoch 7467: train loss = 2108.605786, test loss = 3536.372652\n",
            "Epoch 7468: train loss = 2108.544987, test loss = 3536.368847\n",
            "Epoch 7469: train loss = 2108.484236, test loss = 3536.364964\n",
            "Epoch 7470: train loss = 2108.423534, test loss = 3536.361003\n",
            "Epoch 7471: train loss = 2108.362879, test loss = 3536.356964\n",
            "Epoch 7472: train loss = 2108.302273, test loss = 3536.352846\n",
            "Epoch 7473: train loss = 2108.241715, test loss = 3536.348651\n",
            "Epoch 7474: train loss = 2108.181205, test loss = 3536.344379\n",
            "Epoch 7475: train loss = 2108.120742, test loss = 3536.340029\n",
            "Epoch 7476: train loss = 2108.060328, test loss = 3536.335602\n",
            "Epoch 7477: train loss = 2107.999962, test loss = 3536.331098\n",
            "Epoch 7478: train loss = 2107.939644, test loss = 3536.326517\n",
            "Epoch 7479: train loss = 2107.879373, test loss = 3536.321860\n",
            "Epoch 7480: train loss = 2107.819151, test loss = 3536.317126\n",
            "Epoch 7481: train loss = 2107.758976, test loss = 3536.312317\n",
            "Epoch 7482: train loss = 2107.698848, test loss = 3536.307431\n",
            "Epoch 7483: train loss = 2107.638769, test loss = 3536.302470\n",
            "Epoch 7484: train loss = 2107.578737, test loss = 3536.297434\n",
            "Epoch 7485: train loss = 2107.518752, test loss = 3536.292322\n",
            "Epoch 7486: train loss = 2107.458815, test loss = 3536.287135\n",
            "Epoch 7487: train loss = 2107.398925, test loss = 3536.281873\n",
            "Epoch 7488: train loss = 2107.339083, test loss = 3536.276537\n",
            "Epoch 7489: train loss = 2107.279288, test loss = 3536.271126\n",
            "Epoch 7490: train loss = 2107.219541, test loss = 3536.265641\n",
            "Epoch 7491: train loss = 2107.159841, test loss = 3536.260081\n",
            "Epoch 7492: train loss = 2107.100188, test loss = 3536.254448\n",
            "Epoch 7493: train loss = 2107.040582, test loss = 3536.248742\n",
            "Epoch 7494: train loss = 2106.981023, test loss = 3536.242962\n",
            "Epoch 7495: train loss = 2106.921511, test loss = 3536.237108\n",
            "Epoch 7496: train loss = 2106.862046, test loss = 3536.231182\n",
            "Epoch 7497: train loss = 2106.802629, test loss = 3536.225182\n",
            "Epoch 7498: train loss = 2106.743258, test loss = 3536.219110\n",
            "Epoch 7499: train loss = 2106.683934, test loss = 3536.212966\n",
            "Epoch 7500: train loss = 2106.624657, test loss = 3536.206749\n",
            "Epoch 7501: train loss = 2106.565426, test loss = 3536.200460\n",
            "Epoch 7502: train loss = 2106.506243, test loss = 3536.194100\n",
            "Epoch 7503: train loss = 2106.447106, test loss = 3536.187667\n",
            "Epoch 7504: train loss = 2106.388015, test loss = 3536.181164\n",
            "Epoch 7505: train loss = 2106.328972, test loss = 3536.174588\n",
            "Epoch 7506: train loss = 2106.269974, test loss = 3536.167942\n",
            "Epoch 7507: train loss = 2106.211024, test loss = 3536.161225\n",
            "Epoch 7508: train loss = 2106.152119, test loss = 3536.154437\n",
            "Epoch 7509: train loss = 2106.093261, test loss = 3536.147579\n",
            "Epoch 7510: train loss = 2106.034449, test loss = 3536.140650\n",
            "Epoch 7511: train loss = 2105.975684, test loss = 3536.133651\n",
            "Epoch 7512: train loss = 2105.916965, test loss = 3536.126582\n",
            "Epoch 7513: train loss = 2105.858292, test loss = 3536.119444\n",
            "Epoch 7514: train loss = 2105.799665, test loss = 3536.112236\n",
            "Epoch 7515: train loss = 2105.741084, test loss = 3536.104958\n",
            "Epoch 7516: train loss = 2105.682549, test loss = 3536.097612\n",
            "Epoch 7517: train loss = 2105.624060, test loss = 3536.090196\n",
            "Epoch 7518: train loss = 2105.565617, test loss = 3536.082712\n",
            "Epoch 7519: train loss = 2105.507220, test loss = 3536.075158\n",
            "Epoch 7520: train loss = 2105.448869, test loss = 3536.067537\n",
            "Epoch 7521: train loss = 2105.390564, test loss = 3536.059847\n",
            "Epoch 7522: train loss = 2105.332304, test loss = 3536.052089\n",
            "Epoch 7523: train loss = 2105.274090, test loss = 3536.044264\n",
            "Epoch 7524: train loss = 2105.215921, test loss = 3536.036370\n",
            "Epoch 7525: train loss = 2105.157798, test loss = 3536.028409\n",
            "Epoch 7526: train loss = 2105.099721, test loss = 3536.020381\n",
            "Epoch 7527: train loss = 2105.041689, test loss = 3536.012286\n",
            "Epoch 7528: train loss = 2104.983702, test loss = 3536.004123\n",
            "Epoch 7529: train loss = 2104.925761, test loss = 3535.995894\n",
            "Epoch 7530: train loss = 2104.867865, test loss = 3535.987598\n",
            "Epoch 7531: train loss = 2104.810015, test loss = 3535.979236\n",
            "Epoch 7532: train loss = 2104.752209, test loss = 3535.970808\n",
            "Epoch 7533: train loss = 2104.694449, test loss = 3535.962314\n",
            "Epoch 7534: train loss = 2104.636734, test loss = 3535.953753\n",
            "Epoch 7535: train loss = 2104.579063, test loss = 3535.945128\n",
            "Epoch 7536: train loss = 2104.521438, test loss = 3535.936436\n",
            "Epoch 7537: train loss = 2104.463858, test loss = 3535.927679\n",
            "Epoch 7538: train loss = 2104.406323, test loss = 3535.918858\n",
            "Epoch 7539: train loss = 2104.348832, test loss = 3535.909971\n",
            "Epoch 7540: train loss = 2104.291386, test loss = 3535.901019\n",
            "Epoch 7541: train loss = 2104.233985, test loss = 3535.892003\n",
            "Epoch 7542: train loss = 2104.176629, test loss = 3535.882922\n",
            "Epoch 7543: train loss = 2104.119317, test loss = 3535.873777\n",
            "Epoch 7544: train loss = 2104.062050, test loss = 3535.864568\n",
            "Epoch 7545: train loss = 2104.004828, test loss = 3535.855296\n",
            "Epoch 7546: train loss = 2103.947649, test loss = 3535.845959\n",
            "Epoch 7547: train loss = 2103.890516, test loss = 3535.836559\n",
            "Epoch 7548: train loss = 2103.833426, test loss = 3535.827095\n",
            "Epoch 7549: train loss = 2103.776381, test loss = 3535.817569\n",
            "Epoch 7550: train loss = 2103.719381, test loss = 3535.807979\n",
            "Epoch 7551: train loss = 2103.662424, test loss = 3535.798326\n",
            "Epoch 7552: train loss = 2103.605511, test loss = 3535.788611\n",
            "Epoch 7553: train loss = 2103.548643, test loss = 3535.778833\n",
            "Epoch 7554: train loss = 2103.491819, test loss = 3535.768993\n",
            "Epoch 7555: train loss = 2103.435038, test loss = 3535.759091\n",
            "Epoch 7556: train loss = 2103.378302, test loss = 3535.749127\n",
            "Epoch 7557: train loss = 2103.321610, test loss = 3535.739101\n",
            "Epoch 7558: train loss = 2103.264961, test loss = 3535.729013\n",
            "Epoch 7559: train loss = 2103.208356, test loss = 3535.718864\n",
            "Epoch 7560: train loss = 2103.151795, test loss = 3535.708654\n",
            "Epoch 7561: train loss = 2103.095277, test loss = 3535.698382\n",
            "Epoch 7562: train loss = 2103.038803, test loss = 3535.688050\n",
            "Epoch 7563: train loss = 2102.982373, test loss = 3535.677657\n",
            "Epoch 7564: train loss = 2102.925986, test loss = 3535.667203\n",
            "Epoch 7565: train loss = 2102.869643, test loss = 3535.656688\n",
            "Epoch 7566: train loss = 2102.813343, test loss = 3535.646114\n",
            "Epoch 7567: train loss = 2102.757086, test loss = 3535.635479\n",
            "Epoch 7568: train loss = 2102.700873, test loss = 3535.624784\n",
            "Epoch 7569: train loss = 2102.644703, test loss = 3535.614030\n",
            "Epoch 7570: train loss = 2102.588576, test loss = 3535.603215\n",
            "Epoch 7571: train loss = 2102.532492, test loss = 3535.592342\n",
            "Epoch 7572: train loss = 2102.476452, test loss = 3535.581409\n",
            "Epoch 7573: train loss = 2102.420454, test loss = 3535.570417\n",
            "Epoch 7574: train loss = 2102.364499, test loss = 3535.559366\n",
            "Epoch 7575: train loss = 2102.308588, test loss = 3535.548256\n",
            "Epoch 7576: train loss = 2102.252719, test loss = 3535.537088\n",
            "Epoch 7577: train loss = 2102.196893, test loss = 3535.525861\n",
            "Epoch 7578: train loss = 2102.141109, test loss = 3535.514576\n",
            "Epoch 7579: train loss = 2102.085369, test loss = 3535.503233\n",
            "Epoch 7580: train loss = 2102.029671, test loss = 3535.491831\n",
            "Epoch 7581: train loss = 2101.974015, test loss = 3535.480372\n",
            "Epoch 7582: train loss = 2101.918403, test loss = 3535.468855\n",
            "Epoch 7583: train loss = 2101.862832, test loss = 3535.457281\n",
            "Epoch 7584: train loss = 2101.807304, test loss = 3535.445650\n",
            "Epoch 7585: train loss = 2101.751819, test loss = 3535.433961\n",
            "Epoch 7586: train loss = 2101.696376, test loss = 3535.422215\n",
            "Epoch 7587: train loss = 2101.640975, test loss = 3535.410412\n",
            "Epoch 7588: train loss = 2101.585616, test loss = 3535.398553\n",
            "Epoch 7589: train loss = 2101.530299, test loss = 3535.386637\n",
            "Epoch 7590: train loss = 2101.475025, test loss = 3535.374665\n",
            "Epoch 7591: train loss = 2101.419792, test loss = 3535.362637\n",
            "Epoch 7592: train loss = 2101.364602, test loss = 3535.350552\n",
            "Epoch 7593: train loss = 2101.309453, test loss = 3535.338412\n",
            "Epoch 7594: train loss = 2101.254347, test loss = 3535.326215\n",
            "Epoch 7595: train loss = 2101.199282, test loss = 3535.313964\n",
            "Epoch 7596: train loss = 2101.144259, test loss = 3535.301656\n",
            "Epoch 7597: train loss = 2101.089278, test loss = 3535.289294\n",
            "Epoch 7598: train loss = 2101.034338, test loss = 3535.276876\n",
            "Epoch 7599: train loss = 2100.979440, test loss = 3535.264404\n",
            "Epoch 7600: train loss = 2100.924584, test loss = 3535.251876\n",
            "Epoch 7601: train loss = 2100.869769, test loss = 3535.239294\n",
            "Epoch 7602: train loss = 2100.814995, test loss = 3535.226658\n",
            "Epoch 7603: train loss = 2100.760263, test loss = 3535.213967\n",
            "Epoch 7604: train loss = 2100.705572, test loss = 3535.201221\n",
            "Epoch 7605: train loss = 2100.650923, test loss = 3535.188422\n",
            "Epoch 7606: train loss = 2100.596315, test loss = 3535.175569\n",
            "Epoch 7607: train loss = 2100.541747, test loss = 3535.162662\n",
            "Epoch 7608: train loss = 2100.487222, test loss = 3535.149702\n",
            "Epoch 7609: train loss = 2100.432737, test loss = 3535.136688\n",
            "Epoch 7610: train loss = 2100.378293, test loss = 3535.123621\n",
            "Epoch 7611: train loss = 2100.323890, test loss = 3535.110500\n",
            "Epoch 7612: train loss = 2100.269528, test loss = 3535.097327\n",
            "Epoch 7613: train loss = 2100.215207, test loss = 3535.084101\n",
            "Epoch 7614: train loss = 2100.160926, test loss = 3535.070822\n",
            "Epoch 7615: train loss = 2100.106687, test loss = 3535.057491\n",
            "Epoch 7616: train loss = 2100.052488, test loss = 3535.044107\n",
            "Epoch 7617: train loss = 2099.998329, test loss = 3535.030671\n",
            "Epoch 7618: train loss = 2099.944212, test loss = 3535.017183\n",
            "Epoch 7619: train loss = 2099.890134, test loss = 3535.003643\n",
            "Epoch 7620: train loss = 2099.836098, test loss = 3534.990051\n",
            "Epoch 7621: train loss = 2099.782101, test loss = 3534.976407\n",
            "Epoch 7622: train loss = 2099.728145, test loss = 3534.962712\n",
            "Epoch 7623: train loss = 2099.674230, test loss = 3534.948966\n",
            "Epoch 7624: train loss = 2099.620354, test loss = 3534.935168\n",
            "Epoch 7625: train loss = 2099.566519, test loss = 3534.921319\n",
            "Epoch 7626: train loss = 2099.512724, test loss = 3534.907419\n",
            "Epoch 7627: train loss = 2099.458969, test loss = 3534.893469\n",
            "Epoch 7628: train loss = 2099.405254, test loss = 3534.879468\n",
            "Epoch 7629: train loss = 2099.351579, test loss = 3534.865416\n",
            "Epoch 7630: train loss = 2099.297943, test loss = 3534.851314\n",
            "Epoch 7631: train loss = 2099.244348, test loss = 3534.837162\n",
            "Epoch 7632: train loss = 2099.190793, test loss = 3534.822960\n",
            "Epoch 7633: train loss = 2099.137277, test loss = 3534.808707\n",
            "Epoch 7634: train loss = 2099.083801, test loss = 3534.794405\n",
            "Epoch 7635: train loss = 2099.030364, test loss = 3534.780054\n",
            "Epoch 7636: train loss = 2098.976968, test loss = 3534.765653\n",
            "Epoch 7637: train loss = 2098.923610, test loss = 3534.751202\n",
            "Epoch 7638: train loss = 2098.870293, test loss = 3534.736702\n",
            "Epoch 7639: train loss = 2098.817014, test loss = 3534.722154\n",
            "Epoch 7640: train loss = 2098.763775, test loss = 3534.707556\n",
            "Epoch 7641: train loss = 2098.710576, test loss = 3534.692909\n",
            "Epoch 7642: train loss = 2098.657415, test loss = 3534.678214\n",
            "Epoch 7643: train loss = 2098.604294, test loss = 3534.663470\n",
            "Epoch 7644: train loss = 2098.551212, test loss = 3534.648678\n",
            "Epoch 7645: train loss = 2098.498169, test loss = 3534.633838\n",
            "Epoch 7646: train loss = 2098.445165, test loss = 3534.618949\n",
            "Epoch 7647: train loss = 2098.392200, test loss = 3534.604013\n",
            "Epoch 7648: train loss = 2098.339274, test loss = 3534.589029\n",
            "Epoch 7649: train loss = 2098.286387, test loss = 3534.573997\n",
            "Epoch 7650: train loss = 2098.233538, test loss = 3534.558917\n",
            "Epoch 7651: train loss = 2098.180729, test loss = 3534.543790\n",
            "Epoch 7652: train loss = 2098.127958, test loss = 3534.528616\n",
            "Epoch 7653: train loss = 2098.075226, test loss = 3534.513394\n",
            "Epoch 7654: train loss = 2098.022532, test loss = 3534.498126\n",
            "Epoch 7655: train loss = 2097.969877, test loss = 3534.482810\n",
            "Epoch 7656: train loss = 2097.917261, test loss = 3534.467448\n",
            "Epoch 7657: train loss = 2097.864682, test loss = 3534.452040\n",
            "Epoch 7658: train loss = 2097.812143, test loss = 3534.436584\n",
            "Epoch 7659: train loss = 2097.759641, test loss = 3534.421083\n",
            "Epoch 7660: train loss = 2097.707178, test loss = 3534.405535\n",
            "Epoch 7661: train loss = 2097.654753, test loss = 3534.389941\n",
            "Epoch 7662: train loss = 2097.602367, test loss = 3534.374301\n",
            "Epoch 7663: train loss = 2097.550018, test loss = 3534.358616\n",
            "Epoch 7664: train loss = 2097.497707, test loss = 3534.342884\n",
            "Epoch 7665: train loss = 2097.445435, test loss = 3534.327107\n",
            "Epoch 7666: train loss = 2097.393200, test loss = 3534.311285\n",
            "Epoch 7667: train loss = 2097.341004, test loss = 3534.295417\n",
            "Epoch 7668: train loss = 2097.288845, test loss = 3534.279504\n",
            "Epoch 7669: train loss = 2097.236724, test loss = 3534.263547\n",
            "Epoch 7670: train loss = 2097.184640, test loss = 3534.247544\n",
            "Epoch 7671: train loss = 2097.132595, test loss = 3534.231496\n",
            "Epoch 7672: train loss = 2097.080587, test loss = 3534.215404\n",
            "Epoch 7673: train loss = 2097.028616, test loss = 3534.199267\n",
            "Epoch 7674: train loss = 2096.976683, test loss = 3534.183086\n",
            "Epoch 7675: train loss = 2096.924788, test loss = 3534.166861\n",
            "Epoch 7676: train loss = 2096.872930, test loss = 3534.150591\n",
            "Epoch 7677: train loss = 2096.821109, test loss = 3534.134278\n",
            "Epoch 7678: train loss = 2096.769326, test loss = 3534.117920\n",
            "Epoch 7679: train loss = 2096.717579, test loss = 3534.101519\n",
            "Epoch 7680: train loss = 2096.665870, test loss = 3534.085074\n",
            "Epoch 7681: train loss = 2096.614199, test loss = 3534.068586\n",
            "Epoch 7682: train loss = 2096.562564, test loss = 3534.052054\n",
            "Epoch 7683: train loss = 2096.510966, test loss = 3534.035479\n",
            "Epoch 7684: train loss = 2096.459405, test loss = 3534.018861\n",
            "Epoch 7685: train loss = 2096.407882, test loss = 3534.002200\n",
            "Epoch 7686: train loss = 2096.356395, test loss = 3533.985496\n",
            "Epoch 7687: train loss = 2096.304944, test loss = 3533.968749\n",
            "Epoch 7688: train loss = 2096.253531, test loss = 3533.951960\n",
            "Epoch 7689: train loss = 2096.202154, test loss = 3533.935128\n",
            "Epoch 7690: train loss = 2096.150814, test loss = 3533.918254\n",
            "Epoch 7691: train loss = 2096.099511, test loss = 3533.901337\n",
            "Epoch 7692: train loss = 2096.048244, test loss = 3533.884379\n",
            "Epoch 7693: train loss = 2095.997014, test loss = 3533.867378\n",
            "Epoch 7694: train loss = 2095.945820, test loss = 3533.850335\n",
            "Epoch 7695: train loss = 2095.894662, test loss = 3533.833251\n",
            "Epoch 7696: train loss = 2095.843541, test loss = 3533.816125\n",
            "Epoch 7697: train loss = 2095.792456, test loss = 3533.798957\n",
            "Epoch 7698: train loss = 2095.741408, test loss = 3533.781748\n",
            "Epoch 7699: train loss = 2095.690395, test loss = 3533.764498\n",
            "Epoch 7700: train loss = 2095.639419, test loss = 3533.747206\n",
            "Epoch 7701: train loss = 2095.588478, test loss = 3533.729874\n",
            "Epoch 7702: train loss = 2095.537574, test loss = 3533.712500\n",
            "Epoch 7703: train loss = 2095.486706, test loss = 3533.695086\n",
            "Epoch 7704: train loss = 2095.435873, test loss = 3533.677631\n",
            "Epoch 7705: train loss = 2095.385077, test loss = 3533.660136\n",
            "Epoch 7706: train loss = 2095.334316, test loss = 3533.642599\n",
            "Epoch 7707: train loss = 2095.283591, test loss = 3533.625023\n",
            "Epoch 7708: train loss = 2095.232901, test loss = 3533.607406\n",
            "Epoch 7709: train loss = 2095.182248, test loss = 3533.589750\n",
            "Epoch 7710: train loss = 2095.131630, test loss = 3533.572053\n",
            "Epoch 7711: train loss = 2095.081047, test loss = 3533.554316\n",
            "Epoch 7712: train loss = 2095.030500, test loss = 3533.536540\n",
            "Epoch 7713: train loss = 2094.979988, test loss = 3533.518724\n",
            "Epoch 7714: train loss = 2094.929512, test loss = 3533.500868\n",
            "Epoch 7715: train loss = 2094.879071, test loss = 3533.482974\n",
            "Epoch 7716: train loss = 2094.828665, test loss = 3533.465039\n",
            "Epoch 7717: train loss = 2094.778295, test loss = 3533.447066\n",
            "Epoch 7718: train loss = 2094.727960, test loss = 3533.429053\n",
            "Epoch 7719: train loss = 2094.677659, test loss = 3533.411002\n",
            "Epoch 7720: train loss = 2094.627394, test loss = 3533.392911\n",
            "Epoch 7721: train loss = 2094.577164, test loss = 3533.374782\n",
            "Epoch 7722: train loss = 2094.526969, test loss = 3533.356615\n",
            "Epoch 7723: train loss = 2094.476809, test loss = 3533.338409\n",
            "Epoch 7724: train loss = 2094.426683, test loss = 3533.320164\n",
            "Epoch 7725: train loss = 2094.376593, test loss = 3533.301881\n",
            "Epoch 7726: train loss = 2094.326537, test loss = 3533.283560\n",
            "Epoch 7727: train loss = 2094.276516, test loss = 3533.265201\n",
            "Epoch 7728: train loss = 2094.226529, test loss = 3533.246804\n",
            "Epoch 7729: train loss = 2094.176578, test loss = 3533.228369\n",
            "Epoch 7730: train loss = 2094.126660, test loss = 3533.209897\n",
            "Epoch 7731: train loss = 2094.076777, test loss = 3533.191386\n",
            "Epoch 7732: train loss = 2094.026929, test loss = 3533.172839\n",
            "Epoch 7733: train loss = 2093.977115, test loss = 3533.154253\n",
            "Epoch 7734: train loss = 2093.927335, test loss = 3533.135631\n",
            "Epoch 7735: train loss = 2093.877590, test loss = 3533.116972\n",
            "Epoch 7736: train loss = 2093.827878, test loss = 3533.098275\n",
            "Epoch 7737: train loss = 2093.778201, test loss = 3533.079541\n",
            "Epoch 7738: train loss = 2093.728559, test loss = 3533.060771\n",
            "Epoch 7739: train loss = 2093.678950, test loss = 3533.041964\n",
            "Epoch 7740: train loss = 2093.629375, test loss = 3533.023120\n",
            "Epoch 7741: train loss = 2093.579834, test loss = 3533.004240\n",
            "Epoch 7742: train loss = 2093.530327, test loss = 3532.985323\n",
            "Epoch 7743: train loss = 2093.480854, test loss = 3532.966370\n",
            "Epoch 7744: train loss = 2093.431415, test loss = 3532.947380\n",
            "Epoch 7745: train loss = 2093.382010, test loss = 3532.928355\n",
            "Epoch 7746: train loss = 2093.332638, test loss = 3532.909294\n",
            "Epoch 7747: train loss = 2093.283300, test loss = 3532.890196\n",
            "Epoch 7748: train loss = 2093.233995, test loss = 3532.871063\n",
            "Epoch 7749: train loss = 2093.184724, test loss = 3532.851895\n",
            "Epoch 7750: train loss = 2093.135487, test loss = 3532.832690\n",
            "Epoch 7751: train loss = 2093.086283, test loss = 3532.813451\n",
            "Epoch 7752: train loss = 2093.037112, test loss = 3532.794175\n",
            "Epoch 7753: train loss = 2092.987975, test loss = 3532.774865\n",
            "Epoch 7754: train loss = 2092.938871, test loss = 3532.755520\n",
            "Epoch 7755: train loss = 2092.889801, test loss = 3532.736139\n",
            "Epoch 7756: train loss = 2092.840763, test loss = 3532.716723\n",
            "Epoch 7757: train loss = 2092.791759, test loss = 3532.697273\n",
            "Epoch 7758: train loss = 2092.742788, test loss = 3532.677788\n",
            "Epoch 7759: train loss = 2092.693850, test loss = 3532.658268\n",
            "Epoch 7760: train loss = 2092.644945, test loss = 3532.638714\n",
            "Epoch 7761: train loss = 2092.596073, test loss = 3532.619125\n",
            "Epoch 7762: train loss = 2092.547233, test loss = 3532.599502\n",
            "Epoch 7763: train loss = 2092.498427, test loss = 3532.579845\n",
            "Epoch 7764: train loss = 2092.449653, test loss = 3532.560153\n",
            "Epoch 7765: train loss = 2092.400913, test loss = 3532.540428\n",
            "Epoch 7766: train loss = 2092.352204, test loss = 3532.520669\n",
            "Epoch 7767: train loss = 2092.303529, test loss = 3532.500875\n",
            "Epoch 7768: train loss = 2092.254886, test loss = 3532.481049\n",
            "Epoch 7769: train loss = 2092.206276, test loss = 3532.461188\n",
            "Epoch 7770: train loss = 2092.157698, test loss = 3532.441294\n",
            "Epoch 7771: train loss = 2092.109152, test loss = 3532.421366\n",
            "Epoch 7772: train loss = 2092.060639, test loss = 3532.401406\n",
            "Epoch 7773: train loss = 2092.012159, test loss = 3532.381411\n",
            "Epoch 7774: train loss = 2091.963710, test loss = 3532.361384\n",
            "Epoch 7775: train loss = 2091.915294, test loss = 3532.341324\n",
            "Epoch 7776: train loss = 2091.866910, test loss = 3532.321231\n",
            "Epoch 7777: train loss = 2091.818559, test loss = 3532.301105\n",
            "Epoch 7778: train loss = 2091.770239, test loss = 3532.280946\n",
            "Epoch 7779: train loss = 2091.721951, test loss = 3532.260755\n",
            "Epoch 7780: train loss = 2091.673696, test loss = 3532.240531\n",
            "Epoch 7781: train loss = 2091.625472, test loss = 3532.220274\n",
            "Epoch 7782: train loss = 2091.577280, test loss = 3532.199985\n",
            "Epoch 7783: train loss = 2091.529120, test loss = 3532.179664\n",
            "Epoch 7784: train loss = 2091.480992, test loss = 3532.159311\n",
            "Epoch 7785: train loss = 2091.432896, test loss = 3532.138926\n",
            "Epoch 7786: train loss = 2091.384831, test loss = 3532.118509\n",
            "Epoch 7787: train loss = 2091.336798, test loss = 3532.098059\n",
            "Epoch 7788: train loss = 2091.288797, test loss = 3532.077579\n",
            "Epoch 7789: train loss = 2091.240827, test loss = 3532.057066\n",
            "Epoch 7790: train loss = 2091.192889, test loss = 3532.036522\n",
            "Epoch 7791: train loss = 2091.144982, test loss = 3532.015946\n",
            "Epoch 7792: train loss = 2091.097106, test loss = 3531.995339\n",
            "Epoch 7793: train loss = 2091.049262, test loss = 3531.974701\n",
            "Epoch 7794: train loss = 2091.001450, test loss = 3531.954031\n",
            "Epoch 7795: train loss = 2090.953668, test loss = 3531.933330\n",
            "Epoch 7796: train loss = 2090.905918, test loss = 3531.912598\n",
            "Epoch 7797: train loss = 2090.858199, test loss = 3531.891836\n",
            "Epoch 7798: train loss = 2090.810511, test loss = 3531.871042\n",
            "Epoch 7799: train loss = 2090.762854, test loss = 3531.850218\n",
            "Epoch 7800: train loss = 2090.715228, test loss = 3531.829363\n",
            "Epoch 7801: train loss = 2090.667633, test loss = 3531.808477\n",
            "Epoch 7802: train loss = 2090.620069, test loss = 3531.787561\n",
            "Epoch 7803: train loss = 2090.572536, test loss = 3531.766615\n",
            "Epoch 7804: train loss = 2090.525034, test loss = 3531.745638\n",
            "Epoch 7805: train loss = 2090.477562, test loss = 3531.724631\n",
            "Epoch 7806: train loss = 2090.430121, test loss = 3531.703594\n",
            "Epoch 7807: train loss = 2090.382711, test loss = 3531.682527\n",
            "Epoch 7808: train loss = 2090.335332, test loss = 3531.661430\n",
            "Epoch 7809: train loss = 2090.287983, test loss = 3531.640303\n",
            "Epoch 7810: train loss = 2090.240665, test loss = 3531.619146\n",
            "Epoch 7811: train loss = 2090.193377, test loss = 3531.597960\n",
            "Epoch 7812: train loss = 2090.146119, test loss = 3531.576744\n",
            "Epoch 7813: train loss = 2090.098892, test loss = 3531.555498\n",
            "Epoch 7814: train loss = 2090.051696, test loss = 3531.534224\n",
            "Epoch 7815: train loss = 2090.004529, test loss = 3531.512919\n",
            "Epoch 7816: train loss = 2089.957393, test loss = 3531.491586\n",
            "Epoch 7817: train loss = 2089.910287, test loss = 3531.470223\n",
            "Epoch 7818: train loss = 2089.863212, test loss = 3531.448832\n",
            "Epoch 7819: train loss = 2089.816166, test loss = 3531.427411\n",
            "Epoch 7820: train loss = 2089.769151, test loss = 3531.405962\n",
            "Epoch 7821: train loss = 2089.722165, test loss = 3531.384484\n",
            "Epoch 7822: train loss = 2089.675210, test loss = 3531.362977\n",
            "Epoch 7823: train loss = 2089.628284, test loss = 3531.341441\n",
            "Epoch 7824: train loss = 2089.581388, test loss = 3531.319877\n",
            "Epoch 7825: train loss = 2089.534522, test loss = 3531.298285\n",
            "Epoch 7826: train loss = 2089.487686, test loss = 3531.276664\n",
            "Epoch 7827: train loss = 2089.440880, test loss = 3531.255015\n",
            "Epoch 7828: train loss = 2089.394103, test loss = 3531.233337\n",
            "Epoch 7829: train loss = 2089.347356, test loss = 3531.211632\n",
            "Epoch 7830: train loss = 2089.300639, test loss = 3531.189898\n",
            "Epoch 7831: train loss = 2089.253951, test loss = 3531.168137\n",
            "Epoch 7832: train loss = 2089.207292, test loss = 3531.146347\n",
            "Epoch 7833: train loss = 2089.160663, test loss = 3531.124530\n",
            "Epoch 7834: train loss = 2089.114064, test loss = 3531.102686\n",
            "Epoch 7835: train loss = 2089.067494, test loss = 3531.080813\n",
            "Epoch 7836: train loss = 2089.020953, test loss = 3531.058913\n",
            "Epoch 7837: train loss = 2088.974441, test loss = 3531.036986\n",
            "Epoch 7838: train loss = 2088.927959, test loss = 3531.015031\n",
            "Epoch 7839: train loss = 2088.881506, test loss = 3530.993049\n",
            "Epoch 7840: train loss = 2088.835082, test loss = 3530.971040\n",
            "Epoch 7841: train loss = 2088.788687, test loss = 3530.949004\n",
            "Epoch 7842: train loss = 2088.742321, test loss = 3530.926941\n",
            "Epoch 7843: train loss = 2088.695984, test loss = 3530.904851\n",
            "Epoch 7844: train loss = 2088.649676, test loss = 3530.882733\n",
            "Epoch 7845: train loss = 2088.603397, test loss = 3530.860590\n",
            "Epoch 7846: train loss = 2088.557147, test loss = 3530.838419\n",
            "Epoch 7847: train loss = 2088.510926, test loss = 3530.816222\n",
            "Epoch 7848: train loss = 2088.464733, test loss = 3530.793998\n",
            "Epoch 7849: train loss = 2088.418569, test loss = 3530.771748\n",
            "Epoch 7850: train loss = 2088.372434, test loss = 3530.749471\n",
            "Epoch 7851: train loss = 2088.326328, test loss = 3530.727168\n",
            "Epoch 7852: train loss = 2088.280250, test loss = 3530.704839\n",
            "Epoch 7853: train loss = 2088.234200, test loss = 3530.682484\n",
            "Epoch 7854: train loss = 2088.188179, test loss = 3530.660103\n",
            "Epoch 7855: train loss = 2088.142187, test loss = 3530.637696\n",
            "Epoch 7856: train loss = 2088.096223, test loss = 3530.615262\n",
            "Epoch 7857: train loss = 2088.050287, test loss = 3530.592803\n",
            "Epoch 7858: train loss = 2088.004380, test loss = 3530.570319\n",
            "Epoch 7859: train loss = 2087.958501, test loss = 3530.547808\n",
            "Epoch 7860: train loss = 2087.912650, test loss = 3530.525272\n",
            "Epoch 7861: train loss = 2087.866827, test loss = 3530.502711\n",
            "Epoch 7862: train loss = 2087.821033, test loss = 3530.480124\n",
            "Epoch 7863: train loss = 2087.775266, test loss = 3530.457511\n",
            "Epoch 7864: train loss = 2087.729528, test loss = 3530.434874\n",
            "Epoch 7865: train loss = 2087.683818, test loss = 3530.412211\n",
            "Epoch 7866: train loss = 2087.638135, test loss = 3530.389523\n",
            "Epoch 7867: train loss = 2087.592481, test loss = 3530.366810\n",
            "Epoch 7868: train loss = 2087.546854, test loss = 3530.344072\n",
            "Epoch 7869: train loss = 2087.501255, test loss = 3530.321309\n",
            "Epoch 7870: train loss = 2087.455684, test loss = 3530.298522\n",
            "Epoch 7871: train loss = 2087.410141, test loss = 3530.275709\n",
            "Epoch 7872: train loss = 2087.364625, test loss = 3530.252872\n",
            "Epoch 7873: train loss = 2087.319137, test loss = 3530.230011\n",
            "Epoch 7874: train loss = 2087.273677, test loss = 3530.207124\n",
            "Epoch 7875: train loss = 2087.228244, test loss = 3530.184214\n",
            "Epoch 7876: train loss = 2087.182839, test loss = 3530.161279\n",
            "Epoch 7877: train loss = 2087.137461, test loss = 3530.138319\n",
            "Epoch 7878: train loss = 2087.092111, test loss = 3530.115336\n",
            "Epoch 7879: train loss = 2087.046788, test loss = 3530.092328\n",
            "Epoch 7880: train loss = 2087.001492, test loss = 3530.069297\n",
            "Epoch 7881: train loss = 2086.956224, test loss = 3530.046241\n",
            "Epoch 7882: train loss = 2086.910983, test loss = 3530.023161\n",
            "Epoch 7883: train loss = 2086.865769, test loss = 3530.000058\n",
            "Epoch 7884: train loss = 2086.820582, test loss = 3529.976930\n",
            "Epoch 7885: train loss = 2086.775423, test loss = 3529.953779\n",
            "Epoch 7886: train loss = 2086.730290, test loss = 3529.930605\n",
            "Epoch 7887: train loss = 2086.685185, test loss = 3529.907406\n",
            "Epoch 7888: train loss = 2086.640107, test loss = 3529.884185\n",
            "Epoch 7889: train loss = 2086.595055, test loss = 3529.860940\n",
            "Epoch 7890: train loss = 2086.550031, test loss = 3529.837671\n",
            "Epoch 7891: train loss = 2086.505033, test loss = 3529.814379\n",
            "Epoch 7892: train loss = 2086.460062, test loss = 3529.791064\n",
            "Epoch 7893: train loss = 2086.415118, test loss = 3529.767726\n",
            "Epoch 7894: train loss = 2086.370201, test loss = 3529.744365\n",
            "Epoch 7895: train loss = 2086.325310, test loss = 3529.720981\n",
            "Epoch 7896: train loss = 2086.280446, test loss = 3529.697574\n",
            "Epoch 7897: train loss = 2086.235609, test loss = 3529.674144\n",
            "Epoch 7898: train loss = 2086.190798, test loss = 3529.650691\n",
            "Epoch 7899: train loss = 2086.146014, test loss = 3529.627216\n",
            "Epoch 7900: train loss = 2086.101256, test loss = 3529.603718\n",
            "Epoch 7901: train loss = 2086.056525, test loss = 3529.580197\n",
            "Epoch 7902: train loss = 2086.011820, test loss = 3529.556654\n",
            "Epoch 7903: train loss = 2085.967142, test loss = 3529.533089\n",
            "Epoch 7904: train loss = 2085.922490, test loss = 3529.509501\n",
            "Epoch 7905: train loss = 2085.877864, test loss = 3529.485891\n",
            "Epoch 7906: train loss = 2085.833264, test loss = 3529.462258\n",
            "Epoch 7907: train loss = 2085.788690, test loss = 3529.438604\n",
            "Epoch 7908: train loss = 2085.744143, test loss = 3529.414927\n",
            "Epoch 7909: train loss = 2085.699622, test loss = 3529.391229\n",
            "Epoch 7910: train loss = 2085.655127, test loss = 3529.367508\n",
            "Epoch 7911: train loss = 2085.610658, test loss = 3529.343766\n",
            "Epoch 7912: train loss = 2085.566214, test loss = 3529.320001\n",
            "Epoch 7913: train loss = 2085.521797, test loss = 3529.296215\n",
            "Epoch 7914: train loss = 2085.477406, test loss = 3529.272408\n",
            "Epoch 7915: train loss = 2085.433041, test loss = 3529.248578\n",
            "Epoch 7916: train loss = 2085.388701, test loss = 3529.224727\n",
            "Epoch 7917: train loss = 2085.344387, test loss = 3529.200855\n",
            "Epoch 7918: train loss = 2085.300099, test loss = 3529.176961\n",
            "Epoch 7919: train loss = 2085.255837, test loss = 3529.153046\n",
            "Epoch 7920: train loss = 2085.211600, test loss = 3529.129110\n",
            "Epoch 7921: train loss = 2085.167389, test loss = 3529.105152\n",
            "Epoch 7922: train loss = 2085.123203, test loss = 3529.081173\n",
            "Epoch 7923: train loss = 2085.079043, test loss = 3529.057174\n",
            "Epoch 7924: train loss = 2085.034909, test loss = 3529.033153\n",
            "Epoch 7925: train loss = 2084.990800, test loss = 3529.009111\n",
            "Epoch 7926: train loss = 2084.946716, test loss = 3528.985048\n",
            "Epoch 7927: train loss = 2084.902658, test loss = 3528.960965\n",
            "Epoch 7928: train loss = 2084.858625, test loss = 3528.936861\n",
            "Epoch 7929: train loss = 2084.814617, test loss = 3528.912736\n",
            "Epoch 7930: train loss = 2084.770635, test loss = 3528.888590\n",
            "Epoch 7931: train loss = 2084.726677, test loss = 3528.864424\n",
            "Epoch 7932: train loss = 2084.682745, test loss = 3528.840237\n",
            "Epoch 7933: train loss = 2084.638838, test loss = 3528.816030\n",
            "Epoch 7934: train loss = 2084.594956, test loss = 3528.791803\n",
            "Epoch 7935: train loss = 2084.551100, test loss = 3528.767555\n",
            "Epoch 7936: train loss = 2084.507268, test loss = 3528.743287\n",
            "Epoch 7937: train loss = 2084.463461, test loss = 3528.718999\n",
            "Epoch 7938: train loss = 2084.419679, test loss = 3528.694691\n",
            "Epoch 7939: train loss = 2084.375922, test loss = 3528.670363\n",
            "Epoch 7940: train loss = 2084.332190, test loss = 3528.646014\n",
            "Epoch 7941: train loss = 2084.288483, test loss = 3528.621646\n",
            "Epoch 7942: train loss = 2084.244800, test loss = 3528.597258\n",
            "Epoch 7943: train loss = 2084.201142, test loss = 3528.572850\n",
            "Epoch 7944: train loss = 2084.157509, test loss = 3528.548422\n",
            "Epoch 7945: train loss = 2084.113901, test loss = 3528.523975\n",
            "Epoch 7946: train loss = 2084.070317, test loss = 3528.499508\n",
            "Epoch 7947: train loss = 2084.026757, test loss = 3528.475022\n",
            "Epoch 7948: train loss = 2083.983223, test loss = 3528.450516\n",
            "Epoch 7949: train loss = 2083.939712, test loss = 3528.425990\n",
            "Epoch 7950: train loss = 2083.896227, test loss = 3528.401445\n",
            "Epoch 7951: train loss = 2083.852765, test loss = 3528.376881\n",
            "Epoch 7952: train loss = 2083.809328, test loss = 3528.352298\n",
            "Epoch 7953: train loss = 2083.765915, test loss = 3528.327695\n",
            "Epoch 7954: train loss = 2083.722527, test loss = 3528.303074\n",
            "Epoch 7955: train loss = 2083.679163, test loss = 3528.278433\n",
            "Epoch 7956: train loss = 2083.635823, test loss = 3528.253773\n",
            "Epoch 7957: train loss = 2083.592507, test loss = 3528.229095\n",
            "Epoch 7958: train loss = 2083.549215, test loss = 3528.204397\n",
            "Epoch 7959: train loss = 2083.505948, test loss = 3528.179681\n",
            "Epoch 7960: train loss = 2083.462704, test loss = 3528.154945\n",
            "Epoch 7961: train loss = 2083.419485, test loss = 3528.130191\n",
            "Epoch 7962: train loss = 2083.376290, test loss = 3528.105419\n",
            "Epoch 7963: train loss = 2083.333118, test loss = 3528.080628\n",
            "Epoch 7964: train loss = 2083.289970, test loss = 3528.055818\n",
            "Epoch 7965: train loss = 2083.246847, test loss = 3528.030990\n",
            "Epoch 7966: train loss = 2083.203747, test loss = 3528.006143\n",
            "Epoch 7967: train loss = 2083.160671, test loss = 3527.981278\n",
            "Epoch 7968: train loss = 2083.117618, test loss = 3527.956395\n",
            "Epoch 7969: train loss = 2083.074590, test loss = 3527.931494\n",
            "Epoch 7970: train loss = 2083.031585, test loss = 3527.906574\n",
            "Epoch 7971: train loss = 2082.988604, test loss = 3527.881636\n",
            "Epoch 7972: train loss = 2082.945646, test loss = 3527.856680\n",
            "Epoch 7973: train loss = 2082.902712, test loss = 3527.831706\n",
            "Epoch 7974: train loss = 2082.859801, test loss = 3527.806715\n",
            "Epoch 7975: train loss = 2082.816914, test loss = 3527.781705\n",
            "Epoch 7976: train loss = 2082.774051, test loss = 3527.756677\n",
            "Epoch 7977: train loss = 2082.731210, test loss = 3527.731632\n",
            "Epoch 7978: train loss = 2082.688394, test loss = 3527.706569\n",
            "Epoch 7979: train loss = 2082.645600, test loss = 3527.681488\n",
            "Epoch 7980: train loss = 2082.602830, test loss = 3527.656390\n",
            "Epoch 7981: train loss = 2082.560083, test loss = 3527.631274\n",
            "Epoch 7982: train loss = 2082.517359, test loss = 3527.606140\n",
            "Epoch 7983: train loss = 2082.474659, test loss = 3527.580990\n",
            "Epoch 7984: train loss = 2082.431982, test loss = 3527.555821\n",
            "Epoch 7985: train loss = 2082.389327, test loss = 3527.530636\n",
            "Epoch 7986: train loss = 2082.346696, test loss = 3527.505433\n",
            "Epoch 7987: train loss = 2082.304088, test loss = 3527.480213\n",
            "Epoch 7988: train loss = 2082.261503, test loss = 3527.454975\n",
            "Epoch 7989: train loss = 2082.218941, test loss = 3527.429721\n",
            "Epoch 7990: train loss = 2082.176402, test loss = 3527.404450\n",
            "Epoch 7991: train loss = 2082.133886, test loss = 3527.379161\n",
            "Epoch 7992: train loss = 2082.091392, test loss = 3527.353856\n",
            "Epoch 7993: train loss = 2082.048922, test loss = 3527.328533\n",
            "Epoch 7994: train loss = 2082.006474, test loss = 3527.303194\n",
            "Epoch 7995: train loss = 2081.964049, test loss = 3527.277838\n",
            "Epoch 7996: train loss = 2081.921646, test loss = 3527.252466\n",
            "Epoch 7997: train loss = 2081.879267, test loss = 3527.227076\n",
            "Epoch 7998: train loss = 2081.836910, test loss = 3527.201670\n",
            "Epoch 7999: train loss = 2081.794575, test loss = 3527.176248\n",
            "Epoch 8000: train loss = 2081.752264, test loss = 3527.150809\n",
            "Epoch 8001: train loss = 2081.709974, test loss = 3527.125353\n",
            "Epoch 8002: train loss = 2081.667708, test loss = 3527.099881\n",
            "Epoch 8003: train loss = 2081.625463, test loss = 3527.074393\n",
            "Epoch 8004: train loss = 2081.583241, test loss = 3527.048888\n",
            "Epoch 8005: train loss = 2081.541042, test loss = 3527.023367\n",
            "Epoch 8006: train loss = 2081.498865, test loss = 3526.997830\n",
            "Epoch 8007: train loss = 2081.456710, test loss = 3526.972277\n",
            "Epoch 8008: train loss = 2081.414577, test loss = 3526.946707\n",
            "Epoch 8009: train loss = 2081.372467, test loss = 3526.921122\n",
            "Epoch 8010: train loss = 2081.330379, test loss = 3526.895521\n",
            "Epoch 8011: train loss = 2081.288313, test loss = 3526.869903\n",
            "Epoch 8012: train loss = 2081.246269, test loss = 3526.844270\n",
            "Epoch 8013: train loss = 2081.204248, test loss = 3526.818621\n",
            "Epoch 8014: train loss = 2081.162248, test loss = 3526.792956\n",
            "Epoch 8015: train loss = 2081.120270, test loss = 3526.767276\n",
            "Epoch 8016: train loss = 2081.078315, test loss = 3526.741579\n",
            "Epoch 8017: train loss = 2081.036381, test loss = 3526.715867\n",
            "Epoch 8018: train loss = 2080.994470, test loss = 3526.690140\n",
            "Epoch 8019: train loss = 2080.952580, test loss = 3526.664397\n",
            "Epoch 8020: train loss = 2080.910712, test loss = 3526.638638\n",
            "Epoch 8021: train loss = 2080.868866, test loss = 3526.612864\n",
            "Epoch 8022: train loss = 2080.827041, test loss = 3526.587075\n",
            "Epoch 8023: train loss = 2080.785239, test loss = 3526.561270\n",
            "Epoch 8024: train loss = 2080.743458, test loss = 3526.535450\n",
            "Epoch 8025: train loss = 2080.701699, test loss = 3526.509615\n",
            "Epoch 8026: train loss = 2080.659961, test loss = 3526.483765\n",
            "Epoch 8027: train loss = 2080.618246, test loss = 3526.457899\n",
            "Epoch 8028: train loss = 2080.576551, test loss = 3526.432019\n",
            "Epoch 8029: train loss = 2080.534878, test loss = 3526.406123\n",
            "Epoch 8030: train loss = 2080.493227, test loss = 3526.380213\n",
            "Epoch 8031: train loss = 2080.451597, test loss = 3526.354287\n",
            "Epoch 8032: train loss = 2080.409989, test loss = 3526.328347\n",
            "Epoch 8033: train loss = 2080.368402, test loss = 3526.302391\n",
            "Epoch 8034: train loss = 2080.326837, test loss = 3526.276421\n",
            "Epoch 8035: train loss = 2080.285292, test loss = 3526.250437\n",
            "Epoch 8036: train loss = 2080.243770, test loss = 3526.224437\n",
            "Epoch 8037: train loss = 2080.202268, test loss = 3526.198423\n",
            "Epoch 8038: train loss = 2080.160788, test loss = 3526.172394\n",
            "Epoch 8039: train loss = 2080.119328, test loss = 3526.146351\n",
            "Epoch 8040: train loss = 2080.077890, test loss = 3526.120293\n",
            "Epoch 8041: train loss = 2080.036474, test loss = 3526.094221\n",
            "Epoch 8042: train loss = 2079.995078, test loss = 3526.068134\n",
            "Epoch 8043: train loss = 2079.953703, test loss = 3526.042033\n",
            "Epoch 8044: train loss = 2079.912349, test loss = 3526.015918\n",
            "Epoch 8045: train loss = 2079.871017, test loss = 3525.989788\n",
            "Epoch 8046: train loss = 2079.829705, test loss = 3525.963645\n",
            "Epoch 8047: train loss = 2079.788414, test loss = 3525.937487\n",
            "Epoch 8048: train loss = 2079.747144, test loss = 3525.911315\n",
            "Epoch 8049: train loss = 2079.705895, test loss = 3525.885128\n",
            "Epoch 8050: train loss = 2079.664667, test loss = 3525.858928\n",
            "Epoch 8051: train loss = 2079.623460, test loss = 3525.832714\n",
            "Epoch 8052: train loss = 2079.582273, test loss = 3525.806486\n",
            "Epoch 8053: train loss = 2079.541107, test loss = 3525.780244\n",
            "Epoch 8054: train loss = 2079.499962, test loss = 3525.753988\n",
            "Epoch 8055: train loss = 2079.458838, test loss = 3525.727719\n",
            "Epoch 8056: train loss = 2079.417734, test loss = 3525.701435\n",
            "Epoch 8057: train loss = 2079.376651, test loss = 3525.675138\n",
            "Epoch 8058: train loss = 2079.335588, test loss = 3525.648827\n",
            "Epoch 8059: train loss = 2079.294546, test loss = 3525.622503\n",
            "Epoch 8060: train loss = 2079.253524, test loss = 3525.596165\n",
            "Epoch 8061: train loss = 2079.212523, test loss = 3525.569814\n",
            "Epoch 8062: train loss = 2079.171542, test loss = 3525.543449\n",
            "Epoch 8063: train loss = 2079.130582, test loss = 3525.517070\n",
            "Epoch 8064: train loss = 2079.089642, test loss = 3525.490679\n",
            "Epoch 8065: train loss = 2079.048723, test loss = 3525.464273\n",
            "Epoch 8066: train loss = 2079.007823, test loss = 3525.437855\n",
            "Epoch 8067: train loss = 2078.966944, test loss = 3525.411423\n",
            "Epoch 8068: train loss = 2078.926085, test loss = 3525.384979\n",
            "Epoch 8069: train loss = 2078.885247, test loss = 3525.358520\n",
            "Epoch 8070: train loss = 2078.844429, test loss = 3525.332049\n",
            "Epoch 8071: train loss = 2078.803630, test loss = 3525.305565\n",
            "Epoch 8072: train loss = 2078.762852, test loss = 3525.279068\n",
            "Epoch 8073: train loss = 2078.722094, test loss = 3525.252558\n",
            "Epoch 8074: train loss = 2078.681356, test loss = 3525.226034\n",
            "Epoch 8075: train loss = 2078.640638, test loss = 3525.199498\n",
            "Epoch 8076: train loss = 2078.599940, test loss = 3525.172949\n",
            "Epoch 8077: train loss = 2078.559262, test loss = 3525.146388\n",
            "Epoch 8078: train loss = 2078.518604, test loss = 3525.119813\n",
            "Epoch 8079: train loss = 2078.477966, test loss = 3525.093226\n",
            "Epoch 8080: train loss = 2078.437348, test loss = 3525.066626\n",
            "Epoch 8081: train loss = 2078.396749, test loss = 3525.040013\n",
            "Epoch 8082: train loss = 2078.356170, test loss = 3525.013388\n",
            "Epoch 8083: train loss = 2078.315611, test loss = 3524.986750\n",
            "Epoch 8084: train loss = 2078.275072, test loss = 3524.960100\n",
            "Epoch 8085: train loss = 2078.234553, test loss = 3524.933437\n",
            "Epoch 8086: train loss = 2078.194053, test loss = 3524.906762\n",
            "Epoch 8087: train loss = 2078.153573, test loss = 3524.880075\n",
            "Epoch 8088: train loss = 2078.113112, test loss = 3524.853375\n",
            "Epoch 8089: train loss = 2078.072671, test loss = 3524.826662\n",
            "Epoch 8090: train loss = 2078.032249, test loss = 3524.799938\n",
            "Epoch 8091: train loss = 2077.991847, test loss = 3524.773201\n",
            "Epoch 8092: train loss = 2077.951465, test loss = 3524.746452\n",
            "Epoch 8093: train loss = 2077.911102, test loss = 3524.719691\n",
            "Epoch 8094: train loss = 2077.870758, test loss = 3524.692918\n",
            "Epoch 8095: train loss = 2077.830434, test loss = 3524.666133\n",
            "Epoch 8096: train loss = 2077.790129, test loss = 3524.639336\n",
            "Epoch 8097: train loss = 2077.749844, test loss = 3524.612527\n",
            "Epoch 8098: train loss = 2077.709578, test loss = 3524.585706\n",
            "Epoch 8099: train loss = 2077.669331, test loss = 3524.558873\n",
            "Epoch 8100: train loss = 2077.629103, test loss = 3524.532028\n",
            "Epoch 8101: train loss = 2077.588894, test loss = 3524.505171\n",
            "Epoch 8102: train loss = 2077.548705, test loss = 3524.478303\n",
            "Epoch 8103: train loss = 2077.508535, test loss = 3524.451423\n",
            "Epoch 8104: train loss = 2077.468384, test loss = 3524.424531\n",
            "Epoch 8105: train loss = 2077.428252, test loss = 3524.397628\n",
            "Epoch 8106: train loss = 2077.388139, test loss = 3524.370713\n",
            "Epoch 8107: train loss = 2077.348045, test loss = 3524.343786\n",
            "Epoch 8108: train loss = 2077.307970, test loss = 3524.316848\n",
            "Epoch 8109: train loss = 2077.267914, test loss = 3524.289898\n",
            "Epoch 8110: train loss = 2077.227877, test loss = 3524.262937\n",
            "Epoch 8111: train loss = 2077.187859, test loss = 3524.235965\n",
            "Epoch 8112: train loss = 2077.147860, test loss = 3524.208981\n",
            "Epoch 8113: train loss = 2077.107879, test loss = 3524.181986\n",
            "Epoch 8114: train loss = 2077.067918, test loss = 3524.154980\n",
            "Epoch 8115: train loss = 2077.027975, test loss = 3524.127962\n",
            "Epoch 8116: train loss = 2076.988051, test loss = 3524.100933\n",
            "Epoch 8117: train loss = 2076.948145, test loss = 3524.073893\n",
            "Epoch 8118: train loss = 2076.908259, test loss = 3524.046842\n",
            "Epoch 8119: train loss = 2076.868391, test loss = 3524.019779\n",
            "Epoch 8120: train loss = 2076.828542, test loss = 3523.992706\n",
            "Epoch 8121: train loss = 2076.788711, test loss = 3523.965622\n",
            "Epoch 8122: train loss = 2076.748899, test loss = 3523.938526\n",
            "Epoch 8123: train loss = 2076.709105, test loss = 3523.911420\n",
            "Epoch 8124: train loss = 2076.669330, test loss = 3523.884303\n",
            "Epoch 8125: train loss = 2076.629573, test loss = 3523.857175\n",
            "Epoch 8126: train loss = 2076.589835, test loss = 3523.830036\n",
            "Epoch 8127: train loss = 2076.550116, test loss = 3523.802886\n",
            "Epoch 8128: train loss = 2076.510414, test loss = 3523.775726\n",
            "Epoch 8129: train loss = 2076.470731, test loss = 3523.748555\n",
            "Epoch 8130: train loss = 2076.431067, test loss = 3523.721373\n",
            "Epoch 8131: train loss = 2076.391420, test loss = 3523.694181\n",
            "Epoch 8132: train loss = 2076.351792, test loss = 3523.666978\n",
            "Epoch 8133: train loss = 2076.312183, test loss = 3523.639764\n",
            "Epoch 8134: train loss = 2076.272591, test loss = 3523.612540\n",
            "Epoch 8135: train loss = 2076.233018, test loss = 3523.585305\n",
            "Epoch 8136: train loss = 2076.193463, test loss = 3523.558060\n",
            "Epoch 8137: train loss = 2076.153926, test loss = 3523.530805\n",
            "Epoch 8138: train loss = 2076.114407, test loss = 3523.503539\n",
            "Epoch 8139: train loss = 2076.074906, test loss = 3523.476263\n",
            "Epoch 8140: train loss = 2076.035423, test loss = 3523.448977\n",
            "Epoch 8141: train loss = 2075.995959, test loss = 3523.421680\n",
            "Epoch 8142: train loss = 2075.956512, test loss = 3523.394373\n",
            "Epoch 8143: train loss = 2075.917083, test loss = 3523.367056\n",
            "Epoch 8144: train loss = 2075.877672, test loss = 3523.339729\n",
            "Epoch 8145: train loss = 2075.838279, test loss = 3523.312391\n",
            "Epoch 8146: train loss = 2075.798904, test loss = 3523.285044\n",
            "Epoch 8147: train loss = 2075.759547, test loss = 3523.257686\n",
            "Epoch 8148: train loss = 2075.720208, test loss = 3523.230319\n",
            "Epoch 8149: train loss = 2075.680886, test loss = 3523.202941\n",
            "Epoch 8150: train loss = 2075.641582, test loss = 3523.175554\n",
            "Epoch 8151: train loss = 2075.602296, test loss = 3523.148157\n",
            "Epoch 8152: train loss = 2075.563028, test loss = 3523.120749\n",
            "Epoch 8153: train loss = 2075.523777, test loss = 3523.093332\n",
            "Epoch 8154: train loss = 2075.484544, test loss = 3523.065906\n",
            "Epoch 8155: train loss = 2075.445329, test loss = 3523.038469\n",
            "Epoch 8156: train loss = 2075.406131, test loss = 3523.011023\n",
            "Epoch 8157: train loss = 2075.366951, test loss = 3522.983567\n",
            "Epoch 8158: train loss = 2075.327789, test loss = 3522.956101\n",
            "Epoch 8159: train loss = 2075.288643, test loss = 3522.928626\n",
            "Epoch 8160: train loss = 2075.249516, test loss = 3522.901141\n",
            "Epoch 8161: train loss = 2075.210406, test loss = 3522.873647\n",
            "Epoch 8162: train loss = 2075.171313, test loss = 3522.846143\n",
            "Epoch 8163: train loss = 2075.132238, test loss = 3522.818629\n",
            "Epoch 8164: train loss = 2075.093180, test loss = 3522.791106\n",
            "Epoch 8165: train loss = 2075.054139, test loss = 3522.763574\n",
            "Epoch 8166: train loss = 2075.015116, test loss = 3522.736033\n",
            "Epoch 8167: train loss = 2074.976110, test loss = 3522.708482\n",
            "Epoch 8168: train loss = 2074.937121, test loss = 3522.680921\n",
            "Epoch 8169: train loss = 2074.898150, test loss = 3522.653352\n",
            "Epoch 8170: train loss = 2074.859196, test loss = 3522.625773\n",
            "Epoch 8171: train loss = 2074.820259, test loss = 3522.598185\n",
            "Epoch 8172: train loss = 2074.781339, test loss = 3522.570588\n",
            "Epoch 8173: train loss = 2074.742436, test loss = 3522.542982\n",
            "Epoch 8174: train loss = 2074.703550, test loss = 3522.515366\n",
            "Epoch 8175: train loss = 2074.664682, test loss = 3522.487742\n",
            "Epoch 8176: train loss = 2074.625831, test loss = 3522.460108\n",
            "Epoch 8177: train loss = 2074.586996, test loss = 3522.432466\n",
            "Epoch 8178: train loss = 2074.548179, test loss = 3522.404814\n",
            "Epoch 8179: train loss = 2074.509378, test loss = 3522.377154\n",
            "Epoch 8180: train loss = 2074.470595, test loss = 3522.349484\n",
            "Epoch 8181: train loss = 2074.431828, test loss = 3522.321806\n",
            "Epoch 8182: train loss = 2074.393078, test loss = 3522.294119\n",
            "Epoch 8183: train loss = 2074.354346, test loss = 3522.266423\n",
            "Epoch 8184: train loss = 2074.315630, test loss = 3522.238719\n",
            "Epoch 8185: train loss = 2074.276930, test loss = 3522.211005\n",
            "Epoch 8186: train loss = 2074.238248, test loss = 3522.183283\n",
            "Epoch 8187: train loss = 2074.199582, test loss = 3522.155552\n",
            "Epoch 8188: train loss = 2074.160934, test loss = 3522.127813\n",
            "Epoch 8189: train loss = 2074.122301, test loss = 3522.100065\n",
            "Epoch 8190: train loss = 2074.083686, test loss = 3522.072309\n",
            "Epoch 8191: train loss = 2074.045087, test loss = 3522.044543\n",
            "Epoch 8192: train loss = 2074.006505, test loss = 3522.016770\n",
            "Epoch 8193: train loss = 2073.967939, test loss = 3521.988988\n",
            "Epoch 8194: train loss = 2073.929390, test loss = 3521.961197\n",
            "Epoch 8195: train loss = 2073.890858, test loss = 3521.933398\n",
            "Epoch 8196: train loss = 2073.852342, test loss = 3521.905591\n",
            "Epoch 8197: train loss = 2073.813843, test loss = 3521.877775\n",
            "Epoch 8198: train loss = 2073.775360, test loss = 3521.849951\n",
            "Epoch 8199: train loss = 2073.736893, test loss = 3521.822119\n",
            "Epoch 8200: train loss = 2073.698443, test loss = 3521.794278\n",
            "Epoch 8201: train loss = 2073.660009, test loss = 3521.766429\n",
            "Epoch 8202: train loss = 2073.621592, test loss = 3521.738572\n",
            "Epoch 8203: train loss = 2073.583191, test loss = 3521.710707\n",
            "Epoch 8204: train loss = 2073.544807, test loss = 3521.682834\n",
            "Epoch 8205: train loss = 2073.506438, test loss = 3521.654952\n",
            "Epoch 8206: train loss = 2073.468086, test loss = 3521.627063\n",
            "Epoch 8207: train loss = 2073.429750, test loss = 3521.599165\n",
            "Epoch 8208: train loss = 2073.391431, test loss = 3521.571260\n",
            "Epoch 8209: train loss = 2073.353127, test loss = 3521.543346\n",
            "Epoch 8210: train loss = 2073.314840, test loss = 3521.515425\n",
            "Epoch 8211: train loss = 2073.276569, test loss = 3521.487495\n",
            "Epoch 8212: train loss = 2073.238314, test loss = 3521.459558\n",
            "Epoch 8213: train loss = 2073.200075, test loss = 3521.431613\n",
            "Epoch 8214: train loss = 2073.161853, test loss = 3521.403660\n",
            "Epoch 8215: train loss = 2073.123646, test loss = 3521.375699\n",
            "Epoch 8216: train loss = 2073.085455, test loss = 3521.347731\n",
            "Epoch 8217: train loss = 2073.047280, test loss = 3521.319754\n",
            "Epoch 8218: train loss = 2073.009122, test loss = 3521.291770\n",
            "Epoch 8219: train loss = 2072.970979, test loss = 3521.263779\n",
            "Epoch 8220: train loss = 2072.932852, test loss = 3521.235779\n",
            "Epoch 8221: train loss = 2072.894741, test loss = 3521.207773\n",
            "Epoch 8222: train loss = 2072.856646, test loss = 3521.179758\n",
            "Epoch 8223: train loss = 2072.818566, test loss = 3521.151736\n",
            "Epoch 8224: train loss = 2072.780503, test loss = 3521.123706\n",
            "Epoch 8225: train loss = 2072.742455, test loss = 3521.095669\n",
            "Epoch 8226: train loss = 2072.704423, test loss = 3521.067625\n",
            "Epoch 8227: train loss = 2072.666407, test loss = 3521.039573\n",
            "Epoch 8228: train loss = 2072.628407, test loss = 3521.011513\n",
            "Epoch 8229: train loss = 2072.590422, test loss = 3520.983447\n",
            "Epoch 8230: train loss = 2072.552453, test loss = 3520.955373\n",
            "Epoch 8231: train loss = 2072.514499, test loss = 3520.927291\n",
            "Epoch 8232: train loss = 2072.476561, test loss = 3520.899202\n",
            "Epoch 8233: train loss = 2072.438639, test loss = 3520.871106\n",
            "Epoch 8234: train loss = 2072.400733, test loss = 3520.843003\n",
            "Epoch 8235: train loss = 2072.362842, test loss = 3520.814893\n",
            "Epoch 8236: train loss = 2072.324966, test loss = 3520.786775\n",
            "Epoch 8237: train loss = 2072.287106, test loss = 3520.758650\n",
            "Epoch 8238: train loss = 2072.249261, test loss = 3520.730518\n",
            "Epoch 8239: train loss = 2072.211432, test loss = 3520.702379\n",
            "Epoch 8240: train loss = 2072.173618, test loss = 3520.674233\n",
            "Epoch 8241: train loss = 2072.135820, test loss = 3520.646080\n",
            "Epoch 8242: train loss = 2072.098037, test loss = 3520.617920\n",
            "Epoch 8243: train loss = 2072.060270, test loss = 3520.589753\n",
            "Epoch 8244: train loss = 2072.022517, test loss = 3520.561579\n",
            "Epoch 8245: train loss = 2071.984780, test loss = 3520.533398\n",
            "Epoch 8246: train loss = 2071.947059, test loss = 3520.505210\n",
            "Epoch 8247: train loss = 2071.909352, test loss = 3520.477015\n",
            "Epoch 8248: train loss = 2071.871661, test loss = 3520.448814\n",
            "Epoch 8249: train loss = 2071.833985, test loss = 3520.420605\n",
            "Epoch 8250: train loss = 2071.796325, test loss = 3520.392390\n",
            "Epoch 8251: train loss = 2071.758679, test loss = 3520.364168\n",
            "Epoch 8252: train loss = 2071.721049, test loss = 3520.335940\n",
            "Epoch 8253: train loss = 2071.683433, test loss = 3520.307704\n",
            "Epoch 8254: train loss = 2071.645833, test loss = 3520.279462\n",
            "Epoch 8255: train loss = 2071.608248, test loss = 3520.251214\n",
            "Epoch 8256: train loss = 2071.570678, test loss = 3520.222959\n",
            "Epoch 8257: train loss = 2071.533123, test loss = 3520.194697\n",
            "Epoch 8258: train loss = 2071.495583, test loss = 3520.166428\n",
            "Epoch 8259: train loss = 2071.458058, test loss = 3520.138153\n",
            "Epoch 8260: train loss = 2071.420548, test loss = 3520.109872\n",
            "Epoch 8261: train loss = 2071.383052, test loss = 3520.081584\n",
            "Epoch 8262: train loss = 2071.345572, test loss = 3520.053289\n",
            "Epoch 8263: train loss = 2071.308107, test loss = 3520.024989\n",
            "Epoch 8264: train loss = 2071.270656, test loss = 3519.996681\n",
            "Epoch 8265: train loss = 2071.233221, test loss = 3519.968368\n",
            "Epoch 8266: train loss = 2071.195800, test loss = 3519.940048\n",
            "Epoch 8267: train loss = 2071.158394, test loss = 3519.911721\n",
            "Epoch 8268: train loss = 2071.121002, test loss = 3519.883389\n",
            "Epoch 8269: train loss = 2071.083626, test loss = 3519.855050\n",
            "Epoch 8270: train loss = 2071.046264, test loss = 3519.826705\n",
            "Epoch 8271: train loss = 2071.008917, test loss = 3519.798354\n",
            "Epoch 8272: train loss = 2070.971585, test loss = 3519.769996\n",
            "Epoch 8273: train loss = 2070.934267, test loss = 3519.741632\n",
            "Epoch 8274: train loss = 2070.896964, test loss = 3519.713263\n",
            "Epoch 8275: train loss = 2070.859675, test loss = 3519.684887\n",
            "Epoch 8276: train loss = 2070.822401, test loss = 3519.656505\n",
            "Epoch 8277: train loss = 2070.785142, test loss = 3519.628116\n",
            "Epoch 8278: train loss = 2070.747897, test loss = 3519.599722\n",
            "Epoch 8279: train loss = 2070.710667, test loss = 3519.571322\n",
            "Epoch 8280: train loss = 2070.673451, test loss = 3519.542916\n",
            "Epoch 8281: train loss = 2070.636250, test loss = 3519.514504\n",
            "Epoch 8282: train loss = 2070.599063, test loss = 3519.486086\n",
            "Epoch 8283: train loss = 2070.561890, test loss = 3519.457662\n",
            "Epoch 8284: train loss = 2070.524732, test loss = 3519.429232\n",
            "Epoch 8285: train loss = 2070.487589, test loss = 3519.400797\n",
            "Epoch 8286: train loss = 2070.450459, test loss = 3519.372355\n",
            "Epoch 8287: train loss = 2070.413345, test loss = 3519.343908\n",
            "Epoch 8288: train loss = 2070.376244, test loss = 3519.315455\n",
            "Epoch 8289: train loss = 2070.339158, test loss = 3519.286996\n",
            "Epoch 8290: train loss = 2070.302086, test loss = 3519.258532\n",
            "Epoch 8291: train loss = 2070.265028, test loss = 3519.230061\n",
            "Epoch 8292: train loss = 2070.227984, test loss = 3519.201585\n",
            "Epoch 8293: train loss = 2070.190955, test loss = 3519.173104\n",
            "Epoch 8294: train loss = 2070.153940, test loss = 3519.144617\n",
            "Epoch 8295: train loss = 2070.116939, test loss = 3519.116124\n",
            "Epoch 8296: train loss = 2070.079952, test loss = 3519.087625\n",
            "Epoch 8297: train loss = 2070.042980, test loss = 3519.059122\n",
            "Epoch 8298: train loss = 2070.006021, test loss = 3519.030612\n",
            "Epoch 8299: train loss = 2069.969076, test loss = 3519.002097\n",
            "Epoch 8300: train loss = 2069.932146, test loss = 3518.973577\n",
            "Epoch 8301: train loss = 2069.895230, test loss = 3518.945051\n",
            "Epoch 8302: train loss = 2069.858327, test loss = 3518.916519\n",
            "Epoch 8303: train loss = 2069.821439, test loss = 3518.887983\n",
            "Epoch 8304: train loss = 2069.784565, test loss = 3518.859440\n",
            "Epoch 8305: train loss = 2069.747704, test loss = 3518.830893\n",
            "Epoch 8306: train loss = 2069.710858, test loss = 3518.802340\n",
            "Epoch 8307: train loss = 2069.674025, test loss = 3518.773782\n",
            "Epoch 8308: train loss = 2069.637206, test loss = 3518.745218\n",
            "Epoch 8309: train loss = 2069.600401, test loss = 3518.716650\n",
            "Epoch 8310: train loss = 2069.563610, test loss = 3518.688076\n",
            "Epoch 8311: train loss = 2069.526833, test loss = 3518.659497\n",
            "Epoch 8312: train loss = 2069.490070, test loss = 3518.630912\n",
            "Epoch 8313: train loss = 2069.453320, test loss = 3518.602323\n",
            "Epoch 8314: train loss = 2069.416584, test loss = 3518.573728\n",
            "Epoch 8315: train loss = 2069.379862, test loss = 3518.545128\n",
            "Epoch 8316: train loss = 2069.343154, test loss = 3518.516523\n",
            "Epoch 8317: train loss = 2069.306459, test loss = 3518.487913\n",
            "Epoch 8318: train loss = 2069.269778, test loss = 3518.459298\n",
            "Epoch 8319: train loss = 2069.233111, test loss = 3518.430678\n",
            "Epoch 8320: train loss = 2069.196457, test loss = 3518.402053\n",
            "Epoch 8321: train loss = 2069.159817, test loss = 3518.373423\n",
            "Epoch 8322: train loss = 2069.123191, test loss = 3518.344788\n",
            "Epoch 8323: train loss = 2069.086578, test loss = 3518.316148\n",
            "Epoch 8324: train loss = 2069.049978, test loss = 3518.287503\n",
            "Epoch 8325: train loss = 2069.013392, test loss = 3518.258853\n",
            "Epoch 8326: train loss = 2068.976820, test loss = 3518.230199\n",
            "Epoch 8327: train loss = 2068.940261, test loss = 3518.201539\n",
            "Epoch 8328: train loss = 2068.903716, test loss = 3518.172875\n",
            "Epoch 8329: train loss = 2068.867184, test loss = 3518.144206\n",
            "Epoch 8330: train loss = 2068.830665, test loss = 3518.115532\n",
            "Epoch 8331: train loss = 2068.794160, test loss = 3518.086854\n",
            "Epoch 8332: train loss = 2068.757669, test loss = 3518.058170\n",
            "Epoch 8333: train loss = 2068.721190, test loss = 3518.029482\n",
            "Epoch 8334: train loss = 2068.684725, test loss = 3518.000789\n",
            "Epoch 8335: train loss = 2068.648274, test loss = 3517.972092\n",
            "Epoch 8336: train loss = 2068.611835, test loss = 3517.943390\n",
            "Epoch 8337: train loss = 2068.575410, test loss = 3517.914683\n",
            "Epoch 8338: train loss = 2068.538998, test loss = 3517.885972\n",
            "Epoch 8339: train loss = 2068.502600, test loss = 3517.857256\n",
            "Epoch 8340: train loss = 2068.466214, test loss = 3517.828536\n",
            "Epoch 8341: train loss = 2068.429842, test loss = 3517.799811\n",
            "Epoch 8342: train loss = 2068.393483, test loss = 3517.771082\n",
            "Epoch 8343: train loss = 2068.357138, test loss = 3517.742348\n",
            "Epoch 8344: train loss = 2068.320805, test loss = 3517.713610\n",
            "Epoch 8345: train loss = 2068.284485, test loss = 3517.684867\n",
            "Epoch 8346: train loss = 2068.248179, test loss = 3517.656120\n",
            "Epoch 8347: train loss = 2068.211886, test loss = 3517.627368\n",
            "Epoch 8348: train loss = 2068.175605, test loss = 3517.598612\n",
            "Epoch 8349: train loss = 2068.139338, test loss = 3517.569852\n",
            "Epoch 8350: train loss = 2068.103084, test loss = 3517.541087\n",
            "Epoch 8351: train loss = 2068.066843, test loss = 3517.512318\n",
            "Epoch 8352: train loss = 2068.030614, test loss = 3517.483545\n",
            "Epoch 8353: train loss = 2067.994399, test loss = 3517.454767\n",
            "Epoch 8354: train loss = 2067.958197, test loss = 3517.425985\n",
            "Epoch 8355: train loss = 2067.922008, test loss = 3517.397199\n",
            "Epoch 8356: train loss = 2067.885831, test loss = 3517.368409\n",
            "Epoch 8357: train loss = 2067.849667, test loss = 3517.339615\n",
            "Epoch 8358: train loss = 2067.813517, test loss = 3517.310816\n",
            "Epoch 8359: train loss = 2067.777379, test loss = 3517.282014\n",
            "Epoch 8360: train loss = 2067.741254, test loss = 3517.253207\n",
            "Epoch 8361: train loss = 2067.705142, test loss = 3517.224396\n",
            "Epoch 8362: train loss = 2067.669042, test loss = 3517.195581\n",
            "Epoch 8363: train loss = 2067.632955, test loss = 3517.166762\n",
            "Epoch 8364: train loss = 2067.596882, test loss = 3517.137939\n",
            "Epoch 8365: train loss = 2067.560820, test loss = 3517.109112\n",
            "Epoch 8366: train loss = 2067.524772, test loss = 3517.080281\n",
            "Epoch 8367: train loss = 2067.488736, test loss = 3517.051446\n",
            "Epoch 8368: train loss = 2067.452713, test loss = 3517.022607\n",
            "Epoch 8369: train loss = 2067.416702, test loss = 3516.993764\n",
            "Epoch 8370: train loss = 2067.380705, test loss = 3516.964917\n",
            "Epoch 8371: train loss = 2067.344719, test loss = 3516.936066\n",
            "Epoch 8372: train loss = 2067.308747, test loss = 3516.907212\n",
            "Epoch 8373: train loss = 2067.272787, test loss = 3516.878353\n",
            "Epoch 8374: train loss = 2067.236839, test loss = 3516.849491\n",
            "Epoch 8375: train loss = 2067.200904, test loss = 3516.820625\n",
            "Epoch 8376: train loss = 2067.164982, test loss = 3516.791755\n",
            "Epoch 8377: train loss = 2067.129072, test loss = 3516.762881\n",
            "Epoch 8378: train loss = 2067.093175, test loss = 3516.734004\n",
            "Epoch 8379: train loss = 2067.057290, test loss = 3516.705123\n",
            "Epoch 8380: train loss = 2067.021417, test loss = 3516.676238\n",
            "Epoch 8381: train loss = 2066.985557, test loss = 3516.647350\n",
            "Epoch 8382: train loss = 2066.949709, test loss = 3516.618458\n",
            "Epoch 8383: train loss = 2066.913874, test loss = 3516.589562\n",
            "Epoch 8384: train loss = 2066.878051, test loss = 3516.560662\n",
            "Epoch 8385: train loss = 2066.842240, test loss = 3516.531759\n",
            "Epoch 8386: train loss = 2066.806442, test loss = 3516.502853\n",
            "Epoch 8387: train loss = 2066.770656, test loss = 3516.473943\n",
            "Epoch 8388: train loss = 2066.734882, test loss = 3516.445029\n",
            "Epoch 8389: train loss = 2066.699121, test loss = 3516.416112\n",
            "Epoch 8390: train loss = 2066.663372, test loss = 3516.387191\n",
            "Epoch 8391: train loss = 2066.627635, test loss = 3516.358267\n",
            "Epoch 8392: train loss = 2066.591910, test loss = 3516.329339\n",
            "Epoch 8393: train loss = 2066.556198, test loss = 3516.300408\n",
            "Epoch 8394: train loss = 2066.520498, test loss = 3516.271473\n",
            "Epoch 8395: train loss = 2066.484810, test loss = 3516.242535\n",
            "Epoch 8396: train loss = 2066.449134, test loss = 3516.213594\n",
            "Epoch 8397: train loss = 2066.413470, test loss = 3516.184649\n",
            "Epoch 8398: train loss = 2066.377818, test loss = 3516.155701\n",
            "Epoch 8399: train loss = 2066.342179, test loss = 3516.126750\n",
            "Epoch 8400: train loss = 2066.306551, test loss = 3516.097795\n",
            "Epoch 8401: train loss = 2066.270935, test loss = 3516.068837\n",
            "Epoch 8402: train loss = 2066.235332, test loss = 3516.039875\n",
            "Epoch 8403: train loss = 2066.199741, test loss = 3516.010911\n",
            "Epoch 8404: train loss = 2066.164161, test loss = 3515.981943\n",
            "Epoch 8405: train loss = 2066.128594, test loss = 3515.952972\n",
            "Epoch 8406: train loss = 2066.093038, test loss = 3515.923998\n",
            "Epoch 8407: train loss = 2066.057495, test loss = 3515.895021\n",
            "Epoch 8408: train loss = 2066.021963, test loss = 3515.866040\n",
            "Epoch 8409: train loss = 2065.986443, test loss = 3515.837056\n",
            "Epoch 8410: train loss = 2065.950936, test loss = 3515.808070\n",
            "Epoch 8411: train loss = 2065.915440, test loss = 3515.779080\n",
            "Epoch 8412: train loss = 2065.879956, test loss = 3515.750087\n",
            "Epoch 8413: train loss = 2065.844483, test loss = 3515.721091\n",
            "Epoch 8414: train loss = 2065.809023, test loss = 3515.692091\n",
            "Epoch 8415: train loss = 2065.773575, test loss = 3515.663089\n",
            "Epoch 8416: train loss = 2065.738138, test loss = 3515.634084\n",
            "Epoch 8417: train loss = 2065.702713, test loss = 3515.605076\n",
            "Epoch 8418: train loss = 2065.667299, test loss = 3515.576065\n",
            "Epoch 8419: train loss = 2065.631898, test loss = 3515.547051\n",
            "Epoch 8420: train loss = 2065.596508, test loss = 3515.518034\n",
            "Epoch 8421: train loss = 2065.561130, test loss = 3515.489014\n",
            "Epoch 8422: train loss = 2065.525764, test loss = 3515.459991\n",
            "Epoch 8423: train loss = 2065.490409, test loss = 3515.430965\n",
            "Epoch 8424: train loss = 2065.455066, test loss = 3515.401937\n",
            "Epoch 8425: train loss = 2065.419734, test loss = 3515.372906\n",
            "Epoch 8426: train loss = 2065.384414, test loss = 3515.343871\n",
            "Epoch 8427: train loss = 2065.349106, test loss = 3515.314834\n",
            "Epoch 8428: train loss = 2065.313809, test loss = 3515.285794\n",
            "Epoch 8429: train loss = 2065.278524, test loss = 3515.256752\n",
            "Epoch 8430: train loss = 2065.243251, test loss = 3515.227706\n",
            "Epoch 8431: train loss = 2065.207988, test loss = 3515.198658\n",
            "Epoch 8432: train loss = 2065.172738, test loss = 3515.169608\n",
            "Epoch 8433: train loss = 2065.137499, test loss = 3515.140554\n",
            "Epoch 8434: train loss = 2065.102271, test loss = 3515.111498\n",
            "Epoch 8435: train loss = 2065.067055, test loss = 3515.082439\n",
            "Epoch 8436: train loss = 2065.031850, test loss = 3515.053377\n",
            "Epoch 8437: train loss = 2064.996657, test loss = 3515.024313\n",
            "Epoch 8438: train loss = 2064.961475, test loss = 3514.995247\n",
            "Epoch 8439: train loss = 2064.926305, test loss = 3514.966177\n",
            "Epoch 8440: train loss = 2064.891146, test loss = 3514.937105\n",
            "Epoch 8441: train loss = 2064.855998, test loss = 3514.908031\n",
            "Epoch 8442: train loss = 2064.820861, test loss = 3514.878954\n",
            "Epoch 8443: train loss = 2064.785736, test loss = 3514.849874\n",
            "Epoch 8444: train loss = 2064.750623, test loss = 3514.820792\n",
            "Epoch 8445: train loss = 2064.715520, test loss = 3514.791708\n",
            "Epoch 8446: train loss = 2064.680429, test loss = 3514.762621\n",
            "Epoch 8447: train loss = 2064.645349, test loss = 3514.733531\n",
            "Epoch 8448: train loss = 2064.610280, test loss = 3514.704439\n",
            "Epoch 8449: train loss = 2064.575223, test loss = 3514.675345\n",
            "Epoch 8450: train loss = 2064.540176, test loss = 3514.646248\n",
            "Epoch 8451: train loss = 2064.505141, test loss = 3514.617149\n",
            "Epoch 8452: train loss = 2064.470117, test loss = 3514.588047\n",
            "Epoch 8453: train loss = 2064.435104, test loss = 3514.558944\n",
            "Epoch 8454: train loss = 2064.400103, test loss = 3514.529837\n",
            "Epoch 8455: train loss = 2064.365112, test loss = 3514.500729\n",
            "Epoch 8456: train loss = 2064.330133, test loss = 3514.471618\n",
            "Epoch 8457: train loss = 2064.295165, test loss = 3514.442505\n",
            "Epoch 8458: train loss = 2064.260207, test loss = 3514.413390\n",
            "Epoch 8459: train loss = 2064.225261, test loss = 3514.384272\n",
            "Epoch 8460: train loss = 2064.190326, test loss = 3514.355152\n",
            "Epoch 8461: train loss = 2064.155402, test loss = 3514.326030\n",
            "Epoch 8462: train loss = 2064.120489, test loss = 3514.296906\n",
            "Epoch 8463: train loss = 2064.085586, test loss = 3514.267779\n",
            "Epoch 8464: train loss = 2064.050695, test loss = 3514.238651\n",
            "Epoch 8465: train loss = 2064.015815, test loss = 3514.209520\n",
            "Epoch 8466: train loss = 2063.980946, test loss = 3514.180387\n",
            "Epoch 8467: train loss = 2063.946087, test loss = 3514.151252\n",
            "Epoch 8468: train loss = 2063.911240, test loss = 3514.122115\n",
            "Epoch 8469: train loss = 2063.876403, test loss = 3514.092976\n",
            "Epoch 8470: train loss = 2063.841578, test loss = 3514.063834\n",
            "Epoch 8471: train loss = 2063.806763, test loss = 3514.034691\n",
            "Epoch 8472: train loss = 2063.771959, test loss = 3514.005546\n",
            "Epoch 8473: train loss = 2063.737166, test loss = 3513.976398\n",
            "Epoch 8474: train loss = 2063.702383, test loss = 3513.947249\n",
            "Epoch 8475: train loss = 2063.667612, test loss = 3513.918097\n",
            "Epoch 8476: train loss = 2063.632851, test loss = 3513.888944\n",
            "Epoch 8477: train loss = 2063.598101, test loss = 3513.859789\n",
            "Epoch 8478: train loss = 2063.563362, test loss = 3513.830632\n",
            "Epoch 8479: train loss = 2063.528633, test loss = 3513.801472\n",
            "Epoch 8480: train loss = 2063.493916, test loss = 3513.772311\n",
            "Epoch 8481: train loss = 2063.459209, test loss = 3513.743148\n",
            "Epoch 8482: train loss = 2063.424512, test loss = 3513.713983\n",
            "Epoch 8483: train loss = 2063.389827, test loss = 3513.684817\n",
            "Epoch 8484: train loss = 2063.355151, test loss = 3513.655648\n",
            "Epoch 8485: train loss = 2063.320487, test loss = 3513.626478\n",
            "Epoch 8486: train loss = 2063.285833, test loss = 3513.597306\n",
            "Epoch 8487: train loss = 2063.251190, test loss = 3513.568132\n",
            "Epoch 8488: train loss = 2063.216557, test loss = 3513.538956\n",
            "Epoch 8489: train loss = 2063.181935, test loss = 3513.509778\n",
            "Epoch 8490: train loss = 2063.147324, test loss = 3513.480599\n",
            "Epoch 8491: train loss = 2063.112723, test loss = 3513.451418\n",
            "Epoch 8492: train loss = 2063.078133, test loss = 3513.422235\n",
            "Epoch 8493: train loss = 2063.043553, test loss = 3513.393051\n",
            "Epoch 8494: train loss = 2063.008983, test loss = 3513.363865\n",
            "Epoch 8495: train loss = 2062.974424, test loss = 3513.334677\n",
            "Epoch 8496: train loss = 2062.939876, test loss = 3513.305487\n",
            "Epoch 8497: train loss = 2062.905338, test loss = 3513.276296\n",
            "Epoch 8498: train loss = 2062.870810, test loss = 3513.247104\n",
            "Epoch 8499: train loss = 2062.836293, test loss = 3513.217909\n",
            "Epoch 8500: train loss = 2062.801787, test loss = 3513.188713\n",
            "Epoch 8501: train loss = 2062.767290, test loss = 3513.159516\n",
            "Epoch 8502: train loss = 2062.732804, test loss = 3513.130317\n",
            "Epoch 8503: train loss = 2062.698329, test loss = 3513.101116\n",
            "Epoch 8504: train loss = 2062.663863, test loss = 3513.071914\n",
            "Epoch 8505: train loss = 2062.629409, test loss = 3513.042710\n",
            "Epoch 8506: train loss = 2062.594964, test loss = 3513.013505\n",
            "Epoch 8507: train loss = 2062.560530, test loss = 3512.984299\n",
            "Epoch 8508: train loss = 2062.526106, test loss = 3512.955091\n",
            "Epoch 8509: train loss = 2062.491692, test loss = 3512.925881\n",
            "Epoch 8510: train loss = 2062.457288, test loss = 3512.896670\n",
            "Epoch 8511: train loss = 2062.422895, test loss = 3512.867458\n",
            "Epoch 8512: train loss = 2062.388512, test loss = 3512.838244\n",
            "Epoch 8513: train loss = 2062.354139, test loss = 3512.809028\n",
            "Epoch 8514: train loss = 2062.319777, test loss = 3512.779812\n",
            "Epoch 8515: train loss = 2062.285424, test loss = 3512.750594\n",
            "Epoch 8516: train loss = 2062.251082, test loss = 3512.721374\n",
            "Epoch 8517: train loss = 2062.216750, test loss = 3512.692154\n",
            "Epoch 8518: train loss = 2062.182428, test loss = 3512.662932\n",
            "Epoch 8519: train loss = 2062.148116, test loss = 3512.633708\n",
            "Epoch 8520: train loss = 2062.113814, test loss = 3512.604484\n",
            "Epoch 8521: train loss = 2062.079523, test loss = 3512.575258\n",
            "Epoch 8522: train loss = 2062.045241, test loss = 3512.546031\n",
            "Epoch 8523: train loss = 2062.010969, test loss = 3512.516802\n",
            "Epoch 8524: train loss = 2061.976708, test loss = 3512.487573\n",
            "Epoch 8525: train loss = 2061.942457, test loss = 3512.458342\n",
            "Epoch 8526: train loss = 2061.908215, test loss = 3512.429110\n",
            "Epoch 8527: train loss = 2061.873984, test loss = 3512.399876\n",
            "Epoch 8528: train loss = 2061.839762, test loss = 3512.370642\n",
            "Epoch 8529: train loss = 2061.805551, test loss = 3512.341406\n",
            "Epoch 8530: train loss = 2061.771349, test loss = 3512.312170\n",
            "Epoch 8531: train loss = 2061.737158, test loss = 3512.282932\n",
            "Epoch 8532: train loss = 2061.702976, test loss = 3512.253693\n",
            "Epoch 8533: train loss = 2061.668804, test loss = 3512.224453\n",
            "Epoch 8534: train loss = 2061.634643, test loss = 3512.195211\n",
            "Epoch 8535: train loss = 2061.600491, test loss = 3512.165969\n",
            "Epoch 8536: train loss = 2061.566349, test loss = 3512.136726\n",
            "Epoch 8537: train loss = 2061.532216, test loss = 3512.107481\n",
            "Epoch 8538: train loss = 2061.498094, test loss = 3512.078236\n",
            "Epoch 8539: train loss = 2061.463981, test loss = 3512.048989\n",
            "Epoch 8540: train loss = 2061.429879, test loss = 3512.019742\n",
            "Epoch 8541: train loss = 2061.395786, test loss = 3511.990494\n",
            "Epoch 8542: train loss = 2061.361703, test loss = 3511.961244\n",
            "Epoch 8543: train loss = 2061.327629, test loss = 3511.931994\n",
            "Epoch 8544: train loss = 2061.293566, test loss = 3511.902742\n",
            "Epoch 8545: train loss = 2061.259512, test loss = 3511.873490\n",
            "Epoch 8546: train loss = 2061.225468, test loss = 3511.844237\n",
            "Epoch 8547: train loss = 2061.191433, test loss = 3511.814983\n",
            "Epoch 8548: train loss = 2061.157409, test loss = 3511.785728\n",
            "Epoch 8549: train loss = 2061.123394, test loss = 3511.756472\n",
            "Epoch 8550: train loss = 2061.089388, test loss = 3511.727215\n",
            "Epoch 8551: train loss = 2061.055393, test loss = 3511.697958\n",
            "Epoch 8552: train loss = 2061.021407, test loss = 3511.668699\n",
            "Epoch 8553: train loss = 2060.987430, test loss = 3511.639440\n",
            "Epoch 8554: train loss = 2060.953464, test loss = 3511.610180\n",
            "Epoch 8555: train loss = 2060.919506, test loss = 3511.580919\n",
            "Epoch 8556: train loss = 2060.885559, test loss = 3511.551657\n",
            "Epoch 8557: train loss = 2060.851621, test loss = 3511.522395\n",
            "Epoch 8558: train loss = 2060.817692, test loss = 3511.493132\n",
            "Epoch 8559: train loss = 2060.783773, test loss = 3511.463868\n",
            "Epoch 8560: train loss = 2060.749864, test loss = 3511.434603\n",
            "Epoch 8561: train loss = 2060.715964, test loss = 3511.405338\n",
            "Epoch 8562: train loss = 2060.682074, test loss = 3511.376071\n",
            "Epoch 8563: train loss = 2060.648193, test loss = 3511.346805\n",
            "Epoch 8564: train loss = 2060.614322, test loss = 3511.317537\n",
            "Epoch 8565: train loss = 2060.580460, test loss = 3511.288269\n",
            "Epoch 8566: train loss = 2060.546607, test loss = 3511.259000\n",
            "Epoch 8567: train loss = 2060.512764, test loss = 3511.229731\n",
            "Epoch 8568: train loss = 2060.478931, test loss = 3511.200461\n",
            "Epoch 8569: train loss = 2060.445107, test loss = 3511.171190\n",
            "Epoch 8570: train loss = 2060.411292, test loss = 3511.141919\n",
            "Epoch 8571: train loss = 2060.377487, test loss = 3511.112647\n",
            "Epoch 8572: train loss = 2060.343691, test loss = 3511.083375\n",
            "Epoch 8573: train loss = 2060.309904, test loss = 3511.054102\n",
            "Epoch 8574: train loss = 2060.276127, test loss = 3511.024828\n",
            "Epoch 8575: train loss = 2060.242359, test loss = 3510.995554\n",
            "Epoch 8576: train loss = 2060.208600, test loss = 3510.966280\n",
            "Epoch 8577: train loss = 2060.174851, test loss = 3510.937005\n",
            "Epoch 8578: train loss = 2060.141111, test loss = 3510.907729\n",
            "Epoch 8579: train loss = 2060.107380, test loss = 3510.878453\n",
            "Epoch 8580: train loss = 2060.073658, test loss = 3510.849177\n",
            "Epoch 8581: train loss = 2060.039946, test loss = 3510.819900\n",
            "Epoch 8582: train loss = 2060.006243, test loss = 3510.790622\n",
            "Epoch 8583: train loss = 2059.972549, test loss = 3510.761344\n",
            "Epoch 8584: train loss = 2059.938865, test loss = 3510.732066\n",
            "Epoch 8585: train loss = 2059.905189, test loss = 3510.702788\n",
            "Epoch 8586: train loss = 2059.871523, test loss = 3510.673509\n",
            "Epoch 8587: train loss = 2059.837866, test loss = 3510.644229\n",
            "Epoch 8588: train loss = 2059.804218, test loss = 3510.614949\n",
            "Epoch 8589: train loss = 2059.770579, test loss = 3510.585669\n",
            "Epoch 8590: train loss = 2059.736950, test loss = 3510.556389\n",
            "Epoch 8591: train loss = 2059.703329, test loss = 3510.527108\n",
            "Epoch 8592: train loss = 2059.669718, test loss = 3510.497827\n",
            "Epoch 8593: train loss = 2059.636116, test loss = 3510.468546\n",
            "Epoch 8594: train loss = 2059.602522, test loss = 3510.439264\n",
            "Epoch 8595: train loss = 2059.568938, test loss = 3510.409982\n",
            "Epoch 8596: train loss = 2059.535363, test loss = 3510.380700\n",
            "Epoch 8597: train loss = 2059.501797, test loss = 3510.351417\n",
            "Epoch 8598: train loss = 2059.468240, test loss = 3510.322135\n",
            "Epoch 8599: train loss = 2059.434692, test loss = 3510.292852\n",
            "Epoch 8600: train loss = 2059.401153, test loss = 3510.263569\n",
            "Epoch 8601: train loss = 2059.367623, test loss = 3510.234285\n",
            "Epoch 8602: train loss = 2059.334102, test loss = 3510.205002\n",
            "Epoch 8603: train loss = 2059.300590, test loss = 3510.175718\n",
            "Epoch 8604: train loss = 2059.267087, test loss = 3510.146434\n",
            "Epoch 8605: train loss = 2059.233592, test loss = 3510.117150\n",
            "Epoch 8606: train loss = 2059.200107, test loss = 3510.087866\n",
            "Epoch 8607: train loss = 2059.166631, test loss = 3510.058582\n",
            "Epoch 8608: train loss = 2059.133163, test loss = 3510.029297\n",
            "Epoch 8609: train loss = 2059.099705, test loss = 3510.000013\n",
            "Epoch 8610: train loss = 2059.066255, test loss = 3509.970728\n",
            "Epoch 8611: train loss = 2059.032814, test loss = 3509.941444\n",
            "Epoch 8612: train loss = 2058.999382, test loss = 3509.912159\n",
            "Epoch 8613: train loss = 2058.965959, test loss = 3509.882874\n",
            "Epoch 8614: train loss = 2058.932545, test loss = 3509.853589\n",
            "Epoch 8615: train loss = 2058.899139, test loss = 3509.824305\n",
            "Epoch 8616: train loss = 2058.865743, test loss = 3509.795020\n",
            "Epoch 8617: train loss = 2058.832355, test loss = 3509.765735\n",
            "Epoch 8618: train loss = 2058.798975, test loss = 3509.736450\n",
            "Epoch 8619: train loss = 2058.765605, test loss = 3509.707165\n",
            "Epoch 8620: train loss = 2058.732243, test loss = 3509.677880\n",
            "Epoch 8621: train loss = 2058.698890, test loss = 3509.648596\n",
            "Epoch 8622: train loss = 2058.665546, test loss = 3509.619311\n",
            "Epoch 8623: train loss = 2058.632211, test loss = 3509.590026\n",
            "Epoch 8624: train loss = 2058.598884, test loss = 3509.560742\n",
            "Epoch 8625: train loss = 2058.565566, test loss = 3509.531457\n",
            "Epoch 8626: train loss = 2058.532256, test loss = 3509.502173\n",
            "Epoch 8627: train loss = 2058.498956, test loss = 3509.472889\n",
            "Epoch 8628: train loss = 2058.465664, test loss = 3509.443605\n",
            "Epoch 8629: train loss = 2058.432380, test loss = 3509.414321\n",
            "Epoch 8630: train loss = 2058.399105, test loss = 3509.385037\n",
            "Epoch 8631: train loss = 2058.365839, test loss = 3509.355753\n",
            "Epoch 8632: train loss = 2058.332581, test loss = 3509.326470\n",
            "Epoch 8633: train loss = 2058.299332, test loss = 3509.297186\n",
            "Epoch 8634: train loss = 2058.266092, test loss = 3509.267903\n",
            "Epoch 8635: train loss = 2058.232860, test loss = 3509.238620\n",
            "Epoch 8636: train loss = 2058.199637, test loss = 3509.209338\n",
            "Epoch 8637: train loss = 2058.166422, test loss = 3509.180055\n",
            "Epoch 8638: train loss = 2058.133215, test loss = 3509.150773\n",
            "Epoch 8639: train loss = 2058.100018, test loss = 3509.121491\n",
            "Epoch 8640: train loss = 2058.066828, test loss = 3509.092210\n",
            "Epoch 8641: train loss = 2058.033648, test loss = 3509.062928\n",
            "Epoch 8642: train loss = 2058.000475, test loss = 3509.033647\n",
            "Epoch 8643: train loss = 2057.967311, test loss = 3509.004366\n",
            "Epoch 8644: train loss = 2057.934156, test loss = 3508.975086\n",
            "Epoch 8645: train loss = 2057.901009, test loss = 3508.945806\n",
            "Epoch 8646: train loss = 2057.867871, test loss = 3508.916526\n",
            "Epoch 8647: train loss = 2057.834740, test loss = 3508.887246\n",
            "Epoch 8648: train loss = 2057.801619, test loss = 3508.857967\n",
            "Epoch 8649: train loss = 2057.768506, test loss = 3508.828688\n",
            "Epoch 8650: train loss = 2057.735401, test loss = 3508.799410\n",
            "Epoch 8651: train loss = 2057.702304, test loss = 3508.770132\n",
            "Epoch 8652: train loss = 2057.669216, test loss = 3508.740855\n",
            "Epoch 8653: train loss = 2057.636136, test loss = 3508.711577\n",
            "Epoch 8654: train loss = 2057.603065, test loss = 3508.682301\n",
            "Epoch 8655: train loss = 2057.570001, test loss = 3508.653024\n",
            "Epoch 8656: train loss = 2057.536947, test loss = 3508.623748\n",
            "Epoch 8657: train loss = 2057.503900, test loss = 3508.594473\n",
            "Epoch 8658: train loss = 2057.470862, test loss = 3508.565198\n",
            "Epoch 8659: train loss = 2057.437832, test loss = 3508.535924\n",
            "Epoch 8660: train loss = 2057.404810, test loss = 3508.506650\n",
            "Epoch 8661: train loss = 2057.371797, test loss = 3508.477376\n",
            "Epoch 8662: train loss = 2057.338791, test loss = 3508.448103\n",
            "Epoch 8663: train loss = 2057.305794, test loss = 3508.418831\n",
            "Epoch 8664: train loss = 2057.272806, test loss = 3508.389559\n",
            "Epoch 8665: train loss = 2057.239825, test loss = 3508.360288\n",
            "Epoch 8666: train loss = 2057.206853, test loss = 3508.331017\n",
            "Epoch 8667: train loss = 2057.173888, test loss = 3508.301747\n",
            "Epoch 8668: train loss = 2057.140932, test loss = 3508.272477\n",
            "Epoch 8669: train loss = 2057.107985, test loss = 3508.243208\n",
            "Epoch 8670: train loss = 2057.075045, test loss = 3508.213940\n",
            "Epoch 8671: train loss = 2057.042113, test loss = 3508.184672\n",
            "Epoch 8672: train loss = 2057.009190, test loss = 3508.155405\n",
            "Epoch 8673: train loss = 2056.976275, test loss = 3508.126139\n",
            "Epoch 8674: train loss = 2056.943367, test loss = 3508.096873\n",
            "Epoch 8675: train loss = 2056.910468, test loss = 3508.067607\n",
            "Epoch 8676: train loss = 2056.877577, test loss = 3508.038343\n",
            "Epoch 8677: train loss = 2056.844694, test loss = 3508.009079\n",
            "Epoch 8678: train loss = 2056.811819, test loss = 3507.979816\n",
            "Epoch 8679: train loss = 2056.778953, test loss = 3507.950553\n",
            "Epoch 8680: train loss = 2056.746094, test loss = 3507.921292\n",
            "Epoch 8681: train loss = 2056.713243, test loss = 3507.892031\n",
            "Epoch 8682: train loss = 2056.680400, test loss = 3507.862770\n",
            "Epoch 8683: train loss = 2056.647565, test loss = 3507.833511\n",
            "Epoch 8684: train loss = 2056.614739, test loss = 3507.804252\n",
            "Epoch 8685: train loss = 2056.581920, test loss = 3507.774994\n",
            "Epoch 8686: train loss = 2056.549109, test loss = 3507.745737\n",
            "Epoch 8687: train loss = 2056.516306, test loss = 3507.716480\n",
            "Epoch 8688: train loss = 2056.483511, test loss = 3507.687224\n",
            "Epoch 8689: train loss = 2056.450724, test loss = 3507.657970\n",
            "Epoch 8690: train loss = 2056.417945, test loss = 3507.628715\n",
            "Epoch 8691: train loss = 2056.385174, test loss = 3507.599462\n",
            "Epoch 8692: train loss = 2056.352411, test loss = 3507.570210\n",
            "Epoch 8693: train loss = 2056.319655, test loss = 3507.540958\n",
            "Epoch 8694: train loss = 2056.286908, test loss = 3507.511708\n",
            "Epoch 8695: train loss = 2056.254168, test loss = 3507.482458\n",
            "Epoch 8696: train loss = 2056.221436, test loss = 3507.453209\n",
            "Epoch 8697: train loss = 2056.188713, test loss = 3507.423961\n",
            "Epoch 8698: train loss = 2056.155996, test loss = 3507.394714\n",
            "Epoch 8699: train loss = 2056.123288, test loss = 3507.365467\n",
            "Epoch 8700: train loss = 2056.090588, test loss = 3507.336222\n",
            "Epoch 8701: train loss = 2056.057895, test loss = 3507.306978\n",
            "Epoch 8702: train loss = 2056.025210, test loss = 3507.277734\n",
            "Epoch 8703: train loss = 2055.992533, test loss = 3507.248492\n",
            "Epoch 8704: train loss = 2055.959864, test loss = 3507.219250\n",
            "Epoch 8705: train loss = 2055.927203, test loss = 3507.190010\n",
            "Epoch 8706: train loss = 2055.894549, test loss = 3507.160770\n",
            "Epoch 8707: train loss = 2055.861903, test loss = 3507.131532\n",
            "Epoch 8708: train loss = 2055.829264, test loss = 3507.102294\n",
            "Epoch 8709: train loss = 2055.796634, test loss = 3507.073058\n",
            "Epoch 8710: train loss = 2055.764011, test loss = 3507.043822\n",
            "Epoch 8711: train loss = 2055.731396, test loss = 3507.014588\n",
            "Epoch 8712: train loss = 2055.698788, test loss = 3506.985354\n",
            "Epoch 8713: train loss = 2055.666189, test loss = 3506.956122\n",
            "Epoch 8714: train loss = 2055.633596, test loss = 3506.926891\n",
            "Epoch 8715: train loss = 2055.601012, test loss = 3506.897660\n",
            "Epoch 8716: train loss = 2055.568435, test loss = 3506.868431\n",
            "Epoch 8717: train loss = 2055.535866, test loss = 3506.839203\n",
            "Epoch 8718: train loss = 2055.503304, test loss = 3506.809976\n",
            "Epoch 8719: train loss = 2055.470750, test loss = 3506.780750\n",
            "Epoch 8720: train loss = 2055.438204, test loss = 3506.751526\n",
            "Epoch 8721: train loss = 2055.405665, test loss = 3506.722302\n",
            "Epoch 8722: train loss = 2055.373134, test loss = 3506.693080\n",
            "Epoch 8723: train loss = 2055.340610, test loss = 3506.663858\n",
            "Epoch 8724: train loss = 2055.308094, test loss = 3506.634638\n",
            "Epoch 8725: train loss = 2055.275585, test loss = 3506.605419\n",
            "Epoch 8726: train loss = 2055.243084, test loss = 3506.576202\n",
            "Epoch 8727: train loss = 2055.210591, test loss = 3506.546985\n",
            "Epoch 8728: train loss = 2055.178105, test loss = 3506.517770\n",
            "Epoch 8729: train loss = 2055.145627, test loss = 3506.488556\n",
            "Epoch 8730: train loss = 2055.113156, test loss = 3506.459343\n",
            "Epoch 8731: train loss = 2055.080692, test loss = 3506.430131\n",
            "Epoch 8732: train loss = 2055.048236, test loss = 3506.400921\n",
            "Epoch 8733: train loss = 2055.015788, test loss = 3506.371711\n",
            "Epoch 8734: train loss = 2054.983346, test loss = 3506.342504\n",
            "Epoch 8735: train loss = 2054.950913, test loss = 3506.313297\n",
            "Epoch 8736: train loss = 2054.918487, test loss = 3506.284092\n",
            "Epoch 8737: train loss = 2054.886068, test loss = 3506.254888\n",
            "Epoch 8738: train loss = 2054.853656, test loss = 3506.225685\n",
            "Epoch 8739: train loss = 2054.821253, test loss = 3506.196483\n",
            "Epoch 8740: train loss = 2054.788856, test loss = 3506.167283\n",
            "Epoch 8741: train loss = 2054.756467, test loss = 3506.138085\n",
            "Epoch 8742: train loss = 2054.724085, test loss = 3506.108887\n",
            "Epoch 8743: train loss = 2054.691710, test loss = 3506.079691\n",
            "Epoch 8744: train loss = 2054.659343, test loss = 3506.050496\n",
            "Epoch 8745: train loss = 2054.626984, test loss = 3506.021303\n",
            "Epoch 8746: train loss = 2054.594631, test loss = 3505.992111\n",
            "Epoch 8747: train loss = 2054.562286, test loss = 3505.962920\n",
            "Epoch 8748: train loss = 2054.529948, test loss = 3505.933731\n",
            "Epoch 8749: train loss = 2054.497618, test loss = 3505.904543\n",
            "Epoch 8750: train loss = 2054.465295, test loss = 3505.875357\n",
            "Epoch 8751: train loss = 2054.432979, test loss = 3505.846172\n",
            "Epoch 8752: train loss = 2054.400670, test loss = 3505.816989\n",
            "Epoch 8753: train loss = 2054.368369, test loss = 3505.787807\n",
            "Epoch 8754: train loss = 2054.336074, test loss = 3505.758626\n",
            "Epoch 8755: train loss = 2054.303788, test loss = 3505.729447\n",
            "Epoch 8756: train loss = 2054.271508, test loss = 3505.700269\n",
            "Epoch 8757: train loss = 2054.239235, test loss = 3505.671093\n",
            "Epoch 8758: train loss = 2054.206970, test loss = 3505.641919\n",
            "Epoch 8759: train loss = 2054.174712, test loss = 3505.612746\n",
            "Epoch 8760: train loss = 2054.142461, test loss = 3505.583574\n",
            "Epoch 8761: train loss = 2054.110218, test loss = 3505.554404\n",
            "Epoch 8762: train loss = 2054.077981, test loss = 3505.525235\n",
            "Epoch 8763: train loss = 2054.045752, test loss = 3505.496068\n",
            "Epoch 8764: train loss = 2054.013530, test loss = 3505.466903\n",
            "Epoch 8765: train loss = 2053.981315, test loss = 3505.437739\n",
            "Epoch 8766: train loss = 2053.949107, test loss = 3505.408576\n",
            "Epoch 8767: train loss = 2053.916906, test loss = 3505.379416\n",
            "Epoch 8768: train loss = 2053.884712, test loss = 3505.350257\n",
            "Epoch 8769: train loss = 2053.852526, test loss = 3505.321099\n",
            "Epoch 8770: train loss = 2053.820346, test loss = 3505.291943\n",
            "Epoch 8771: train loss = 2053.788174, test loss = 3505.262789\n",
            "Epoch 8772: train loss = 2053.756009, test loss = 3505.233636\n",
            "Epoch 8773: train loss = 2053.723850, test loss = 3505.204485\n",
            "Epoch 8774: train loss = 2053.691699, test loss = 3505.175335\n",
            "Epoch 8775: train loss = 2053.659555, test loss = 3505.146188\n",
            "Epoch 8776: train loss = 2053.627418, test loss = 3505.117041\n",
            "Epoch 8777: train loss = 2053.595288, test loss = 3505.087897\n",
            "Epoch 8778: train loss = 2053.563165, test loss = 3505.058754\n",
            "Epoch 8779: train loss = 2053.531049, test loss = 3505.029613\n",
            "Epoch 8780: train loss = 2053.498940, test loss = 3505.000474\n",
            "Epoch 8781: train loss = 2053.466838, test loss = 3504.971336\n",
            "Epoch 8782: train loss = 2053.434742, test loss = 3504.942200\n",
            "Epoch 8783: train loss = 2053.402654, test loss = 3504.913066\n",
            "Epoch 8784: train loss = 2053.370573, test loss = 3504.883933\n",
            "Epoch 8785: train loss = 2053.338499, test loss = 3504.854803\n",
            "Epoch 8786: train loss = 2053.306432, test loss = 3504.825674\n",
            "Epoch 8787: train loss = 2053.274371, test loss = 3504.796546\n",
            "Epoch 8788: train loss = 2053.242318, test loss = 3504.767421\n",
            "Epoch 8789: train loss = 2053.210271, test loss = 3504.738297\n",
            "Epoch 8790: train loss = 2053.178232, test loss = 3504.709175\n",
            "Epoch 8791: train loss = 2053.146199, test loss = 3504.680055\n",
            "Epoch 8792: train loss = 2053.114173, test loss = 3504.650937\n",
            "Epoch 8793: train loss = 2053.082154, test loss = 3504.621820\n",
            "Epoch 8794: train loss = 2053.050142, test loss = 3504.592706\n",
            "Epoch 8795: train loss = 2053.018137, test loss = 3504.563593\n",
            "Epoch 8796: train loss = 2052.986138, test loss = 3504.534482\n",
            "Epoch 8797: train loss = 2052.954147, test loss = 3504.505373\n",
            "Epoch 8798: train loss = 2052.922162, test loss = 3504.476266\n",
            "Epoch 8799: train loss = 2052.890184, test loss = 3504.447160\n",
            "Epoch 8800: train loss = 2052.858213, test loss = 3504.418057\n",
            "Epoch 8801: train loss = 2052.826249, test loss = 3504.388955\n",
            "Epoch 8802: train loss = 2052.794291, test loss = 3504.359855\n",
            "Epoch 8803: train loss = 2052.762341, test loss = 3504.330758\n",
            "Epoch 8804: train loss = 2052.730397, test loss = 3504.301662\n",
            "Epoch 8805: train loss = 2052.698459, test loss = 3504.272568\n",
            "Epoch 8806: train loss = 2052.666529, test loss = 3504.243476\n",
            "Epoch 8807: train loss = 2052.634605, test loss = 3504.214385\n",
            "Epoch 8808: train loss = 2052.602688, test loss = 3504.185297\n",
            "Epoch 8809: train loss = 2052.570778, test loss = 3504.156211\n",
            "Epoch 8810: train loss = 2052.538874, test loss = 3504.127127\n",
            "Epoch 8811: train loss = 2052.506978, test loss = 3504.098045\n",
            "Epoch 8812: train loss = 2052.475087, test loss = 3504.068964\n",
            "Epoch 8813: train loss = 2052.443204, test loss = 3504.039886\n",
            "Epoch 8814: train loss = 2052.411327, test loss = 3504.010810\n",
            "Epoch 8815: train loss = 2052.379457, test loss = 3503.981736\n",
            "Epoch 8816: train loss = 2052.347594, test loss = 3503.952663\n",
            "Epoch 8817: train loss = 2052.315737, test loss = 3503.923593\n",
            "Epoch 8818: train loss = 2052.283887, test loss = 3503.894525\n",
            "Epoch 8819: train loss = 2052.252043, test loss = 3503.865459\n",
            "Epoch 8820: train loss = 2052.220207, test loss = 3503.836395\n",
            "Epoch 8821: train loss = 2052.188376, test loss = 3503.807333\n",
            "Epoch 8822: train loss = 2052.156553, test loss = 3503.778273\n",
            "Epoch 8823: train loss = 2052.124736, test loss = 3503.749215\n",
            "Epoch 8824: train loss = 2052.092925, test loss = 3503.720159\n",
            "Epoch 8825: train loss = 2052.061121, test loss = 3503.691105\n",
            "Epoch 8826: train loss = 2052.029324, test loss = 3503.662054\n",
            "Epoch 8827: train loss = 2051.997533, test loss = 3503.633004\n",
            "Epoch 8828: train loss = 2051.965749, test loss = 3503.603957\n",
            "Epoch 8829: train loss = 2051.933971, test loss = 3503.574912\n",
            "Epoch 8830: train loss = 2051.902200, test loss = 3503.545869\n",
            "Epoch 8831: train loss = 2051.870436, test loss = 3503.516828\n",
            "Epoch 8832: train loss = 2051.838678, test loss = 3503.487789\n",
            "Epoch 8833: train loss = 2051.806926, test loss = 3503.458753\n",
            "Epoch 8834: train loss = 2051.775181, test loss = 3503.429718\n",
            "Epoch 8835: train loss = 2051.743443, test loss = 3503.400686\n",
            "Epoch 8836: train loss = 2051.711710, test loss = 3503.371656\n",
            "Epoch 8837: train loss = 2051.679985, test loss = 3503.342628\n",
            "Epoch 8838: train loss = 2051.648266, test loss = 3503.313602\n",
            "Epoch 8839: train loss = 2051.616553, test loss = 3503.284579\n",
            "Epoch 8840: train loss = 2051.584847, test loss = 3503.255558\n",
            "Epoch 8841: train loss = 2051.553147, test loss = 3503.226539\n",
            "Epoch 8842: train loss = 2051.521454, test loss = 3503.197522\n",
            "Epoch 8843: train loss = 2051.489767, test loss = 3503.168508\n",
            "Epoch 8844: train loss = 2051.458086, test loss = 3503.139496\n",
            "Epoch 8845: train loss = 2051.426412, test loss = 3503.110486\n",
            "Epoch 8846: train loss = 2051.394745, test loss = 3503.081478\n",
            "Epoch 8847: train loss = 2051.363083, test loss = 3503.052473\n",
            "Epoch 8848: train loss = 2051.331428, test loss = 3503.023469\n",
            "Epoch 8849: train loss = 2051.299780, test loss = 3502.994469\n",
            "Epoch 8850: train loss = 2051.268138, test loss = 3502.965470\n",
            "Epoch 8851: train loss = 2051.236502, test loss = 3502.936474\n",
            "Epoch 8852: train loss = 2051.204872, test loss = 3502.907480\n",
            "Epoch 8853: train loss = 2051.173249, test loss = 3502.878489\n",
            "Epoch 8854: train loss = 2051.141632, test loss = 3502.849499\n",
            "Epoch 8855: train loss = 2051.110022, test loss = 3502.820513\n",
            "Epoch 8856: train loss = 2051.078417, test loss = 3502.791528\n",
            "Epoch 8857: train loss = 2051.046820, test loss = 3502.762546\n",
            "Epoch 8858: train loss = 2051.015228, test loss = 3502.733566\n",
            "Epoch 8859: train loss = 2050.983643, test loss = 3502.704589\n",
            "Epoch 8860: train loss = 2050.952064, test loss = 3502.675614\n",
            "Epoch 8861: train loss = 2050.920491, test loss = 3502.646642\n",
            "Epoch 8862: train loss = 2050.888924, test loss = 3502.617671\n",
            "Epoch 8863: train loss = 2050.857364, test loss = 3502.588704\n",
            "Epoch 8864: train loss = 2050.825810, test loss = 3502.559738\n",
            "Epoch 8865: train loss = 2050.794262, test loss = 3502.530776\n",
            "Epoch 8866: train loss = 2050.762721, test loss = 3502.501815\n",
            "Epoch 8867: train loss = 2050.731185, test loss = 3502.472857\n",
            "Epoch 8868: train loss = 2050.699656, test loss = 3502.443902\n",
            "Epoch 8869: train loss = 2050.668133, test loss = 3502.414949\n",
            "Epoch 8870: train loss = 2050.636617, test loss = 3502.385998\n",
            "Epoch 8871: train loss = 2050.605106, test loss = 3502.357050\n",
            "Epoch 8872: train loss = 2050.573602, test loss = 3502.328104\n",
            "Epoch 8873: train loss = 2050.542103, test loss = 3502.299161\n",
            "Epoch 8874: train loss = 2050.510611, test loss = 3502.270221\n",
            "Epoch 8875: train loss = 2050.479126, test loss = 3502.241283\n",
            "Epoch 8876: train loss = 2050.447646, test loss = 3502.212347\n",
            "Epoch 8877: train loss = 2050.416172, test loss = 3502.183414\n",
            "Epoch 8878: train loss = 2050.384705, test loss = 3502.154483\n",
            "Epoch 8879: train loss = 2050.353243, test loss = 3502.125556\n",
            "Epoch 8880: train loss = 2050.321788, test loss = 3502.096630\n",
            "Epoch 8881: train loss = 2050.290339, test loss = 3502.067707\n",
            "Epoch 8882: train loss = 2050.258896, test loss = 3502.038787\n",
            "Epoch 8883: train loss = 2050.227459, test loss = 3502.009869\n",
            "Epoch 8884: train loss = 2050.196028, test loss = 3501.980954\n",
            "Epoch 8885: train loss = 2050.164603, test loss = 3501.952042\n",
            "Epoch 8886: train loss = 2050.133185, test loss = 3501.923132\n",
            "Epoch 8887: train loss = 2050.101772, test loss = 3501.894224\n",
            "Epoch 8888: train loss = 2050.070365, test loss = 3501.865320\n",
            "Epoch 8889: train loss = 2050.038965, test loss = 3501.836418\n",
            "Epoch 8890: train loss = 2050.007570, test loss = 3501.807518\n",
            "Epoch 8891: train loss = 2049.976182, test loss = 3501.778622\n",
            "Epoch 8892: train loss = 2049.944799, test loss = 3501.749727\n",
            "Epoch 8893: train loss = 2049.913422, test loss = 3501.720836\n",
            "Epoch 8894: train loss = 2049.882052, test loss = 3501.691947\n",
            "Epoch 8895: train loss = 2049.850687, test loss = 3501.663061\n",
            "Epoch 8896: train loss = 2049.819329, test loss = 3501.634178\n",
            "Epoch 8897: train loss = 2049.787976, test loss = 3501.605297\n",
            "Epoch 8898: train loss = 2049.756630, test loss = 3501.576419\n",
            "Epoch 8899: train loss = 2049.725289, test loss = 3501.547543\n",
            "Epoch 8900: train loss = 2049.693954, test loss = 3501.518671\n",
            "Epoch 8901: train loss = 2049.662625, test loss = 3501.489801\n",
            "Epoch 8902: train loss = 2049.631303, test loss = 3501.460934\n",
            "Epoch 8903: train loss = 2049.599986, test loss = 3501.432069\n",
            "Epoch 8904: train loss = 2049.568675, test loss = 3501.403207\n",
            "Epoch 8905: train loss = 2049.537370, test loss = 3501.374349\n",
            "Epoch 8906: train loss = 2049.506070, test loss = 3501.345492\n",
            "Epoch 8907: train loss = 2049.474777, test loss = 3501.316639\n",
            "Epoch 8908: train loss = 2049.443490, test loss = 3501.287788\n",
            "Epoch 8909: train loss = 2049.412208, test loss = 3501.258940\n",
            "Epoch 8910: train loss = 2049.380933, test loss = 3501.230095\n",
            "Epoch 8911: train loss = 2049.349663, test loss = 3501.201253\n",
            "Epoch 8912: train loss = 2049.318399, test loss = 3501.172414\n",
            "Epoch 8913: train loss = 2049.287141, test loss = 3501.143577\n",
            "Epoch 8914: train loss = 2049.255889, test loss = 3501.114743\n",
            "Epoch 8915: train loss = 2049.224642, test loss = 3501.085912\n",
            "Epoch 8916: train loss = 2049.193402, test loss = 3501.057084\n",
            "Epoch 8917: train loss = 2049.162167, test loss = 3501.028259\n",
            "Epoch 8918: train loss = 2049.130938, test loss = 3500.999436\n",
            "Epoch 8919: train loss = 2049.099715, test loss = 3500.970616\n",
            "Epoch 8920: train loss = 2049.068497, test loss = 3500.941800\n",
            "Epoch 8921: train loss = 2049.037286, test loss = 3500.912986\n",
            "Epoch 8922: train loss = 2049.006080, test loss = 3500.884175\n",
            "Epoch 8923: train loss = 2048.974880, test loss = 3500.855367\n",
            "Epoch 8924: train loss = 2048.943686, test loss = 3500.826562\n",
            "Epoch 8925: train loss = 2048.912497, test loss = 3500.797759\n",
            "Epoch 8926: train loss = 2048.881314, test loss = 3500.768960\n",
            "Epoch 8927: train loss = 2048.850137, test loss = 3500.740164\n",
            "Epoch 8928: train loss = 2048.818966, test loss = 3500.711370\n",
            "Epoch 8929: train loss = 2048.787801, test loss = 3500.682580\n",
            "Epoch 8930: train loss = 2048.756641, test loss = 3500.653792\n",
            "Epoch 8931: train loss = 2048.725487, test loss = 3500.625007\n",
            "Epoch 8932: train loss = 2048.694338, test loss = 3500.596226\n",
            "Epoch 8933: train loss = 2048.663195, test loss = 3500.567447\n",
            "Epoch 8934: train loss = 2048.632058, test loss = 3500.538671\n",
            "Epoch 8935: train loss = 2048.600927, test loss = 3500.509898\n",
            "Epoch 8936: train loss = 2048.569801, test loss = 3500.481128\n",
            "Epoch 8937: train loss = 2048.538681, test loss = 3500.452362\n",
            "Epoch 8938: train loss = 2048.507567, test loss = 3500.423598\n",
            "Epoch 8939: train loss = 2048.476458, test loss = 3500.394837\n",
            "Epoch 8940: train loss = 2048.445355, test loss = 3500.366079\n",
            "Epoch 8941: train loss = 2048.414258, test loss = 3500.337325\n",
            "Epoch 8942: train loss = 2048.383166, test loss = 3500.308573\n",
            "Epoch 8943: train loss = 2048.352080, test loss = 3500.279824\n",
            "Epoch 8944: train loss = 2048.320999, test loss = 3500.251079\n",
            "Epoch 8945: train loss = 2048.289924, test loss = 3500.222336\n",
            "Epoch 8946: train loss = 2048.258855, test loss = 3500.193597\n",
            "Epoch 8947: train loss = 2048.227791, test loss = 3500.164861\n",
            "Epoch 8948: train loss = 2048.196733, test loss = 3500.136127\n",
            "Epoch 8949: train loss = 2048.165680, test loss = 3500.107397\n",
            "Epoch 8950: train loss = 2048.134633, test loss = 3500.078670\n",
            "Epoch 8951: train loss = 2048.103592, test loss = 3500.049946\n",
            "Epoch 8952: train loss = 2048.072556, test loss = 3500.021225\n",
            "Epoch 8953: train loss = 2048.041525, test loss = 3499.992507\n",
            "Epoch 8954: train loss = 2048.010500, test loss = 3499.963793\n",
            "Epoch 8955: train loss = 2047.979481, test loss = 3499.935081\n",
            "Epoch 8956: train loss = 2047.948467, test loss = 3499.906373\n",
            "Epoch 8957: train loss = 2047.917459, test loss = 3499.877668\n",
            "Epoch 8958: train loss = 2047.886456, test loss = 3499.848966\n",
            "Epoch 8959: train loss = 2047.855459, test loss = 3499.820267\n",
            "Epoch 8960: train loss = 2047.824467, test loss = 3499.791571\n",
            "Epoch 8961: train loss = 2047.793481, test loss = 3499.762879\n",
            "Epoch 8962: train loss = 2047.762500, test loss = 3499.734189\n",
            "Epoch 8963: train loss = 2047.731525, test loss = 3499.705503\n",
            "Epoch 8964: train loss = 2047.700555, test loss = 3499.676820\n",
            "Epoch 8965: train loss = 2047.669590, test loss = 3499.648141\n",
            "Epoch 8966: train loss = 2047.638631, test loss = 3499.619464\n",
            "Epoch 8967: train loss = 2047.607678, test loss = 3499.590791\n",
            "Epoch 8968: train loss = 2047.576730, test loss = 3499.562121\n",
            "Epoch 8969: train loss = 2047.545787, test loss = 3499.533454\n",
            "Epoch 8970: train loss = 2047.514850, test loss = 3499.504790\n",
            "Epoch 8971: train loss = 2047.483918, test loss = 3499.476130\n",
            "Epoch 8972: train loss = 2047.452992, test loss = 3499.447473\n",
            "Epoch 8973: train loss = 2047.422071, test loss = 3499.418819\n",
            "Epoch 8974: train loss = 2047.391155, test loss = 3499.390169\n",
            "Epoch 8975: train loss = 2047.360245, test loss = 3499.361522\n",
            "Epoch 8976: train loss = 2047.329340, test loss = 3499.332878\n",
            "Epoch 8977: train loss = 2047.298441, test loss = 3499.304237\n",
            "Epoch 8978: train loss = 2047.267547, test loss = 3499.275600\n",
            "Epoch 8979: train loss = 2047.236658, test loss = 3499.246966\n",
            "Epoch 8980: train loss = 2047.205774, test loss = 3499.218335\n",
            "Epoch 8981: train loss = 2047.174896, test loss = 3499.189708\n",
            "Epoch 8982: train loss = 2047.144024, test loss = 3499.161084\n",
            "Epoch 8983: train loss = 2047.113156, test loss = 3499.132463\n",
            "Epoch 8984: train loss = 2047.082294, test loss = 3499.103846\n",
            "Epoch 8985: train loss = 2047.051438, test loss = 3499.075232\n",
            "Epoch 8986: train loss = 2047.020586, test loss = 3499.046622\n",
            "Epoch 8987: train loss = 2046.989740, test loss = 3499.018014\n",
            "Epoch 8988: train loss = 2046.958899, test loss = 3498.989410\n",
            "Epoch 8989: train loss = 2046.928064, test loss = 3498.960810\n",
            "Epoch 8990: train loss = 2046.897233, test loss = 3498.932213\n",
            "Epoch 8991: train loss = 2046.866408, test loss = 3498.903619\n",
            "Epoch 8992: train loss = 2046.835589, test loss = 3498.875029\n",
            "Epoch 8993: train loss = 2046.804774, test loss = 3498.846442\n",
            "Epoch 8994: train loss = 2046.773965, test loss = 3498.817859\n",
            "Epoch 8995: train loss = 2046.743161, test loss = 3498.789279\n",
            "Epoch 8996: train loss = 2046.712362, test loss = 3498.760703\n",
            "Epoch 8997: train loss = 2046.681569, test loss = 3498.732130\n",
            "Epoch 8998: train loss = 2046.650781, test loss = 3498.703560\n",
            "Epoch 8999: train loss = 2046.619998, test loss = 3498.674994\n",
            "Epoch 9000: train loss = 2046.589220, test loss = 3498.646431\n",
            "Epoch 9001: train loss = 2046.558447, test loss = 3498.617872\n",
            "Epoch 9002: train loss = 2046.527680, test loss = 3498.589316\n",
            "Epoch 9003: train loss = 2046.496918, test loss = 3498.560764\n",
            "Epoch 9004: train loss = 2046.466161, test loss = 3498.532215\n",
            "Epoch 9005: train loss = 2046.435409, test loss = 3498.503670\n",
            "Epoch 9006: train loss = 2046.404662, test loss = 3498.475128\n",
            "Epoch 9007: train loss = 2046.373921, test loss = 3498.446590\n",
            "Epoch 9008: train loss = 2046.343184, test loss = 3498.418056\n",
            "Epoch 9009: train loss = 2046.312453, test loss = 3498.389525\n",
            "Epoch 9010: train loss = 2046.281727, test loss = 3498.360997\n",
            "Epoch 9011: train loss = 2046.251006, test loss = 3498.332473\n",
            "Epoch 9012: train loss = 2046.220290, test loss = 3498.303952\n",
            "Epoch 9013: train loss = 2046.189580, test loss = 3498.275436\n",
            "Epoch 9014: train loss = 2046.158874, test loss = 3498.246922\n",
            "Epoch 9015: train loss = 2046.128174, test loss = 3498.218412\n",
            "Epoch 9016: train loss = 2046.097478, test loss = 3498.189906\n",
            "Epoch 9017: train loss = 2046.066788, test loss = 3498.161404\n",
            "Epoch 9018: train loss = 2046.036103, test loss = 3498.132905\n",
            "Epoch 9019: train loss = 2046.005423, test loss = 3498.104409\n",
            "Epoch 9020: train loss = 2045.974748, test loss = 3498.075918\n",
            "Epoch 9021: train loss = 2045.944078, test loss = 3498.047429\n",
            "Epoch 9022: train loss = 2045.913413, test loss = 3498.018945\n",
            "Epoch 9023: train loss = 2045.882754, test loss = 3497.990464\n",
            "Epoch 9024: train loss = 2045.852099, test loss = 3497.961987\n",
            "Epoch 9025: train loss = 2045.821449, test loss = 3497.933513\n",
            "Epoch 9026: train loss = 2045.790805, test loss = 3497.905043\n",
            "Epoch 9027: train loss = 2045.760165, test loss = 3497.876577\n",
            "Epoch 9028: train loss = 2045.729531, test loss = 3497.848115\n",
            "Epoch 9029: train loss = 2045.698901, test loss = 3497.819656\n",
            "Epoch 9030: train loss = 2045.668276, test loss = 3497.791200\n",
            "Epoch 9031: train loss = 2045.637657, test loss = 3497.762749\n",
            "Epoch 9032: train loss = 2045.607042, test loss = 3497.734301\n",
            "Epoch 9033: train loss = 2045.576433, test loss = 3497.705857\n",
            "Epoch 9034: train loss = 2045.545828, test loss = 3497.677417\n",
            "Epoch 9035: train loss = 2045.515229, test loss = 3497.648980\n",
            "Epoch 9036: train loss = 2045.484634, test loss = 3497.620547\n",
            "Epoch 9037: train loss = 2045.454044, test loss = 3497.592118\n",
            "Epoch 9038: train loss = 2045.423460, test loss = 3497.563692\n",
            "Epoch 9039: train loss = 2045.392880, test loss = 3497.535271\n",
            "Epoch 9040: train loss = 2045.362305, test loss = 3497.506853\n",
            "Epoch 9041: train loss = 2045.331736, test loss = 3497.478439\n",
            "Epoch 9042: train loss = 2045.301171, test loss = 3497.450028\n",
            "Epoch 9043: train loss = 2045.270611, test loss = 3497.421621\n",
            "Epoch 9044: train loss = 2045.240056, test loss = 3497.393219\n",
            "Epoch 9045: train loss = 2045.209505, test loss = 3497.364820\n",
            "Epoch 9046: train loss = 2045.178960, test loss = 3497.336424\n",
            "Epoch 9047: train loss = 2045.148420, test loss = 3497.308033\n",
            "Epoch 9048: train loss = 2045.117885, test loss = 3497.279645\n",
            "Epoch 9049: train loss = 2045.087354, test loss = 3497.251261\n",
            "Epoch 9050: train loss = 2045.056828, test loss = 3497.222882\n",
            "Epoch 9051: train loss = 2045.026308, test loss = 3497.194505\n",
            "Epoch 9052: train loss = 2044.995792, test loss = 3497.166133\n",
            "Epoch 9053: train loss = 2044.965281, test loss = 3497.137765\n",
            "Epoch 9054: train loss = 2044.934774, test loss = 3497.109400\n",
            "Epoch 9055: train loss = 2044.904273, test loss = 3497.081039\n",
            "Epoch 9056: train loss = 2044.873777, test loss = 3497.052683\n",
            "Epoch 9057: train loss = 2044.843285, test loss = 3497.024330\n",
            "Epoch 9058: train loss = 2044.812798, test loss = 3496.995980\n",
            "Epoch 9059: train loss = 2044.782316, test loss = 3496.967635\n",
            "Epoch 9060: train loss = 2044.751839, test loss = 3496.939294\n",
            "Epoch 9061: train loss = 2044.721367, test loss = 3496.910957\n",
            "Epoch 9062: train loss = 2044.690899, test loss = 3496.882623\n",
            "Epoch 9063: train loss = 2044.660436, test loss = 3496.854294\n",
            "Epoch 9064: train loss = 2044.629978, test loss = 3496.825968\n",
            "Epoch 9065: train loss = 2044.599525, test loss = 3496.797646\n",
            "Epoch 9066: train loss = 2044.569077, test loss = 3496.769329\n",
            "Epoch 9067: train loss = 2044.538633, test loss = 3496.741015\n",
            "Epoch 9068: train loss = 2044.508195, test loss = 3496.712705\n",
            "Epoch 9069: train loss = 2044.477760, test loss = 3496.684399\n",
            "Epoch 9070: train loss = 2044.447331, test loss = 3496.656097\n",
            "Epoch 9071: train loss = 2044.416907, test loss = 3496.627799\n",
            "Epoch 9072: train loss = 2044.386487, test loss = 3496.599506\n",
            "Epoch 9073: train loss = 2044.356072, test loss = 3496.571216\n",
            "Epoch 9074: train loss = 2044.325661, test loss = 3496.542930\n",
            "Epoch 9075: train loss = 2044.295256, test loss = 3496.514648\n",
            "Epoch 9076: train loss = 2044.264855, test loss = 3496.486370\n",
            "Epoch 9077: train loss = 2044.234459, test loss = 3496.458096\n",
            "Epoch 9078: train loss = 2044.204067, test loss = 3496.429826\n",
            "Epoch 9079: train loss = 2044.173680, test loss = 3496.401560\n",
            "Epoch 9080: train loss = 2044.143298, test loss = 3496.373299\n",
            "Epoch 9081: train loss = 2044.112921, test loss = 3496.345041\n",
            "Epoch 9082: train loss = 2044.082548, test loss = 3496.316787\n",
            "Epoch 9083: train loss = 2044.052180, test loss = 3496.288538\n",
            "Epoch 9084: train loss = 2044.021817, test loss = 3496.260292\n",
            "Epoch 9085: train loss = 2043.991458, test loss = 3496.232051\n",
            "Epoch 9086: train loss = 2043.961104, test loss = 3496.203814\n",
            "Epoch 9087: train loss = 2043.930755, test loss = 3496.175580\n",
            "Epoch 9088: train loss = 2043.900410, test loss = 3496.147351\n",
            "Epoch 9089: train loss = 2043.870070, test loss = 3496.119126\n",
            "Epoch 9090: train loss = 2043.839735, test loss = 3496.090905\n",
            "Epoch 9091: train loss = 2043.809404, test loss = 3496.062688\n",
            "Epoch 9092: train loss = 2043.779078, test loss = 3496.034476\n",
            "Epoch 9093: train loss = 2043.748756, test loss = 3496.006267\n",
            "Epoch 9094: train loss = 2043.718439, test loss = 3495.978063\n",
            "Epoch 9095: train loss = 2043.688127, test loss = 3495.949862\n",
            "Epoch 9096: train loss = 2043.657819, test loss = 3495.921666\n",
            "Epoch 9097: train loss = 2043.627516, test loss = 3495.893474\n",
            "Epoch 9098: train loss = 2043.597217, test loss = 3495.865287\n",
            "Epoch 9099: train loss = 2043.566923, test loss = 3495.837103\n",
            "Epoch 9100: train loss = 2043.536634, test loss = 3495.808924\n",
            "Epoch 9101: train loss = 2043.506349, test loss = 3495.780749\n",
            "Epoch 9102: train loss = 2043.476069, test loss = 3495.752578\n",
            "Epoch 9103: train loss = 2043.445793, test loss = 3495.724411\n",
            "Epoch 9104: train loss = 2043.415522, test loss = 3495.696248\n",
            "Epoch 9105: train loss = 2043.385255, test loss = 3495.668090\n",
            "Epoch 9106: train loss = 2043.354993, test loss = 3495.639936\n",
            "Epoch 9107: train loss = 2043.324735, test loss = 3495.611786\n",
            "Epoch 9108: train loss = 2043.294482, test loss = 3495.583640\n",
            "Epoch 9109: train loss = 2043.264234, test loss = 3495.555499\n",
            "Epoch 9110: train loss = 2043.233990, test loss = 3495.527362\n",
            "Epoch 9111: train loss = 2043.203750, test loss = 3495.499229\n",
            "Epoch 9112: train loss = 2043.173515, test loss = 3495.471100\n",
            "Epoch 9113: train loss = 2043.143285, test loss = 3495.442976\n",
            "Epoch 9114: train loss = 2043.113059, test loss = 3495.414856\n",
            "Epoch 9115: train loss = 2043.082837, test loss = 3495.386740\n",
            "Epoch 9116: train loss = 2043.052620, test loss = 3495.358628\n",
            "Epoch 9117: train loss = 2043.022408, test loss = 3495.330521\n",
            "Epoch 9118: train loss = 2042.992200, test loss = 3495.302418\n",
            "Epoch 9119: train loss = 2042.961996, test loss = 3495.274320\n",
            "Epoch 9120: train loss = 2042.931797, test loss = 3495.246226\n",
            "Epoch 9121: train loss = 2042.901602, test loss = 3495.218136\n",
            "Epoch 9122: train loss = 2042.871412, test loss = 3495.190050\n",
            "Epoch 9123: train loss = 2042.841226, test loss = 3495.161969\n",
            "Epoch 9124: train loss = 2042.811045, test loss = 3495.133892\n",
            "Epoch 9125: train loss = 2042.780868, test loss = 3495.105820\n",
            "Epoch 9126: train loss = 2042.750695, test loss = 3495.077752\n",
            "Epoch 9127: train loss = 2042.720527, test loss = 3495.049688\n",
            "Epoch 9128: train loss = 2042.690363, test loss = 3495.021629\n",
            "Epoch 9129: train loss = 2042.660204, test loss = 3494.993574\n",
            "Epoch 9130: train loss = 2042.630049, test loss = 3494.965523\n",
            "Epoch 9131: train loss = 2042.599899, test loss = 3494.937477\n",
            "Epoch 9132: train loss = 2042.569753, test loss = 3494.909435\n",
            "Epoch 9133: train loss = 2042.539611, test loss = 3494.881398\n",
            "Epoch 9134: train loss = 2042.509474, test loss = 3494.853365\n",
            "Epoch 9135: train loss = 2042.479341, test loss = 3494.825337\n",
            "Epoch 9136: train loss = 2042.449212, test loss = 3494.797313\n",
            "Epoch 9137: train loss = 2042.419088, test loss = 3494.769293\n",
            "Epoch 9138: train loss = 2042.388968, test loss = 3494.741278\n",
            "Epoch 9139: train loss = 2042.358852, test loss = 3494.713267\n",
            "Epoch 9140: train loss = 2042.328741, test loss = 3494.685261\n",
            "Epoch 9141: train loss = 2042.298634, test loss = 3494.657260\n",
            "Epoch 9142: train loss = 2042.268532, test loss = 3494.629262\n",
            "Epoch 9143: train loss = 2042.238433, test loss = 3494.601270\n",
            "Epoch 9144: train loss = 2042.208339, test loss = 3494.573281\n",
            "Epoch 9145: train loss = 2042.178250, test loss = 3494.545298\n",
            "Epoch 9146: train loss = 2042.148165, test loss = 3494.517318\n",
            "Epoch 9147: train loss = 2042.118084, test loss = 3494.489344\n",
            "Epoch 9148: train loss = 2042.088007, test loss = 3494.461374\n",
            "Epoch 9149: train loss = 2042.057935, test loss = 3494.433408\n",
            "Epoch 9150: train loss = 2042.027866, test loss = 3494.405447\n",
            "Epoch 9151: train loss = 2041.997803, test loss = 3494.377490\n",
            "Epoch 9152: train loss = 2041.967743, test loss = 3494.349538\n",
            "Epoch 9153: train loss = 2041.937688, test loss = 3494.321591\n",
            "Epoch 9154: train loss = 2041.907637, test loss = 3494.293648\n",
            "Epoch 9155: train loss = 2041.877590, test loss = 3494.265710\n",
            "Epoch 9156: train loss = 2041.847548, test loss = 3494.237776\n",
            "Epoch 9157: train loss = 2041.817509, test loss = 3494.209847\n",
            "Epoch 9158: train loss = 2041.787475, test loss = 3494.181922\n",
            "Epoch 9159: train loss = 2041.757446, test loss = 3494.154002\n",
            "Epoch 9160: train loss = 2041.727420, test loss = 3494.126087\n",
            "Epoch 9161: train loss = 2041.697399, test loss = 3494.098176\n",
            "Epoch 9162: train loss = 2041.667382, test loss = 3494.070270\n",
            "Epoch 9163: train loss = 2041.637369, test loss = 3494.042369\n",
            "Epoch 9164: train loss = 2041.607360, test loss = 3494.014472\n",
            "Epoch 9165: train loss = 2041.577356, test loss = 3493.986580\n",
            "Epoch 9166: train loss = 2041.547356, test loss = 3493.958692\n",
            "Epoch 9167: train loss = 2041.517359, test loss = 3493.930810\n",
            "Epoch 9168: train loss = 2041.487368, test loss = 3493.902931\n",
            "Epoch 9169: train loss = 2041.457380, test loss = 3493.875058\n",
            "Epoch 9170: train loss = 2041.427396, test loss = 3493.847189\n",
            "Epoch 9171: train loss = 2041.397417, test loss = 3493.819325\n",
            "Epoch 9172: train loss = 2041.367442, test loss = 3493.791465\n",
            "Epoch 9173: train loss = 2041.337471, test loss = 3493.763611\n",
            "Epoch 9174: train loss = 2041.307504, test loss = 3493.735761\n",
            "Epoch 9175: train loss = 2041.277541, test loss = 3493.707915\n",
            "Epoch 9176: train loss = 2041.247583, test loss = 3493.680075\n",
            "Epoch 9177: train loss = 2041.217628, test loss = 3493.652239\n",
            "Epoch 9178: train loss = 2041.187678, test loss = 3493.624408\n",
            "Epoch 9179: train loss = 2041.157732, test loss = 3493.596581\n",
            "Epoch 9180: train loss = 2041.127790, test loss = 3493.568760\n",
            "Epoch 9181: train loss = 2041.097852, test loss = 3493.540943\n",
            "Epoch 9182: train loss = 2041.067918, test loss = 3493.513131\n",
            "Epoch 9183: train loss = 2041.037988, test loss = 3493.485324\n",
            "Epoch 9184: train loss = 2041.008063, test loss = 3493.457521\n",
            "Epoch 9185: train loss = 2040.978141, test loss = 3493.429723\n",
            "Epoch 9186: train loss = 2040.948224, test loss = 3493.401930\n",
            "Epoch 9187: train loss = 2040.918310, test loss = 3493.374142\n",
            "Epoch 9188: train loss = 2040.888401, test loss = 3493.346359\n",
            "Epoch 9189: train loss = 2040.858496, test loss = 3493.318580\n",
            "Epoch 9190: train loss = 2040.828595, test loss = 3493.290807\n",
            "Epoch 9191: train loss = 2040.798698, test loss = 3493.263038\n",
            "Epoch 9192: train loss = 2040.768805, test loss = 3493.235274\n",
            "Epoch 9193: train loss = 2040.738916, test loss = 3493.207515\n",
            "Epoch 9194: train loss = 2040.709031, test loss = 3493.179760\n",
            "Epoch 9195: train loss = 2040.679150, test loss = 3493.152011\n",
            "Epoch 9196: train loss = 2040.649273, test loss = 3493.124266\n",
            "Epoch 9197: train loss = 2040.619401, test loss = 3493.096526\n",
            "Epoch 9198: train loss = 2040.589532, test loss = 3493.068792\n",
            "Epoch 9199: train loss = 2040.559667, test loss = 3493.041062\n",
            "Epoch 9200: train loss = 2040.529806, test loss = 3493.013337\n",
            "Epoch 9201: train loss = 2040.499950, test loss = 3492.985616\n",
            "Epoch 9202: train loss = 2040.470097, test loss = 3492.957901\n",
            "Epoch 9203: train loss = 2040.440248, test loss = 3492.930191\n",
            "Epoch 9204: train loss = 2040.410403, test loss = 3492.902485\n",
            "Epoch 9205: train loss = 2040.380563, test loss = 3492.874785\n",
            "Epoch 9206: train loss = 2040.350726, test loss = 3492.847089\n",
            "Epoch 9207: train loss = 2040.320893, test loss = 3492.819399\n",
            "Epoch 9208: train loss = 2040.291065, test loss = 3492.791713\n",
            "Epoch 9209: train loss = 2040.261240, test loss = 3492.764033\n",
            "Epoch 9210: train loss = 2040.231419, test loss = 3492.736357\n",
            "Epoch 9211: train loss = 2040.201602, test loss = 3492.708686\n",
            "Epoch 9212: train loss = 2040.171789, test loss = 3492.681020\n",
            "Epoch 9213: train loss = 2040.141980, test loss = 3492.653360\n",
            "Epoch 9214: train loss = 2040.112175, test loss = 3492.625704\n",
            "Epoch 9215: train loss = 2040.082374, test loss = 3492.598053\n",
            "Epoch 9216: train loss = 2040.052577, test loss = 3492.570407\n",
            "Epoch 9217: train loss = 2040.022784, test loss = 3492.542767\n",
            "Epoch 9218: train loss = 2039.992994, test loss = 3492.515131\n",
            "Epoch 9219: train loss = 2039.963209, test loss = 3492.487500\n",
            "Epoch 9220: train loss = 2039.933427, test loss = 3492.459875\n",
            "Epoch 9221: train loss = 2039.903650, test loss = 3492.432254\n",
            "Epoch 9222: train loss = 2039.873876, test loss = 3492.404639\n",
            "Epoch 9223: train loss = 2039.844106, test loss = 3492.377028\n",
            "Epoch 9224: train loss = 2039.814340, test loss = 3492.349423\n",
            "Epoch 9225: train loss = 2039.784578, test loss = 3492.321822\n",
            "Epoch 9226: train loss = 2039.754820, test loss = 3492.294227\n",
            "Epoch 9227: train loss = 2039.725066, test loss = 3492.266637\n",
            "Epoch 9228: train loss = 2039.695316, test loss = 3492.239052\n",
            "Epoch 9229: train loss = 2039.665569, test loss = 3492.211472\n",
            "Epoch 9230: train loss = 2039.635826, test loss = 3492.183897\n",
            "Epoch 9231: train loss = 2039.606088, test loss = 3492.156327\n",
            "Epoch 9232: train loss = 2039.576353, test loss = 3492.128763\n",
            "Epoch 9233: train loss = 2039.546622, test loss = 3492.101203\n",
            "Epoch 9234: train loss = 2039.516894, test loss = 3492.073649\n",
            "Epoch 9235: train loss = 2039.487171, test loss = 3492.046100\n",
            "Epoch 9236: train loss = 2039.457451, test loss = 3492.018556\n",
            "Epoch 9237: train loss = 2039.427736, test loss = 3491.991017\n",
            "Epoch 9238: train loss = 2039.398024, test loss = 3491.963483\n",
            "Epoch 9239: train loss = 2039.368316, test loss = 3491.935955\n",
            "Epoch 9240: train loss = 2039.338611, test loss = 3491.908432\n",
            "Epoch 9241: train loss = 2039.308911, test loss = 3491.880914\n",
            "Epoch 9242: train loss = 2039.279214, test loss = 3491.853401\n",
            "Epoch 9243: train loss = 2039.249521, test loss = 3491.825893\n",
            "Epoch 9244: train loss = 2039.219832, test loss = 3491.798391\n",
            "Epoch 9245: train loss = 2039.190147, test loss = 3491.770893\n",
            "Epoch 9246: train loss = 2039.160465, test loss = 3491.743401\n",
            "Epoch 9247: train loss = 2039.130787, test loss = 3491.715914\n",
            "Epoch 9248: train loss = 2039.101113, test loss = 3491.688433\n",
            "Epoch 9249: train loss = 2039.071443, test loss = 3491.660957\n",
            "Epoch 9250: train loss = 2039.041777, test loss = 3491.633486\n",
            "Epoch 9251: train loss = 2039.012114, test loss = 3491.606020\n",
            "Epoch 9252: train loss = 2038.982455, test loss = 3491.578559\n",
            "Epoch 9253: train loss = 2038.952800, test loss = 3491.551104\n",
            "Epoch 9254: train loss = 2038.923148, test loss = 3491.523654\n",
            "Epoch 9255: train loss = 2038.893500, test loss = 3491.496210\n",
            "Epoch 9256: train loss = 2038.863856, test loss = 3491.468770\n",
            "Epoch 9257: train loss = 2038.834216, test loss = 3491.441336\n",
            "Epoch 9258: train loss = 2038.804580, test loss = 3491.413907\n",
            "Epoch 9259: train loss = 2038.774947, test loss = 3491.386484\n",
            "Epoch 9260: train loss = 2038.745318, test loss = 3491.359066\n",
            "Epoch 9261: train loss = 2038.715692, test loss = 3491.331653\n",
            "Epoch 9262: train loss = 2038.686070, test loss = 3491.304246\n",
            "Epoch 9263: train loss = 2038.656452, test loss = 3491.276844\n",
            "Epoch 9264: train loss = 2038.626838, test loss = 3491.249447\n",
            "Epoch 9265: train loss = 2038.597227, test loss = 3491.222056\n",
            "Epoch 9266: train loss = 2038.567620, test loss = 3491.194670\n",
            "Epoch 9267: train loss = 2038.538017, test loss = 3491.167290\n",
            "Epoch 9268: train loss = 2038.508418, test loss = 3491.139915\n",
            "Epoch 9269: train loss = 2038.478822, test loss = 3491.112545\n",
            "Epoch 9270: train loss = 2038.449229, test loss = 3491.085181\n",
            "Epoch 9271: train loss = 2038.419641, test loss = 3491.057822\n",
            "Epoch 9272: train loss = 2038.390056, test loss = 3491.030468\n",
            "Epoch 9273: train loss = 2038.360474, test loss = 3491.003120\n",
            "Epoch 9274: train loss = 2038.330897, test loss = 3490.975778\n",
            "Epoch 9275: train loss = 2038.301323, test loss = 3490.948440\n",
            "Epoch 9276: train loss = 2038.271752, test loss = 3490.921109\n",
            "Epoch 9277: train loss = 2038.242185, test loss = 3490.893783\n",
            "Epoch 9278: train loss = 2038.212622, test loss = 3490.866462\n",
            "Epoch 9279: train loss = 2038.183063, test loss = 3490.839146\n",
            "Epoch 9280: train loss = 2038.153507, test loss = 3490.811837\n",
            "Epoch 9281: train loss = 2038.123954, test loss = 3490.784532\n",
            "Epoch 9282: train loss = 2038.094406, test loss = 3490.757233\n",
            "Epoch 9283: train loss = 2038.064861, test loss = 3490.729940\n",
            "Epoch 9284: train loss = 2038.035319, test loss = 3490.702652\n",
            "Epoch 9285: train loss = 2038.005781, test loss = 3490.675370\n",
            "Epoch 9286: train loss = 2037.976247, test loss = 3490.648093\n",
            "Epoch 9287: train loss = 2037.946716, test loss = 3490.620822\n",
            "Epoch 9288: train loss = 2037.917189, test loss = 3490.593556\n",
            "Epoch 9289: train loss = 2037.887665, test loss = 3490.566296\n",
            "Epoch 9290: train loss = 2037.858145, test loss = 3490.539042\n",
            "Epoch 9291: train loss = 2037.828629, test loss = 3490.511793\n",
            "Epoch 9292: train loss = 2037.799116, test loss = 3490.484549\n",
            "Epoch 9293: train loss = 2037.769606, test loss = 3490.457311\n",
            "Epoch 9294: train loss = 2037.740100, test loss = 3490.430079\n",
            "Epoch 9295: train loss = 2037.710598, test loss = 3490.402852\n",
            "Epoch 9296: train loss = 2037.681099, test loss = 3490.375631\n",
            "Epoch 9297: train loss = 2037.651604, test loss = 3490.348416\n",
            "Epoch 9298: train loss = 2037.622113, test loss = 3490.321206\n",
            "Epoch 9299: train loss = 2037.592624, test loss = 3490.294002\n",
            "Epoch 9300: train loss = 2037.563140, test loss = 3490.266803\n",
            "Epoch 9301: train loss = 2037.533659, test loss = 3490.239610\n",
            "Epoch 9302: train loss = 2037.504181, test loss = 3490.212423\n",
            "Epoch 9303: train loss = 2037.474707, test loss = 3490.185241\n",
            "Epoch 9304: train loss = 2037.445236, test loss = 3490.158065\n",
            "Epoch 9305: train loss = 2037.415769, test loss = 3490.130895\n",
            "Epoch 9306: train loss = 2037.386305, test loss = 3490.103730\n",
            "Epoch 9307: train loss = 2037.356845, test loss = 3490.076571\n",
            "Epoch 9308: train loss = 2037.327389, test loss = 3490.049418\n",
            "Epoch 9309: train loss = 2037.297935, test loss = 3490.022270\n",
            "Epoch 9310: train loss = 2037.268486, test loss = 3489.995129\n",
            "Epoch 9311: train loss = 2037.239039, test loss = 3489.967992\n",
            "Epoch 9312: train loss = 2037.209597, test loss = 3489.940862\n",
            "Epoch 9313: train loss = 2037.180157, test loss = 3489.913737\n",
            "Epoch 9314: train loss = 2037.150721, test loss = 3489.886618\n",
            "Epoch 9315: train loss = 2037.121289, test loss = 3489.859505\n",
            "Epoch 9316: train loss = 2037.091860, test loss = 3489.832398\n",
            "Epoch 9317: train loss = 2037.062435, test loss = 3489.805296\n",
            "Epoch 9318: train loss = 2037.033012, test loss = 3489.778200\n",
            "Epoch 9319: train loss = 2037.003594, test loss = 3489.751110\n",
            "Epoch 9320: train loss = 2036.974179, test loss = 3489.724026\n",
            "Epoch 9321: train loss = 2036.944767, test loss = 3489.696947\n",
            "Epoch 9322: train loss = 2036.915358, test loss = 3489.669875\n",
            "Epoch 9323: train loss = 2036.885953, test loss = 3489.642808\n",
            "Epoch 9324: train loss = 2036.856552, test loss = 3489.615747\n",
            "Epoch 9325: train loss = 2036.827153, test loss = 3489.588691\n",
            "Epoch 9326: train loss = 2036.797759, test loss = 3489.561642\n",
            "Epoch 9327: train loss = 2036.768367, test loss = 3489.534598\n",
            "Epoch 9328: train loss = 2036.738979, test loss = 3489.507561\n",
            "Epoch 9329: train loss = 2036.709595, test loss = 3489.480529\n",
            "Epoch 9330: train loss = 2036.680213, test loss = 3489.453503\n",
            "Epoch 9331: train loss = 2036.650836, test loss = 3489.426482\n",
            "Epoch 9332: train loss = 2036.621461, test loss = 3489.399468\n",
            "Epoch 9333: train loss = 2036.592090, test loss = 3489.372460\n",
            "Epoch 9334: train loss = 2036.562722, test loss = 3489.345457\n",
            "Epoch 9335: train loss = 2036.533358, test loss = 3489.318461\n",
            "Epoch 9336: train loss = 2036.503997, test loss = 3489.291470\n",
            "Epoch 9337: train loss = 2036.474639, test loss = 3489.264485\n",
            "Epoch 9338: train loss = 2036.445285, test loss = 3489.237506\n",
            "Epoch 9339: train loss = 2036.415934, test loss = 3489.210534\n",
            "Epoch 9340: train loss = 2036.386586, test loss = 3489.183567\n",
            "Epoch 9341: train loss = 2036.357242, test loss = 3489.156606\n",
            "Epoch 9342: train loss = 2036.327900, test loss = 3489.129651\n",
            "Epoch 9343: train loss = 2036.298563, test loss = 3489.102701\n",
            "Epoch 9344: train loss = 2036.269228, test loss = 3489.075758\n",
            "Epoch 9345: train loss = 2036.239897, test loss = 3489.048821\n",
            "Epoch 9346: train loss = 2036.210569, test loss = 3489.021890\n",
            "Epoch 9347: train loss = 2036.181245, test loss = 3488.994965\n",
            "Epoch 9348: train loss = 2036.151924, test loss = 3488.968046\n",
            "Epoch 9349: train loss = 2036.122606, test loss = 3488.941133\n",
            "Epoch 9350: train loss = 2036.093291, test loss = 3488.914226\n",
            "Epoch 9351: train loss = 2036.063980, test loss = 3488.887325\n",
            "Epoch 9352: train loss = 2036.034672, test loss = 3488.860430\n",
            "Epoch 9353: train loss = 2036.005367, test loss = 3488.833541\n",
            "Epoch 9354: train loss = 2035.976066, test loss = 3488.806658\n",
            "Epoch 9355: train loss = 2035.946767, test loss = 3488.779781\n",
            "Epoch 9356: train loss = 2035.917473, test loss = 3488.752910\n",
            "Epoch 9357: train loss = 2035.888181, test loss = 3488.726045\n",
            "Epoch 9358: train loss = 2035.858892, test loss = 3488.699187\n",
            "Epoch 9359: train loss = 2035.829607, test loss = 3488.672334\n",
            "Epoch 9360: train loss = 2035.800325, test loss = 3488.645488\n",
            "Epoch 9361: train loss = 2035.771046, test loss = 3488.618647\n",
            "Epoch 9362: train loss = 2035.741771, test loss = 3488.591813\n",
            "Epoch 9363: train loss = 2035.712499, test loss = 3488.564985\n",
            "Epoch 9364: train loss = 2035.683230, test loss = 3488.538163\n",
            "Epoch 9365: train loss = 2035.653964, test loss = 3488.511347\n",
            "Epoch 9366: train loss = 2035.624701, test loss = 3488.484538\n",
            "Epoch 9367: train loss = 2035.595442, test loss = 3488.457734\n",
            "Epoch 9368: train loss = 2035.566186, test loss = 3488.430937\n",
            "Epoch 9369: train loss = 2035.536933, test loss = 3488.404146\n",
            "Epoch 9370: train loss = 2035.507683, test loss = 3488.377361\n",
            "Epoch 9371: train loss = 2035.478437, test loss = 3488.350582\n",
            "Epoch 9372: train loss = 2035.449193, test loss = 3488.323809\n",
            "Epoch 9373: train loss = 2035.419953, test loss = 3488.297043\n",
            "Epoch 9374: train loss = 2035.390716, test loss = 3488.270283\n",
            "Epoch 9375: train loss = 2035.361482, test loss = 3488.243529\n",
            "Epoch 9376: train loss = 2035.332251, test loss = 3488.216781\n",
            "Epoch 9377: train loss = 2035.303024, test loss = 3488.190040\n",
            "Epoch 9378: train loss = 2035.273800, test loss = 3488.163305\n",
            "Epoch 9379: train loss = 2035.244579, test loss = 3488.136576\n",
            "Epoch 9380: train loss = 2035.215361, test loss = 3488.109853\n",
            "Epoch 9381: train loss = 2035.186146, test loss = 3488.083137\n",
            "Epoch 9382: train loss = 2035.156934, test loss = 3488.056426\n",
            "Epoch 9383: train loss = 2035.127725, test loss = 3488.029723\n",
            "Epoch 9384: train loss = 2035.098520, test loss = 3488.003025\n",
            "Epoch 9385: train loss = 2035.069318, test loss = 3487.976334\n",
            "Epoch 9386: train loss = 2035.040119, test loss = 3487.949649\n",
            "Epoch 9387: train loss = 2035.010922, test loss = 3487.922970\n",
            "Epoch 9388: train loss = 2034.981730, test loss = 3487.896298\n",
            "Epoch 9389: train loss = 2034.952540, test loss = 3487.869632\n",
            "Epoch 9390: train loss = 2034.923353, test loss = 3487.842972\n",
            "Epoch 9391: train loss = 2034.894170, test loss = 3487.816319\n",
            "Epoch 9392: train loss = 2034.864989, test loss = 3487.789672\n",
            "Epoch 9393: train loss = 2034.835812, test loss = 3487.763032\n",
            "Epoch 9394: train loss = 2034.806637, test loss = 3487.736398\n",
            "Epoch 9395: train loss = 2034.777466, test loss = 3487.709770\n",
            "Epoch 9396: train loss = 2034.748298, test loss = 3487.683149\n",
            "Epoch 9397: train loss = 2034.719133, test loss = 3487.656534\n",
            "Epoch 9398: train loss = 2034.689971, test loss = 3487.629925\n",
            "Epoch 9399: train loss = 2034.660812, test loss = 3487.603323\n",
            "Epoch 9400: train loss = 2034.631656, test loss = 3487.576727\n",
            "Epoch 9401: train loss = 2034.602504, test loss = 3487.550138\n",
            "Epoch 9402: train loss = 2034.573354, test loss = 3487.523555\n",
            "Epoch 9403: train loss = 2034.544207, test loss = 3487.496979\n",
            "Epoch 9404: train loss = 2034.515064, test loss = 3487.470409\n",
            "Epoch 9405: train loss = 2034.485923, test loss = 3487.443846\n",
            "Epoch 9406: train loss = 2034.456786, test loss = 3487.417289\n",
            "Epoch 9407: train loss = 2034.427651, test loss = 3487.390738\n",
            "Epoch 9408: train loss = 2034.398520, test loss = 3487.364194\n",
            "Epoch 9409: train loss = 2034.369392, test loss = 3487.337657\n",
            "Epoch 9410: train loss = 2034.340266, test loss = 3487.311126\n",
            "Epoch 9411: train loss = 2034.311144, test loss = 3487.284602\n",
            "Epoch 9412: train loss = 2034.282025, test loss = 3487.258084\n",
            "Epoch 9413: train loss = 2034.252908, test loss = 3487.231573\n",
            "Epoch 9414: train loss = 2034.223795, test loss = 3487.205068\n",
            "Epoch 9415: train loss = 2034.194685, test loss = 3487.178570\n",
            "Epoch 9416: train loss = 2034.165577, test loss = 3487.152078\n",
            "Epoch 9417: train loss = 2034.136473, test loss = 3487.125593\n",
            "Epoch 9418: train loss = 2034.107372, test loss = 3487.099114\n",
            "Epoch 9419: train loss = 2034.078274, test loss = 3487.072643\n",
            "Epoch 9420: train loss = 2034.049178, test loss = 3487.046177\n",
            "Epoch 9421: train loss = 2034.020086, test loss = 3487.019719\n",
            "Epoch 9422: train loss = 2033.990997, test loss = 3486.993266\n",
            "Epoch 9423: train loss = 2033.961910, test loss = 3486.966821\n",
            "Epoch 9424: train loss = 2033.932827, test loss = 3486.940382\n",
            "Epoch 9425: train loss = 2033.903746, test loss = 3486.913950\n",
            "Epoch 9426: train loss = 2033.874669, test loss = 3486.887525\n",
            "Epoch 9427: train loss = 2033.845594, test loss = 3486.861106\n",
            "Epoch 9428: train loss = 2033.816522, test loss = 3486.834694\n",
            "Epoch 9429: train loss = 2033.787454, test loss = 3486.808288\n",
            "Epoch 9430: train loss = 2033.758388, test loss = 3486.781889\n",
            "Epoch 9431: train loss = 2033.729325, test loss = 3486.755497\n",
            "Epoch 9432: train loss = 2033.700265, test loss = 3486.729112\n",
            "Epoch 9433: train loss = 2033.671208, test loss = 3486.702733\n",
            "Epoch 9434: train loss = 2033.642154, test loss = 3486.676361\n",
            "Epoch 9435: train loss = 2033.613103, test loss = 3486.649996\n",
            "Epoch 9436: train loss = 2033.584055, test loss = 3486.623637\n",
            "Epoch 9437: train loss = 2033.555010, test loss = 3486.597285\n",
            "Epoch 9438: train loss = 2033.525968, test loss = 3486.570940\n",
            "Epoch 9439: train loss = 2033.496928, test loss = 3486.544602\n",
            "Epoch 9440: train loss = 2033.467892, test loss = 3486.518271\n",
            "Epoch 9441: train loss = 2033.438858, test loss = 3486.491946\n",
            "Epoch 9442: train loss = 2033.409827, test loss = 3486.465628\n",
            "Epoch 9443: train loss = 2033.380799, test loss = 3486.439317\n",
            "Epoch 9444: train loss = 2033.351774, test loss = 3486.413013\n",
            "Epoch 9445: train loss = 2033.322752, test loss = 3486.386715\n",
            "Epoch 9446: train loss = 2033.293733, test loss = 3486.360424\n",
            "Epoch 9447: train loss = 2033.264716, test loss = 3486.334141\n",
            "Epoch 9448: train loss = 2033.235703, test loss = 3486.307864\n",
            "Epoch 9449: train loss = 2033.206692, test loss = 3486.281594\n",
            "Epoch 9450: train loss = 2033.177685, test loss = 3486.255330\n",
            "Epoch 9451: train loss = 2033.148680, test loss = 3486.229074\n",
            "Epoch 9452: train loss = 2033.119677, test loss = 3486.202824\n",
            "Epoch 9453: train loss = 2033.090678, test loss = 3486.176582\n",
            "Epoch 9454: train loss = 2033.061682, test loss = 3486.150346\n",
            "Epoch 9455: train loss = 2033.032688, test loss = 3486.124117\n",
            "Epoch 9456: train loss = 2033.003697, test loss = 3486.097895\n",
            "Epoch 9457: train loss = 2032.974710, test loss = 3486.071680\n",
            "Epoch 9458: train loss = 2032.945724, test loss = 3486.045472\n",
            "Epoch 9459: train loss = 2032.916742, test loss = 3486.019271\n",
            "Epoch 9460: train loss = 2032.887763, test loss = 3485.993077\n",
            "Epoch 9461: train loss = 2032.858786, test loss = 3485.966890\n",
            "Epoch 9462: train loss = 2032.829812, test loss = 3485.940710\n",
            "Epoch 9463: train loss = 2032.800841, test loss = 3485.914536\n",
            "Epoch 9464: train loss = 2032.771873, test loss = 3485.888370\n",
            "Epoch 9465: train loss = 2032.742907, test loss = 3485.862211\n",
            "Epoch 9466: train loss = 2032.713945, test loss = 3485.836059\n",
            "Epoch 9467: train loss = 2032.684985, test loss = 3485.809914\n",
            "Epoch 9468: train loss = 2032.656028, test loss = 3485.783775\n",
            "Epoch 9469: train loss = 2032.627073, test loss = 3485.757644\n",
            "Epoch 9470: train loss = 2032.598122, test loss = 3485.731520\n",
            "Epoch 9471: train loss = 2032.569173, test loss = 3485.705403\n",
            "Epoch 9472: train loss = 2032.540227, test loss = 3485.679293\n",
            "Epoch 9473: train loss = 2032.511284, test loss = 3485.653190\n",
            "Epoch 9474: train loss = 2032.482343, test loss = 3485.627094\n",
            "Epoch 9475: train loss = 2032.453406, test loss = 3485.601005\n",
            "Epoch 9476: train loss = 2032.424471, test loss = 3485.574924\n",
            "Epoch 9477: train loss = 2032.395538, test loss = 3485.548849\n",
            "Epoch 9478: train loss = 2032.366609, test loss = 3485.522782\n",
            "Epoch 9479: train loss = 2032.337682, test loss = 3485.496721\n",
            "Epoch 9480: train loss = 2032.308758, test loss = 3485.470668\n",
            "Epoch 9481: train loss = 2032.279837, test loss = 3485.444622\n",
            "Epoch 9482: train loss = 2032.250918, test loss = 3485.418583\n",
            "Epoch 9483: train loss = 2032.222002, test loss = 3485.392551\n",
            "Epoch 9484: train loss = 2032.193089, test loss = 3485.366527\n",
            "Epoch 9485: train loss = 2032.164179, test loss = 3485.340509\n",
            "Epoch 9486: train loss = 2032.135271, test loss = 3485.314499\n",
            "Epoch 9487: train loss = 2032.106366, test loss = 3485.288496\n",
            "Epoch 9488: train loss = 2032.077464, test loss = 3485.262500\n",
            "Epoch 9489: train loss = 2032.048564, test loss = 3485.236512\n",
            "Epoch 9490: train loss = 2032.019667, test loss = 3485.210531\n",
            "Epoch 9491: train loss = 2031.990773, test loss = 3485.184557\n",
            "Epoch 9492: train loss = 2031.961881, test loss = 3485.158590\n",
            "Epoch 9493: train loss = 2031.932992, test loss = 3485.132630\n",
            "Epoch 9494: train loss = 2031.904106, test loss = 3485.106678\n",
            "Epoch 9495: train loss = 2031.875223, test loss = 3485.080733\n",
            "Epoch 9496: train loss = 2031.846342, test loss = 3485.054795\n",
            "Epoch 9497: train loss = 2031.817464, test loss = 3485.028864\n",
            "Epoch 9498: train loss = 2031.788588, test loss = 3485.002941\n",
            "Epoch 9499: train loss = 2031.759715, test loss = 3484.977025\n",
            "Epoch 9500: train loss = 2031.730845, test loss = 3484.951117\n",
            "Epoch 9501: train loss = 2031.701977, test loss = 3484.925216\n",
            "Epoch 9502: train loss = 2031.673112, test loss = 3484.899322\n",
            "Epoch 9503: train loss = 2031.644250, test loss = 3484.873435\n",
            "Epoch 9504: train loss = 2031.615390, test loss = 3484.847556\n",
            "Epoch 9505: train loss = 2031.586533, test loss = 3484.821684\n",
            "Epoch 9506: train loss = 2031.557679, test loss = 3484.795820\n",
            "Epoch 9507: train loss = 2031.528827, test loss = 3484.769963\n",
            "Epoch 9508: train loss = 2031.499978, test loss = 3484.744113\n",
            "Epoch 9509: train loss = 2031.471131, test loss = 3484.718271\n",
            "Epoch 9510: train loss = 2031.442287, test loss = 3484.692436\n",
            "Epoch 9511: train loss = 2031.413446, test loss = 3484.666609\n",
            "Epoch 9512: train loss = 2031.384607, test loss = 3484.640789\n",
            "Epoch 9513: train loss = 2031.355771, test loss = 3484.614977\n",
            "Epoch 9514: train loss = 2031.326937, test loss = 3484.589172\n",
            "Epoch 9515: train loss = 2031.298106, test loss = 3484.563374\n",
            "Epoch 9516: train loss = 2031.269278, test loss = 3484.537584\n",
            "Epoch 9517: train loss = 2031.240452, test loss = 3484.511801\n",
            "Epoch 9518: train loss = 2031.211629, test loss = 3484.486026\n",
            "Epoch 9519: train loss = 2031.182808, test loss = 3484.460259\n",
            "Epoch 9520: train loss = 2031.153990, test loss = 3484.434499\n",
            "Epoch 9521: train loss = 2031.125175, test loss = 3484.408746\n",
            "Epoch 9522: train loss = 2031.096362, test loss = 3484.383001\n",
            "Epoch 9523: train loss = 2031.067551, test loss = 3484.357264\n",
            "Epoch 9524: train loss = 2031.038744, test loss = 3484.331534\n",
            "Epoch 9525: train loss = 2031.009938, test loss = 3484.305812\n",
            "Epoch 9526: train loss = 2030.981136, test loss = 3484.280097\n",
            "Epoch 9527: train loss = 2030.952335, test loss = 3484.254390\n",
            "Epoch 9528: train loss = 2030.923538, test loss = 3484.228690\n",
            "Epoch 9529: train loss = 2030.894743, test loss = 3484.202998\n",
            "Epoch 9530: train loss = 2030.865950, test loss = 3484.177314\n",
            "Epoch 9531: train loss = 2030.837160, test loss = 3484.151637\n",
            "Epoch 9532: train loss = 2030.808372, test loss = 3484.125968\n",
            "Epoch 9533: train loss = 2030.779587, test loss = 3484.100307\n",
            "Epoch 9534: train loss = 2030.750805, test loss = 3484.074653\n",
            "Epoch 9535: train loss = 2030.722025, test loss = 3484.049007\n",
            "Epoch 9536: train loss = 2030.693247, test loss = 3484.023368\n",
            "Epoch 9537: train loss = 2030.664472, test loss = 3483.997737\n",
            "Epoch 9538: train loss = 2030.635700, test loss = 3483.972114\n",
            "Epoch 9539: train loss = 2030.606930, test loss = 3483.946499\n",
            "Epoch 9540: train loss = 2030.578162, test loss = 3483.920891\n",
            "Epoch 9541: train loss = 2030.549397, test loss = 3483.895291\n",
            "Epoch 9542: train loss = 2030.520634, test loss = 3483.869699\n",
            "Epoch 9543: train loss = 2030.491874, test loss = 3483.844115\n",
            "Epoch 9544: train loss = 2030.463117, test loss = 3483.818538\n",
            "Epoch 9545: train loss = 2030.434361, test loss = 3483.792969\n",
            "Epoch 9546: train loss = 2030.405609, test loss = 3483.767408\n",
            "Epoch 9547: train loss = 2030.376859, test loss = 3483.741855\n",
            "Epoch 9548: train loss = 2030.348111, test loss = 3483.716309\n",
            "Epoch 9549: train loss = 2030.319365, test loss = 3483.690771\n",
            "Epoch 9550: train loss = 2030.290623, test loss = 3483.665241\n",
            "Epoch 9551: train loss = 2030.261882, test loss = 3483.639719\n",
            "Epoch 9552: train loss = 2030.233144, test loss = 3483.614205\n",
            "Epoch 9553: train loss = 2030.204409, test loss = 3483.588699\n",
            "Epoch 9554: train loss = 2030.175676, test loss = 3483.563200\n",
            "Epoch 9555: train loss = 2030.146945, test loss = 3483.537709\n",
            "Epoch 9556: train loss = 2030.118217, test loss = 3483.512227\n",
            "Epoch 9557: train loss = 2030.089491, test loss = 3483.486752\n",
            "Epoch 9558: train loss = 2030.060767, test loss = 3483.461285\n",
            "Epoch 9559: train loss = 2030.032046, test loss = 3483.435826\n",
            "Epoch 9560: train loss = 2030.003328, test loss = 3483.410374\n",
            "Epoch 9561: train loss = 2029.974612, test loss = 3483.384931\n",
            "Epoch 9562: train loss = 2029.945898, test loss = 3483.359496\n",
            "Epoch 9563: train loss = 2029.917186, test loss = 3483.334069\n",
            "Epoch 9564: train loss = 2029.888477, test loss = 3483.308649\n",
            "Epoch 9565: train loss = 2029.859771, test loss = 3483.283238\n",
            "Epoch 9566: train loss = 2029.831066, test loss = 3483.257834\n",
            "Epoch 9567: train loss = 2029.802365, test loss = 3483.232439\n",
            "Epoch 9568: train loss = 2029.773665, test loss = 3483.207051\n",
            "Epoch 9569: train loss = 2029.744968, test loss = 3483.181672\n",
            "Epoch 9570: train loss = 2029.716273, test loss = 3483.156301\n",
            "Epoch 9571: train loss = 2029.687581, test loss = 3483.130937\n",
            "Epoch 9572: train loss = 2029.658891, test loss = 3483.105582\n",
            "Epoch 9573: train loss = 2029.630203, test loss = 3483.080235\n",
            "Epoch 9574: train loss = 2029.601518, test loss = 3483.054896\n",
            "Epoch 9575: train loss = 2029.572835, test loss = 3483.029564\n",
            "Epoch 9576: train loss = 2029.544154, test loss = 3483.004241\n",
            "Epoch 9577: train loss = 2029.515476, test loss = 3482.978927\n",
            "Epoch 9578: train loss = 2029.486800, test loss = 3482.953620\n",
            "Epoch 9579: train loss = 2029.458127, test loss = 3482.928321\n",
            "Epoch 9580: train loss = 2029.429455, test loss = 3482.903031\n",
            "Epoch 9581: train loss = 2029.400787, test loss = 3482.877748\n",
            "Epoch 9582: train loss = 2029.372120, test loss = 3482.852474\n",
            "Epoch 9583: train loss = 2029.343456, test loss = 3482.827208\n",
            "Epoch 9584: train loss = 2029.314794, test loss = 3482.801950\n",
            "Epoch 9585: train loss = 2029.286134, test loss = 3482.776700\n",
            "Epoch 9586: train loss = 2029.257477, test loss = 3482.751459\n",
            "Epoch 9587: train loss = 2029.228822, test loss = 3482.726225\n",
            "Epoch 9588: train loss = 2029.200169, test loss = 3482.701000\n",
            "Epoch 9589: train loss = 2029.171518, test loss = 3482.675784\n",
            "Epoch 9590: train loss = 2029.142870, test loss = 3482.650575\n",
            "Epoch 9591: train loss = 2029.114224, test loss = 3482.625375\n",
            "Epoch 9592: train loss = 2029.085581, test loss = 3482.600182\n",
            "Epoch 9593: train loss = 2029.056939, test loss = 3482.574999\n",
            "Epoch 9594: train loss = 2029.028300, test loss = 3482.549823\n",
            "Epoch 9595: train loss = 2028.999664, test loss = 3482.524656\n",
            "Epoch 9596: train loss = 2028.971029, test loss = 3482.499497\n",
            "Epoch 9597: train loss = 2028.942397, test loss = 3482.474346\n",
            "Epoch 9598: train loss = 2028.913767, test loss = 3482.449204\n",
            "Epoch 9599: train loss = 2028.885139, test loss = 3482.424070\n",
            "Epoch 9600: train loss = 2028.856514, test loss = 3482.398945\n",
            "Epoch 9601: train loss = 2028.827890, test loss = 3482.373827\n",
            "Epoch 9602: train loss = 2028.799269, test loss = 3482.348719\n",
            "Epoch 9603: train loss = 2028.770650, test loss = 3482.323618\n",
            "Epoch 9604: train loss = 2028.742034, test loss = 3482.298526\n",
            "Epoch 9605: train loss = 2028.713420, test loss = 3482.273442\n",
            "Epoch 9606: train loss = 2028.684808, test loss = 3482.248367\n",
            "Epoch 9607: train loss = 2028.656198, test loss = 3482.223300\n",
            "Epoch 9608: train loss = 2028.627590, test loss = 3482.198242\n",
            "Epoch 9609: train loss = 2028.598985, test loss = 3482.173192\n",
            "Epoch 9610: train loss = 2028.570381, test loss = 3482.148151\n",
            "Epoch 9611: train loss = 2028.541780, test loss = 3482.123118\n",
            "Epoch 9612: train loss = 2028.513182, test loss = 3482.098093\n",
            "Epoch 9613: train loss = 2028.484585, test loss = 3482.073077\n",
            "Epoch 9614: train loss = 2028.455991, test loss = 3482.048070\n",
            "Epoch 9615: train loss = 2028.427398, test loss = 3482.023071\n",
            "Epoch 9616: train loss = 2028.398808, test loss = 3481.998081\n",
            "Epoch 9617: train loss = 2028.370221, test loss = 3481.973099\n",
            "Epoch 9618: train loss = 2028.341635, test loss = 3481.948125\n",
            "Epoch 9619: train loss = 2028.313051, test loss = 3481.923161\n",
            "Epoch 9620: train loss = 2028.284470, test loss = 3481.898205\n",
            "Epoch 9621: train loss = 2028.255891, test loss = 3481.873257\n",
            "Epoch 9622: train loss = 2028.227314, test loss = 3481.848318\n",
            "Epoch 9623: train loss = 2028.198739, test loss = 3481.823388\n",
            "Epoch 9624: train loss = 2028.170166, test loss = 3481.798466\n",
            "Epoch 9625: train loss = 2028.141596, test loss = 3481.773553\n",
            "Epoch 9626: train loss = 2028.113027, test loss = 3481.748648\n",
            "Epoch 9627: train loss = 2028.084461, test loss = 3481.723753\n",
            "Epoch 9628: train loss = 2028.055897, test loss = 3481.698866\n",
            "Epoch 9629: train loss = 2028.027335, test loss = 3481.673987\n",
            "Epoch 9630: train loss = 2027.998775, test loss = 3481.649117\n",
            "Epoch 9631: train loss = 2027.970218, test loss = 3481.624256\n",
            "Epoch 9632: train loss = 2027.941662, test loss = 3481.599404\n",
            "Epoch 9633: train loss = 2027.913109, test loss = 3481.574561\n",
            "Epoch 9634: train loss = 2027.884557, test loss = 3481.549726\n",
            "Epoch 9635: train loss = 2027.856008, test loss = 3481.524900\n",
            "Epoch 9636: train loss = 2027.827461, test loss = 3481.500082\n",
            "Epoch 9637: train loss = 2027.798916, test loss = 3481.475274\n",
            "Epoch 9638: train loss = 2027.770373, test loss = 3481.450474\n",
            "Epoch 9639: train loss = 2027.741832, test loss = 3481.425683\n",
            "Epoch 9640: train loss = 2027.713293, test loss = 3481.400901\n",
            "Epoch 9641: train loss = 2027.684757, test loss = 3481.376128\n",
            "Epoch 9642: train loss = 2027.656222, test loss = 3481.351363\n",
            "Epoch 9643: train loss = 2027.627690, test loss = 3481.326608\n",
            "Epoch 9644: train loss = 2027.599159, test loss = 3481.301861\n",
            "Epoch 9645: train loss = 2027.570631, test loss = 3481.277123\n",
            "Epoch 9646: train loss = 2027.542105, test loss = 3481.252394\n",
            "Epoch 9647: train loss = 2027.513580, test loss = 3481.227674\n",
            "Epoch 9648: train loss = 2027.485058, test loss = 3481.202963\n",
            "Epoch 9649: train loss = 2027.456538, test loss = 3481.178261\n",
            "Epoch 9650: train loss = 2027.428020, test loss = 3481.153568\n",
            "Epoch 9651: train loss = 2027.399504, test loss = 3481.128883\n",
            "Epoch 9652: train loss = 2027.370990, test loss = 3481.104208\n",
            "Epoch 9653: train loss = 2027.342478, test loss = 3481.079541\n",
            "Epoch 9654: train loss = 2027.313968, test loss = 3481.054884\n",
            "Epoch 9655: train loss = 2027.285460, test loss = 3481.030235\n",
            "Epoch 9656: train loss = 2027.256955, test loss = 3481.005596\n",
            "Epoch 9657: train loss = 2027.228451, test loss = 3480.980966\n",
            "Epoch 9658: train loss = 2027.199949, test loss = 3480.956344\n",
            "Epoch 9659: train loss = 2027.171449, test loss = 3480.931732\n",
            "Epoch 9660: train loss = 2027.142952, test loss = 3480.907129\n",
            "Epoch 9661: train loss = 2027.114456, test loss = 3480.882534\n",
            "Epoch 9662: train loss = 2027.085962, test loss = 3480.857949\n",
            "Epoch 9663: train loss = 2027.057470, test loss = 3480.833373\n",
            "Epoch 9664: train loss = 2027.028981, test loss = 3480.808806\n",
            "Epoch 9665: train loss = 2027.000493, test loss = 3480.784249\n",
            "Epoch 9666: train loss = 2026.972007, test loss = 3480.759700\n",
            "Epoch 9667: train loss = 2026.943523, test loss = 3480.735160\n",
            "Epoch 9668: train loss = 2026.915042, test loss = 3480.710630\n",
            "Epoch 9669: train loss = 2026.886562, test loss = 3480.686109\n",
            "Epoch 9670: train loss = 2026.858084, test loss = 3480.661597\n",
            "Epoch 9671: train loss = 2026.829608, test loss = 3480.637094\n",
            "Epoch 9672: train loss = 2026.801134, test loss = 3480.612600\n",
            "Epoch 9673: train loss = 2026.772662, test loss = 3480.588116\n",
            "Epoch 9674: train loss = 2026.744192, test loss = 3480.563641\n",
            "Epoch 9675: train loss = 2026.715724, test loss = 3480.539175\n",
            "Epoch 9676: train loss = 2026.687258, test loss = 3480.514718\n",
            "Epoch 9677: train loss = 2026.658794, test loss = 3480.490271\n",
            "Epoch 9678: train loss = 2026.630332, test loss = 3480.465833\n",
            "Epoch 9679: train loss = 2026.601871, test loss = 3480.441404\n",
            "Epoch 9680: train loss = 2026.573413, test loss = 3480.416984\n",
            "Epoch 9681: train loss = 2026.544957, test loss = 3480.392574\n",
            "Epoch 9682: train loss = 2026.516502, test loss = 3480.368173\n",
            "Epoch 9683: train loss = 2026.488050, test loss = 3480.343782\n",
            "Epoch 9684: train loss = 2026.459599, test loss = 3480.319400\n",
            "Epoch 9685: train loss = 2026.431150, test loss = 3480.295027\n",
            "Epoch 9686: train loss = 2026.402703, test loss = 3480.270663\n",
            "Epoch 9687: train loss = 2026.374259, test loss = 3480.246310\n",
            "Epoch 9688: train loss = 2026.345816, test loss = 3480.221965\n",
            "Epoch 9689: train loss = 2026.317374, test loss = 3480.197630\n",
            "Epoch 9690: train loss = 2026.288935, test loss = 3480.173304\n",
            "Epoch 9691: train loss = 2026.260498, test loss = 3480.148988\n",
            "Epoch 9692: train loss = 2026.232062, test loss = 3480.124681\n",
            "Epoch 9693: train loss = 2026.203629, test loss = 3480.100384\n",
            "Epoch 9694: train loss = 2026.175197, test loss = 3480.076096\n",
            "Epoch 9695: train loss = 2026.146767, test loss = 3480.051818\n",
            "Epoch 9696: train loss = 2026.118339, test loss = 3480.027549\n",
            "Epoch 9697: train loss = 2026.089913, test loss = 3480.003290\n",
            "Epoch 9698: train loss = 2026.061489, test loss = 3479.979040\n",
            "Epoch 9699: train loss = 2026.033067, test loss = 3479.954800\n",
            "Epoch 9700: train loss = 2026.004646, test loss = 3479.930569\n",
            "Epoch 9701: train loss = 2025.976227, test loss = 3479.906348\n",
            "Epoch 9702: train loss = 2025.947811, test loss = 3479.882137\n",
            "Epoch 9703: train loss = 2025.919396, test loss = 3479.857935\n",
            "Epoch 9704: train loss = 2025.890982, test loss = 3479.833743\n",
            "Epoch 9705: train loss = 2025.862571, test loss = 3479.809560\n",
            "Epoch 9706: train loss = 2025.834162, test loss = 3479.785387\n",
            "Epoch 9707: train loss = 2025.805754, test loss = 3479.761224\n",
            "Epoch 9708: train loss = 2025.777348, test loss = 3479.737071\n",
            "Epoch 9709: train loss = 2025.748944, test loss = 3479.712927\n",
            "Epoch 9710: train loss = 2025.720542, test loss = 3479.688793\n",
            "Epoch 9711: train loss = 2025.692141, test loss = 3479.664668\n",
            "Epoch 9712: train loss = 2025.663743, test loss = 3479.640554\n",
            "Epoch 9713: train loss = 2025.635346, test loss = 3479.616449\n",
            "Epoch 9714: train loss = 2025.606951, test loss = 3479.592354\n",
            "Epoch 9715: train loss = 2025.578558, test loss = 3479.568268\n",
            "Epoch 9716: train loss = 2025.550166, test loss = 3479.544193\n",
            "Epoch 9717: train loss = 2025.521776, test loss = 3479.520127\n",
            "Epoch 9718: train loss = 2025.493389, test loss = 3479.496071\n",
            "Epoch 9719: train loss = 2025.465002, test loss = 3479.472025\n",
            "Epoch 9720: train loss = 2025.436618, test loss = 3479.447988\n",
            "Epoch 9721: train loss = 2025.408236, test loss = 3479.423962\n",
            "Epoch 9722: train loss = 2025.379855, test loss = 3479.399945\n",
            "Epoch 9723: train loss = 2025.351476, test loss = 3479.375938\n",
            "Epoch 9724: train loss = 2025.323098, test loss = 3479.351942\n",
            "Epoch 9725: train loss = 2025.294723, test loss = 3479.327955\n",
            "Epoch 9726: train loss = 2025.266349, test loss = 3479.303978\n",
            "Epoch 9727: train loss = 2025.237977, test loss = 3479.280011\n",
            "Epoch 9728: train loss = 2025.209607, test loss = 3479.256054\n",
            "Epoch 9729: train loss = 2025.181238, test loss = 3479.232107\n",
            "Epoch 9730: train loss = 2025.152871, test loss = 3479.208169\n",
            "Epoch 9731: train loss = 2025.124506, test loss = 3479.184242\n",
            "Epoch 9732: train loss = 2025.096143, test loss = 3479.160325\n",
            "Epoch 9733: train loss = 2025.067781, test loss = 3479.136418\n",
            "Epoch 9734: train loss = 2025.039421, test loss = 3479.112521\n",
            "Epoch 9735: train loss = 2025.011063, test loss = 3479.088634\n",
            "Epoch 9736: train loss = 2024.982706, test loss = 3479.064757\n",
            "Epoch 9737: train loss = 2024.954352, test loss = 3479.040890\n",
            "Epoch 9738: train loss = 2024.925998, test loss = 3479.017033\n",
            "Epoch 9739: train loss = 2024.897647, test loss = 3478.993187\n",
            "Epoch 9740: train loss = 2024.869297, test loss = 3478.969350\n",
            "Epoch 9741: train loss = 2024.840949, test loss = 3478.945524\n",
            "Epoch 9742: train loss = 2024.812603, test loss = 3478.921708\n",
            "Epoch 9743: train loss = 2024.784258, test loss = 3478.897901\n",
            "Epoch 9744: train loss = 2024.755915, test loss = 3478.874106\n",
            "Epoch 9745: train loss = 2024.727574, test loss = 3478.850320\n",
            "Epoch 9746: train loss = 2024.699234, test loss = 3478.826544\n",
            "Epoch 9747: train loss = 2024.670896, test loss = 3478.802779\n",
            "Epoch 9748: train loss = 2024.642559, test loss = 3478.779024\n",
            "Epoch 9749: train loss = 2024.614225, test loss = 3478.755279\n",
            "Epoch 9750: train loss = 2024.585892, test loss = 3478.731545\n",
            "Epoch 9751: train loss = 2024.557560, test loss = 3478.707821\n",
            "Epoch 9752: train loss = 2024.529230, test loss = 3478.684107\n",
            "Epoch 9753: train loss = 2024.500902, test loss = 3478.660403\n",
            "Epoch 9754: train loss = 2024.472576, test loss = 3478.636710\n",
            "Epoch 9755: train loss = 2024.444251, test loss = 3478.613027\n",
            "Epoch 9756: train loss = 2024.415928, test loss = 3478.589355\n",
            "Epoch 9757: train loss = 2024.387606, test loss = 3478.565692\n",
            "Epoch 9758: train loss = 2024.359286, test loss = 3478.542041\n",
            "Epoch 9759: train loss = 2024.330967, test loss = 3478.518399\n",
            "Epoch 9760: train loss = 2024.302651, test loss = 3478.494768\n",
            "Epoch 9761: train loss = 2024.274335, test loss = 3478.471148\n",
            "Epoch 9762: train loss = 2024.246022, test loss = 3478.447538\n",
            "Epoch 9763: train loss = 2024.217710, test loss = 3478.423938\n",
            "Epoch 9764: train loss = 2024.189399, test loss = 3478.400349\n",
            "Epoch 9765: train loss = 2024.161090, test loss = 3478.376770\n",
            "Epoch 9766: train loss = 2024.132783, test loss = 3478.353202\n",
            "Epoch 9767: train loss = 2024.104477, test loss = 3478.329644\n",
            "Epoch 9768: train loss = 2024.076173, test loss = 3478.306097\n",
            "Epoch 9769: train loss = 2024.047871, test loss = 3478.282561\n",
            "Epoch 9770: train loss = 2024.019570, test loss = 3478.259035\n",
            "Epoch 9771: train loss = 2023.991270, test loss = 3478.235519\n",
            "Epoch 9772: train loss = 2023.962973, test loss = 3478.212015\n",
            "Epoch 9773: train loss = 2023.934676, test loss = 3478.188521\n",
            "Epoch 9774: train loss = 2023.906382, test loss = 3478.165037\n",
            "Epoch 9775: train loss = 2023.878088, test loss = 3478.141564\n",
            "Epoch 9776: train loss = 2023.849797, test loss = 3478.118102\n",
            "Epoch 9777: train loss = 2023.821506, test loss = 3478.094650\n",
            "Epoch 9778: train loss = 2023.793218, test loss = 3478.071210\n",
            "Epoch 9779: train loss = 2023.764931, test loss = 3478.047779\n",
            "Epoch 9780: train loss = 2023.736645, test loss = 3478.024360\n",
            "Epoch 9781: train loss = 2023.708361, test loss = 3478.000951\n",
            "Epoch 9782: train loss = 2023.680079, test loss = 3477.977553\n",
            "Epoch 9783: train loss = 2023.651798, test loss = 3477.954166\n",
            "Epoch 9784: train loss = 2023.623518, test loss = 3477.930790\n",
            "Epoch 9785: train loss = 2023.595240, test loss = 3477.907424\n",
            "Epoch 9786: train loss = 2023.566964, test loss = 3477.884070\n",
            "Epoch 9787: train loss = 2023.538689, test loss = 3477.860726\n",
            "Epoch 9788: train loss = 2023.510415, test loss = 3477.837393\n",
            "Epoch 9789: train loss = 2023.482143, test loss = 3477.814070\n",
            "Epoch 9790: train loss = 2023.453873, test loss = 3477.790759\n",
            "Epoch 9791: train loss = 2023.425604, test loss = 3477.767459\n",
            "Epoch 9792: train loss = 2023.397336, test loss = 3477.744169\n",
            "Epoch 9793: train loss = 2023.369070, test loss = 3477.720891\n",
            "Epoch 9794: train loss = 2023.340805, test loss = 3477.697623\n",
            "Epoch 9795: train loss = 2023.312542, test loss = 3477.674366\n",
            "Epoch 9796: train loss = 2023.284280, test loss = 3477.651121\n",
            "Epoch 9797: train loss = 2023.256020, test loss = 3477.627886\n",
            "Epoch 9798: train loss = 2023.227761, test loss = 3477.604662\n",
            "Epoch 9799: train loss = 2023.199504, test loss = 3477.581449\n",
            "Epoch 9800: train loss = 2023.171248, test loss = 3477.558248\n",
            "Epoch 9801: train loss = 2023.142993, test loss = 3477.535057\n",
            "Epoch 9802: train loss = 2023.114740, test loss = 3477.511878\n",
            "Epoch 9803: train loss = 2023.086489, test loss = 3477.488709\n",
            "Epoch 9804: train loss = 2023.058238, test loss = 3477.465552\n",
            "Epoch 9805: train loss = 2023.029990, test loss = 3477.442406\n",
            "Epoch 9806: train loss = 2023.001742, test loss = 3477.419271\n",
            "Epoch 9807: train loss = 2022.973496, test loss = 3477.396147\n",
            "Epoch 9808: train loss = 2022.945252, test loss = 3477.373034\n",
            "Epoch 9809: train loss = 2022.917008, test loss = 3477.349933\n",
            "Epoch 9810: train loss = 2022.888767, test loss = 3477.326843\n",
            "Epoch 9811: train loss = 2022.860526, test loss = 3477.303763\n",
            "Epoch 9812: train loss = 2022.832287, test loss = 3477.280696\n",
            "Epoch 9813: train loss = 2022.804050, test loss = 3477.257639\n",
            "Epoch 9814: train loss = 2022.775813, test loss = 3477.234594\n",
            "Epoch 9815: train loss = 2022.747578, test loss = 3477.211560\n",
            "Epoch 9816: train loss = 2022.719345, test loss = 3477.188537\n",
            "Epoch 9817: train loss = 2022.691113, test loss = 3477.165526\n",
            "Epoch 9818: train loss = 2022.662882, test loss = 3477.142525\n",
            "Epoch 9819: train loss = 2022.634653, test loss = 3477.119537\n",
            "Epoch 9820: train loss = 2022.606425, test loss = 3477.096559\n",
            "Epoch 9821: train loss = 2022.578198, test loss = 3477.073593\n",
            "Epoch 9822: train loss = 2022.549972, test loss = 3477.050639\n",
            "Epoch 9823: train loss = 2022.521748, test loss = 3477.027696\n",
            "Epoch 9824: train loss = 2022.493526, test loss = 3477.004764\n",
            "Epoch 9825: train loss = 2022.465304, test loss = 3476.981844\n",
            "Epoch 9826: train loss = 2022.437084, test loss = 3476.958935\n",
            "Epoch 9827: train loss = 2022.408866, test loss = 3476.936038\n",
            "Epoch 9828: train loss = 2022.380648, test loss = 3476.913152\n",
            "Epoch 9829: train loss = 2022.352432, test loss = 3476.890278\n",
            "Epoch 9830: train loss = 2022.324217, test loss = 3476.867415\n",
            "Epoch 9831: train loss = 2022.296004, test loss = 3476.844564\n",
            "Epoch 9832: train loss = 2022.267792, test loss = 3476.821725\n",
            "Epoch 9833: train loss = 2022.239581, test loss = 3476.798897\n",
            "Epoch 9834: train loss = 2022.211371, test loss = 3476.776080\n",
            "Epoch 9835: train loss = 2022.183163, test loss = 3476.753276\n",
            "Epoch 9836: train loss = 2022.154956, test loss = 3476.730483\n",
            "Epoch 9837: train loss = 2022.126750, test loss = 3476.707701\n",
            "Epoch 9838: train loss = 2022.098546, test loss = 3476.684932\n",
            "Epoch 9839: train loss = 2022.070343, test loss = 3476.662174\n",
            "Epoch 9840: train loss = 2022.042141, test loss = 3476.639427\n",
            "Epoch 9841: train loss = 2022.013940, test loss = 3476.616693\n",
            "Epoch 9842: train loss = 2021.985741, test loss = 3476.593970\n",
            "Epoch 9843: train loss = 2021.957543, test loss = 3476.571259\n",
            "Epoch 9844: train loss = 2021.929346, test loss = 3476.548560\n",
            "Epoch 9845: train loss = 2021.901150, test loss = 3476.525873\n",
            "Epoch 9846: train loss = 2021.872956, test loss = 3476.503197\n",
            "Epoch 9847: train loss = 2021.844763, test loss = 3476.480533\n",
            "Epoch 9848: train loss = 2021.816571, test loss = 3476.457881\n",
            "Epoch 9849: train loss = 2021.788380, test loss = 3476.435241\n",
            "Epoch 9850: train loss = 2021.760190, test loss = 3476.412613\n",
            "Epoch 9851: train loss = 2021.732002, test loss = 3476.389997\n",
            "Epoch 9852: train loss = 2021.703815, test loss = 3476.367393\n",
            "Epoch 9853: train loss = 2021.675629, test loss = 3476.344801\n",
            "Epoch 9854: train loss = 2021.647445, test loss = 3476.322220\n",
            "Epoch 9855: train loss = 2021.619261, test loss = 3476.299652\n",
            "Epoch 9856: train loss = 2021.591079, test loss = 3476.277096\n",
            "Epoch 9857: train loss = 2021.562898, test loss = 3476.254551\n",
            "Epoch 9858: train loss = 2021.534718, test loss = 3476.232019\n",
            "Epoch 9859: train loss = 2021.506539, test loss = 3476.209499\n",
            "Epoch 9860: train loss = 2021.478362, test loss = 3476.186991\n",
            "Epoch 9861: train loss = 2021.450185, test loss = 3476.164495\n",
            "Epoch 9862: train loss = 2021.422010, test loss = 3476.142011\n",
            "Epoch 9863: train loss = 2021.393836, test loss = 3476.119539\n",
            "Epoch 9864: train loss = 2021.365663, test loss = 3476.097079\n",
            "Epoch 9865: train loss = 2021.337491, test loss = 3476.074632\n",
            "Epoch 9866: train loss = 2021.309321, test loss = 3476.052196\n",
            "Epoch 9867: train loss = 2021.281151, test loss = 3476.029773\n",
            "Epoch 9868: train loss = 2021.252983, test loss = 3476.007363\n",
            "Epoch 9869: train loss = 2021.224816, test loss = 3475.984964\n",
            "Epoch 9870: train loss = 2021.196650, test loss = 3475.962578\n",
            "Epoch 9871: train loss = 2021.168485, test loss = 3475.940204\n",
            "Epoch 9872: train loss = 2021.140321, test loss = 3475.917842\n",
            "Epoch 9873: train loss = 2021.112159, test loss = 3475.895493\n",
            "Epoch 9874: train loss = 2021.083997, test loss = 3475.873155\n",
            "Epoch 9875: train loss = 2021.055837, test loss = 3475.850831\n",
            "Epoch 9876: train loss = 2021.027677, test loss = 3475.828518\n",
            "Epoch 9877: train loss = 2020.999519, test loss = 3475.806219\n",
            "Epoch 9878: train loss = 2020.971362, test loss = 3475.783931\n",
            "Epoch 9879: train loss = 2020.943206, test loss = 3475.761656\n",
            "Epoch 9880: train loss = 2020.915051, test loss = 3475.739393\n",
            "Epoch 9881: train loss = 2020.886897, test loss = 3475.717143\n",
            "Epoch 9882: train loss = 2020.858744, test loss = 3475.694906\n",
            "Epoch 9883: train loss = 2020.830593, test loss = 3475.672680\n",
            "Epoch 9884: train loss = 2020.802442, test loss = 3475.650468\n",
            "Epoch 9885: train loss = 2020.774292, test loss = 3475.628268\n",
            "Epoch 9886: train loss = 2020.746144, test loss = 3475.606080\n",
            "Epoch 9887: train loss = 2020.717996, test loss = 3475.583905\n",
            "Epoch 9888: train loss = 2020.689850, test loss = 3475.561743\n",
            "Epoch 9889: train loss = 2020.661705, test loss = 3475.539593\n",
            "Epoch 9890: train loss = 2020.633560, test loss = 3475.517456\n",
            "Epoch 9891: train loss = 2020.605417, test loss = 3475.495332\n",
            "Epoch 9892: train loss = 2020.577275, test loss = 3475.473220\n",
            "Epoch 9893: train loss = 2020.549133, test loss = 3475.451122\n",
            "Epoch 9894: train loss = 2020.520993, test loss = 3475.429035\n",
            "Epoch 9895: train loss = 2020.492854, test loss = 3475.406962\n",
            "Epoch 9896: train loss = 2020.464716, test loss = 3475.384901\n",
            "Epoch 9897: train loss = 2020.436578, test loss = 3475.362853\n",
            "Epoch 9898: train loss = 2020.408442, test loss = 3475.340818\n",
            "Epoch 9899: train loss = 2020.380307, test loss = 3475.318796\n",
            "Epoch 9900: train loss = 2020.352173, test loss = 3475.296787\n",
            "Epoch 9901: train loss = 2020.324039, test loss = 3475.274790\n",
            "Epoch 9902: train loss = 2020.295907, test loss = 3475.252806\n",
            "Epoch 9903: train loss = 2020.267776, test loss = 3475.230836\n",
            "Epoch 9904: train loss = 2020.239646, test loss = 3475.208878\n",
            "Epoch 9905: train loss = 2020.211516, test loss = 3475.186933\n",
            "Epoch 9906: train loss = 2020.183388, test loss = 3475.165001\n",
            "Epoch 9907: train loss = 2020.155260, test loss = 3475.143082\n",
            "Epoch 9908: train loss = 2020.127134, test loss = 3475.121176\n",
            "Epoch 9909: train loss = 2020.099008, test loss = 3475.099283\n",
            "Epoch 9910: train loss = 2020.070884, test loss = 3475.077404\n",
            "Epoch 9911: train loss = 2020.042760, test loss = 3475.055537\n",
            "Epoch 9912: train loss = 2020.014637, test loss = 3475.033683\n",
            "Epoch 9913: train loss = 2019.986516, test loss = 3475.011843\n",
            "Epoch 9914: train loss = 2019.958395, test loss = 3474.990015\n",
            "Epoch 9915: train loss = 2019.930275, test loss = 3474.968201\n",
            "Epoch 9916: train loss = 2019.902156, test loss = 3474.946400\n",
            "Epoch 9917: train loss = 2019.874038, test loss = 3474.924612\n",
            "Epoch 9918: train loss = 2019.845921, test loss = 3474.902838\n",
            "Epoch 9919: train loss = 2019.817804, test loss = 3474.881076\n",
            "Epoch 9920: train loss = 2019.789689, test loss = 3474.859328\n",
            "Epoch 9921: train loss = 2019.761574, test loss = 3474.837593\n",
            "Epoch 9922: train loss = 2019.733461, test loss = 3474.815872\n",
            "Epoch 9923: train loss = 2019.705348, test loss = 3474.794163\n",
            "Epoch 9924: train loss = 2019.677236, test loss = 3474.772468\n",
            "Epoch 9925: train loss = 2019.649125, test loss = 3474.750787\n",
            "Epoch 9926: train loss = 2019.621015, test loss = 3474.729119\n",
            "Epoch 9927: train loss = 2019.592906, test loss = 3474.707464\n",
            "Epoch 9928: train loss = 2019.564797, test loss = 3474.685823\n",
            "Epoch 9929: train loss = 2019.536690, test loss = 3474.664195\n",
            "Epoch 9930: train loss = 2019.508583, test loss = 3474.642581\n",
            "Epoch 9931: train loss = 2019.480477, test loss = 3474.620980\n",
            "Epoch 9932: train loss = 2019.452372, test loss = 3474.599392\n",
            "Epoch 9933: train loss = 2019.424268, test loss = 3474.577818\n",
            "Epoch 9934: train loss = 2019.396164, test loss = 3474.556258\n",
            "Epoch 9935: train loss = 2019.368062, test loss = 3474.534711\n",
            "Epoch 9936: train loss = 2019.339960, test loss = 3474.513178\n",
            "Epoch 9937: train loss = 2019.311859, test loss = 3474.491659\n",
            "Epoch 9938: train loss = 2019.283759, test loss = 3474.470153\n",
            "Epoch 9939: train loss = 2019.255660, test loss = 3474.448661\n",
            "Epoch 9940: train loss = 2019.227561, test loss = 3474.427182\n",
            "Epoch 9941: train loss = 2019.199463, test loss = 3474.405718\n",
            "Epoch 9942: train loss = 2019.171366, test loss = 3474.384267\n",
            "Epoch 9943: train loss = 2019.143270, test loss = 3474.362829\n",
            "Epoch 9944: train loss = 2019.115175, test loss = 3474.341406\n",
            "Epoch 9945: train loss = 2019.087080, test loss = 3474.319996\n",
            "Epoch 9946: train loss = 2019.058986, test loss = 3474.298600\n",
            "Epoch 9947: train loss = 2019.030893, test loss = 3474.277218\n",
            "Epoch 9948: train loss = 2019.002801, test loss = 3474.255850\n",
            "Epoch 9949: train loss = 2018.974709, test loss = 3474.234496\n",
            "Epoch 9950: train loss = 2018.946619, test loss = 3474.213156\n",
            "Epoch 9951: train loss = 2018.918528, test loss = 3474.191829\n",
            "Epoch 9952: train loss = 2018.890439, test loss = 3474.170517\n",
            "Epoch 9953: train loss = 2018.862350, test loss = 3474.149218\n",
            "Epoch 9954: train loss = 2018.834263, test loss = 3474.127934\n",
            "Epoch 9955: train loss = 2018.806175, test loss = 3474.106663\n",
            "Epoch 9956: train loss = 2018.778089, test loss = 3474.085407\n",
            "Epoch 9957: train loss = 2018.750003, test loss = 3474.064165\n",
            "Epoch 9958: train loss = 2018.721918, test loss = 3474.042936\n",
            "Epoch 9959: train loss = 2018.693834, test loss = 3474.021722\n",
            "Epoch 9960: train loss = 2018.665750, test loss = 3474.000522\n",
            "Epoch 9961: train loss = 2018.637667, test loss = 3473.979336\n",
            "Epoch 9962: train loss = 2018.609585, test loss = 3473.958165\n",
            "Epoch 9963: train loss = 2018.581503, test loss = 3473.937007\n",
            "Epoch 9964: train loss = 2018.553422, test loss = 3473.915864\n",
            "Epoch 9965: train loss = 2018.525342, test loss = 3473.894735\n",
            "Epoch 9966: train loss = 2018.497262, test loss = 3473.873621\n",
            "Epoch 9967: train loss = 2018.469183, test loss = 3473.852520\n",
            "Epoch 9968: train loss = 2018.441105, test loss = 3473.831434\n",
            "Epoch 9969: train loss = 2018.413028, test loss = 3473.810363\n",
            "Epoch 9970: train loss = 2018.384951, test loss = 3473.789305\n",
            "Epoch 9971: train loss = 2018.356874, test loss = 3473.768262\n",
            "Epoch 9972: train loss = 2018.328798, test loss = 3473.747234\n",
            "Epoch 9973: train loss = 2018.300723, test loss = 3473.726220\n",
            "Epoch 9974: train loss = 2018.272649, test loss = 3473.705220\n",
            "Epoch 9975: train loss = 2018.244575, test loss = 3473.684235\n",
            "Epoch 9976: train loss = 2018.216502, test loss = 3473.663265\n",
            "Epoch 9977: train loss = 2018.188429, test loss = 3473.642308\n",
            "Epoch 9978: train loss = 2018.160357, test loss = 3473.621367\n",
            "Epoch 9979: train loss = 2018.132286, test loss = 3473.600440\n",
            "Epoch 9980: train loss = 2018.104215, test loss = 3473.579528\n",
            "Epoch 9981: train loss = 2018.076145, test loss = 3473.558630\n",
            "Epoch 9982: train loss = 2018.048075, test loss = 3473.537747\n",
            "Epoch 9983: train loss = 2018.020006, test loss = 3473.516879\n",
            "Epoch 9984: train loss = 2017.991937, test loss = 3473.496025\n",
            "Epoch 9985: train loss = 2017.963869, test loss = 3473.475186\n",
            "Epoch 9986: train loss = 2017.935802, test loss = 3473.454362\n",
            "Epoch 9987: train loss = 2017.907735, test loss = 3473.433553\n",
            "Epoch 9988: train loss = 2017.879669, test loss = 3473.412758\n",
            "Epoch 9989: train loss = 2017.851603, test loss = 3473.391979\n",
            "Epoch 9990: train loss = 2017.823538, test loss = 3473.371214\n",
            "Epoch 9991: train loss = 2017.795473, test loss = 3473.350464\n",
            "Epoch 9992: train loss = 2017.767409, test loss = 3473.329729\n",
            "Epoch 9993: train loss = 2017.739345, test loss = 3473.309009\n",
            "Epoch 9994: train loss = 2017.711282, test loss = 3473.288304\n",
            "Epoch 9995: train loss = 2017.683219, test loss = 3473.267613\n",
            "Epoch 9996: train loss = 2017.655157, test loss = 3473.246938\n",
            "Epoch 9997: train loss = 2017.627096, test loss = 3473.226278\n",
            "Epoch 9998: train loss = 2017.599034, test loss = 3473.205633\n",
            "Epoch 9999: train loss = 2017.570974, test loss = 3473.185003\n",
            "Epoch 10000: train loss = 2017.542914, test loss = 3473.164388\n"
          ]
        }
      ],
      "source": [
        "## Train the 2-hidden layer neural network (8 nodes, 8 nodes followed by 1 node)\n",
        "## using batch training with batch size = 100\n",
        "learning_rate = 1e-04 # learning rate\n",
        "batch_size = 32 # batch size\n",
        "nepochs = 10000 # number of epochs\n",
        "reg_strength = 1.0 # regularization strength\n",
        "# Create empty array to store training losses over each epoch\n",
        "loss_train_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "# Create empty array to store test losses over each epoch\n",
        "loss_test_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "\n",
        "\n",
        "# Neural network architecture\n",
        "\n",
        "dlayer1 = Dense(num_features, 16, reg_strength) # define dense layer 1\n",
        "alayer1 = Tanh() # ReLU activation layer 1\n",
        "dlayer2 = Dense(16, 1, reg_strength) # define dense layer 2\n",
        "\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  for b in range(len(batch_indices)):\n",
        "    # Forward propagation for training data\n",
        "    dlayer1.forward(X_train_transformed[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n",
        "    alayer1.forward(dlayer1.output) # forward prop activation layer 1\n",
        "    dlayer2.forward(alayer1.output) # forward prop dense layer 2\n",
        "    # Calculate training data loss\n",
        "    loss += mse(Y_train[batch_indices[b]], dlayer2.output)\n",
        "    # Add the regularization losses\n",
        "    loss += dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "    # Backward prop starts here\n",
        "    grad = mse_gradient(Y_train[batch_indices[b]], dlayer2.output)\n",
        "    grad = dlayer2.backward(grad, learning_rate)\n",
        "    grad = alayer1.backward(grad)\n",
        "    grad = dlayer1.backward(grad, learning_rate)\n",
        "  # Calculate the average training loss for the current epoch\n",
        "  loss_train_epoch[epoch] = loss/len(batch_indices)\n",
        "\n",
        "  # Forward propagation for test data\n",
        "  dlayer1.forward(X_test_transformed)\n",
        "  alayer1.forward(dlayer1.output)\n",
        "  dlayer2.forward(alayer1.output)\n",
        "\n",
        "  # Calculate test data loss plus regularization loss\n",
        "  loss_test_epoch[epoch] =  mse(Y_test, dlayer2.output) + dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "  print('Epoch %d: train loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n",
        "  epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXEFASk-Tkv"
      },
      "source": [
        "---\n",
        "\n",
        "Plot training loss vs. epoch\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Iv3k23SlCqGf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot(loss_train_epoch, 'b', label = 'Train')\n",
        "ax.plot(loss_test_epoch, 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 1..0', fontsize = 14);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaLoOOWK-WBj"
      },
      "source": [
        "---\n",
        "\n",
        "Test performance on test data\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "d7AEbmpcKcPY"
      },
      "outputs": [],
      "source": [
        "dlayer1.forward(X_test_transformed)\n",
        "alayer1.forward(dlayer1.output)\n",
        "dlayer2.forward(alayer1.output)\n",
        "ypred = dlayer2.output.flatten()\n",
        "ytrue = Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL88EDPu2cKU"
      },
      "source": [
        "---\n",
        "\n",
        "Define neural network architecture for regression\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "k_YHFR9c2Zqq"
      },
      "outputs": [],
      "source": [
        "# Define neural network architecture for regression\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(X_train_transformed.T.shape[1], ), kernel_regularizer = keras.regularizers.l2(l = 1.0)),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu', input_shape=(X_train_transformed.T.shape[1], ), kernel_regularizer = keras.regularizers.l2(l = 1.0)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNvESd_2eyX"
      },
      "source": [
        "---\n",
        "\n",
        "Compile the neural network\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bVRF1u3i2hcA"
      },
      "outputs": [],
      "source": [
        "# Compile the neural network model\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-04)\n",
        "model.compile(optimizer = opt, loss = 'mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGVN5fi62mHY"
      },
      "source": [
        "---\n",
        "\n",
        "Train the model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TPdXEpps2k5G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10000\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 29595.3125 - val_loss: 26986.8477\n",
            "Epoch 2/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 29582.8984 - val_loss: 26975.0332\n",
            "Epoch 3/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29570.4844 - val_loss: 26963.2695\n",
            "Epoch 4/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29558.2441 - val_loss: 26951.6016\n",
            "Epoch 5/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29545.4648 - val_loss: 26939.6133\n",
            "Epoch 6/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29532.5430 - val_loss: 26926.7500\n",
            "Epoch 7/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29518.5996 - val_loss: 26912.9160\n",
            "Epoch 8/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29503.9043 - val_loss: 26898.9473\n",
            "Epoch 9/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29488.5977 - val_loss: 26883.2559\n",
            "Epoch 10/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29471.6289 - val_loss: 26866.6621\n",
            "Epoch 11/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29454.4297 - val_loss: 26849.3770\n",
            "Epoch 12/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29435.5859 - val_loss: 26829.9297\n",
            "Epoch 13/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29414.9453 - val_loss: 26808.8828\n",
            "Epoch 14/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29392.2637 - val_loss: 26786.8418\n",
            "Epoch 15/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29368.6973 - val_loss: 26762.5957\n",
            "Epoch 16/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29342.4785 - val_loss: 26736.5117\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 29313.6934 - val_loss: 26707.5000\n",
            "Epoch 18/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29282.4707 - val_loss: 26675.0332\n",
            "Epoch 19/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29246.6094 - val_loss: 26639.2031\n",
            "Epoch 20/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 29208.2090 - val_loss: 26600.2949\n",
            "Epoch 21/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29166.4023 - val_loss: 26557.0508\n",
            "Epoch 22/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29118.4648 - val_loss: 26510.6426\n",
            "Epoch 23/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29068.7285 - val_loss: 26459.0957\n",
            "Epoch 24/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29012.2695 - val_loss: 26403.2109\n",
            "Epoch 25/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28951.7988 - val_loss: 26341.6602\n",
            "Epoch 26/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28884.7363 - val_loss: 26276.3867\n",
            "Epoch 27/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28812.4219 - val_loss: 26205.0332\n",
            "Epoch 28/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28734.0078 - val_loss: 26126.4297\n",
            "Epoch 29/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28644.6562 - val_loss: 26038.0703\n",
            "Epoch 30/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28550.4668 - val_loss: 25948.9766\n",
            "Epoch 31/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28453.0059 - val_loss: 25854.6172\n",
            "Epoch 32/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28348.7852 - val_loss: 25753.1289\n",
            "Epoch 33/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28236.3770 - val_loss: 25643.9766\n",
            "Epoch 34/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28109.0703 - val_loss: 25518.7070\n",
            "Epoch 35/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 27976.1055 - val_loss: 25392.3379\n",
            "Epoch 36/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 27834.4297 - val_loss: 25256.1875\n",
            "Epoch 37/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 27684.6211 - val_loss: 25117.4277\n",
            "Epoch 38/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27531.9238 - val_loss: 24965.8633\n",
            "Epoch 39/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27359.2207 - val_loss: 24796.9160\n",
            "Epoch 40/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27172.0938 - val_loss: 24620.6914\n",
            "Epoch 41/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26974.3027 - val_loss: 24423.8320\n",
            "Epoch 42/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26755.1582 - val_loss: 24220.4551\n",
            "Epoch 43/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 26537.5547 - val_loss: 24014.9570\n",
            "Epoch 44/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26306.5547 - val_loss: 23799.6719\n",
            "Epoch 45/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26067.0586 - val_loss: 23571.7441\n",
            "Epoch 46/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 25815.7715 - val_loss: 23333.4629\n",
            "Epoch 47/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 25554.0566 - val_loss: 23083.1738\n",
            "Epoch 48/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 25272.4277 - val_loss: 22825.2305\n",
            "Epoch 49/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 24984.2832 - val_loss: 22540.6211\n",
            "Epoch 50/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 24654.2949 - val_loss: 22237.2168\n",
            "Epoch 51/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 24329.0000 - val_loss: 21938.9297\n",
            "Epoch 52/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 23999.4219 - val_loss: 21634.1406\n",
            "Epoch 53/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 23658.6562 - val_loss: 21317.7891\n",
            "Epoch 54/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 23303.5625 - val_loss: 20994.8105\n",
            "Epoch 55/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 22947.5273 - val_loss: 20651.6797\n",
            "Epoch 56/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 22560.4980 - val_loss: 20290.6973\n",
            "Epoch 57/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 22153.2051 - val_loss: 19931.8066\n",
            "Epoch 58/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21747.1738 - val_loss: 19547.4297\n",
            "Epoch 59/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21321.3574 - val_loss: 19157.9160\n",
            "Epoch 60/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20880.3828 - val_loss: 18753.5879\n",
            "Epoch 61/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20437.2793 - val_loss: 18327.8516\n",
            "Epoch 62/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19931.6191 - val_loss: 17879.4102\n",
            "Epoch 63/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19454.0332 - val_loss: 17445.0684\n",
            "Epoch 64/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18967.5352 - val_loss: 17017.7324\n",
            "Epoch 65/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18481.6445 - val_loss: 16567.6973\n",
            "Epoch 66/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17970.8418 - val_loss: 16116.3232\n",
            "Epoch 67/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17472.4570 - val_loss: 15664.9414\n",
            "Epoch 68/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16958.6426 - val_loss: 15185.7412\n",
            "Epoch 69/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 16408.3066 - val_loss: 14694.6250\n",
            "Epoch 70/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15862.5195 - val_loss: 14233.2568\n",
            "Epoch 71/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15348.8018 - val_loss: 13751.8203\n",
            "Epoch 72/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14816.1699 - val_loss: 13292.7988\n",
            "Epoch 73/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14306.1816 - val_loss: 12830.2363\n",
            "Epoch 74/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13754.9395 - val_loss: 12342.8721\n",
            "Epoch 75/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13221.7871 - val_loss: 11870.7471\n",
            "Epoch 76/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 12706.8496 - val_loss: 11435.5449\n",
            "Epoch 77/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12204.7080 - val_loss: 10986.7744\n",
            "Epoch 78/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11729.7617 - val_loss: 10577.6855\n",
            "Epoch 79/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11256.2539 - val_loss: 10154.3945\n",
            "Epoch 80/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10756.6768 - val_loss: 9704.9541\n",
            "Epoch 81/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10285.3467 - val_loss: 9312.9141\n",
            "Epoch 82/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9818.4854 - val_loss: 8879.4160\n",
            "Epoch 83/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9343.8867 - val_loss: 8473.1016\n",
            "Epoch 84/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8907.2783 - val_loss: 8114.6758\n",
            "Epoch 85/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8497.3955 - val_loss: 7761.5273\n",
            "Epoch 86/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8116.0991 - val_loss: 7451.8765\n",
            "Epoch 87/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7800.4668 - val_loss: 7173.2695\n",
            "Epoch 88/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7498.2402 - val_loss: 6900.5723\n",
            "Epoch 89/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7176.7373 - val_loss: 6619.1294\n",
            "Epoch 90/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6863.3682 - val_loss: 6352.0156\n",
            "Epoch 91/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6576.3794 - val_loss: 6124.4487\n",
            "Epoch 92/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6335.0757 - val_loss: 5927.5645\n",
            "Epoch 93/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6104.0923 - val_loss: 5724.3750\n",
            "Epoch 94/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5893.1357 - val_loss: 5530.8511\n",
            "Epoch 95/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5689.4985 - val_loss: 5373.5322\n",
            "Epoch 96/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5501.9971 - val_loss: 5210.3110\n",
            "Epoch 97/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5328.9038 - val_loss: 5066.8511\n",
            "Epoch 98/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5191.3228 - val_loss: 4955.7612\n",
            "Epoch 99/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5075.0107 - val_loss: 4861.9985\n",
            "Epoch 100/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4972.7954 - val_loss: 4781.2393\n",
            "Epoch 101/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 4886.5728 - val_loss: 4702.2334\n",
            "Epoch 102/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4805.8477 - val_loss: 4644.7231\n",
            "Epoch 103/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4734.1309 - val_loss: 4570.0713\n",
            "Epoch 104/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4632.9062 - val_loss: 4486.2012\n",
            "Epoch 105/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4559.9541 - val_loss: 4432.6045\n",
            "Epoch 106/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4496.3843 - val_loss: 4380.1333\n",
            "Epoch 107/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4445.0396 - val_loss: 4332.0781\n",
            "Epoch 108/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4392.1606 - val_loss: 4290.2310\n",
            "Epoch 109/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4346.6587 - val_loss: 4257.4565\n",
            "Epoch 110/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4298.9609 - val_loss: 4215.0815\n",
            "Epoch 111/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4254.8628 - val_loss: 4173.7080\n",
            "Epoch 112/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4202.9814 - val_loss: 4128.1895\n",
            "Epoch 113/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4162.3184 - val_loss: 4101.4097\n",
            "Epoch 114/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4133.3125 - val_loss: 4081.6714\n",
            "Epoch 115/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 4110.1372 - val_loss: 4059.5273\n",
            "Epoch 116/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 4083.1873 - val_loss: 4040.3892\n",
            "Epoch 117/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 4059.9346 - val_loss: 4020.7739\n",
            "Epoch 118/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4038.0425 - val_loss: 3997.3733\n",
            "Epoch 119/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4015.0068 - val_loss: 3981.9922\n",
            "Epoch 120/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3993.6370 - val_loss: 3965.1243\n",
            "Epoch 121/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3975.9675 - val_loss: 3949.5422\n",
            "Epoch 122/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3958.4082 - val_loss: 3938.7927\n",
            "Epoch 123/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3944.6138 - val_loss: 3938.5161\n",
            "Epoch 124/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3937.0613 - val_loss: 3929.3630\n",
            "Epoch 125/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3922.8804 - val_loss: 3915.9592\n",
            "Epoch 126/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3904.9985 - val_loss: 3897.1177\n",
            "Epoch 127/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3888.1016 - val_loss: 3893.3491\n",
            "Epoch 128/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3868.2043 - val_loss: 3870.1121\n",
            "Epoch 129/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3845.0100 - val_loss: 3852.3125\n",
            "Epoch 130/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3830.5520 - val_loss: 3836.0081\n",
            "Epoch 131/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3809.0740 - val_loss: 3825.1074\n",
            "Epoch 132/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3795.9600 - val_loss: 3813.6697\n",
            "Epoch 133/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3780.3027 - val_loss: 3800.9653\n",
            "Epoch 134/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3766.4102 - val_loss: 3792.2854\n",
            "Epoch 135/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3751.7310 - val_loss: 3779.6477\n",
            "Epoch 136/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3741.4370 - val_loss: 3769.2185\n",
            "Epoch 137/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3727.6252 - val_loss: 3760.7971\n",
            "Epoch 138/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3716.9985 - val_loss: 3751.1660\n",
            "Epoch 139/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3706.5613 - val_loss: 3739.9326\n",
            "Epoch 140/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3696.2124 - val_loss: 3730.3542\n",
            "Epoch 141/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3683.2639 - val_loss: 3718.1978\n",
            "Epoch 142/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3671.8232 - val_loss: 3709.6162\n",
            "Epoch 143/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3662.7092 - val_loss: 3701.6079\n",
            "Epoch 144/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3653.0896 - val_loss: 3694.7297\n",
            "Epoch 145/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3642.4253 - val_loss: 3688.9722\n",
            "Epoch 146/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3632.7830 - val_loss: 3680.3496\n",
            "Epoch 147/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3620.9873 - val_loss: 3675.1797\n",
            "Epoch 148/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3613.6753 - val_loss: 3670.0586\n",
            "Epoch 149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3605.2864 - val_loss: 3662.6819\n",
            "Epoch 150/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3596.7830 - val_loss: 3657.0752\n",
            "Epoch 151/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3589.3037 - val_loss: 3646.3604\n",
            "Epoch 152/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3579.0339 - val_loss: 3640.8765\n",
            "Epoch 153/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3571.0637 - val_loss: 3633.2261\n",
            "Epoch 154/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3561.9614 - val_loss: 3626.0505\n",
            "Epoch 155/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3556.3931 - val_loss: 3618.3430\n",
            "Epoch 156/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3547.6909 - val_loss: 3609.3918\n",
            "Epoch 157/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3538.5925 - val_loss: 3603.1025\n",
            "Epoch 158/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3530.2468 - val_loss: 3597.4583\n",
            "Epoch 159/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3522.6665 - val_loss: 3592.4221\n",
            "Epoch 160/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3514.6265 - val_loss: 3588.1135\n",
            "Epoch 161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3506.0867 - val_loss: 3585.1257\n",
            "Epoch 162/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3496.5701 - val_loss: 3576.6716\n",
            "Epoch 163/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3487.5950 - val_loss: 3570.2332\n",
            "Epoch 164/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3477.9790 - val_loss: 3567.6411\n",
            "Epoch 165/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3465.9805 - val_loss: 3562.4753\n",
            "Epoch 166/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3458.5486 - val_loss: 3558.7803\n",
            "Epoch 167/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3452.0859 - val_loss: 3555.4541\n",
            "Epoch 168/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3446.2593 - val_loss: 3550.6165\n",
            "Epoch 169/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3438.6895 - val_loss: 3545.2756\n",
            "Epoch 170/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3431.9387 - val_loss: 3540.8940\n",
            "Epoch 171/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3425.5789 - val_loss: 3536.1765\n",
            "Epoch 172/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3420.1360 - val_loss: 3533.2756\n",
            "Epoch 173/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3413.5820 - val_loss: 3529.0359\n",
            "Epoch 174/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3407.0803 - val_loss: 3524.1748\n",
            "Epoch 175/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3401.0776 - val_loss: 3520.1516\n",
            "Epoch 176/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3395.0237 - val_loss: 3515.3831\n",
            "Epoch 177/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3387.7627 - val_loss: 3509.0227\n",
            "Epoch 178/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3381.4856 - val_loss: 3503.8533\n",
            "Epoch 179/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3374.5291 - val_loss: 3497.7964\n",
            "Epoch 180/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3367.9236 - val_loss: 3494.5273\n",
            "Epoch 181/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3362.7336 - val_loss: 3494.4890\n",
            "Epoch 182/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3359.2683 - val_loss: 3491.5872\n",
            "Epoch 183/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3352.8909 - val_loss: 3489.0403\n",
            "Epoch 184/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3347.2925 - val_loss: 3484.6040\n",
            "Epoch 185/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3341.5808 - val_loss: 3480.7261\n",
            "Epoch 186/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3339.8311 - val_loss: 3481.4150\n",
            "Epoch 187/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3330.6641 - val_loss: 3474.3638\n",
            "Epoch 188/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3324.8535 - val_loss: 3468.9658\n",
            "Epoch 189/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3318.6323 - val_loss: 3464.3079\n",
            "Epoch 190/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3313.2136 - val_loss: 3462.7278\n",
            "Epoch 191/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3309.5959 - val_loss: 3459.1812\n",
            "Epoch 192/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3303.6279 - val_loss: 3454.0762\n",
            "Epoch 193/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3297.7529 - val_loss: 3447.4583\n",
            "Epoch 194/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3291.4404 - val_loss: 3443.4297\n",
            "Epoch 195/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3286.2446 - val_loss: 3440.8726\n",
            "Epoch 196/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3281.5566 - val_loss: 3437.9517\n",
            "Epoch 197/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3278.1243 - val_loss: 3439.9368\n",
            "Epoch 198/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3275.7915 - val_loss: 3440.5225\n",
            "Epoch 199/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3271.1863 - val_loss: 3439.4087\n",
            "Epoch 200/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3270.0859 - val_loss: 3446.5281\n",
            "Epoch 201/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3268.7390 - val_loss: 3445.2581\n",
            "Epoch 202/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3262.6838 - val_loss: 3439.2747\n",
            "Epoch 203/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3257.8308 - val_loss: 3434.8047\n",
            "Epoch 204/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3250.8279 - val_loss: 3423.2588\n",
            "Epoch 205/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3248.9585 - val_loss: 3424.3752\n",
            "Epoch 206/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3242.0127 - val_loss: 3420.0940\n",
            "Epoch 207/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3238.3665 - val_loss: 3412.1780\n",
            "Epoch 208/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3232.9631 - val_loss: 3411.0842\n",
            "Epoch 209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3229.5261 - val_loss: 3409.3071\n",
            "Epoch 210/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3225.4734 - val_loss: 3407.2478\n",
            "Epoch 211/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3221.1863 - val_loss: 3404.5928\n",
            "Epoch 212/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3216.4365 - val_loss: 3398.0259\n",
            "Epoch 213/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3212.1963 - val_loss: 3392.0649\n",
            "Epoch 214/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3208.7666 - val_loss: 3385.0818\n",
            "Epoch 215/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3204.4119 - val_loss: 3382.0632\n",
            "Epoch 216/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3200.7458 - val_loss: 3378.8933\n",
            "Epoch 217/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3194.8752 - val_loss: 3377.9978\n",
            "Epoch 218/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3190.8340 - val_loss: 3378.6267\n",
            "Epoch 219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3185.9062 - val_loss: 3376.6475\n",
            "Epoch 220/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3182.0229 - val_loss: 3374.9094\n",
            "Epoch 221/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3177.8772 - val_loss: 3371.9517\n",
            "Epoch 222/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3173.5559 - val_loss: 3370.5176\n",
            "Epoch 223/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3169.0063 - val_loss: 3368.3020\n",
            "Epoch 224/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3165.3726 - val_loss: 3365.6265\n",
            "Epoch 225/10000\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 3161.8145 - val_loss: 3363.7061\n",
            "Epoch 226/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3158.2620 - val_loss: 3360.5627\n",
            "Epoch 227/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3155.7227 - val_loss: 3358.5161\n",
            "Epoch 228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3148.4165 - val_loss: 3351.1729\n",
            "Epoch 229/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3147.2217 - val_loss: 3345.3904\n",
            "Epoch 230/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3142.2593 - val_loss: 3342.5786\n",
            "Epoch 231/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3137.4104 - val_loss: 3342.4263\n",
            "Epoch 232/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3135.0227 - val_loss: 3339.6729\n",
            "Epoch 233/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3132.3223 - val_loss: 3336.2734\n",
            "Epoch 234/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3127.2188 - val_loss: 3336.1675\n",
            "Epoch 235/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3124.3972 - val_loss: 3336.8518\n",
            "Epoch 236/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3121.4475 - val_loss: 3340.0322\n",
            "Epoch 237/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3119.1775 - val_loss: 3339.4585\n",
            "Epoch 238/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3116.6794 - val_loss: 3342.9622\n",
            "Epoch 239/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3114.3442 - val_loss: 3341.5947\n",
            "Epoch 240/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3110.6619 - val_loss: 3336.8455\n",
            "Epoch 241/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3107.6980 - val_loss: 3337.3252\n",
            "Epoch 242/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3103.1975 - val_loss: 3333.1572\n",
            "Epoch 243/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3098.3162 - val_loss: 3328.4841\n",
            "Epoch 244/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3094.9958 - val_loss: 3325.1084\n",
            "Epoch 245/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3090.5864 - val_loss: 3320.5298\n",
            "Epoch 246/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3087.9050 - val_loss: 3317.7444\n",
            "Epoch 247/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3084.9333 - val_loss: 3316.4607\n",
            "Epoch 248/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3081.6885 - val_loss: 3313.9753\n",
            "Epoch 249/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3079.5872 - val_loss: 3305.3887\n",
            "Epoch 250/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3077.0615 - val_loss: 3304.7615\n",
            "Epoch 251/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3078.9417 - val_loss: 3295.2935\n",
            "Epoch 252/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3069.4253 - val_loss: 3294.2935\n",
            "Epoch 253/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3066.4600 - val_loss: 3291.7549\n",
            "Epoch 254/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3065.3528 - val_loss: 3287.5112\n",
            "Epoch 255/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3061.8850 - val_loss: 3287.0127\n",
            "Epoch 256/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3058.3296 - val_loss: 3286.1292\n",
            "Epoch 257/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3055.7148 - val_loss: 3282.8589\n",
            "Epoch 258/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3053.0315 - val_loss: 3279.3828\n",
            "Epoch 259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3050.3984 - val_loss: 3276.7097\n",
            "Epoch 260/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3048.1794 - val_loss: 3275.8484\n",
            "Epoch 261/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3045.7136 - val_loss: 3281.8428\n",
            "Epoch 262/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3042.0886 - val_loss: 3277.8093\n",
            "Epoch 263/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3038.8774 - val_loss: 3280.8391\n",
            "Epoch 264/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3035.2419 - val_loss: 3281.3511\n",
            "Epoch 265/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3033.6204 - val_loss: 3282.2886\n",
            "Epoch 266/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3031.1575 - val_loss: 3282.4592\n",
            "Epoch 267/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3029.0779 - val_loss: 3282.9722\n",
            "Epoch 268/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 3027.5361 - val_loss: 3285.1714\n",
            "Epoch 269/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3025.5977 - val_loss: 3285.2004\n",
            "Epoch 270/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 3022.2319 - val_loss: 3282.6677\n",
            "Epoch 271/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3019.2712 - val_loss: 3276.2122\n",
            "Epoch 272/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3016.7168 - val_loss: 3272.8877\n",
            "Epoch 273/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3013.2886 - val_loss: 3269.2859\n",
            "Epoch 274/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3010.6814 - val_loss: 3260.8767\n",
            "Epoch 275/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3006.6338 - val_loss: 3257.7812\n",
            "Epoch 276/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3008.7156 - val_loss: 3253.6208\n",
            "Epoch 277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3003.8262 - val_loss: 3252.0947\n",
            "Epoch 278/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3002.6150 - val_loss: 3249.8157\n",
            "Epoch 279/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2999.9868 - val_loss: 3250.3110\n",
            "Epoch 280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2997.3113 - val_loss: 3251.6846\n",
            "Epoch 281/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2994.3501 - val_loss: 3250.0930\n",
            "Epoch 282/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2993.1646 - val_loss: 3247.5706\n",
            "Epoch 283/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2991.6821 - val_loss: 3246.9910\n",
            "Epoch 284/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2990.3201 - val_loss: 3243.4656\n",
            "Epoch 285/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2992.2861 - val_loss: 3240.1440\n",
            "Epoch 286/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2987.9670 - val_loss: 3242.7822\n",
            "Epoch 287/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2983.6208 - val_loss: 3244.9480\n",
            "Epoch 288/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2979.0049 - val_loss: 3250.6721\n",
            "Epoch 289/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2980.5344 - val_loss: 3254.3408\n",
            "Epoch 290/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2978.4268 - val_loss: 3252.9602\n",
            "Epoch 291/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2976.1362 - val_loss: 3249.1621\n",
            "Epoch 292/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2974.0898 - val_loss: 3247.5779\n",
            "Epoch 293/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2971.5320 - val_loss: 3247.7261\n",
            "Epoch 294/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2968.9771 - val_loss: 3247.7554\n",
            "Epoch 295/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2967.3438 - val_loss: 3245.2244\n",
            "Epoch 296/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2964.3843 - val_loss: 3240.2781\n",
            "Epoch 297/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2961.9104 - val_loss: 3241.0254\n",
            "Epoch 298/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2960.2910 - val_loss: 3240.4717\n",
            "Epoch 299/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2958.0642 - val_loss: 3240.5908\n",
            "Epoch 300/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2956.6414 - val_loss: 3240.4346\n",
            "Epoch 301/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2955.6838 - val_loss: 3238.3040\n",
            "Epoch 302/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2952.7817 - val_loss: 3233.1113\n",
            "Epoch 303/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2950.6304 - val_loss: 3231.6152\n",
            "Epoch 304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2948.6819 - val_loss: 3226.3308\n",
            "Epoch 305/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2947.1946 - val_loss: 3223.8728\n",
            "Epoch 306/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2945.4353 - val_loss: 3222.3953\n",
            "Epoch 307/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2943.8291 - val_loss: 3221.8540\n",
            "Epoch 308/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2942.3450 - val_loss: 3220.7229\n",
            "Epoch 309/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2941.0295 - val_loss: 3226.2278\n",
            "Epoch 310/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2939.2612 - val_loss: 3227.7622\n",
            "Epoch 311/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2938.6877 - val_loss: 3225.7668\n",
            "Epoch 312/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2936.0535 - val_loss: 3229.5681\n",
            "Epoch 313/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2934.8513 - val_loss: 3228.2891\n",
            "Epoch 314/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2932.5981 - val_loss: 3220.2515\n",
            "Epoch 315/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2931.6169 - val_loss: 3218.2554\n",
            "Epoch 316/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2929.2815 - val_loss: 3212.3496\n",
            "Epoch 317/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2929.0828 - val_loss: 3211.3066\n",
            "Epoch 318/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2927.7417 - val_loss: 3212.3726\n",
            "Epoch 319/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2926.2590 - val_loss: 3214.2141\n",
            "Epoch 320/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2924.8743 - val_loss: 3213.4951\n",
            "Epoch 321/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2923.3867 - val_loss: 3207.9985\n",
            "Epoch 322/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2923.1863 - val_loss: 3208.2302\n",
            "Epoch 323/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2922.5396 - val_loss: 3199.9087\n",
            "Epoch 324/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2920.4338 - val_loss: 3196.7668\n",
            "Epoch 325/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2921.5974 - val_loss: 3195.4175\n",
            "Epoch 326/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2919.5537 - val_loss: 3194.9399\n",
            "Epoch 327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2917.2014 - val_loss: 3195.8228\n",
            "Epoch 328/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2915.8406 - val_loss: 3196.5498\n",
            "Epoch 329/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2913.7505 - val_loss: 3199.4976\n",
            "Epoch 330/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2912.7568 - val_loss: 3199.3267\n",
            "Epoch 331/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2911.2383 - val_loss: 3197.6201\n",
            "Epoch 332/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2909.1948 - val_loss: 3198.4241\n",
            "Epoch 333/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2907.9651 - val_loss: 3192.5015\n",
            "Epoch 334/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2907.4844 - val_loss: 3188.1484\n",
            "Epoch 335/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2907.4287 - val_loss: 3183.8516\n",
            "Epoch 336/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2905.6489 - val_loss: 3186.0447\n",
            "Epoch 337/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2904.1548 - val_loss: 3183.7566\n",
            "Epoch 338/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2903.1038 - val_loss: 3183.0120\n",
            "Epoch 339/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2901.5618 - val_loss: 3179.8782\n",
            "Epoch 340/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2901.5693 - val_loss: 3180.1099\n",
            "Epoch 341/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2900.5864 - val_loss: 3177.9016\n",
            "Epoch 342/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2901.5183 - val_loss: 3177.5879\n",
            "Epoch 343/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2898.7078 - val_loss: 3179.3059\n",
            "Epoch 344/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2895.0317 - val_loss: 3178.6243\n",
            "Epoch 345/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2893.9185 - val_loss: 3178.2302\n",
            "Epoch 346/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2892.7805 - val_loss: 3179.6523\n",
            "Epoch 347/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2889.1284 - val_loss: 3182.5200\n",
            "Epoch 348/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2886.0925 - val_loss: 3186.0627\n",
            "Epoch 349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2882.1001 - val_loss: 3186.5684\n",
            "Epoch 350/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2880.3594 - val_loss: 3186.8704\n",
            "Epoch 351/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2874.0791 - val_loss: 3197.7612\n",
            "Epoch 352/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2875.9944 - val_loss: 3201.4937\n",
            "Epoch 353/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2876.9731 - val_loss: 3203.9465\n",
            "Epoch 354/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2875.4702 - val_loss: 3199.6106\n",
            "Epoch 355/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2872.8655 - val_loss: 3199.5078\n",
            "Epoch 356/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2871.6318 - val_loss: 3199.0122\n",
            "Epoch 357/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2870.6670 - val_loss: 3198.3977\n",
            "Epoch 358/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2868.7314 - val_loss: 3195.9719\n",
            "Epoch 359/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2867.9087 - val_loss: 3201.3918\n",
            "Epoch 360/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2868.3662 - val_loss: 3208.4690\n",
            "Epoch 361/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2871.1470 - val_loss: 3213.4214\n",
            "Epoch 362/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2868.0537 - val_loss: 3202.5215\n",
            "Epoch 363/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2863.9661 - val_loss: 3198.8765\n",
            "Epoch 364/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2862.3730 - val_loss: 3195.2644\n",
            "Epoch 365/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2862.1016 - val_loss: 3186.9368\n",
            "Epoch 366/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2859.8528 - val_loss: 3183.0042\n",
            "Epoch 367/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2857.6970 - val_loss: 3180.1467\n",
            "Epoch 368/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2857.1628 - val_loss: 3180.5828\n",
            "Epoch 369/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2855.9055 - val_loss: 3181.0022\n",
            "Epoch 370/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2854.6501 - val_loss: 3178.5520\n",
            "Epoch 371/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2852.8257 - val_loss: 3175.1660\n",
            "Epoch 372/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2854.0835 - val_loss: 3175.4465\n",
            "Epoch 373/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2853.1016 - val_loss: 3175.1089\n",
            "Epoch 374/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2851.3423 - val_loss: 3175.8584\n",
            "Epoch 375/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2850.7690 - val_loss: 3173.9585\n",
            "Epoch 376/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2849.9990 - val_loss: 3174.2717\n",
            "Epoch 377/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2848.3445 - val_loss: 3175.8420\n",
            "Epoch 378/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2845.3806 - val_loss: 3178.5908\n",
            "Epoch 379/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2844.2385 - val_loss: 3177.1611\n",
            "Epoch 380/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2843.6838 - val_loss: 3176.2893\n",
            "Epoch 381/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2842.4314 - val_loss: 3175.2915\n",
            "Epoch 382/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2842.7200 - val_loss: 3174.5186\n",
            "Epoch 383/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2842.1108 - val_loss: 3175.5120\n",
            "Epoch 384/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2840.5637 - val_loss: 3170.3413\n",
            "Epoch 385/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2840.9387 - val_loss: 3169.0923\n",
            "Epoch 386/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2844.6814 - val_loss: 3166.5359\n",
            "Epoch 387/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2843.7039 - val_loss: 3167.3652\n",
            "Epoch 388/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2841.7712 - val_loss: 3169.5334\n",
            "Epoch 389/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2839.5012 - val_loss: 3167.5359\n",
            "Epoch 390/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2838.3340 - val_loss: 3168.2458\n",
            "Epoch 391/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2836.5103 - val_loss: 3169.3013\n",
            "Epoch 392/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2836.0913 - val_loss: 3168.7830\n",
            "Epoch 393/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2836.2036 - val_loss: 3169.3245\n",
            "Epoch 394/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2834.5115 - val_loss: 3172.6184\n",
            "Epoch 395/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2831.2034 - val_loss: 3174.6226\n",
            "Epoch 396/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2830.4609 - val_loss: 3176.5200\n",
            "Epoch 397/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2828.2439 - val_loss: 3176.7483\n",
            "Epoch 398/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2826.6541 - val_loss: 3179.1116\n",
            "Epoch 399/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2826.5435 - val_loss: 3184.1179\n",
            "Epoch 400/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2825.8416 - val_loss: 3188.0535\n",
            "Epoch 401/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2823.9001 - val_loss: 3187.3560\n",
            "Epoch 402/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2823.1362 - val_loss: 3188.1160\n",
            "Epoch 403/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2821.5437 - val_loss: 3186.7434\n",
            "Epoch 404/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2821.1987 - val_loss: 3190.0544\n",
            "Epoch 405/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2820.6389 - val_loss: 3187.7114\n",
            "Epoch 406/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2820.0005 - val_loss: 3186.9441\n",
            "Epoch 407/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2819.0286 - val_loss: 3186.9341\n",
            "Epoch 408/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2817.6311 - val_loss: 3184.1865\n",
            "Epoch 409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2817.2410 - val_loss: 3183.3965\n",
            "Epoch 410/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2816.1086 - val_loss: 3180.2190\n",
            "Epoch 411/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2815.8083 - val_loss: 3181.8652\n",
            "Epoch 412/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2815.5449 - val_loss: 3189.3118\n",
            "Epoch 413/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2817.3574 - val_loss: 3198.8977\n",
            "Epoch 414/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2817.1689 - val_loss: 3194.7710\n",
            "Epoch 415/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2813.2136 - val_loss: 3184.9397\n",
            "Epoch 416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2812.5125 - val_loss: 3184.2412\n",
            "Epoch 417/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2811.4229 - val_loss: 3181.1130\n",
            "Epoch 418/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2811.0518 - val_loss: 3181.8357\n",
            "Epoch 419/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2810.2686 - val_loss: 3181.1545\n",
            "Epoch 420/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2809.7141 - val_loss: 3177.7021\n",
            "Epoch 421/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2809.7939 - val_loss: 3179.0242\n",
            "Epoch 422/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2808.9741 - val_loss: 3175.0046\n",
            "Epoch 423/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2807.1384 - val_loss: 3184.7153\n",
            "Epoch 424/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2806.8840 - val_loss: 3181.2021\n",
            "Epoch 425/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2804.0671 - val_loss: 3190.3835\n",
            "Epoch 426/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2805.6963 - val_loss: 3187.0371\n",
            "Epoch 427/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2804.4731 - val_loss: 3180.2051\n",
            "Epoch 428/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2803.4966 - val_loss: 3177.2444\n",
            "Epoch 429/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2805.8491 - val_loss: 3166.8328\n",
            "Epoch 430/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2802.8289 - val_loss: 3161.4734\n",
            "Epoch 431/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2801.9580 - val_loss: 3165.2246\n",
            "Epoch 432/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2800.6755 - val_loss: 3167.0747\n",
            "Epoch 433/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2802.2546 - val_loss: 3165.1177\n",
            "Epoch 434/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2801.7573 - val_loss: 3162.9529\n",
            "Epoch 435/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2799.6201 - val_loss: 3163.9246\n",
            "Epoch 436/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2797.8174 - val_loss: 3164.7942\n",
            "Epoch 437/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2797.0903 - val_loss: 3169.6450\n",
            "Epoch 438/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2794.8127 - val_loss: 3172.3044\n",
            "Epoch 439/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2796.3198 - val_loss: 3174.9341\n",
            "Epoch 440/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2794.0129 - val_loss: 3174.2332\n",
            "Epoch 441/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2793.4456 - val_loss: 3174.6450\n",
            "Epoch 442/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2793.8418 - val_loss: 3179.1772\n",
            "Epoch 443/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2792.9946 - val_loss: 3178.6521\n",
            "Epoch 444/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2792.2695 - val_loss: 3179.0471\n",
            "Epoch 445/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2791.5513 - val_loss: 3176.7959\n",
            "Epoch 446/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2790.4294 - val_loss: 3176.4148\n",
            "Epoch 447/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2789.7126 - val_loss: 3173.1858\n",
            "Epoch 448/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2786.8369 - val_loss: 3167.5249\n",
            "Epoch 449/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2788.0569 - val_loss: 3164.2053\n",
            "Epoch 450/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2788.1680 - val_loss: 3165.2563\n",
            "Epoch 451/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2787.6086 - val_loss: 3168.2524\n",
            "Epoch 452/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2785.7976 - val_loss: 3169.5767\n",
            "Epoch 453/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2785.5283 - val_loss: 3172.4087\n",
            "Epoch 454/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2785.9580 - val_loss: 3178.7646\n",
            "Epoch 455/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2784.3320 - val_loss: 3177.8921\n",
            "Epoch 456/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2783.5327 - val_loss: 3174.3640\n",
            "Epoch 457/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2781.4661 - val_loss: 3167.4558\n",
            "Epoch 458/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2782.4316 - val_loss: 3164.4048\n",
            "Epoch 459/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2781.4031 - val_loss: 3160.5784\n",
            "Epoch 460/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2782.3682 - val_loss: 3162.0842\n",
            "Epoch 461/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2780.9700 - val_loss: 3163.4490\n",
            "Epoch 462/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2780.3330 - val_loss: 3165.8228\n",
            "Epoch 463/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2778.7354 - val_loss: 3172.4382\n",
            "Epoch 464/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2777.8186 - val_loss: 3175.7148\n",
            "Epoch 465/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2779.8296 - val_loss: 3184.0994\n",
            "Epoch 466/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2778.9604 - val_loss: 3179.8672\n",
            "Epoch 467/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2778.9280 - val_loss: 3174.3271\n",
            "Epoch 468/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2775.9104 - val_loss: 3175.1685\n",
            "Epoch 469/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2775.0903 - val_loss: 3173.8389\n",
            "Epoch 470/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2774.3572 - val_loss: 3176.4465\n",
            "Epoch 471/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2773.4861 - val_loss: 3170.7466\n",
            "Epoch 472/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2773.5371 - val_loss: 3170.1130\n",
            "Epoch 473/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2772.4333 - val_loss: 3168.6604\n",
            "Epoch 474/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2771.7122 - val_loss: 3165.9722\n",
            "Epoch 475/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2773.4155 - val_loss: 3167.6096\n",
            "Epoch 476/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2772.7146 - val_loss: 3160.7163\n",
            "Epoch 477/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2770.7488 - val_loss: 3162.1201\n",
            "Epoch 478/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2769.7637 - val_loss: 3165.1340\n",
            "Epoch 479/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2768.7371 - val_loss: 3167.5962\n",
            "Epoch 480/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2769.5422 - val_loss: 3168.7017\n",
            "Epoch 481/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2768.6035 - val_loss: 3165.8599\n",
            "Epoch 482/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2766.7368 - val_loss: 3172.9941\n",
            "Epoch 483/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2768.3191 - val_loss: 3177.8560\n",
            "Epoch 484/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2769.0642 - val_loss: 3180.3728\n",
            "Epoch 485/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2768.7129 - val_loss: 3178.1643\n",
            "Epoch 486/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2766.0493 - val_loss: 3174.3943\n",
            "Epoch 487/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2765.9231 - val_loss: 3172.7651\n",
            "Epoch 488/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2764.5156 - val_loss: 3167.9985\n",
            "Epoch 489/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2764.8701 - val_loss: 3166.0259\n",
            "Epoch 490/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2763.0884 - val_loss: 3163.7332\n",
            "Epoch 491/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2763.6060 - val_loss: 3161.3364\n",
            "Epoch 492/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2762.7446 - val_loss: 3160.6160\n",
            "Epoch 493/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2762.2180 - val_loss: 3159.3479\n",
            "Epoch 494/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2764.0339 - val_loss: 3153.1956\n",
            "Epoch 495/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2763.6194 - val_loss: 3153.2947\n",
            "Epoch 496/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2762.9912 - val_loss: 3150.9104\n",
            "Epoch 497/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2768.2471 - val_loss: 3146.4778\n",
            "Epoch 498/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2770.0837 - val_loss: 3144.9185\n",
            "Epoch 499/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2770.0686 - val_loss: 3145.8159\n",
            "Epoch 500/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2767.2356 - val_loss: 3147.7837\n",
            "Epoch 501/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2764.3953 - val_loss: 3150.2141\n",
            "Epoch 502/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2761.3467 - val_loss: 3150.7471\n",
            "Epoch 503/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2760.5396 - val_loss: 3152.1584\n",
            "Epoch 504/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2760.4817 - val_loss: 3153.9453\n",
            "Epoch 505/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2758.4133 - val_loss: 3152.0354\n",
            "Epoch 506/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2761.4417 - val_loss: 3148.6421\n",
            "Epoch 507/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2758.8333 - val_loss: 3153.0457\n",
            "Epoch 508/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2755.5728 - val_loss: 3156.8958\n",
            "Epoch 509/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2755.6077 - val_loss: 3159.1987\n",
            "Epoch 510/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2754.4180 - val_loss: 3157.7935\n",
            "Epoch 511/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2752.8533 - val_loss: 3158.9978\n",
            "Epoch 512/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2751.5957 - val_loss: 3163.1572\n",
            "Epoch 513/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2750.6738 - val_loss: 3166.5085\n",
            "Epoch 514/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2750.2917 - val_loss: 3166.7190\n",
            "Epoch 515/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2750.1838 - val_loss: 3167.7683\n",
            "Epoch 516/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2749.4087 - val_loss: 3166.8640\n",
            "Epoch 517/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2748.8933 - val_loss: 3166.5417\n",
            "Epoch 518/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2748.6252 - val_loss: 3164.6860\n",
            "Epoch 519/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2746.5359 - val_loss: 3159.6226\n",
            "Epoch 520/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2748.8831 - val_loss: 3159.4207\n",
            "Epoch 521/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2749.1719 - val_loss: 3158.0337\n",
            "Epoch 522/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2746.1382 - val_loss: 3164.4517\n",
            "Epoch 523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2745.6196 - val_loss: 3168.3267\n",
            "Epoch 524/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2745.0793 - val_loss: 3175.0569\n",
            "Epoch 525/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2744.4417 - val_loss: 3180.7349\n",
            "Epoch 526/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2746.2529 - val_loss: 3182.0105\n",
            "Epoch 527/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2745.4719 - val_loss: 3179.0916\n",
            "Epoch 528/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2744.7271 - val_loss: 3172.6833\n",
            "Epoch 529/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2741.4902 - val_loss: 3169.3591\n",
            "Epoch 530/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2740.2710 - val_loss: 3162.5383\n",
            "Epoch 531/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2742.8816 - val_loss: 3161.0122\n",
            "Epoch 532/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2743.0925 - val_loss: 3160.7178\n",
            "Epoch 533/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2741.6941 - val_loss: 3162.5273\n",
            "Epoch 534/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2741.5764 - val_loss: 3165.3789\n",
            "Epoch 535/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2741.9688 - val_loss: 3159.4878\n",
            "Epoch 536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2741.2310 - val_loss: 3160.7661\n",
            "Epoch 537/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2739.5571 - val_loss: 3164.8860\n",
            "Epoch 538/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2739.1531 - val_loss: 3166.1729\n",
            "Epoch 539/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2739.6379 - val_loss: 3161.6555\n",
            "Epoch 540/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2739.7583 - val_loss: 3161.9512\n",
            "Epoch 541/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2738.9712 - val_loss: 3163.2524\n",
            "Epoch 542/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2738.2336 - val_loss: 3167.0610\n",
            "Epoch 543/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2734.9302 - val_loss: 3173.6047\n",
            "Epoch 544/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2735.4011 - val_loss: 3174.1433\n",
            "Epoch 545/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2736.3960 - val_loss: 3170.2815\n",
            "Epoch 546/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2734.4556 - val_loss: 3172.3621\n",
            "Epoch 547/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2734.6807 - val_loss: 3171.7917\n",
            "Epoch 548/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2733.5168 - val_loss: 3169.6885\n",
            "Epoch 549/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2734.1201 - val_loss: 3169.7002\n",
            "Epoch 550/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2732.8689 - val_loss: 3169.7141\n",
            "Epoch 551/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2732.7388 - val_loss: 3169.1306\n",
            "Epoch 552/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2732.1907 - val_loss: 3166.7742\n",
            "Epoch 553/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2731.5750 - val_loss: 3160.9734\n",
            "Epoch 554/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2732.8042 - val_loss: 3162.3181\n",
            "Epoch 555/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2731.7180 - val_loss: 3163.5835\n",
            "Epoch 556/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2729.4565 - val_loss: 3167.8542\n",
            "Epoch 557/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2730.5269 - val_loss: 3170.9375\n",
            "Epoch 558/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2730.5176 - val_loss: 3180.0571\n",
            "Epoch 559/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2731.5476 - val_loss: 3178.0906\n",
            "Epoch 560/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2731.2358 - val_loss: 3173.7092\n",
            "Epoch 561/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2730.0286 - val_loss: 3170.9895\n",
            "Epoch 562/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2728.5630 - val_loss: 3169.6897\n",
            "Epoch 563/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2727.3379 - val_loss: 3167.7642\n",
            "Epoch 564/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2727.3838 - val_loss: 3167.1594\n",
            "Epoch 565/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2727.6250 - val_loss: 3167.9253\n",
            "Epoch 566/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2726.8840 - val_loss: 3167.6025\n",
            "Epoch 567/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2726.4216 - val_loss: 3168.1765\n",
            "Epoch 568/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2726.0647 - val_loss: 3166.0386\n",
            "Epoch 569/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2726.0872 - val_loss: 3163.5024\n",
            "Epoch 570/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2724.5864 - val_loss: 3160.5217\n",
            "Epoch 571/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2724.3838 - val_loss: 3161.8784\n",
            "Epoch 572/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2724.5415 - val_loss: 3158.5994\n",
            "Epoch 573/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2725.0510 - val_loss: 3158.7683\n",
            "Epoch 574/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2724.7957 - val_loss: 3162.3765\n",
            "Epoch 575/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2722.8191 - val_loss: 3172.2173\n",
            "Epoch 576/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2725.6382 - val_loss: 3179.3804\n",
            "Epoch 577/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2724.6812 - val_loss: 3174.8142\n",
            "Epoch 578/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2723.6865 - val_loss: 3163.7866\n",
            "Epoch 579/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2721.8804 - val_loss: 3160.1257\n",
            "Epoch 580/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2720.2866 - val_loss: 3159.0242\n",
            "Epoch 581/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2719.1252 - val_loss: 3167.7078\n",
            "Epoch 582/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2720.3416 - val_loss: 3172.4719\n",
            "Epoch 583/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2721.8303 - val_loss: 3176.6841\n",
            "Epoch 584/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2721.6755 - val_loss: 3176.1787\n",
            "Epoch 585/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2720.0178 - val_loss: 3170.7021\n",
            "Epoch 586/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2718.5137 - val_loss: 3167.7451\n",
            "Epoch 587/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2717.7468 - val_loss: 3163.6953\n",
            "Epoch 588/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2718.6150 - val_loss: 3154.5991\n",
            "Epoch 589/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2716.1387 - val_loss: 3153.0327\n",
            "Epoch 590/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2715.4858 - val_loss: 3154.3091\n",
            "Epoch 591/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2713.5945 - val_loss: 3158.5586\n",
            "Epoch 592/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2714.5435 - val_loss: 3157.9922\n",
            "Epoch 593/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2716.8992 - val_loss: 3168.9055\n",
            "Epoch 594/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2716.7375 - val_loss: 3176.9368\n",
            "Epoch 595/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2716.5552 - val_loss: 3174.3950\n",
            "Epoch 596/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2718.0654 - val_loss: 3181.2356\n",
            "Epoch 597/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2717.3364 - val_loss: 3171.0134\n",
            "Epoch 598/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2715.0215 - val_loss: 3170.5000\n",
            "Epoch 599/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2713.2405 - val_loss: 3166.9648\n",
            "Epoch 600/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2712.4917 - val_loss: 3165.2971\n",
            "Epoch 601/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2711.5330 - val_loss: 3165.1851\n",
            "Epoch 602/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2711.5820 - val_loss: 3169.0154\n",
            "Epoch 603/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2715.4131 - val_loss: 3178.6733\n",
            "Epoch 604/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2718.7942 - val_loss: 3191.3921\n",
            "Epoch 605/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2720.9346 - val_loss: 3193.6140\n",
            "Epoch 606/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2720.5979 - val_loss: 3191.4971\n",
            "Epoch 607/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2717.4790 - val_loss: 3181.8848\n",
            "Epoch 608/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2710.2949 - val_loss: 3168.5623\n",
            "Epoch 609/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2709.9780 - val_loss: 3162.9309\n",
            "Epoch 610/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2708.4546 - val_loss: 3152.4409\n",
            "Epoch 611/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2707.0364 - val_loss: 3147.5801\n",
            "Epoch 612/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2707.5037 - val_loss: 3145.3328\n",
            "Epoch 613/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2708.9885 - val_loss: 3140.5935\n",
            "Epoch 614/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2709.6216 - val_loss: 3137.9072\n",
            "Epoch 615/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2709.3481 - val_loss: 3137.9150\n",
            "Epoch 616/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2708.1697 - val_loss: 3140.0024\n",
            "Epoch 617/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2705.9260 - val_loss: 3142.0916\n",
            "Epoch 618/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2705.4163 - val_loss: 3144.0667\n",
            "Epoch 619/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2705.5046 - val_loss: 3141.9834\n",
            "Epoch 620/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2704.9663 - val_loss: 3143.5842\n",
            "Epoch 621/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2705.2258 - val_loss: 3141.6946\n",
            "Epoch 622/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2704.5085 - val_loss: 3144.2690\n",
            "Epoch 623/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2702.4519 - val_loss: 3153.5134\n",
            "Epoch 624/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2702.6099 - val_loss: 3153.6050\n",
            "Epoch 625/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2699.7532 - val_loss: 3168.0154\n",
            "Epoch 626/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2704.2688 - val_loss: 3175.9878\n",
            "Epoch 627/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2705.6353 - val_loss: 3177.8604\n",
            "Epoch 628/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2706.2229 - val_loss: 3176.9958\n",
            "Epoch 629/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2704.7224 - val_loss: 3173.6418\n",
            "Epoch 630/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2703.5879 - val_loss: 3170.8406\n",
            "Epoch 631/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2702.9407 - val_loss: 3171.2510\n",
            "Epoch 632/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2701.4109 - val_loss: 3162.6677\n",
            "Epoch 633/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2700.1865 - val_loss: 3159.1587\n",
            "Epoch 634/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2698.5356 - val_loss: 3167.4255\n",
            "Epoch 635/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2701.4050 - val_loss: 3170.1628\n",
            "Epoch 636/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2699.9185 - val_loss: 3168.7676\n",
            "Epoch 637/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2699.7473 - val_loss: 3168.0491\n",
            "Epoch 638/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2699.2896 - val_loss: 3169.2419\n",
            "Epoch 639/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2700.5120 - val_loss: 3172.6636\n",
            "Epoch 640/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2698.9421 - val_loss: 3171.4575\n",
            "Epoch 641/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2700.5276 - val_loss: 3177.1790\n",
            "Epoch 642/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2700.1763 - val_loss: 3172.0632\n",
            "Epoch 643/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2699.6226 - val_loss: 3164.8423\n",
            "Epoch 644/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2697.1118 - val_loss: 3161.3503\n",
            "Epoch 645/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2696.7207 - val_loss: 3164.0303\n",
            "Epoch 646/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2695.2400 - val_loss: 3160.5591\n",
            "Epoch 647/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2694.9868 - val_loss: 3154.9854\n",
            "Epoch 648/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2694.5154 - val_loss: 3154.3860\n",
            "Epoch 649/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2693.9766 - val_loss: 3150.2676\n",
            "Epoch 650/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2694.0879 - val_loss: 3151.5303\n",
            "Epoch 651/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2695.3464 - val_loss: 3149.4807\n",
            "Epoch 652/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2693.8679 - val_loss: 3149.0073\n",
            "Epoch 653/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2693.1467 - val_loss: 3153.3027\n",
            "Epoch 654/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2693.3840 - val_loss: 3155.4971\n",
            "Epoch 655/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2693.0203 - val_loss: 3154.0576\n",
            "Epoch 656/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2692.0972 - val_loss: 3155.4790\n",
            "Epoch 657/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2691.5513 - val_loss: 3158.4116\n",
            "Epoch 658/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2691.4822 - val_loss: 3161.5720\n",
            "Epoch 659/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2691.4590 - val_loss: 3166.3459\n",
            "Epoch 660/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2692.5854 - val_loss: 3166.4229\n",
            "Epoch 661/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2692.0098 - val_loss: 3158.8047\n",
            "Epoch 662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2690.4280 - val_loss: 3157.4072\n",
            "Epoch 663/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2690.4707 - val_loss: 3153.3564\n",
            "Epoch 664/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2690.4043 - val_loss: 3154.3765\n",
            "Epoch 665/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2687.9224 - val_loss: 3162.3484\n",
            "Epoch 666/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2691.2776 - val_loss: 3167.0947\n",
            "Epoch 667/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2692.7317 - val_loss: 3168.7871\n",
            "Epoch 668/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2692.2246 - val_loss: 3168.7454\n",
            "Epoch 669/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2691.3826 - val_loss: 3166.5432\n",
            "Epoch 670/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2690.4578 - val_loss: 3163.2190\n",
            "Epoch 671/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2689.7622 - val_loss: 3162.6362\n",
            "Epoch 672/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2689.0398 - val_loss: 3160.9761\n",
            "Epoch 673/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2688.5388 - val_loss: 3160.4849\n",
            "Epoch 674/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2688.8269 - val_loss: 3159.8672\n",
            "Epoch 675/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2688.2183 - val_loss: 3164.0425\n",
            "Epoch 676/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2688.8972 - val_loss: 3157.8308\n",
            "Epoch 677/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2686.4553 - val_loss: 3153.5322\n",
            "Epoch 678/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2686.7043 - val_loss: 3151.3740\n",
            "Epoch 679/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2687.0317 - val_loss: 3146.4536\n",
            "Epoch 680/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2685.9333 - val_loss: 3145.9038\n",
            "Epoch 681/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2685.9880 - val_loss: 3146.4561\n",
            "Epoch 682/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2685.4941 - val_loss: 3147.5928\n",
            "Epoch 683/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2685.9258 - val_loss: 3151.6938\n",
            "Epoch 684/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2684.1421 - val_loss: 3154.5791\n",
            "Epoch 685/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2683.7908 - val_loss: 3157.6597\n",
            "Epoch 686/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2684.7495 - val_loss: 3154.8582\n",
            "Epoch 687/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2685.8552 - val_loss: 3149.5259\n",
            "Epoch 688/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2683.4146 - val_loss: 3157.4128\n",
            "Epoch 689/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2683.9016 - val_loss: 3160.9937\n",
            "Epoch 690/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2683.8008 - val_loss: 3162.1482\n",
            "Epoch 691/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2683.4153 - val_loss: 3161.7815\n",
            "Epoch 692/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2682.6985 - val_loss: 3158.7354\n",
            "Epoch 693/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2682.6094 - val_loss: 3157.9246\n",
            "Epoch 694/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2682.2314 - val_loss: 3155.1179\n",
            "Epoch 695/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2681.7532 - val_loss: 3157.5369\n",
            "Epoch 696/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2683.0547 - val_loss: 3167.4241\n",
            "Epoch 697/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2684.9800 - val_loss: 3167.7251\n",
            "Epoch 698/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2684.1787 - val_loss: 3165.2563\n",
            "Epoch 699/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2682.6052 - val_loss: 3159.7429\n",
            "Epoch 700/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2681.8037 - val_loss: 3158.3928\n",
            "Epoch 701/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2681.8037 - val_loss: 3156.6826\n",
            "Epoch 702/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2680.1714 - val_loss: 3155.9414\n",
            "Epoch 703/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2680.0054 - val_loss: 3157.6023\n",
            "Epoch 704/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2678.0671 - val_loss: 3152.0029\n",
            "Epoch 705/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2678.7388 - val_loss: 3151.4551\n",
            "Epoch 706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2677.9346 - val_loss: 3158.2991\n",
            "Epoch 707/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2678.7051 - val_loss: 3160.4880\n",
            "Epoch 708/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2678.8108 - val_loss: 3161.6328\n",
            "Epoch 709/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2677.3813 - val_loss: 3159.1421\n",
            "Epoch 710/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2682.2595 - val_loss: 3166.6794\n",
            "Epoch 711/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2679.5918 - val_loss: 3162.2571\n",
            "Epoch 712/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2678.7964 - val_loss: 3160.7380\n",
            "Epoch 713/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2678.7246 - val_loss: 3161.5801\n",
            "Epoch 714/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2678.1794 - val_loss: 3160.9790\n",
            "Epoch 715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2678.3521 - val_loss: 3166.3765\n",
            "Epoch 716/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2678.9539 - val_loss: 3168.2554\n",
            "Epoch 717/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2678.7368 - val_loss: 3165.9167\n",
            "Epoch 718/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2677.8462 - val_loss: 3166.0649\n",
            "Epoch 719/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2679.1692 - val_loss: 3169.7756\n",
            "Epoch 720/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2678.9333 - val_loss: 3168.9971\n",
            "Epoch 721/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2676.7820 - val_loss: 3164.8503\n",
            "Epoch 722/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2676.3225 - val_loss: 3168.0872\n",
            "Epoch 723/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2676.9473 - val_loss: 3167.7690\n",
            "Epoch 724/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2676.7041 - val_loss: 3160.3252\n",
            "Epoch 725/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2675.1716 - val_loss: 3154.7747\n",
            "Epoch 726/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2674.5569 - val_loss: 3149.6665\n",
            "Epoch 727/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2676.3977 - val_loss: 3147.9285\n",
            "Epoch 728/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2676.6987 - val_loss: 3149.3533\n",
            "Epoch 729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2675.7549 - val_loss: 3150.5583\n",
            "Epoch 730/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2674.7310 - val_loss: 3153.3933\n",
            "Epoch 731/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2674.6455 - val_loss: 3153.3918\n",
            "Epoch 732/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2673.8545 - val_loss: 3155.3574\n",
            "Epoch 733/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2673.7834 - val_loss: 3157.0664\n",
            "Epoch 734/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2673.0874 - val_loss: 3156.3958\n",
            "Epoch 735/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2672.8860 - val_loss: 3155.5884\n",
            "Epoch 736/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2673.0452 - val_loss: 3158.7017\n",
            "Epoch 737/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2671.4248 - val_loss: 3161.9880\n",
            "Epoch 738/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2671.3682 - val_loss: 3162.0513\n",
            "Epoch 739/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2670.6538 - val_loss: 3158.8589\n",
            "Epoch 740/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2670.6260 - val_loss: 3159.1516\n",
            "Epoch 741/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2669.9548 - val_loss: 3155.0359\n",
            "Epoch 742/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2670.2793 - val_loss: 3152.0769\n",
            "Epoch 743/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2673.2852 - val_loss: 3149.6523\n",
            "Epoch 744/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2671.8989 - val_loss: 3151.6990\n",
            "Epoch 745/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2671.3977 - val_loss: 3158.4121\n",
            "Epoch 746/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2668.7388 - val_loss: 3155.2236\n",
            "Epoch 747/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2668.7271 - val_loss: 3151.9463\n",
            "Epoch 748/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2667.7871 - val_loss: 3156.7954\n",
            "Epoch 749/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2666.9097 - val_loss: 3158.1084\n",
            "Epoch 750/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2666.6638 - val_loss: 3158.4758\n",
            "Epoch 751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2666.7593 - val_loss: 3155.5640\n",
            "Epoch 752/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2666.8669 - val_loss: 3153.3223\n",
            "Epoch 753/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2667.3843 - val_loss: 3153.2900\n",
            "Epoch 754/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2667.1184 - val_loss: 3154.6216\n",
            "Epoch 755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2667.0679 - val_loss: 3162.8799\n",
            "Epoch 756/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2665.0737 - val_loss: 3161.3357\n",
            "Epoch 757/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2664.8789 - val_loss: 3158.6211\n",
            "Epoch 758/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2667.6738 - val_loss: 3151.8062\n",
            "Epoch 759/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2666.2695 - val_loss: 3152.3599\n",
            "Epoch 760/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2666.2222 - val_loss: 3152.5867\n",
            "Epoch 761/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2665.2839 - val_loss: 3153.7390\n",
            "Epoch 762/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2665.8352 - val_loss: 3150.7434\n",
            "Epoch 763/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2666.1897 - val_loss: 3149.7314\n",
            "Epoch 764/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2668.9460 - val_loss: 3146.1729\n",
            "Epoch 765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2667.6174 - val_loss: 3145.1096\n",
            "Epoch 766/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2669.3799 - val_loss: 3141.4087\n",
            "Epoch 767/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2670.5642 - val_loss: 3141.3308\n",
            "Epoch 768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2669.1409 - val_loss: 3142.6970\n",
            "Epoch 769/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2666.7593 - val_loss: 3145.0203\n",
            "Epoch 770/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2663.2822 - val_loss: 3146.2942\n",
            "Epoch 771/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2664.3979 - val_loss: 3146.5801\n",
            "Epoch 772/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2663.2080 - val_loss: 3146.8047\n",
            "Epoch 773/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2664.1841 - val_loss: 3143.4529\n",
            "Epoch 774/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2661.8743 - val_loss: 3148.8596\n",
            "Epoch 775/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2662.9385 - val_loss: 3146.1506\n",
            "Epoch 776/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2662.8752 - val_loss: 3148.3040\n",
            "Epoch 777/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2663.3208 - val_loss: 3146.0793\n",
            "Epoch 778/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2661.7534 - val_loss: 3149.3398\n",
            "Epoch 779/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2660.9243 - val_loss: 3151.2241\n",
            "Epoch 780/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2660.3232 - val_loss: 3152.9399\n",
            "Epoch 781/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2659.7969 - val_loss: 3155.8892\n",
            "Epoch 782/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2661.2300 - val_loss: 3152.2991\n",
            "Epoch 783/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2659.0083 - val_loss: 3152.8352\n",
            "Epoch 784/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2658.7822 - val_loss: 3152.4641\n",
            "Epoch 785/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2658.3857 - val_loss: 3152.9832\n",
            "Epoch 786/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2658.7393 - val_loss: 3155.8091\n",
            "Epoch 787/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2657.4675 - val_loss: 3158.0793\n",
            "Epoch 788/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2657.4460 - val_loss: 3160.4204\n",
            "Epoch 789/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2657.3215 - val_loss: 3160.5273\n",
            "Epoch 790/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2656.8633 - val_loss: 3161.3406\n",
            "Epoch 791/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2657.2129 - val_loss: 3162.9485\n",
            "Epoch 792/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2656.3347 - val_loss: 3163.1741\n",
            "Epoch 793/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2656.2551 - val_loss: 3164.0459\n",
            "Epoch 794/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2657.3997 - val_loss: 3167.0688\n",
            "Epoch 795/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2656.3625 - val_loss: 3162.9241\n",
            "Epoch 796/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2655.1602 - val_loss: 3164.9502\n",
            "Epoch 797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2655.4456 - val_loss: 3159.5520\n",
            "Epoch 798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2653.8782 - val_loss: 3153.5042\n",
            "Epoch 799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2654.7805 - val_loss: 3154.5784\n",
            "Epoch 800/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2654.8882 - val_loss: 3155.5811\n",
            "Epoch 801/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2654.6836 - val_loss: 3154.0535\n",
            "Epoch 802/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2655.0896 - val_loss: 3152.8960\n",
            "Epoch 803/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2654.7327 - val_loss: 3158.1221\n",
            "Epoch 804/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2653.6018 - val_loss: 3158.0728\n",
            "Epoch 805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2652.7700 - val_loss: 3157.7153\n",
            "Epoch 806/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2653.0471 - val_loss: 3155.0991\n",
            "Epoch 807/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2654.7104 - val_loss: 3149.5920\n",
            "Epoch 808/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2655.7148 - val_loss: 3147.9922\n",
            "Epoch 809/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2655.1572 - val_loss: 3148.3469\n",
            "Epoch 810/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2655.3347 - val_loss: 3148.6577\n",
            "Epoch 811/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2654.2461 - val_loss: 3149.5098\n",
            "Epoch 812/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2653.5647 - val_loss: 3150.6313\n",
            "Epoch 813/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2652.9314 - val_loss: 3149.7114\n",
            "Epoch 814/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2653.3696 - val_loss: 3149.8518\n",
            "Epoch 815/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2652.8882 - val_loss: 3149.7871\n",
            "Epoch 816/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2654.5322 - val_loss: 3149.0786\n",
            "Epoch 817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2653.0430 - val_loss: 3150.5811\n",
            "Epoch 818/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2651.9421 - val_loss: 3154.0679\n",
            "Epoch 819/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2650.2312 - val_loss: 3156.0935\n",
            "Epoch 820/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2648.7793 - val_loss: 3159.7742\n",
            "Epoch 821/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2648.8201 - val_loss: 3163.5029\n",
            "Epoch 822/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2649.1802 - val_loss: 3167.3821\n",
            "Epoch 823/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2657.4463 - val_loss: 3186.5852\n",
            "Epoch 824/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2655.5469 - val_loss: 3185.9712\n",
            "Epoch 825/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2653.8555 - val_loss: 3183.0154\n",
            "Epoch 826/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2651.8960 - val_loss: 3177.8076\n",
            "Epoch 827/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2649.2874 - val_loss: 3172.8564\n",
            "Epoch 828/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2649.2383 - val_loss: 3169.4185\n",
            "Epoch 829/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2648.0627 - val_loss: 3169.2942\n",
            "Epoch 830/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2648.1575 - val_loss: 3173.1292\n",
            "Epoch 831/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2649.4485 - val_loss: 3175.7654\n",
            "Epoch 832/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2651.0564 - val_loss: 3175.8118\n",
            "Epoch 833/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2649.1575 - val_loss: 3173.7415\n",
            "Epoch 834/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2648.5933 - val_loss: 3171.6538\n",
            "Epoch 835/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2647.9795 - val_loss: 3170.4199\n",
            "Epoch 836/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2647.1448 - val_loss: 3168.6570\n",
            "Epoch 837/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2645.9673 - val_loss: 3161.1079\n",
            "Epoch 838/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2645.6736 - val_loss: 3162.3596\n",
            "Epoch 839/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2645.8301 - val_loss: 3162.6018\n",
            "Epoch 840/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2644.8218 - val_loss: 3159.9866\n",
            "Epoch 841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2645.0442 - val_loss: 3159.6729\n",
            "Epoch 842/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2645.0203 - val_loss: 3157.7300\n",
            "Epoch 843/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2644.5486 - val_loss: 3158.8071\n",
            "Epoch 844/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2644.3718 - val_loss: 3158.7422\n",
            "Epoch 845/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2643.4751 - val_loss: 3161.4575\n",
            "Epoch 846/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2644.6062 - val_loss: 3165.4272\n",
            "Epoch 847/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2646.1797 - val_loss: 3158.5134\n",
            "Epoch 848/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2643.8931 - val_loss: 3160.3135\n",
            "Epoch 849/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2643.4294 - val_loss: 3159.6309\n",
            "Epoch 850/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2643.0435 - val_loss: 3160.1790\n",
            "Epoch 851/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2641.5564 - val_loss: 3154.6060\n",
            "Epoch 852/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2641.1726 - val_loss: 3144.6628\n",
            "Epoch 853/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2644.5012 - val_loss: 3143.2734\n",
            "Epoch 854/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2645.1182 - val_loss: 3142.9736\n",
            "Epoch 855/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2643.2258 - val_loss: 3146.2532\n",
            "Epoch 856/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2643.8406 - val_loss: 3153.3005\n",
            "Epoch 857/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2641.3853 - val_loss: 3156.2117\n",
            "Epoch 858/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2642.2195 - val_loss: 3150.3223\n",
            "Epoch 859/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2641.4180 - val_loss: 3144.9802\n",
            "Epoch 860/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2642.5193 - val_loss: 3146.4758\n",
            "Epoch 861/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2642.5820 - val_loss: 3144.7500\n",
            "Epoch 862/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2641.8000 - val_loss: 3146.6741\n",
            "Epoch 863/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2641.4312 - val_loss: 3149.9375\n",
            "Epoch 864/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2641.7937 - val_loss: 3154.5225\n",
            "Epoch 865/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2642.9585 - val_loss: 3147.2332\n",
            "Epoch 866/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2639.9531 - val_loss: 3145.7571\n",
            "Epoch 867/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2640.2229 - val_loss: 3144.8982\n",
            "Epoch 868/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2640.2336 - val_loss: 3146.9568\n",
            "Epoch 869/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2640.8906 - val_loss: 3144.9224\n",
            "Epoch 870/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2639.8452 - val_loss: 3144.2297\n",
            "Epoch 871/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2643.5164 - val_loss: 3138.8423\n",
            "Epoch 872/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2642.3870 - val_loss: 3138.6216\n",
            "Epoch 873/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2643.0447 - val_loss: 3137.5803\n",
            "Epoch 874/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2643.9937 - val_loss: 3139.7388\n",
            "Epoch 875/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2640.8037 - val_loss: 3139.3635\n",
            "Epoch 876/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2637.9446 - val_loss: 3145.1108\n",
            "Epoch 877/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2637.8965 - val_loss: 3150.1934\n",
            "Epoch 878/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2636.6169 - val_loss: 3146.8591\n",
            "Epoch 879/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2637.0969 - val_loss: 3147.6812\n",
            "Epoch 880/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2636.0757 - val_loss: 3151.1147\n",
            "Epoch 881/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2636.8542 - val_loss: 3152.5232\n",
            "Epoch 882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2636.3894 - val_loss: 3151.5171\n",
            "Epoch 883/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2636.4985 - val_loss: 3151.1104\n",
            "Epoch 884/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2636.4202 - val_loss: 3149.5315\n",
            "Epoch 885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2637.8132 - val_loss: 3154.1179\n",
            "Epoch 886/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2634.7781 - val_loss: 3153.3496\n",
            "Epoch 887/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2636.5789 - val_loss: 3162.4602\n",
            "Epoch 888/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2637.2778 - val_loss: 3164.5288\n",
            "Epoch 889/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2637.2222 - val_loss: 3163.7310\n",
            "Epoch 890/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2636.8804 - val_loss: 3166.9817\n",
            "Epoch 891/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2637.4888 - val_loss: 3172.5034\n",
            "Epoch 892/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2638.6484 - val_loss: 3172.0576\n",
            "Epoch 893/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2637.6863 - val_loss: 3170.3035\n",
            "Epoch 894/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2637.4453 - val_loss: 3169.8398\n",
            "Epoch 895/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2638.9268 - val_loss: 3177.6787\n",
            "Epoch 896/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2639.5945 - val_loss: 3177.1672\n",
            "Epoch 897/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2636.4011 - val_loss: 3165.2588\n",
            "Epoch 898/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2636.5154 - val_loss: 3156.2065\n",
            "Epoch 899/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2632.3206 - val_loss: 3153.2542\n",
            "Epoch 900/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2633.3706 - val_loss: 3155.6648\n",
            "Epoch 901/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2631.2161 - val_loss: 3163.1052\n",
            "Epoch 902/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2631.0332 - val_loss: 3171.2747\n",
            "Epoch 903/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2633.8677 - val_loss: 3172.1665\n",
            "Epoch 904/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2633.5671 - val_loss: 3166.5283\n",
            "Epoch 905/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2633.2375 - val_loss: 3164.5371\n",
            "Epoch 906/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2631.2581 - val_loss: 3159.1411\n",
            "Epoch 907/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2632.4021 - val_loss: 3152.1609\n",
            "Epoch 908/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.3901 - val_loss: 3150.3203\n",
            "Epoch 909/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.3950 - val_loss: 3149.0186\n",
            "Epoch 910/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2632.7676 - val_loss: 3148.1699\n",
            "Epoch 911/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2633.7371 - val_loss: 3148.2292\n",
            "Epoch 912/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2635.0195 - val_loss: 3149.0142\n",
            "Epoch 913/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2634.8713 - val_loss: 3151.6819\n",
            "Epoch 914/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.2441 - val_loss: 3149.7307\n",
            "Epoch 915/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.5378 - val_loss: 3149.5723\n",
            "Epoch 916/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2632.2136 - val_loss: 3151.9438\n",
            "Epoch 917/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2632.7402 - val_loss: 3152.3501\n",
            "Epoch 918/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2631.5242 - val_loss: 3150.7073\n",
            "Epoch 919/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.6060 - val_loss: 3147.6362\n",
            "Epoch 920/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2631.4795 - val_loss: 3148.5032\n",
            "Epoch 921/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2629.7573 - val_loss: 3149.8669\n",
            "Epoch 922/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2629.0615 - val_loss: 3155.0032\n",
            "Epoch 923/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2628.2581 - val_loss: 3155.9065\n",
            "Epoch 924/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2628.8005 - val_loss: 3158.0183\n",
            "Epoch 925/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2629.1282 - val_loss: 3164.4919\n",
            "Epoch 926/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2628.7512 - val_loss: 3166.3167\n",
            "Epoch 927/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2629.4561 - val_loss: 3175.7419\n",
            "Epoch 928/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.6499 - val_loss: 3171.0852\n",
            "Epoch 929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2630.3699 - val_loss: 3172.5264\n",
            "Epoch 930/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2632.0518 - val_loss: 3176.0505\n",
            "Epoch 931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2633.7393 - val_loss: 3184.2148\n",
            "Epoch 932/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2635.4851 - val_loss: 3182.7996\n",
            "Epoch 933/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2634.3669 - val_loss: 3180.4688\n",
            "Epoch 934/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2634.3767 - val_loss: 3180.4133\n",
            "Epoch 935/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2631.2397 - val_loss: 3168.4741\n",
            "Epoch 936/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2626.8643 - val_loss: 3153.5962\n",
            "Epoch 937/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2626.3159 - val_loss: 3149.3760\n",
            "Epoch 938/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2626.4333 - val_loss: 3148.5447\n",
            "Epoch 939/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2626.2258 - val_loss: 3149.5398\n",
            "Epoch 940/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2626.9861 - val_loss: 3152.5576\n",
            "Epoch 941/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2625.4470 - val_loss: 3156.8860\n",
            "Epoch 942/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2627.6187 - val_loss: 3162.7219\n",
            "Epoch 943/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2627.7053 - val_loss: 3165.5464\n",
            "Epoch 944/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2627.0505 - val_loss: 3167.1074\n",
            "Epoch 945/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2627.8303 - val_loss: 3168.0471\n",
            "Epoch 946/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2627.0793 - val_loss: 3163.4897\n",
            "Epoch 947/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2624.8748 - val_loss: 3155.1184\n",
            "Epoch 948/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2623.3552 - val_loss: 3152.2227\n",
            "Epoch 949/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2624.1479 - val_loss: 3151.1753\n",
            "Epoch 950/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2624.1401 - val_loss: 3150.3252\n",
            "Epoch 951/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2623.2761 - val_loss: 3147.6628\n",
            "Epoch 952/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2624.5068 - val_loss: 3145.1819\n",
            "Epoch 953/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2627.7458 - val_loss: 3142.3293\n",
            "Epoch 954/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2629.5959 - val_loss: 3143.0906\n",
            "Epoch 955/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2628.0049 - val_loss: 3154.5771\n",
            "Epoch 956/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2623.4126 - val_loss: 3162.5496\n",
            "Epoch 957/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2625.3281 - val_loss: 3171.7991\n",
            "Epoch 958/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2624.4739 - val_loss: 3168.0393\n",
            "Epoch 959/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2624.2097 - val_loss: 3161.4824\n",
            "Epoch 960/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2623.2903 - val_loss: 3159.2983\n",
            "Epoch 961/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2623.0339 - val_loss: 3160.7749\n",
            "Epoch 962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2622.4148 - val_loss: 3164.5066\n",
            "Epoch 963/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2622.7366 - val_loss: 3165.3123\n",
            "Epoch 964/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2622.3491 - val_loss: 3164.0200\n",
            "Epoch 965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2622.7554 - val_loss: 3160.9004\n",
            "Epoch 966/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2622.1626 - val_loss: 3162.1697\n",
            "Epoch 967/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2622.2202 - val_loss: 3163.0234\n",
            "Epoch 968/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.5369 - val_loss: 3162.1677\n",
            "Epoch 969/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.3689 - val_loss: 3159.3870\n",
            "Epoch 970/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.1804 - val_loss: 3158.5859\n",
            "Epoch 971/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.4094 - val_loss: 3160.5303\n",
            "Epoch 972/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2620.7080 - val_loss: 3163.2690\n",
            "Epoch 973/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2620.5371 - val_loss: 3170.9319\n",
            "Epoch 974/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.8914 - val_loss: 3171.2390\n",
            "Epoch 975/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2621.8428 - val_loss: 3172.4136\n",
            "Epoch 976/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2622.5725 - val_loss: 3177.5889\n",
            "Epoch 977/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2623.3943 - val_loss: 3172.0928\n",
            "Epoch 978/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2621.4719 - val_loss: 3171.5322\n",
            "Epoch 979/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2620.9094 - val_loss: 3170.9431\n",
            "Epoch 980/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2620.4592 - val_loss: 3165.6565\n",
            "Epoch 981/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2618.8938 - val_loss: 3160.4922\n",
            "Epoch 982/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.1069 - val_loss: 3158.4333\n",
            "Epoch 983/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2619.4739 - val_loss: 3161.0066\n",
            "Epoch 984/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2619.9849 - val_loss: 3165.0479\n",
            "Epoch 985/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2620.4119 - val_loss: 3172.0234\n",
            "Epoch 986/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2619.8005 - val_loss: 3175.1240\n",
            "Epoch 987/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.2051 - val_loss: 3178.3420\n",
            "Epoch 988/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.1624 - val_loss: 3177.6995\n",
            "Epoch 989/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2621.5356 - val_loss: 3168.7290\n",
            "Epoch 990/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2618.6877 - val_loss: 3171.6265\n",
            "Epoch 991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2618.9033 - val_loss: 3172.1787\n",
            "Epoch 992/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2618.7192 - val_loss: 3175.3662\n",
            "Epoch 993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2618.7378 - val_loss: 3176.7085\n",
            "Epoch 994/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2619.9124 - val_loss: 3181.9878\n",
            "Epoch 995/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2620.9780 - val_loss: 3181.4480\n",
            "Epoch 996/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2620.8831 - val_loss: 3183.2871\n",
            "Epoch 997/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2621.3279 - val_loss: 3178.7959\n",
            "Epoch 998/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2619.0725 - val_loss: 3179.9490\n",
            "Epoch 999/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2620.0632 - val_loss: 3177.2273\n",
            "Epoch 1000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2617.4355 - val_loss: 3181.3408\n",
            "Epoch 1001/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2619.2854 - val_loss: 3184.9773\n",
            "Epoch 1002/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2621.4868 - val_loss: 3192.3918\n",
            "Epoch 1003/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2621.5081 - val_loss: 3180.8096\n",
            "Epoch 1004/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2619.1970 - val_loss: 3176.7170\n",
            "Epoch 1005/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2617.6326 - val_loss: 3176.5415\n",
            "Epoch 1006/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2618.4604 - val_loss: 3182.1196\n",
            "Epoch 1007/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2620.7463 - val_loss: 3191.5723\n",
            "Epoch 1008/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2624.4890 - val_loss: 3196.4758\n",
            "Epoch 1009/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2623.5691 - val_loss: 3192.7612\n",
            "Epoch 1010/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2622.7979 - val_loss: 3188.9404\n",
            "Epoch 1011/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2620.4680 - val_loss: 3188.6064\n",
            "Epoch 1012/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2620.9946 - val_loss: 3193.9390\n",
            "Epoch 1013/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2623.2148 - val_loss: 3195.2715\n",
            "Epoch 1014/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2623.4067 - val_loss: 3194.4087\n",
            "Epoch 1015/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2621.8955 - val_loss: 3189.0842\n",
            "Epoch 1016/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2619.8508 - val_loss: 3183.2571\n",
            "Epoch 1017/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2617.7527 - val_loss: 3177.9048\n",
            "Epoch 1018/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2615.8276 - val_loss: 3170.6492\n",
            "Epoch 1019/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2616.0876 - val_loss: 3171.8752\n",
            "Epoch 1020/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2613.2595 - val_loss: 3167.1748\n",
            "Epoch 1021/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2614.9351 - val_loss: 3169.4438\n",
            "Epoch 1022/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2614.6807 - val_loss: 3171.5527\n",
            "Epoch 1023/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2614.8496 - val_loss: 3169.4592\n",
            "Epoch 1024/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2614.6245 - val_loss: 3168.0015\n",
            "Epoch 1025/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2615.8860 - val_loss: 3159.6714\n",
            "Epoch 1026/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2614.9404 - val_loss: 3159.2310\n",
            "Epoch 1027/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2614.1797 - val_loss: 3161.2654\n",
            "Epoch 1028/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2612.3933 - val_loss: 3156.7451\n",
            "Epoch 1029/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2614.2014 - val_loss: 3155.8984\n",
            "Epoch 1030/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2614.7354 - val_loss: 3155.5129\n",
            "Epoch 1031/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2615.0466 - val_loss: 3150.6377\n",
            "Epoch 1032/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2615.8115 - val_loss: 3147.4229\n",
            "Epoch 1033/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2619.7605 - val_loss: 3147.7329\n",
            "Epoch 1034/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2614.7703 - val_loss: 3153.2266\n",
            "Epoch 1035/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2613.7214 - val_loss: 3152.0779\n",
            "Epoch 1036/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2617.8320 - val_loss: 3150.7146\n",
            "Epoch 1037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2618.0925 - val_loss: 3150.6177\n",
            "Epoch 1038/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2617.2173 - val_loss: 3152.3835\n",
            "Epoch 1039/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2612.9321 - val_loss: 3159.0291\n",
            "Epoch 1040/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2612.8984 - val_loss: 3159.1123\n",
            "Epoch 1041/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2612.6226 - val_loss: 3158.2346\n",
            "Epoch 1042/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2612.2310 - val_loss: 3159.1428\n",
            "Epoch 1043/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2612.7383 - val_loss: 3162.1323\n",
            "Epoch 1044/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2610.8662 - val_loss: 3163.8708\n",
            "Epoch 1045/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.4092 - val_loss: 3164.4136\n",
            "Epoch 1046/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2612.7209 - val_loss: 3161.9473\n",
            "Epoch 1047/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2610.7915 - val_loss: 3163.9128\n",
            "Epoch 1048/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.1262 - val_loss: 3160.2261\n",
            "Epoch 1049/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.9182 - val_loss: 3160.4221\n",
            "Epoch 1050/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.6765 - val_loss: 3158.5291\n",
            "Epoch 1051/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2611.5901 - val_loss: 3158.0176\n",
            "Epoch 1052/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2611.6592 - val_loss: 3156.7910\n",
            "Epoch 1053/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.9355 - val_loss: 3162.7053\n",
            "Epoch 1054/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.2595 - val_loss: 3166.7690\n",
            "Epoch 1055/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2610.9106 - val_loss: 3174.6047\n",
            "Epoch 1056/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2613.1936 - val_loss: 3180.0122\n",
            "Epoch 1057/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2614.6401 - val_loss: 3185.0017\n",
            "Epoch 1058/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2615.3022 - val_loss: 3185.2300\n",
            "Epoch 1059/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2614.9341 - val_loss: 3183.3665\n",
            "Epoch 1060/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2614.4668 - val_loss: 3181.4915\n",
            "Epoch 1061/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2608.8743 - val_loss: 3170.7141\n",
            "Epoch 1062/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2607.3152 - val_loss: 3164.0347\n",
            "Epoch 1063/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.1560 - val_loss: 3159.6138\n",
            "Epoch 1064/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2610.1582 - val_loss: 3161.7522\n",
            "Epoch 1065/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2609.2451 - val_loss: 3165.2434\n",
            "Epoch 1066/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2608.8293 - val_loss: 3167.8484\n",
            "Epoch 1067/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2609.3364 - val_loss: 3170.4045\n",
            "Epoch 1068/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2609.3645 - val_loss: 3172.2461\n",
            "Epoch 1069/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2610.0906 - val_loss: 3176.5266\n",
            "Epoch 1070/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.0706 - val_loss: 3179.1228\n",
            "Epoch 1071/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.0125 - val_loss: 3179.0071\n",
            "Epoch 1072/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2612.0017 - val_loss: 3180.1265\n",
            "Epoch 1073/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2611.7144 - val_loss: 3181.9753\n",
            "Epoch 1074/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2611.6794 - val_loss: 3179.6064\n",
            "Epoch 1075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2610.8113 - val_loss: 3173.3672\n",
            "Epoch 1076/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2611.9438 - val_loss: 3179.4817\n",
            "Epoch 1077/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2610.8020 - val_loss: 3179.7605\n",
            "Epoch 1078/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2610.9714 - val_loss: 3170.0962\n",
            "Epoch 1079/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2608.4866 - val_loss: 3160.5203\n",
            "Epoch 1080/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2608.2036 - val_loss: 3160.9336\n",
            "Epoch 1081/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2607.9285 - val_loss: 3158.2197\n",
            "Epoch 1082/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2607.9160 - val_loss: 3157.3984\n",
            "Epoch 1083/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2608.7104 - val_loss: 3154.1121\n",
            "Epoch 1084/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2608.8706 - val_loss: 3153.5791\n",
            "Epoch 1085/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2607.5312 - val_loss: 3155.4934\n",
            "Epoch 1086/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2607.6118 - val_loss: 3154.3767\n",
            "Epoch 1087/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2608.0718 - val_loss: 3158.6484\n",
            "Epoch 1088/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2606.9236 - val_loss: 3157.8918\n",
            "Epoch 1089/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2607.5820 - val_loss: 3160.0779\n",
            "Epoch 1090/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.9412 - val_loss: 3153.7788\n",
            "Epoch 1091/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2605.4487 - val_loss: 3149.7358\n",
            "Epoch 1092/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2605.6975 - val_loss: 3151.8027\n",
            "Epoch 1093/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2605.1519 - val_loss: 3152.1938\n",
            "Epoch 1094/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2605.3376 - val_loss: 3155.0449\n",
            "Epoch 1095/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2605.0562 - val_loss: 3154.6853\n",
            "Epoch 1096/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.6082 - val_loss: 3153.0315\n",
            "Epoch 1097/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2604.6877 - val_loss: 3153.0828\n",
            "Epoch 1098/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2610.1035 - val_loss: 3164.6714\n",
            "Epoch 1099/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2606.1704 - val_loss: 3163.1787\n",
            "Epoch 1100/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2604.9338 - val_loss: 3161.7766\n",
            "Epoch 1101/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2604.3516 - val_loss: 3159.5022\n",
            "Epoch 1102/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2603.5369 - val_loss: 3161.6541\n",
            "Epoch 1103/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.6169 - val_loss: 3157.2991\n",
            "Epoch 1104/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.0842 - val_loss: 3157.1208\n",
            "Epoch 1105/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.4067 - val_loss: 3154.8821\n",
            "Epoch 1106/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2603.4160 - val_loss: 3154.7971\n",
            "Epoch 1107/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2603.3047 - val_loss: 3155.8335\n",
            "Epoch 1108/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2604.1665 - val_loss: 3153.0667\n",
            "Epoch 1109/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2603.7214 - val_loss: 3150.9800\n",
            "Epoch 1110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2603.1118 - val_loss: 3150.9622\n",
            "Epoch 1111/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2602.6672 - val_loss: 3153.6328\n",
            "Epoch 1112/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2607.2446 - val_loss: 3164.1208\n",
            "Epoch 1113/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2610.0505 - val_loss: 3168.3821\n",
            "Epoch 1114/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2607.2014 - val_loss: 3161.6138\n",
            "Epoch 1115/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2605.6448 - val_loss: 3157.0161\n",
            "Epoch 1116/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2603.9644 - val_loss: 3154.1616\n",
            "Epoch 1117/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2603.2732 - val_loss: 3152.6628\n",
            "Epoch 1118/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2602.4644 - val_loss: 3148.4084\n",
            "Epoch 1119/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2602.1172 - val_loss: 3144.5576\n",
            "Epoch 1120/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2601.5918 - val_loss: 3140.4255\n",
            "Epoch 1121/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2606.1919 - val_loss: 3135.2334\n",
            "Epoch 1122/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2608.0901 - val_loss: 3135.4045\n",
            "Epoch 1123/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2606.0464 - val_loss: 3136.5024\n",
            "Epoch 1124/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2605.1079 - val_loss: 3137.3059\n",
            "Epoch 1125/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2606.1470 - val_loss: 3142.4175\n",
            "Epoch 1126/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2602.6096 - val_loss: 3145.9565\n",
            "Epoch 1127/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2601.2417 - val_loss: 3145.6819\n",
            "Epoch 1128/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2601.0144 - val_loss: 3148.8687\n",
            "Epoch 1129/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2600.8782 - val_loss: 3152.1917\n",
            "Epoch 1130/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2600.8354 - val_loss: 3151.0601\n",
            "Epoch 1131/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2600.4905 - val_loss: 3149.7988\n",
            "Epoch 1132/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2599.8469 - val_loss: 3152.2510\n",
            "Epoch 1133/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2600.2019 - val_loss: 3151.5193\n",
            "Epoch 1134/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2600.6699 - val_loss: 3146.6785\n",
            "Epoch 1135/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2599.1736 - val_loss: 3150.9526\n",
            "Epoch 1136/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2599.9011 - val_loss: 3152.3997\n",
            "Epoch 1137/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2599.6963 - val_loss: 3153.7156\n",
            "Epoch 1138/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2600.6482 - val_loss: 3154.5562\n",
            "Epoch 1139/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2600.0369 - val_loss: 3153.6755\n",
            "Epoch 1140/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2599.7117 - val_loss: 3152.6155\n",
            "Epoch 1141/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2600.5356 - val_loss: 3152.8311\n",
            "Epoch 1142/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2601.4246 - val_loss: 3145.0085\n",
            "Epoch 1143/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2599.4873 - val_loss: 3148.2366\n",
            "Epoch 1144/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2599.4075 - val_loss: 3147.8784\n",
            "Epoch 1145/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.4143 - val_loss: 3144.4983\n",
            "Epoch 1146/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2600.7751 - val_loss: 3150.3223\n",
            "Epoch 1147/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2599.0823 - val_loss: 3152.1770\n",
            "Epoch 1148/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2600.0742 - val_loss: 3142.7034\n",
            "Epoch 1149/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.7183 - val_loss: 3139.6790\n",
            "Epoch 1150/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.4668 - val_loss: 3139.7188\n",
            "Epoch 1151/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2599.4146 - val_loss: 3142.8083\n",
            "Epoch 1152/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.4302 - val_loss: 3147.5547\n",
            "Epoch 1153/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.4246 - val_loss: 3145.0371\n",
            "Epoch 1154/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2598.4587 - val_loss: 3143.5977\n",
            "Epoch 1155/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2597.8767 - val_loss: 3144.0513\n",
            "Epoch 1156/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2597.8557 - val_loss: 3153.5906\n",
            "Epoch 1157/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2598.9724 - val_loss: 3150.3511\n",
            "Epoch 1158/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.7493 - val_loss: 3147.3420\n",
            "Epoch 1159/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2597.7781 - val_loss: 3147.8086\n",
            "Epoch 1160/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2597.3818 - val_loss: 3144.3455\n",
            "Epoch 1161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2597.3503 - val_loss: 3149.3735\n",
            "Epoch 1162/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2597.1150 - val_loss: 3148.9919\n",
            "Epoch 1163/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2596.5203 - val_loss: 3142.5242\n",
            "Epoch 1164/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2596.0168 - val_loss: 3139.6145\n",
            "Epoch 1165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2599.9539 - val_loss: 3138.7722\n",
            "Epoch 1166/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2608.3345 - val_loss: 3134.9666\n",
            "Epoch 1167/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2602.6772 - val_loss: 3136.5977\n",
            "Epoch 1168/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.0496 - val_loss: 3134.4014\n",
            "Epoch 1169/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2604.5229 - val_loss: 3133.9214\n",
            "Epoch 1170/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2604.3411 - val_loss: 3134.8259\n",
            "Epoch 1171/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2600.7104 - val_loss: 3136.9150\n",
            "Epoch 1172/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2600.2847 - val_loss: 3137.7852\n",
            "Epoch 1173/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2599.0859 - val_loss: 3140.0591\n",
            "Epoch 1174/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2596.5303 - val_loss: 3147.0459\n",
            "Epoch 1175/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2598.5950 - val_loss: 3156.5034\n",
            "Epoch 1176/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2597.4084 - val_loss: 3159.8799\n",
            "Epoch 1177/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2599.3625 - val_loss: 3166.1477\n",
            "Epoch 1178/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2600.0537 - val_loss: 3166.8413\n",
            "Epoch 1179/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2598.5825 - val_loss: 3160.0112\n",
            "Epoch 1180/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2596.5388 - val_loss: 3158.4414\n",
            "Epoch 1181/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.9604 - val_loss: 3153.3975\n",
            "Epoch 1182/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.0583 - val_loss: 3151.5786\n",
            "Epoch 1183/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2594.8247 - val_loss: 3150.9517\n",
            "Epoch 1184/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2593.8328 - val_loss: 3149.5295\n",
            "Epoch 1185/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2594.0967 - val_loss: 3146.1882\n",
            "Epoch 1186/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2596.8889 - val_loss: 3142.3967\n",
            "Epoch 1187/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2595.5415 - val_loss: 3142.5234\n",
            "Epoch 1188/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2595.5247 - val_loss: 3143.7585\n",
            "Epoch 1189/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.5488 - val_loss: 3140.6724\n",
            "Epoch 1190/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2595.0261 - val_loss: 3140.7373\n",
            "Epoch 1191/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2594.8118 - val_loss: 3141.9536\n",
            "Epoch 1192/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2594.4382 - val_loss: 3142.3892\n",
            "Epoch 1193/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2594.0085 - val_loss: 3141.9397\n",
            "Epoch 1194/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2594.8545 - val_loss: 3136.2139\n",
            "Epoch 1195/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.8567 - val_loss: 3135.8210\n",
            "Epoch 1196/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2596.7129 - val_loss: 3134.4241\n",
            "Epoch 1197/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.9585 - val_loss: 3136.1875\n",
            "Epoch 1198/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2593.6938 - val_loss: 3139.0576\n",
            "Epoch 1199/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2595.5312 - val_loss: 3134.8062\n",
            "Epoch 1200/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2595.3696 - val_loss: 3134.7466\n",
            "Epoch 1201/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.2429 - val_loss: 3133.6506\n",
            "Epoch 1202/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2596.4656 - val_loss: 3132.6609\n",
            "Epoch 1203/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2596.3374 - val_loss: 3133.8892\n",
            "Epoch 1204/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.5635 - val_loss: 3135.2739\n",
            "Epoch 1205/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2595.8223 - val_loss: 3134.4871\n",
            "Epoch 1206/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2595.1099 - val_loss: 3135.8010\n",
            "Epoch 1207/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2595.7437 - val_loss: 3142.0815\n",
            "Epoch 1208/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2593.5396 - val_loss: 3141.3901\n",
            "Epoch 1209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2591.9268 - val_loss: 3146.3469\n",
            "Epoch 1210/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2592.4482 - val_loss: 3142.8462\n",
            "Epoch 1211/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2593.1558 - val_loss: 3148.9167\n",
            "Epoch 1212/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2590.9407 - val_loss: 3153.3635\n",
            "Epoch 1213/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2592.1091 - val_loss: 3153.6169\n",
            "Epoch 1214/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2592.0383 - val_loss: 3145.6707\n",
            "Epoch 1215/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2590.9390 - val_loss: 3142.9663\n",
            "Epoch 1216/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2591.2483 - val_loss: 3139.6338\n",
            "Epoch 1217/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2591.7085 - val_loss: 3142.7715\n",
            "Epoch 1218/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.6807 - val_loss: 3145.9702\n",
            "Epoch 1219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2593.2302 - val_loss: 3149.4255\n",
            "Epoch 1220/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2590.8276 - val_loss: 3146.4910\n",
            "Epoch 1221/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2590.4011 - val_loss: 3146.7603\n",
            "Epoch 1222/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2589.2839 - val_loss: 3152.2388\n",
            "Epoch 1223/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2591.7761 - val_loss: 3159.8167\n",
            "Epoch 1224/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2591.7463 - val_loss: 3161.9148\n",
            "Epoch 1225/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2592.5481 - val_loss: 3166.6812\n",
            "Epoch 1226/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2593.1492 - val_loss: 3163.8760\n",
            "Epoch 1227/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2592.3306 - val_loss: 3152.3855\n",
            "Epoch 1228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2590.1067 - val_loss: 3150.5818\n",
            "Epoch 1229/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2591.4426 - val_loss: 3147.4321\n",
            "Epoch 1230/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2590.2500 - val_loss: 3149.1924\n",
            "Epoch 1231/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2589.4382 - val_loss: 3151.1121\n",
            "Epoch 1232/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.7876 - val_loss: 3151.5105\n",
            "Epoch 1233/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2590.1072 - val_loss: 3157.1484\n",
            "Epoch 1234/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2591.4983 - val_loss: 3167.3708\n",
            "Epoch 1235/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2594.1743 - val_loss: 3171.1016\n",
            "Epoch 1236/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2596.1753 - val_loss: 3176.4248\n",
            "Epoch 1237/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2596.5535 - val_loss: 3169.6460\n",
            "Epoch 1238/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2595.1506 - val_loss: 3163.3613\n",
            "Epoch 1239/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2591.5635 - val_loss: 3159.3596\n",
            "Epoch 1240/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2589.5549 - val_loss: 3151.0498\n",
            "Epoch 1241/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2590.6213 - val_loss: 3143.8452\n",
            "Epoch 1242/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.1118 - val_loss: 3143.1016\n",
            "Epoch 1243/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.6829 - val_loss: 3144.4897\n",
            "Epoch 1244/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.5776 - val_loss: 3147.8447\n",
            "Epoch 1245/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.2329 - val_loss: 3148.7268\n",
            "Epoch 1246/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2587.9043 - val_loss: 3151.8230\n",
            "Epoch 1247/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2590.4355 - val_loss: 3145.1689\n",
            "Epoch 1248/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.1340 - val_loss: 3144.0259\n",
            "Epoch 1249/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2589.5034 - val_loss: 3147.7781\n",
            "Epoch 1250/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2588.1565 - val_loss: 3153.3184\n",
            "Epoch 1251/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2588.2864 - val_loss: 3158.0938\n",
            "Epoch 1252/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2589.4839 - val_loss: 3161.6387\n",
            "Epoch 1253/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2589.0486 - val_loss: 3162.3323\n",
            "Epoch 1254/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2589.5415 - val_loss: 3162.8015\n",
            "Epoch 1255/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2590.2278 - val_loss: 3157.5559\n",
            "Epoch 1256/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2587.6040 - val_loss: 3155.8245\n",
            "Epoch 1257/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2591.5215 - val_loss: 3162.1689\n",
            "Epoch 1258/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.3386 - val_loss: 3158.8784\n",
            "Epoch 1259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2587.7087 - val_loss: 3158.0745\n",
            "Epoch 1260/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2587.1016 - val_loss: 3158.5166\n",
            "Epoch 1261/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2587.0879 - val_loss: 3159.8604\n",
            "Epoch 1262/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.4348 - val_loss: 3163.0735\n",
            "Epoch 1263/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2587.6279 - val_loss: 3161.2830\n",
            "Epoch 1264/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.4128 - val_loss: 3157.1772\n",
            "Epoch 1265/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2586.1575 - val_loss: 3154.8430\n",
            "Epoch 1266/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2587.5615 - val_loss: 3152.5808\n",
            "Epoch 1267/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2586.1636 - val_loss: 3151.4597\n",
            "Epoch 1268/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2585.4656 - val_loss: 3149.0459\n",
            "Epoch 1269/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2592.4714 - val_loss: 3144.0059\n",
            "Epoch 1270/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2586.1331 - val_loss: 3147.2073\n",
            "Epoch 1271/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2586.6499 - val_loss: 3150.8936\n",
            "Epoch 1272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2585.1313 - val_loss: 3153.0259\n",
            "Epoch 1273/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2585.6047 - val_loss: 3147.8757\n",
            "Epoch 1274/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2587.5718 - val_loss: 3143.7927\n",
            "Epoch 1275/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.2485 - val_loss: 3147.6296\n",
            "Epoch 1276/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2586.6021 - val_loss: 3149.2278\n",
            "Epoch 1277/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2586.1804 - val_loss: 3150.2927\n",
            "Epoch 1278/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2585.1653 - val_loss: 3143.9526\n",
            "Epoch 1279/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2586.0012 - val_loss: 3143.4814\n",
            "Epoch 1280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2585.8833 - val_loss: 3143.3037\n",
            "Epoch 1281/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2586.5464 - val_loss: 3143.1033\n",
            "Epoch 1282/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2586.6233 - val_loss: 3143.8652\n",
            "Epoch 1283/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2585.7339 - val_loss: 3141.7620\n",
            "Epoch 1284/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2588.8162 - val_loss: 3139.0435\n",
            "Epoch 1285/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2589.6616 - val_loss: 3141.1140\n",
            "Epoch 1286/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2589.3940 - val_loss: 3142.5583\n",
            "Epoch 1287/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2587.0691 - val_loss: 3144.8799\n",
            "Epoch 1288/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2584.8203 - val_loss: 3150.4858\n",
            "Epoch 1289/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2583.8152 - val_loss: 3153.2380\n",
            "Epoch 1290/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2583.4573 - val_loss: 3153.6853\n",
            "Epoch 1291/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2583.1401 - val_loss: 3153.5022\n",
            "Epoch 1292/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2583.7915 - val_loss: 3149.2065\n",
            "Epoch 1293/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2586.4854 - val_loss: 3149.7812\n",
            "Epoch 1294/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2585.3760 - val_loss: 3155.7549\n",
            "Epoch 1295/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2583.1667 - val_loss: 3155.7883\n",
            "Epoch 1296/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2583.3611 - val_loss: 3156.8718\n",
            "Epoch 1297/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2584.1858 - val_loss: 3152.9722\n",
            "Epoch 1298/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2584.1987 - val_loss: 3154.6741\n",
            "Epoch 1299/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2582.6704 - val_loss: 3153.5916\n",
            "Epoch 1300/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2584.0820 - val_loss: 3153.5969\n",
            "Epoch 1301/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2584.3513 - val_loss: 3153.7058\n",
            "Epoch 1302/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2584.0183 - val_loss: 3155.5312\n",
            "Epoch 1303/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2583.0188 - val_loss: 3156.6033\n",
            "Epoch 1304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2582.3936 - val_loss: 3157.1553\n",
            "Epoch 1305/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2583.3169 - val_loss: 3153.7117\n",
            "Epoch 1306/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.8184 - val_loss: 3155.7878\n",
            "Epoch 1307/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2582.3123 - val_loss: 3155.3271\n",
            "Epoch 1308/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2581.5405 - val_loss: 3156.2620\n",
            "Epoch 1309/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2581.6743 - val_loss: 3156.4409\n",
            "Epoch 1310/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.4678 - val_loss: 3153.3335\n",
            "Epoch 1311/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2581.5525 - val_loss: 3152.4429\n",
            "Epoch 1312/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.0500 - val_loss: 3154.7935\n",
            "Epoch 1313/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2581.8379 - val_loss: 3159.4790\n",
            "Epoch 1314/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.7017 - val_loss: 3160.5972\n",
            "Epoch 1315/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.9023 - val_loss: 3161.7461\n",
            "Epoch 1316/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2582.0283 - val_loss: 3160.1980\n",
            "Epoch 1317/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.7620 - val_loss: 3154.7017\n",
            "Epoch 1318/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2580.9487 - val_loss: 3153.7661\n",
            "Epoch 1319/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2580.4080 - val_loss: 3149.0232\n",
            "Epoch 1320/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.5073 - val_loss: 3145.7100\n",
            "Epoch 1321/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2581.4700 - val_loss: 3145.6040\n",
            "Epoch 1322/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2583.4121 - val_loss: 3140.0740\n",
            "Epoch 1323/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2584.6167 - val_loss: 3139.4509\n",
            "Epoch 1324/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2585.0171 - val_loss: 3140.2983\n",
            "Epoch 1325/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2585.5808 - val_loss: 3139.6016\n",
            "Epoch 1326/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2584.8298 - val_loss: 3141.3716\n",
            "Epoch 1327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2582.9543 - val_loss: 3142.5586\n",
            "Epoch 1328/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2582.1885 - val_loss: 3143.1082\n",
            "Epoch 1329/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2581.7859 - val_loss: 3143.1152\n",
            "Epoch 1330/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2581.7388 - val_loss: 3144.5850\n",
            "Epoch 1331/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2580.8245 - val_loss: 3145.8655\n",
            "Epoch 1332/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2580.2773 - val_loss: 3147.2405\n",
            "Epoch 1333/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2580.6880 - val_loss: 3146.2727\n",
            "Epoch 1334/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2579.7710 - val_loss: 3148.5811\n",
            "Epoch 1335/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2578.8315 - val_loss: 3151.9172\n",
            "Epoch 1336/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2578.6870 - val_loss: 3152.4033\n",
            "Epoch 1337/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2579.2805 - val_loss: 3151.0361\n",
            "Epoch 1338/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2578.2344 - val_loss: 3150.3940\n",
            "Epoch 1339/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2578.3611 - val_loss: 3149.2891\n",
            "Epoch 1340/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2577.6995 - val_loss: 3145.4038\n",
            "Epoch 1341/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2579.7905 - val_loss: 3142.3311\n",
            "Epoch 1342/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2585.1643 - val_loss: 3143.0471\n",
            "Epoch 1343/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2583.1697 - val_loss: 3144.4614\n",
            "Epoch 1344/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2582.1018 - val_loss: 3143.0344\n",
            "Epoch 1345/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2584.2622 - val_loss: 3139.7788\n",
            "Epoch 1346/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2588.9871 - val_loss: 3139.2803\n",
            "Epoch 1347/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.8538 - val_loss: 3138.8062\n",
            "Epoch 1348/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2589.9629 - val_loss: 3139.4417\n",
            "Epoch 1349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2586.5911 - val_loss: 3144.4294\n",
            "Epoch 1350/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2578.1914 - val_loss: 3149.4143\n",
            "Epoch 1351/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2578.2065 - val_loss: 3150.8435\n",
            "Epoch 1352/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2578.5586 - val_loss: 3150.8005\n",
            "Epoch 1353/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2577.8633 - val_loss: 3151.7197\n",
            "Epoch 1354/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2577.0171 - val_loss: 3154.9854\n",
            "Epoch 1355/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2572.6775 - val_loss: 3164.1697\n",
            "Epoch 1356/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2575.4653 - val_loss: 3164.0256\n",
            "Epoch 1357/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2576.1328 - val_loss: 3166.8076\n",
            "Epoch 1358/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2575.6807 - val_loss: 3156.7366\n",
            "Epoch 1359/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2574.8481 - val_loss: 3154.1226\n",
            "Epoch 1360/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2574.8677 - val_loss: 3154.5720\n",
            "Epoch 1361/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2575.4780 - val_loss: 3159.0281\n",
            "Epoch 1362/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2574.6887 - val_loss: 3159.2068\n",
            "Epoch 1363/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2575.2349 - val_loss: 3157.2205\n",
            "Epoch 1364/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2574.3721 - val_loss: 3155.8779\n",
            "Epoch 1365/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2573.3760 - val_loss: 3152.7791\n",
            "Epoch 1366/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2574.9668 - val_loss: 3153.3269\n",
            "Epoch 1367/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2574.7485 - val_loss: 3152.6235\n",
            "Epoch 1368/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2574.5874 - val_loss: 3153.5273\n",
            "Epoch 1369/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2576.6292 - val_loss: 3151.2251\n",
            "Epoch 1370/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2575.8677 - val_loss: 3153.5449\n",
            "Epoch 1371/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2575.3259 - val_loss: 3151.2397\n",
            "Epoch 1372/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2576.8843 - val_loss: 3145.2170\n",
            "Epoch 1373/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2576.0471 - val_loss: 3146.8469\n",
            "Epoch 1374/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2574.6448 - val_loss: 3148.7954\n",
            "Epoch 1375/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2573.7964 - val_loss: 3149.5063\n",
            "Epoch 1376/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2573.6350 - val_loss: 3148.8315\n",
            "Epoch 1377/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2573.7168 - val_loss: 3146.7478\n",
            "Epoch 1378/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2573.0308 - val_loss: 3147.2715\n",
            "Epoch 1379/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2572.5481 - val_loss: 3144.6787\n",
            "Epoch 1380/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2575.3496 - val_loss: 3145.8062\n",
            "Epoch 1381/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2574.2173 - val_loss: 3148.4121\n",
            "Epoch 1382/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2573.7729 - val_loss: 3151.5688\n",
            "Epoch 1383/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2571.9668 - val_loss: 3150.3223\n",
            "Epoch 1384/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2571.1128 - val_loss: 3154.8911\n",
            "Epoch 1385/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2571.2849 - val_loss: 3157.3892\n",
            "Epoch 1386/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2571.6482 - val_loss: 3154.6121\n",
            "Epoch 1387/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2571.4663 - val_loss: 3154.5415\n",
            "Epoch 1388/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2571.3076 - val_loss: 3154.1787\n",
            "Epoch 1389/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2569.4812 - val_loss: 3149.1653\n",
            "Epoch 1390/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2573.4036 - val_loss: 3147.6777\n",
            "Epoch 1391/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2575.7703 - val_loss: 3144.6816\n",
            "Epoch 1392/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2575.2378 - val_loss: 3144.9958\n",
            "Epoch 1393/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2572.6924 - val_loss: 3147.2134\n",
            "Epoch 1394/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2572.8796 - val_loss: 3147.7917\n",
            "Epoch 1395/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2572.6392 - val_loss: 3152.4736\n",
            "Epoch 1396/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2571.0627 - val_loss: 3149.2756\n",
            "Epoch 1397/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2571.1062 - val_loss: 3149.2798\n",
            "Epoch 1398/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2571.5063 - val_loss: 3149.6843\n",
            "Epoch 1399/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2573.2190 - val_loss: 3146.8560\n",
            "Epoch 1400/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.8074 - val_loss: 3148.1946\n",
            "Epoch 1401/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2571.7844 - val_loss: 3154.5811\n",
            "Epoch 1402/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2569.7151 - val_loss: 3150.4697\n",
            "Epoch 1403/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.0554 - val_loss: 3152.7922\n",
            "Epoch 1404/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2568.9937 - val_loss: 3158.8816\n",
            "Epoch 1405/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2570.1182 - val_loss: 3157.7195\n",
            "Epoch 1406/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2569.3828 - val_loss: 3159.3347\n",
            "Epoch 1407/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2567.4067 - val_loss: 3165.3779\n",
            "Epoch 1408/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2570.7681 - val_loss: 3168.3428\n",
            "Epoch 1409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2568.1877 - val_loss: 3160.1580\n",
            "Epoch 1410/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2568.2156 - val_loss: 3154.7324\n",
            "Epoch 1411/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2569.0735 - val_loss: 3149.3831\n",
            "Epoch 1412/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2576.0400 - val_loss: 3142.4607\n",
            "Epoch 1413/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2571.9636 - val_loss: 3143.6980\n",
            "Epoch 1414/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.8801 - val_loss: 3143.0710\n",
            "Epoch 1415/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.0510 - val_loss: 3144.1145\n",
            "Epoch 1416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.2222 - val_loss: 3144.6313\n",
            "Epoch 1417/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2571.7136 - val_loss: 3148.5818\n",
            "Epoch 1418/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2567.8843 - val_loss: 3152.4922\n",
            "Epoch 1419/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2568.9287 - val_loss: 3159.0129\n",
            "Epoch 1420/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2567.7729 - val_loss: 3159.9607\n",
            "Epoch 1421/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2567.6885 - val_loss: 3156.7437\n",
            "Epoch 1422/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2564.7256 - val_loss: 3149.5898\n",
            "Epoch 1423/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2567.7046 - val_loss: 3147.0969\n",
            "Epoch 1424/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2569.6260 - val_loss: 3145.4360\n",
            "Epoch 1425/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2569.2673 - val_loss: 3147.3179\n",
            "Epoch 1426/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2569.0066 - val_loss: 3148.9631\n",
            "Epoch 1427/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2569.1709 - val_loss: 3145.9878\n",
            "Epoch 1428/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.0964 - val_loss: 3146.0952\n",
            "Epoch 1429/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2567.7266 - val_loss: 3150.0264\n",
            "Epoch 1430/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2567.7871 - val_loss: 3157.1841\n",
            "Epoch 1431/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2568.8044 - val_loss: 3166.3447\n",
            "Epoch 1432/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2567.3572 - val_loss: 3162.4080\n",
            "Epoch 1433/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2567.9553 - val_loss: 3167.0142\n",
            "Epoch 1434/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2568.9893 - val_loss: 3167.2307\n",
            "Epoch 1435/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2568.6794 - val_loss: 3161.5884\n",
            "Epoch 1436/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2570.1357 - val_loss: 3154.3220\n",
            "Epoch 1437/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2566.5200 - val_loss: 3155.6641\n",
            "Epoch 1438/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2566.8071 - val_loss: 3154.7859\n",
            "Epoch 1439/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2566.4412 - val_loss: 3155.2292\n",
            "Epoch 1440/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2566.3481 - val_loss: 3153.6475\n",
            "Epoch 1441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2565.8586 - val_loss: 3156.1577\n",
            "Epoch 1442/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2566.0332 - val_loss: 3156.3511\n",
            "Epoch 1443/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2565.6116 - val_loss: 3153.6770\n",
            "Epoch 1444/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2566.9241 - val_loss: 3151.4192\n",
            "Epoch 1445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2565.6018 - val_loss: 3154.8071\n",
            "Epoch 1446/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2565.4783 - val_loss: 3156.6235\n",
            "Epoch 1447/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2564.3181 - val_loss: 3161.9265\n",
            "Epoch 1448/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2566.3250 - val_loss: 3162.2300\n",
            "Epoch 1449/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2566.0134 - val_loss: 3160.5386\n",
            "Epoch 1450/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2565.4175 - val_loss: 3155.0295\n",
            "Epoch 1451/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2564.1809 - val_loss: 3150.7798\n",
            "Epoch 1452/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2566.0273 - val_loss: 3144.9016\n",
            "Epoch 1453/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2566.4619 - val_loss: 3145.0249\n",
            "Epoch 1454/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2565.6167 - val_loss: 3144.4290\n",
            "Epoch 1455/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2565.3213 - val_loss: 3145.2485\n",
            "Epoch 1456/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2564.9014 - val_loss: 3146.3005\n",
            "Epoch 1457/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2564.7058 - val_loss: 3146.7627\n",
            "Epoch 1458/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2563.6919 - val_loss: 3148.3547\n",
            "Epoch 1459/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2569.7932 - val_loss: 3145.3496\n",
            "Epoch 1460/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2564.5049 - val_loss: 3147.1040\n",
            "Epoch 1461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2563.6050 - val_loss: 3151.0022\n",
            "Epoch 1462/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2563.1094 - val_loss: 3158.2043\n",
            "Epoch 1463/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2564.7495 - val_loss: 3160.8096\n",
            "Epoch 1464/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2562.7988 - val_loss: 3156.9128\n",
            "Epoch 1465/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2562.8809 - val_loss: 3154.2534\n",
            "Epoch 1466/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2562.5039 - val_loss: 3157.7791\n",
            "Epoch 1467/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2562.6731 - val_loss: 3159.8103\n",
            "Epoch 1468/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2563.4250 - val_loss: 3165.0000\n",
            "Epoch 1469/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2563.4050 - val_loss: 3162.3062\n",
            "Epoch 1470/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2562.7825 - val_loss: 3161.5137\n",
            "Epoch 1471/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2562.6299 - val_loss: 3162.1292\n",
            "Epoch 1472/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2562.6541 - val_loss: 3167.7844\n",
            "Epoch 1473/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2563.1694 - val_loss: 3170.1099\n",
            "Epoch 1474/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2563.7869 - val_loss: 3167.7163\n",
            "Epoch 1475/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2563.1963 - val_loss: 3165.5432\n",
            "Epoch 1476/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2562.7034 - val_loss: 3163.6660\n",
            "Epoch 1477/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2562.5769 - val_loss: 3173.3516\n",
            "Epoch 1478/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2565.9719 - val_loss: 3177.4014\n",
            "Epoch 1479/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2567.2458 - val_loss: 3176.2227\n",
            "Epoch 1480/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2561.6995 - val_loss: 3168.3228\n",
            "Epoch 1481/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2562.6746 - val_loss: 3166.4910\n",
            "Epoch 1482/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2560.6113 - val_loss: 3161.2517\n",
            "Epoch 1483/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2561.0066 - val_loss: 3159.4509\n",
            "Epoch 1484/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2560.5496 - val_loss: 3157.4646\n",
            "Epoch 1485/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2561.5286 - val_loss: 3157.5330\n",
            "Epoch 1486/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.9141 - val_loss: 3158.1973\n",
            "Epoch 1487/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.8503 - val_loss: 3159.1108\n",
            "Epoch 1488/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2560.5095 - val_loss: 3159.5657\n",
            "Epoch 1489/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.1841 - val_loss: 3159.2766\n",
            "Epoch 1490/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.0896 - val_loss: 3159.0359\n",
            "Epoch 1491/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.2612 - val_loss: 3160.8535\n",
            "Epoch 1492/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2560.1118 - val_loss: 3159.7065\n",
            "Epoch 1493/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2561.4243 - val_loss: 3162.7195\n",
            "Epoch 1494/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2559.4583 - val_loss: 3160.9551\n",
            "Epoch 1495/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2559.4097 - val_loss: 3157.5754\n",
            "Epoch 1496/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2560.2490 - val_loss: 3163.1475\n",
            "Epoch 1497/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2559.3979 - val_loss: 3164.1409\n",
            "Epoch 1498/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2558.9299 - val_loss: 3172.8672\n",
            "Epoch 1499/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2561.9358 - val_loss: 3175.1248\n",
            "Epoch 1500/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2562.3477 - val_loss: 3175.1899\n",
            "Epoch 1501/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2563.2744 - val_loss: 3172.5227\n",
            "Epoch 1502/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.9976 - val_loss: 3163.0977\n",
            "Epoch 1503/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2559.2886 - val_loss: 3161.6802\n",
            "Epoch 1504/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2559.2075 - val_loss: 3162.4824\n",
            "Epoch 1505/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2558.6035 - val_loss: 3163.6067\n",
            "Epoch 1506/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2558.3193 - val_loss: 3158.3953\n",
            "Epoch 1507/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2558.5522 - val_loss: 3157.7981\n",
            "Epoch 1508/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2560.0625 - val_loss: 3161.1340\n",
            "Epoch 1509/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2557.3103 - val_loss: 3158.6990\n",
            "Epoch 1510/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2557.7537 - val_loss: 3154.4185\n",
            "Epoch 1511/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2559.5325 - val_loss: 3161.9238\n",
            "Epoch 1512/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2557.7493 - val_loss: 3165.5457\n",
            "Epoch 1513/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2559.2676 - val_loss: 3171.3428\n",
            "Epoch 1514/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2559.4497 - val_loss: 3169.2097\n",
            "Epoch 1515/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2558.6160 - val_loss: 3161.2002\n",
            "Epoch 1516/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2556.8853 - val_loss: 3159.8210\n",
            "Epoch 1517/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2556.7727 - val_loss: 3164.5256\n",
            "Epoch 1518/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2557.0146 - val_loss: 3169.9910\n",
            "Epoch 1519/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2558.2949 - val_loss: 3171.6587\n",
            "Epoch 1520/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2559.9365 - val_loss: 3180.3613\n",
            "Epoch 1521/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2562.5198 - val_loss: 3179.6628\n",
            "Epoch 1522/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2561.8955 - val_loss: 3171.4629\n",
            "Epoch 1523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2555.9019 - val_loss: 3162.6084\n",
            "Epoch 1524/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2556.7124 - val_loss: 3159.9158\n",
            "Epoch 1525/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.3049 - val_loss: 3158.1282\n",
            "Epoch 1526/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.9683 - val_loss: 3153.7900\n",
            "Epoch 1527/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.6301 - val_loss: 3153.9871\n",
            "Epoch 1528/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2557.1899 - val_loss: 3156.8174\n",
            "Epoch 1529/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2557.5308 - val_loss: 3163.0203\n",
            "Epoch 1530/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2555.2266 - val_loss: 3168.1658\n",
            "Epoch 1531/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.7334 - val_loss: 3168.2983\n",
            "Epoch 1532/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2556.9700 - val_loss: 3171.0674\n",
            "Epoch 1533/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2558.2424 - val_loss: 3180.8171\n",
            "Epoch 1534/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.8430 - val_loss: 3181.3604\n",
            "Epoch 1535/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.3792 - val_loss: 3179.3525\n",
            "Epoch 1536/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2559.6985 - val_loss: 3176.5928\n",
            "Epoch 1537/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2560.9983 - val_loss: 3181.0728\n",
            "Epoch 1538/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2559.7039 - val_loss: 3176.9521\n",
            "Epoch 1539/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2558.5149 - val_loss: 3166.5449\n",
            "Epoch 1540/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2554.7356 - val_loss: 3165.9895\n",
            "Epoch 1541/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.9058 - val_loss: 3166.8792\n",
            "Epoch 1542/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2555.3135 - val_loss: 3167.1973\n",
            "Epoch 1543/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.1553 - val_loss: 3173.7297\n",
            "Epoch 1544/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.4006 - val_loss: 3165.9199\n",
            "Epoch 1545/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2555.1060 - val_loss: 3165.2871\n",
            "Epoch 1546/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2556.9927 - val_loss: 3153.6545\n",
            "Epoch 1547/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2553.6270 - val_loss: 3146.0186\n",
            "Epoch 1548/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.8540 - val_loss: 3144.5730\n",
            "Epoch 1549/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2555.2986 - val_loss: 3143.3135\n",
            "Epoch 1550/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2555.1306 - val_loss: 3147.2678\n",
            "Epoch 1551/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2554.6211 - val_loss: 3147.1243\n",
            "Epoch 1552/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2554.1392 - val_loss: 3146.9685\n",
            "Epoch 1553/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.2913 - val_loss: 3146.8462\n",
            "Epoch 1554/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.0269 - val_loss: 3147.8774\n",
            "Epoch 1555/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2552.8250 - val_loss: 3152.3550\n",
            "Epoch 1556/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2554.8958 - val_loss: 3150.4663\n",
            "Epoch 1557/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2553.0706 - val_loss: 3152.6299\n",
            "Epoch 1558/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2556.1821 - val_loss: 3158.0786\n",
            "Epoch 1559/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2555.5615 - val_loss: 3161.0378\n",
            "Epoch 1560/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.1450 - val_loss: 3154.0400\n",
            "Epoch 1561/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2553.6208 - val_loss: 3153.8552\n",
            "Epoch 1562/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2554.7122 - val_loss: 3150.7471\n",
            "Epoch 1563/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2553.1948 - val_loss: 3151.4438\n",
            "Epoch 1564/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.1196 - val_loss: 3153.1060\n",
            "Epoch 1565/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2552.8496 - val_loss: 3149.3157\n",
            "Epoch 1566/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2552.4624 - val_loss: 3148.2092\n",
            "Epoch 1567/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2550.7380 - val_loss: 3155.1938\n",
            "Epoch 1568/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2555.5425 - val_loss: 3161.8933\n",
            "Epoch 1569/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.1091 - val_loss: 3159.7935\n",
            "Epoch 1570/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2553.1172 - val_loss: 3158.0713\n",
            "Epoch 1571/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2553.6313 - val_loss: 3154.5161\n",
            "Epoch 1572/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2551.2263 - val_loss: 3152.1960\n",
            "Epoch 1573/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2551.5623 - val_loss: 3151.6404\n",
            "Epoch 1574/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2551.3538 - val_loss: 3153.8372\n",
            "Epoch 1575/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2551.3901 - val_loss: 3151.8560\n",
            "Epoch 1576/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2553.7751 - val_loss: 3146.7983\n",
            "Epoch 1577/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2552.0986 - val_loss: 3145.4229\n",
            "Epoch 1578/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2553.2634 - val_loss: 3143.6538\n",
            "Epoch 1579/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2554.3689 - val_loss: 3142.4658\n",
            "Epoch 1580/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.7446 - val_loss: 3145.8552\n",
            "Epoch 1581/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2551.7549 - val_loss: 3146.1392\n",
            "Epoch 1582/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2551.9524 - val_loss: 3147.4919\n",
            "Epoch 1583/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2551.7349 - val_loss: 3150.1621\n",
            "Epoch 1584/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2551.4971 - val_loss: 3152.2021\n",
            "Epoch 1585/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2550.2871 - val_loss: 3151.1545\n",
            "Epoch 1586/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.5544 - val_loss: 3146.0674\n",
            "Epoch 1587/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2552.0095 - val_loss: 3144.7957\n",
            "Epoch 1588/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2551.9849 - val_loss: 3145.8159\n",
            "Epoch 1589/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2549.0210 - val_loss: 3151.0085\n",
            "Epoch 1590/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2549.4700 - val_loss: 3153.3066\n",
            "Epoch 1591/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2551.8269 - val_loss: 3159.9751\n",
            "Epoch 1592/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2550.3391 - val_loss: 3156.8350\n",
            "Epoch 1593/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2549.7666 - val_loss: 3160.0872\n",
            "Epoch 1594/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2549.6414 - val_loss: 3157.3574\n",
            "Epoch 1595/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2549.3606 - val_loss: 3158.3840\n",
            "Epoch 1596/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.1453 - val_loss: 3160.9976\n",
            "Epoch 1597/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2552.0320 - val_loss: 3164.7771\n",
            "Epoch 1598/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2550.6304 - val_loss: 3164.5945\n",
            "Epoch 1599/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.6724 - val_loss: 3161.7781\n",
            "Epoch 1600/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2549.8572 - val_loss: 3156.6760\n",
            "Epoch 1601/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2548.2881 - val_loss: 3154.7371\n",
            "Epoch 1602/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2550.5386 - val_loss: 3147.4800\n",
            "Epoch 1603/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2549.9436 - val_loss: 3148.4622\n",
            "Epoch 1604/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2549.0117 - val_loss: 3150.8809\n",
            "Epoch 1605/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.0110 - val_loss: 3149.6785\n",
            "Epoch 1606/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2548.8035 - val_loss: 3151.9871\n",
            "Epoch 1607/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2546.7412 - val_loss: 3158.7529\n",
            "Epoch 1608/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.1248 - val_loss: 3158.8191\n",
            "Epoch 1609/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2548.8916 - val_loss: 3157.2122\n",
            "Epoch 1610/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.4993 - val_loss: 3160.2878\n",
            "Epoch 1611/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.1228 - val_loss: 3161.5554\n",
            "Epoch 1612/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2548.8411 - val_loss: 3163.5232\n",
            "Epoch 1613/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.3884 - val_loss: 3170.0730\n",
            "Epoch 1614/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2551.5881 - val_loss: 3173.8936\n",
            "Epoch 1615/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2552.4675 - val_loss: 3170.0435\n",
            "Epoch 1616/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2551.3635 - val_loss: 3162.9702\n",
            "Epoch 1617/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2549.6709 - val_loss: 3156.6033\n",
            "Epoch 1618/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2548.8279 - val_loss: 3152.9985\n",
            "Epoch 1619/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2547.5596 - val_loss: 3155.5691\n",
            "Epoch 1620/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2549.4216 - val_loss: 3160.6082\n",
            "Epoch 1621/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2548.5854 - val_loss: 3162.4192\n",
            "Epoch 1622/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2548.1963 - val_loss: 3163.3054\n",
            "Epoch 1623/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2548.9426 - val_loss: 3165.8315\n",
            "Epoch 1624/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2548.0020 - val_loss: 3161.8972\n",
            "Epoch 1625/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2547.1516 - val_loss: 3161.2458\n",
            "Epoch 1626/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2547.3052 - val_loss: 3161.1160\n",
            "Epoch 1627/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2547.7349 - val_loss: 3164.6704\n",
            "Epoch 1628/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2546.2251 - val_loss: 3157.5625\n",
            "Epoch 1629/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2545.7659 - val_loss: 3156.6472\n",
            "Epoch 1630/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2547.8469 - val_loss: 3164.6587\n",
            "Epoch 1631/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2549.4058 - val_loss: 3172.0583\n",
            "Epoch 1632/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2550.3359 - val_loss: 3170.3467\n",
            "Epoch 1633/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2551.3428 - val_loss: 3177.6724\n",
            "Epoch 1634/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2551.7715 - val_loss: 3178.2678\n",
            "Epoch 1635/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2550.7793 - val_loss: 3175.0640\n",
            "Epoch 1636/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2548.9553 - val_loss: 3171.2266\n",
            "Epoch 1637/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2547.1252 - val_loss: 3163.0955\n",
            "Epoch 1638/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2546.0701 - val_loss: 3156.7549\n",
            "Epoch 1639/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2546.3481 - val_loss: 3157.1284\n",
            "Epoch 1640/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2545.6335 - val_loss: 3159.5771\n",
            "Epoch 1641/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2546.8000 - val_loss: 3162.1980\n",
            "Epoch 1642/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2545.9688 - val_loss: 3160.1650\n",
            "Epoch 1643/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2546.0757 - val_loss: 3157.6138\n",
            "Epoch 1644/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2548.0071 - val_loss: 3152.1812\n",
            "Epoch 1645/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.9636 - val_loss: 3156.5254\n",
            "Epoch 1646/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.5073 - val_loss: 3160.3027\n",
            "Epoch 1647/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.5698 - val_loss: 3161.4966\n",
            "Epoch 1648/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.7529 - val_loss: 3167.7876\n",
            "Epoch 1649/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2545.6118 - val_loss: 3160.7139\n",
            "Epoch 1650/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2544.0447 - val_loss: 3154.0151\n",
            "Epoch 1651/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.7183 - val_loss: 3153.7827\n",
            "Epoch 1652/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2543.9106 - val_loss: 3153.8560\n",
            "Epoch 1653/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.7383 - val_loss: 3155.2507\n",
            "Epoch 1654/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.6660 - val_loss: 3160.8679\n",
            "Epoch 1655/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.0383 - val_loss: 3162.6475\n",
            "Epoch 1656/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2543.5127 - val_loss: 3164.2798\n",
            "Epoch 1657/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.2170 - val_loss: 3164.4807\n",
            "Epoch 1658/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2543.2185 - val_loss: 3161.6284\n",
            "Epoch 1659/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2543.1377 - val_loss: 3160.2847\n",
            "Epoch 1660/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2543.5137 - val_loss: 3165.1018\n",
            "Epoch 1661/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.4187 - val_loss: 3164.8428\n",
            "Epoch 1662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2544.3276 - val_loss: 3160.9111\n",
            "Epoch 1663/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2544.4939 - val_loss: 3156.8269\n",
            "Epoch 1664/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2542.9624 - val_loss: 3158.5315\n",
            "Epoch 1665/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.7734 - val_loss: 3162.5935\n",
            "Epoch 1666/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.8767 - val_loss: 3162.5608\n",
            "Epoch 1667/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2543.7251 - val_loss: 3159.8621\n",
            "Epoch 1668/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.5312 - val_loss: 3161.9753\n",
            "Epoch 1669/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2543.0093 - val_loss: 3163.2092\n",
            "Epoch 1670/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.1750 - val_loss: 3163.7290\n",
            "Epoch 1671/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2542.5122 - val_loss: 3166.1868\n",
            "Epoch 1672/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2542.9229 - val_loss: 3168.3154\n",
            "Epoch 1673/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2542.5371 - val_loss: 3165.8660\n",
            "Epoch 1674/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2542.0098 - val_loss: 3167.6082\n",
            "Epoch 1675/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2541.9695 - val_loss: 3166.6226\n",
            "Epoch 1676/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.0015 - val_loss: 3166.6482\n",
            "Epoch 1677/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.2090 - val_loss: 3169.2012\n",
            "Epoch 1678/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.9031 - val_loss: 3168.5259\n",
            "Epoch 1679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.6970 - val_loss: 3170.9746\n",
            "Epoch 1680/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.8857 - val_loss: 3170.5464\n",
            "Epoch 1681/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2541.6318 - val_loss: 3169.6208\n",
            "Epoch 1682/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2542.3276 - val_loss: 3161.8872\n",
            "Epoch 1683/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2541.2166 - val_loss: 3158.8701\n",
            "Epoch 1684/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2541.3269 - val_loss: 3159.4290\n",
            "Epoch 1685/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.2573 - val_loss: 3162.8630\n",
            "Epoch 1686/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2540.6597 - val_loss: 3164.6565\n",
            "Epoch 1687/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.8953 - val_loss: 3161.0923\n",
            "Epoch 1688/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.6545 - val_loss: 3159.7747\n",
            "Epoch 1689/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2541.4578 - val_loss: 3155.1201\n",
            "Epoch 1690/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.0583 - val_loss: 3155.0386\n",
            "Epoch 1691/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.9746 - val_loss: 3154.3359\n",
            "Epoch 1692/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.6313 - val_loss: 3149.9087\n",
            "Epoch 1693/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2545.2195 - val_loss: 3147.0085\n",
            "Epoch 1694/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2548.0938 - val_loss: 3147.0703\n",
            "Epoch 1695/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2546.6877 - val_loss: 3148.5811\n",
            "Epoch 1696/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2543.9971 - val_loss: 3150.4072\n",
            "Epoch 1697/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2541.9370 - val_loss: 3153.0198\n",
            "Epoch 1698/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.4663 - val_loss: 3156.9910\n",
            "Epoch 1699/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2538.6858 - val_loss: 3161.6265\n",
            "Epoch 1700/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2539.6580 - val_loss: 3165.3848\n",
            "Epoch 1701/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2539.5286 - val_loss: 3167.3623\n",
            "Epoch 1702/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2539.8418 - val_loss: 3167.4192\n",
            "Epoch 1703/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2540.7273 - val_loss: 3173.7891\n",
            "Epoch 1704/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2542.5591 - val_loss: 3178.8989\n",
            "Epoch 1705/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2544.0571 - val_loss: 3187.9910\n",
            "Epoch 1706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2546.6484 - val_loss: 3187.5657\n",
            "Epoch 1707/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2544.8269 - val_loss: 3182.4753\n",
            "Epoch 1708/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2545.1484 - val_loss: 3191.1833\n",
            "Epoch 1709/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2549.2678 - val_loss: 3194.8301\n",
            "Epoch 1710/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2550.2202 - val_loss: 3193.4397\n",
            "Epoch 1711/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2548.9324 - val_loss: 3189.7815\n",
            "Epoch 1712/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2546.7388 - val_loss: 3187.2085\n",
            "Epoch 1713/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2544.9841 - val_loss: 3182.6555\n",
            "Epoch 1714/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.6199 - val_loss: 3176.5391\n",
            "Epoch 1715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2542.2542 - val_loss: 3178.2117\n",
            "Epoch 1716/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2541.7708 - val_loss: 3176.8904\n",
            "Epoch 1717/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2541.6279 - val_loss: 3181.4128\n",
            "Epoch 1718/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2547.8687 - val_loss: 3189.7380\n",
            "Epoch 1719/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2546.4500 - val_loss: 3191.2637\n",
            "Epoch 1720/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2546.5037 - val_loss: 3190.1377\n",
            "Epoch 1721/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2543.5842 - val_loss: 3179.3108\n",
            "Epoch 1722/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2539.6814 - val_loss: 3174.2820\n",
            "Epoch 1723/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2540.1233 - val_loss: 3176.6152\n",
            "Epoch 1724/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2539.3706 - val_loss: 3173.7241\n",
            "Epoch 1725/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2539.0371 - val_loss: 3168.8582\n",
            "Epoch 1726/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2537.8157 - val_loss: 3167.1865\n",
            "Epoch 1727/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2537.6987 - val_loss: 3169.9736\n",
            "Epoch 1728/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2537.3662 - val_loss: 3175.7373\n",
            "Epoch 1729/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2539.5725 - val_loss: 3174.6523\n",
            "Epoch 1730/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2538.4568 - val_loss: 3180.2683\n",
            "Epoch 1731/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2541.3711 - val_loss: 3185.7146\n",
            "Epoch 1732/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2538.1252 - val_loss: 3176.7019\n",
            "Epoch 1733/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2536.8643 - val_loss: 3169.8323\n",
            "Epoch 1734/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2537.2336 - val_loss: 3165.7878\n",
            "Epoch 1735/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.3447 - val_loss: 3165.4497\n",
            "Epoch 1736/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.4204 - val_loss: 3165.4192\n",
            "Epoch 1737/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.4573 - val_loss: 3165.0674\n",
            "Epoch 1738/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2536.2466 - val_loss: 3163.8101\n",
            "Epoch 1739/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.6487 - val_loss: 3163.9919\n",
            "Epoch 1740/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2536.9910 - val_loss: 3165.8684\n",
            "Epoch 1741/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2536.2913 - val_loss: 3165.3916\n",
            "Epoch 1742/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.3740 - val_loss: 3165.5435\n",
            "Epoch 1743/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2537.1179 - val_loss: 3162.8965\n",
            "Epoch 1744/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2535.3303 - val_loss: 3166.2627\n",
            "Epoch 1745/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2535.5300 - val_loss: 3169.1531\n",
            "Epoch 1746/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2538.4331 - val_loss: 3178.1130\n",
            "Epoch 1747/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2538.7942 - val_loss: 3177.7009\n",
            "Epoch 1748/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2538.5164 - val_loss: 3177.5503\n",
            "Epoch 1749/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2538.9558 - val_loss: 3178.6931\n",
            "Epoch 1750/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2538.3928 - val_loss: 3179.9895\n",
            "Epoch 1751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.1885 - val_loss: 3184.4441\n",
            "Epoch 1752/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2540.3157 - val_loss: 3182.4929\n",
            "Epoch 1753/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2537.9094 - val_loss: 3177.6860\n",
            "Epoch 1754/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2538.4175 - val_loss: 3170.1472\n",
            "Epoch 1755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2535.9741 - val_loss: 3172.2310\n",
            "Epoch 1756/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2535.6953 - val_loss: 3174.1138\n",
            "Epoch 1757/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2535.4255 - val_loss: 3171.2009\n",
            "Epoch 1758/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2535.3899 - val_loss: 3173.4600\n",
            "Epoch 1759/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2535.0613 - val_loss: 3170.6741\n",
            "Epoch 1760/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2533.3184 - val_loss: 3165.1033\n",
            "Epoch 1761/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2534.1912 - val_loss: 3164.7292\n",
            "Epoch 1762/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2534.0996 - val_loss: 3167.1252\n",
            "Epoch 1763/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2534.6047 - val_loss: 3169.5667\n",
            "Epoch 1764/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2534.0774 - val_loss: 3170.1755\n",
            "Epoch 1765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.5957 - val_loss: 3171.4551\n",
            "Epoch 1766/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.8418 - val_loss: 3171.5833\n",
            "Epoch 1767/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.4888 - val_loss: 3171.8579\n",
            "Epoch 1768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.4463 - val_loss: 3172.9424\n",
            "Epoch 1769/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.2510 - val_loss: 3181.0183\n",
            "Epoch 1770/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2535.7800 - val_loss: 3191.3496\n",
            "Epoch 1771/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2541.1863 - val_loss: 3201.2307\n",
            "Epoch 1772/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2544.0339 - val_loss: 3206.1099\n",
            "Epoch 1773/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.2739 - val_loss: 3204.2134\n",
            "Epoch 1774/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2543.2258 - val_loss: 3204.7322\n",
            "Epoch 1775/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.8755 - val_loss: 3199.9966\n",
            "Epoch 1776/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2540.8103 - val_loss: 3201.3884\n",
            "Epoch 1777/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2539.6799 - val_loss: 3198.2156\n",
            "Epoch 1778/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.9622 - val_loss: 3189.8960\n",
            "Epoch 1779/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2536.2886 - val_loss: 3198.1707\n",
            "Epoch 1780/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2539.5916 - val_loss: 3198.7756\n",
            "Epoch 1781/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2537.5481 - val_loss: 3187.6704\n",
            "Epoch 1782/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2534.6265 - val_loss: 3179.9509\n",
            "Epoch 1783/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2533.1250 - val_loss: 3179.5510\n",
            "Epoch 1784/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2533.5386 - val_loss: 3183.2451\n",
            "Epoch 1785/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.1484 - val_loss: 3183.4128\n",
            "Epoch 1786/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.6973 - val_loss: 3183.8430\n",
            "Epoch 1787/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.1074 - val_loss: 3184.0618\n",
            "Epoch 1788/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2533.3003 - val_loss: 3184.5110\n",
            "Epoch 1789/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.3286 - val_loss: 3184.7908\n",
            "Epoch 1790/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.4753 - val_loss: 3190.7571\n",
            "Epoch 1791/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2535.5967 - val_loss: 3199.5103\n",
            "Epoch 1792/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2536.2986 - val_loss: 3197.5063\n",
            "Epoch 1793/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2535.2759 - val_loss: 3192.2830\n",
            "Epoch 1794/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2534.2625 - val_loss: 3184.4817\n",
            "Epoch 1795/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2533.8870 - val_loss: 3190.4814\n",
            "Epoch 1796/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2531.6685 - val_loss: 3186.7141\n",
            "Epoch 1797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2528.9385 - val_loss: 3177.1274\n",
            "Epoch 1798/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2530.6604 - val_loss: 3177.1885\n",
            "Epoch 1799/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2530.7307 - val_loss: 3178.0464\n",
            "Epoch 1800/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2531.8987 - val_loss: 3174.9839\n",
            "Epoch 1801/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2530.8628 - val_loss: 3171.7847\n",
            "Epoch 1802/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2530.4155 - val_loss: 3168.1306\n",
            "Epoch 1803/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2530.5886 - val_loss: 3166.0759\n",
            "Epoch 1804/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2532.1560 - val_loss: 3163.6040\n",
            "Epoch 1805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2532.9348 - val_loss: 3163.5896\n",
            "Epoch 1806/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.4082 - val_loss: 3163.5249\n",
            "Epoch 1807/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2533.1125 - val_loss: 3163.2148\n",
            "Epoch 1808/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2532.4546 - val_loss: 3165.3318\n",
            "Epoch 1809/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2528.4971 - val_loss: 3171.8413\n",
            "Epoch 1810/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2530.2817 - val_loss: 3170.0378\n",
            "Epoch 1811/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2529.5571 - val_loss: 3170.0352\n",
            "Epoch 1812/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2530.5083 - val_loss: 3166.5884\n",
            "Epoch 1813/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2530.1821 - val_loss: 3167.1018\n",
            "Epoch 1814/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2531.2197 - val_loss: 3163.4961\n",
            "Epoch 1815/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2531.3040 - val_loss: 3159.6067\n",
            "Epoch 1816/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2534.5642 - val_loss: 3158.7966\n",
            "Epoch 1817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2534.8005 - val_loss: 3160.4546\n",
            "Epoch 1818/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2534.6179 - val_loss: 3161.1819\n",
            "Epoch 1819/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2534.2380 - val_loss: 3162.1953\n",
            "Epoch 1820/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2532.9844 - val_loss: 3164.2683\n",
            "Epoch 1821/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2531.3521 - val_loss: 3165.1306\n",
            "Epoch 1822/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2529.7231 - val_loss: 3167.6392\n",
            "Epoch 1823/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2528.1357 - val_loss: 3171.6130\n",
            "Epoch 1824/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.7690 - val_loss: 3175.3574\n",
            "Epoch 1825/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.1091 - val_loss: 3176.5593\n",
            "Epoch 1826/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.1672 - val_loss: 3176.4966\n",
            "Epoch 1827/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2528.2900 - val_loss: 3180.3091\n",
            "Epoch 1828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2528.3330 - val_loss: 3179.4902\n",
            "Epoch 1829/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2527.7563 - val_loss: 3182.7402\n",
            "Epoch 1830/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2528.9827 - val_loss: 3184.6392\n",
            "Epoch 1831/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2529.8789 - val_loss: 3180.5618\n",
            "Epoch 1832/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2529.1084 - val_loss: 3186.5769\n",
            "Epoch 1833/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2527.9402 - val_loss: 3180.4233\n",
            "Epoch 1834/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2528.2368 - val_loss: 3178.6284\n",
            "Epoch 1835/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2529.6682 - val_loss: 3186.0723\n",
            "Epoch 1836/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2530.1973 - val_loss: 3186.6047\n",
            "Epoch 1837/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.2922 - val_loss: 3182.7258\n",
            "Epoch 1838/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.4590 - val_loss: 3184.5254\n",
            "Epoch 1839/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2526.8215 - val_loss: 3177.1987\n",
            "Epoch 1840/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.4661 - val_loss: 3174.8540\n",
            "Epoch 1841/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.2219 - val_loss: 3171.6233\n",
            "Epoch 1842/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2525.2529 - val_loss: 3167.3728\n",
            "Epoch 1843/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2527.8669 - val_loss: 3166.8347\n",
            "Epoch 1844/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2528.7029 - val_loss: 3167.8220\n",
            "Epoch 1845/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.2024 - val_loss: 3168.0376\n",
            "Epoch 1846/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2525.7393 - val_loss: 3173.3010\n",
            "Epoch 1847/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2531.4402 - val_loss: 3182.9600\n",
            "Epoch 1848/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2527.6357 - val_loss: 3188.0793\n",
            "Epoch 1849/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2527.2639 - val_loss: 3187.1748\n",
            "Epoch 1850/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2531.2632 - val_loss: 3180.6228\n",
            "Epoch 1851/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.1804 - val_loss: 3175.6802\n",
            "Epoch 1852/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.3843 - val_loss: 3175.6514\n",
            "Epoch 1853/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2526.5925 - val_loss: 3174.3921\n",
            "Epoch 1854/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2525.7434 - val_loss: 3173.7131\n",
            "Epoch 1855/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.0991 - val_loss: 3178.4502\n",
            "Epoch 1856/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2524.4624 - val_loss: 3183.8096\n",
            "Epoch 1857/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2525.2383 - val_loss: 3183.2471\n",
            "Epoch 1858/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2525.4084 - val_loss: 3180.6965\n",
            "Epoch 1859/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2525.3594 - val_loss: 3177.6140\n",
            "Epoch 1860/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2527.3857 - val_loss: 3175.1997\n",
            "Epoch 1861/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2528.4563 - val_loss: 3174.6763\n",
            "Epoch 1862/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.9370 - val_loss: 3172.4080\n",
            "Epoch 1863/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.9014 - val_loss: 3172.9543\n",
            "Epoch 1864/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2527.8694 - val_loss: 3171.8398\n",
            "Epoch 1865/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2527.7849 - val_loss: 3172.8328\n",
            "Epoch 1866/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2527.3479 - val_loss: 3173.5969\n",
            "Epoch 1867/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2528.9299 - val_loss: 3170.7478\n",
            "Epoch 1868/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2530.1450 - val_loss: 3171.3711\n",
            "Epoch 1869/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2529.0999 - val_loss: 3171.8809\n",
            "Epoch 1870/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2527.0776 - val_loss: 3173.7598\n",
            "Epoch 1871/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2526.6455 - val_loss: 3174.8181\n",
            "Epoch 1872/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2525.6428 - val_loss: 3176.7476\n",
            "Epoch 1873/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2525.1860 - val_loss: 3177.0671\n",
            "Epoch 1874/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2525.4448 - val_loss: 3176.3103\n",
            "Epoch 1875/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2526.4087 - val_loss: 3173.5823\n",
            "Epoch 1876/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2528.7124 - val_loss: 3173.8181\n",
            "Epoch 1877/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2527.8474 - val_loss: 3174.4854\n",
            "Epoch 1878/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2527.1606 - val_loss: 3175.8750\n",
            "Epoch 1879/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2525.7974 - val_loss: 3176.6953\n",
            "Epoch 1880/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2526.6011 - val_loss: 3173.2830\n",
            "Epoch 1881/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2530.7659 - val_loss: 3177.6692\n",
            "Epoch 1882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2523.5808 - val_loss: 3178.6121\n",
            "Epoch 1883/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2523.6572 - val_loss: 3188.0752\n",
            "Epoch 1884/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2523.2017 - val_loss: 3188.5449\n",
            "Epoch 1885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2523.3413 - val_loss: 3187.6545\n",
            "Epoch 1886/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2522.5830 - val_loss: 3190.4768\n",
            "Epoch 1887/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2522.9441 - val_loss: 3191.4653\n",
            "Epoch 1888/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2522.9253 - val_loss: 3191.6072\n",
            "Epoch 1889/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2523.0269 - val_loss: 3191.7092\n",
            "Epoch 1890/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2523.0112 - val_loss: 3192.5442\n",
            "Epoch 1891/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2522.9084 - val_loss: 3194.0376\n",
            "Epoch 1892/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2525.4888 - val_loss: 3196.5759\n",
            "Epoch 1893/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2524.3889 - val_loss: 3201.0137\n",
            "Epoch 1894/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2527.1560 - val_loss: 3207.4460\n",
            "Epoch 1895/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2527.1733 - val_loss: 3205.3271\n",
            "Epoch 1896/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2524.8232 - val_loss: 3195.1931\n",
            "Epoch 1897/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2522.2891 - val_loss: 3192.5225\n",
            "Epoch 1898/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2523.2876 - val_loss: 3194.3210\n",
            "Epoch 1899/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2522.3381 - val_loss: 3194.3540\n",
            "Epoch 1900/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2522.7935 - val_loss: 3198.7795\n",
            "Epoch 1901/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2523.6699 - val_loss: 3200.1995\n",
            "Epoch 1902/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2523.8972 - val_loss: 3197.7859\n",
            "Epoch 1903/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2523.8721 - val_loss: 3201.1794\n",
            "Epoch 1904/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2523.1924 - val_loss: 3195.7715\n",
            "Epoch 1905/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2522.3555 - val_loss: 3189.6562\n",
            "Epoch 1906/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2521.0967 - val_loss: 3189.1833\n",
            "Epoch 1907/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2522.1792 - val_loss: 3191.1345\n",
            "Epoch 1908/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2521.0513 - val_loss: 3187.4702\n",
            "Epoch 1909/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2520.9849 - val_loss: 3187.6973\n",
            "Epoch 1910/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2520.9512 - val_loss: 3189.5571\n",
            "Epoch 1911/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2521.0566 - val_loss: 3188.0154\n",
            "Epoch 1912/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2520.4675 - val_loss: 3189.7522\n",
            "Epoch 1913/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2521.2200 - val_loss: 3188.0759\n",
            "Epoch 1914/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2520.6616 - val_loss: 3184.9990\n",
            "Epoch 1915/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.4690 - val_loss: 3186.0710\n",
            "Epoch 1916/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2522.1865 - val_loss: 3189.7803\n",
            "Epoch 1917/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2521.8174 - val_loss: 3192.5496\n",
            "Epoch 1918/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2520.9978 - val_loss: 3193.6130\n",
            "Epoch 1919/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2519.7542 - val_loss: 3191.2998\n",
            "Epoch 1920/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.6394 - val_loss: 3191.5752\n",
            "Epoch 1921/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2520.7327 - val_loss: 3190.5129\n",
            "Epoch 1922/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.2146 - val_loss: 3191.8076\n",
            "Epoch 1923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.2246 - val_loss: 3190.1890\n",
            "Epoch 1924/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.1914 - val_loss: 3184.0684\n",
            "Epoch 1925/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2520.4927 - val_loss: 3183.3933\n",
            "Epoch 1926/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2521.3787 - val_loss: 3181.1204\n",
            "Epoch 1927/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.2190 - val_loss: 3188.0146\n",
            "Epoch 1928/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2520.1194 - val_loss: 3195.2234\n",
            "Epoch 1929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.0115 - val_loss: 3198.1777\n",
            "Epoch 1930/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2519.3391 - val_loss: 3194.6665\n",
            "Epoch 1931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2519.3176 - val_loss: 3191.1797\n",
            "Epoch 1932/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2519.1489 - val_loss: 3195.2222\n",
            "Epoch 1933/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.5457 - val_loss: 3190.6931\n",
            "Epoch 1934/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.5159 - val_loss: 3189.8132\n",
            "Epoch 1935/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2517.6782 - val_loss: 3185.9558\n",
            "Epoch 1936/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2519.1484 - val_loss: 3191.4172\n",
            "Epoch 1937/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2518.4629 - val_loss: 3191.7788\n",
            "Epoch 1938/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.1802 - val_loss: 3189.6509\n",
            "Epoch 1939/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2518.4404 - val_loss: 3190.6877\n",
            "Epoch 1940/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.9810 - val_loss: 3186.3772\n",
            "Epoch 1941/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2518.8250 - val_loss: 3182.4880\n",
            "Epoch 1942/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2519.8591 - val_loss: 3182.8240\n",
            "Epoch 1943/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.4028 - val_loss: 3183.0671\n",
            "Epoch 1944/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.9170 - val_loss: 3185.4902\n",
            "Epoch 1945/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2517.6091 - val_loss: 3187.9414\n",
            "Epoch 1946/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2517.9731 - val_loss: 3189.3149\n",
            "Epoch 1947/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2519.8628 - val_loss: 3184.9087\n",
            "Epoch 1948/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2518.0708 - val_loss: 3186.4136\n",
            "Epoch 1949/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.8223 - val_loss: 3192.2346\n",
            "Epoch 1950/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.1118 - val_loss: 3193.1917\n",
            "Epoch 1951/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2517.2979 - val_loss: 3195.4045\n",
            "Epoch 1952/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2518.2734 - val_loss: 3199.2371\n",
            "Epoch 1953/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2519.2058 - val_loss: 3203.3704\n",
            "Epoch 1954/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2519.5261 - val_loss: 3206.1228\n",
            "Epoch 1955/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2518.8335 - val_loss: 3203.0161\n",
            "Epoch 1956/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2517.9766 - val_loss: 3198.7307\n",
            "Epoch 1957/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.1858 - val_loss: 3194.4231\n",
            "Epoch 1958/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2517.4719 - val_loss: 3192.3352\n",
            "Epoch 1959/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.7720 - val_loss: 3195.3040\n",
            "Epoch 1960/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.6323 - val_loss: 3204.2117\n",
            "Epoch 1961/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.5911 - val_loss: 3202.2390\n",
            "Epoch 1962/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.8167 - val_loss: 3199.4128\n",
            "Epoch 1963/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.1724 - val_loss: 3192.7217\n",
            "Epoch 1964/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2516.9951 - val_loss: 3192.5376\n",
            "Epoch 1965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.1570 - val_loss: 3192.9490\n",
            "Epoch 1966/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2517.2563 - val_loss: 3192.1411\n",
            "Epoch 1967/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.6042 - val_loss: 3188.5422\n",
            "Epoch 1968/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2518.1233 - val_loss: 3187.6565\n",
            "Epoch 1969/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.1587 - val_loss: 3185.0703\n",
            "Epoch 1970/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2521.3289 - val_loss: 3184.5674\n",
            "Epoch 1971/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.8174 - val_loss: 3185.5259\n",
            "Epoch 1972/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.5469 - val_loss: 3186.8223\n",
            "Epoch 1973/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2519.3169 - val_loss: 3189.3020\n",
            "Epoch 1974/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.5283 - val_loss: 3190.2585\n",
            "Epoch 1975/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2516.3508 - val_loss: 3192.1499\n",
            "Epoch 1976/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.2424 - val_loss: 3192.7500\n",
            "Epoch 1977/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.9102 - val_loss: 3191.0596\n",
            "Epoch 1978/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2515.8274 - val_loss: 3192.5852\n",
            "Epoch 1979/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.5251 - val_loss: 3190.3462\n",
            "Epoch 1980/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.2139 - val_loss: 3197.1426\n",
            "Epoch 1981/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.0752 - val_loss: 3201.8752\n",
            "Epoch 1982/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.1431 - val_loss: 3202.4597\n",
            "Epoch 1983/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2518.5024 - val_loss: 3198.4041\n",
            "Epoch 1984/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2514.8733 - val_loss: 3201.3547\n",
            "Epoch 1985/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2515.9438 - val_loss: 3204.8647\n",
            "Epoch 1986/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2515.2686 - val_loss: 3196.9536\n",
            "Epoch 1987/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2514.7034 - val_loss: 3194.7959\n",
            "Epoch 1988/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2515.4258 - val_loss: 3194.0657\n",
            "Epoch 1989/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2514.8501 - val_loss: 3193.6416\n",
            "Epoch 1990/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.8323 - val_loss: 3194.5330\n",
            "Epoch 1991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.5049 - val_loss: 3194.3013\n",
            "Epoch 1992/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2514.5818 - val_loss: 3194.8188\n",
            "Epoch 1993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2513.8667 - val_loss: 3195.4312\n",
            "Epoch 1994/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.2168 - val_loss: 3194.8660\n",
            "Epoch 1995/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2515.2461 - val_loss: 3192.9104\n",
            "Epoch 1996/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.9380 - val_loss: 3191.0320\n",
            "Epoch 1997/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2515.1313 - val_loss: 3190.4873\n",
            "Epoch 1998/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2517.0925 - val_loss: 3185.3335\n",
            "Epoch 1999/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2521.9565 - val_loss: 3184.0112\n",
            "Epoch 2000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2520.8875 - val_loss: 3187.0610\n",
            "Epoch 2001/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2516.7310 - val_loss: 3189.3379\n",
            "Epoch 2002/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2516.1709 - val_loss: 3189.7322\n",
            "Epoch 2003/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2515.1770 - val_loss: 3190.8848\n",
            "Epoch 2004/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.7173 - val_loss: 3193.2676\n",
            "Epoch 2005/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.4817 - val_loss: 3192.9856\n",
            "Epoch 2006/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2513.8088 - val_loss: 3193.6538\n",
            "Epoch 2007/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.8142 - val_loss: 3192.5952\n",
            "Epoch 2008/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2517.7368 - val_loss: 3190.2710\n",
            "Epoch 2009/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2519.4185 - val_loss: 3190.9175\n",
            "Epoch 2010/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2517.0894 - val_loss: 3192.5735\n",
            "Epoch 2011/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.4106 - val_loss: 3194.2690\n",
            "Epoch 2012/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.3574 - val_loss: 3194.3384\n",
            "Epoch 2013/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2516.5308 - val_loss: 3194.7690\n",
            "Epoch 2014/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.6431 - val_loss: 3196.7510\n",
            "Epoch 2015/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2514.8130 - val_loss: 3197.4958\n",
            "Epoch 2016/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2515.1794 - val_loss: 3200.8203\n",
            "Epoch 2017/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.4082 - val_loss: 3201.5559\n",
            "Epoch 2018/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.8455 - val_loss: 3203.9480\n",
            "Epoch 2019/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.6709 - val_loss: 3213.5081\n",
            "Epoch 2020/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2515.1196 - val_loss: 3217.4673\n",
            "Epoch 2021/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2514.2324 - val_loss: 3212.9685\n",
            "Epoch 2022/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2512.6189 - val_loss: 3211.3862\n",
            "Epoch 2023/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2512.7927 - val_loss: 3215.5762\n",
            "Epoch 2024/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2515.2886 - val_loss: 3209.8640\n",
            "Epoch 2025/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.3267 - val_loss: 3207.5459\n",
            "Epoch 2026/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.1470 - val_loss: 3203.6313\n",
            "Epoch 2027/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2512.4138 - val_loss: 3206.3616\n",
            "Epoch 2028/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.1091 - val_loss: 3205.6985\n",
            "Epoch 2029/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.7031 - val_loss: 3205.4678\n",
            "Epoch 2030/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.9626 - val_loss: 3206.8879\n",
            "Epoch 2031/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2509.8813 - val_loss: 3201.9241\n",
            "Epoch 2032/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2511.5383 - val_loss: 3201.9214\n",
            "Epoch 2033/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2513.0454 - val_loss: 3200.2515\n",
            "Epoch 2034/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2511.7888 - val_loss: 3198.7283\n",
            "Epoch 2035/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2511.2085 - val_loss: 3199.1892\n",
            "Epoch 2036/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.5969 - val_loss: 3198.5410\n",
            "Epoch 2037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.4966 - val_loss: 3197.2378\n",
            "Epoch 2038/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2511.3792 - val_loss: 3199.0449\n",
            "Epoch 2039/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2510.5605 - val_loss: 3202.2019\n",
            "Epoch 2040/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2511.2361 - val_loss: 3207.1931\n",
            "Epoch 2041/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.0347 - val_loss: 3211.5767\n",
            "Epoch 2042/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.7104 - val_loss: 3212.0967\n",
            "Epoch 2043/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2511.0410 - val_loss: 3213.8645\n",
            "Epoch 2044/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2511.1592 - val_loss: 3215.5344\n",
            "Epoch 2045/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2512.3154 - val_loss: 3218.7109\n",
            "Epoch 2046/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.3284 - val_loss: 3212.4478\n",
            "Epoch 2047/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2511.8665 - val_loss: 3204.9653\n",
            "Epoch 2048/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2510.1279 - val_loss: 3203.2075\n",
            "Epoch 2049/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2510.1504 - val_loss: 3204.5610\n",
            "Epoch 2050/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2511.4373 - val_loss: 3206.7234\n",
            "Epoch 2051/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2510.6575 - val_loss: 3197.4839\n",
            "Epoch 2052/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2510.4216 - val_loss: 3193.9880\n",
            "Epoch 2053/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2509.8745 - val_loss: 3194.0042\n",
            "Epoch 2054/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2509.5237 - val_loss: 3192.2429\n",
            "Epoch 2055/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2508.6079 - val_loss: 3188.4639\n",
            "Epoch 2056/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2513.0403 - val_loss: 3188.2566\n",
            "Epoch 2057/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2513.8875 - val_loss: 3187.1067\n",
            "Epoch 2058/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2515.0061 - val_loss: 3183.5896\n",
            "Epoch 2059/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2517.4419 - val_loss: 3183.3672\n",
            "Epoch 2060/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2515.5852 - val_loss: 3185.7146\n",
            "Epoch 2061/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2514.5510 - val_loss: 3187.4309\n",
            "Epoch 2062/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2510.3032 - val_loss: 3192.2356\n",
            "Epoch 2063/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2509.2620 - val_loss: 3193.0815\n",
            "Epoch 2064/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2509.1738 - val_loss: 3196.2974\n",
            "Epoch 2065/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2508.6418 - val_loss: 3197.0920\n",
            "Epoch 2066/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2508.6467 - val_loss: 3199.7678\n",
            "Epoch 2067/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2508.8281 - val_loss: 3202.9678\n",
            "Epoch 2068/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2509.2844 - val_loss: 3203.6953\n",
            "Epoch 2069/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2508.3994 - val_loss: 3202.2363\n",
            "Epoch 2070/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2508.0635 - val_loss: 3198.8809\n",
            "Epoch 2071/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.7390 - val_loss: 3189.6130\n",
            "Epoch 2072/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2510.3901 - val_loss: 3186.5120\n",
            "Epoch 2073/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2510.7336 - val_loss: 3186.6907\n",
            "Epoch 2074/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2510.5505 - val_loss: 3186.8259\n",
            "Epoch 2075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2510.5002 - val_loss: 3188.1699\n",
            "Epoch 2076/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2509.5732 - val_loss: 3188.2166\n",
            "Epoch 2077/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2509.5298 - val_loss: 3189.3311\n",
            "Epoch 2078/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2511.2708 - val_loss: 3197.3420\n",
            "Epoch 2079/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2506.5876 - val_loss: 3199.7046\n",
            "Epoch 2080/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2507.1833 - val_loss: 3201.0073\n",
            "Epoch 2081/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2508.8513 - val_loss: 3202.4810\n",
            "Epoch 2082/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2505.6106 - val_loss: 3199.3467\n",
            "Epoch 2083/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.6143 - val_loss: 3197.7510\n",
            "Epoch 2084/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.6836 - val_loss: 3196.2842\n",
            "Epoch 2085/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2507.2710 - val_loss: 3195.2590\n",
            "Epoch 2086/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.6201 - val_loss: 3197.8877\n",
            "Epoch 2087/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2506.2427 - val_loss: 3199.6721\n",
            "Epoch 2088/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.5969 - val_loss: 3199.3118\n",
            "Epoch 2089/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2506.3176 - val_loss: 3197.3247\n",
            "Epoch 2090/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2506.8992 - val_loss: 3194.3308\n",
            "Epoch 2091/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2507.4341 - val_loss: 3189.8235\n",
            "Epoch 2092/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2510.2036 - val_loss: 3189.5913\n",
            "Epoch 2093/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2508.7456 - val_loss: 3192.9119\n",
            "Epoch 2094/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2507.6665 - val_loss: 3198.3953\n",
            "Epoch 2095/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2507.6138 - val_loss: 3201.1931\n",
            "Epoch 2096/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.2065 - val_loss: 3197.4404\n",
            "Epoch 2097/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2508.9719 - val_loss: 3197.1025\n",
            "Epoch 2098/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2506.6531 - val_loss: 3199.4441\n",
            "Epoch 2099/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.6179 - val_loss: 3205.0977\n",
            "Epoch 2100/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.5271 - val_loss: 3204.7837\n",
            "Epoch 2101/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.0769 - val_loss: 3209.1987\n",
            "Epoch 2102/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.9033 - val_loss: 3207.6890\n",
            "Epoch 2103/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.4656 - val_loss: 3203.2166\n",
            "Epoch 2104/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.1201 - val_loss: 3201.7571\n",
            "Epoch 2105/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.1118 - val_loss: 3204.3679\n",
            "Epoch 2106/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2505.8452 - val_loss: 3208.6226\n",
            "Epoch 2107/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2506.9185 - val_loss: 3212.6875\n",
            "Epoch 2108/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2505.7085 - val_loss: 3212.1602\n",
            "Epoch 2109/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2505.2314 - val_loss: 3208.4695\n",
            "Epoch 2110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2505.2700 - val_loss: 3207.2766\n",
            "Epoch 2111/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.1641 - val_loss: 3207.5352\n",
            "Epoch 2112/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2504.9312 - val_loss: 3210.8040\n",
            "Epoch 2113/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.3328 - val_loss: 3209.9060\n",
            "Epoch 2114/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.3599 - val_loss: 3213.1716\n",
            "Epoch 2115/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2503.8315 - val_loss: 3208.6140\n",
            "Epoch 2116/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2508.2375 - val_loss: 3202.7380\n",
            "Epoch 2117/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.0522 - val_loss: 3202.8455\n",
            "Epoch 2118/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.3015 - val_loss: 3204.4053\n",
            "Epoch 2119/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2506.8933 - val_loss: 3212.1204\n",
            "Epoch 2120/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2506.2715 - val_loss: 3213.1833\n",
            "Epoch 2121/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2504.2966 - val_loss: 3208.9346\n",
            "Epoch 2122/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.5229 - val_loss: 3204.1917\n",
            "Epoch 2123/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2507.0627 - val_loss: 3199.7483\n",
            "Epoch 2124/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.0107 - val_loss: 3199.7122\n",
            "Epoch 2125/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.6914 - val_loss: 3193.8718\n",
            "Epoch 2126/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.6763 - val_loss: 3192.4978\n",
            "Epoch 2127/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.5776 - val_loss: 3194.3574\n",
            "Epoch 2128/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.1072 - val_loss: 3195.0361\n",
            "Epoch 2129/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2504.1978 - val_loss: 3193.3599\n",
            "Epoch 2130/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2503.9417 - val_loss: 3194.3484\n",
            "Epoch 2131/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2503.7344 - val_loss: 3193.7878\n",
            "Epoch 2132/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2505.7825 - val_loss: 3190.6035\n",
            "Epoch 2133/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.9900 - val_loss: 3189.5288\n",
            "Epoch 2134/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2506.9424 - val_loss: 3186.2581\n",
            "Epoch 2135/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2510.4011 - val_loss: 3182.8303\n",
            "Epoch 2136/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2512.2383 - val_loss: 3183.0168\n",
            "Epoch 2137/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2511.5764 - val_loss: 3185.4165\n",
            "Epoch 2138/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2503.9321 - val_loss: 3190.3262\n",
            "Epoch 2139/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2503.7549 - val_loss: 3192.0225\n",
            "Epoch 2140/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2503.5527 - val_loss: 3191.4062\n",
            "Epoch 2141/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2503.3757 - val_loss: 3190.0667\n",
            "Epoch 2142/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2503.3972 - val_loss: 3190.8220\n",
            "Epoch 2143/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2504.9148 - val_loss: 3193.3323\n",
            "Epoch 2144/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2502.3008 - val_loss: 3195.1846\n",
            "Epoch 2145/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.3208 - val_loss: 3190.8110\n",
            "Epoch 2146/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2502.3044 - val_loss: 3186.8855\n",
            "Epoch 2147/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2503.7205 - val_loss: 3186.2410\n",
            "Epoch 2148/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2505.5994 - val_loss: 3185.2397\n",
            "Epoch 2149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.2129 - val_loss: 3187.4028\n",
            "Epoch 2150/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2502.6223 - val_loss: 3188.6033\n",
            "Epoch 2151/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2503.1074 - val_loss: 3190.2148\n",
            "Epoch 2152/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.6960 - val_loss: 3191.5085\n",
            "Epoch 2153/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.4727 - val_loss: 3195.6340\n",
            "Epoch 2154/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2501.3508 - val_loss: 3195.6699\n",
            "Epoch 2155/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2500.9233 - val_loss: 3194.9890\n",
            "Epoch 2156/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.7502 - val_loss: 3193.8259\n",
            "Epoch 2157/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2500.9861 - val_loss: 3195.4429\n",
            "Epoch 2158/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2499.9778 - val_loss: 3198.6475\n",
            "Epoch 2159/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2501.5947 - val_loss: 3198.2458\n",
            "Epoch 2160/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2500.7566 - val_loss: 3198.6147\n",
            "Epoch 2161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.0371 - val_loss: 3197.8892\n",
            "Epoch 2162/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2500.7668 - val_loss: 3197.9751\n",
            "Epoch 2163/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.6226 - val_loss: 3200.4744\n",
            "Epoch 2164/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2500.4429 - val_loss: 3199.3174\n",
            "Epoch 2165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.1343 - val_loss: 3204.7654\n",
            "Epoch 2166/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.6711 - val_loss: 3209.4521\n",
            "Epoch 2167/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2503.1809 - val_loss: 3212.3240\n",
            "Epoch 2168/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2502.9460 - val_loss: 3212.3323\n",
            "Epoch 2169/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2502.5837 - val_loss: 3209.1733\n",
            "Epoch 2170/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2502.1401 - val_loss: 3207.1243\n",
            "Epoch 2171/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2501.9507 - val_loss: 3211.8806\n",
            "Epoch 2172/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2503.8108 - val_loss: 3215.8511\n",
            "Epoch 2173/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2504.9106 - val_loss: 3211.8638\n",
            "Epoch 2174/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2503.4451 - val_loss: 3210.3743\n",
            "Epoch 2175/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.4121 - val_loss: 3215.1243\n",
            "Epoch 2176/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2504.0256 - val_loss: 3216.2717\n",
            "Epoch 2177/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2505.1775 - val_loss: 3217.3516\n",
            "Epoch 2178/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2504.9922 - val_loss: 3214.1580\n",
            "Epoch 2179/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2504.1816 - val_loss: 3207.1099\n",
            "Epoch 2180/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.5012 - val_loss: 3208.0007\n",
            "Epoch 2181/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2502.8125 - val_loss: 3210.9561\n",
            "Epoch 2182/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2501.8108 - val_loss: 3208.3623\n",
            "Epoch 2183/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2501.7717 - val_loss: 3207.8406\n",
            "Epoch 2184/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2501.6904 - val_loss: 3206.1248\n",
            "Epoch 2185/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2500.4324 - val_loss: 3205.8999\n",
            "Epoch 2186/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2499.9004 - val_loss: 3204.3284\n",
            "Epoch 2187/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2499.5029 - val_loss: 3203.6116\n",
            "Epoch 2188/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2499.6030 - val_loss: 3206.5930\n",
            "Epoch 2189/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2500.2236 - val_loss: 3207.8640\n",
            "Epoch 2190/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2500.4094 - val_loss: 3208.6885\n",
            "Epoch 2191/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2502.6775 - val_loss: 3216.9326\n",
            "Epoch 2192/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2504.4836 - val_loss: 3215.7458\n",
            "Epoch 2193/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2504.2998 - val_loss: 3213.9229\n",
            "Epoch 2194/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2502.6414 - val_loss: 3209.0288\n",
            "Epoch 2195/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2498.6404 - val_loss: 3200.4553\n",
            "Epoch 2196/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2496.9817 - val_loss: 3193.5483\n",
            "Epoch 2197/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2498.4973 - val_loss: 3193.6907\n",
            "Epoch 2198/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2498.9397 - val_loss: 3198.1362\n",
            "Epoch 2199/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2498.5203 - val_loss: 3206.2227\n",
            "Epoch 2200/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2499.3616 - val_loss: 3205.7107\n",
            "Epoch 2201/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2499.3455 - val_loss: 3204.5271\n",
            "Epoch 2202/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2499.0305 - val_loss: 3202.9385\n",
            "Epoch 2203/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2498.0757 - val_loss: 3203.8958\n",
            "Epoch 2204/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2498.0869 - val_loss: 3211.9065\n",
            "Epoch 2205/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2500.3740 - val_loss: 3215.4070\n",
            "Epoch 2206/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2501.6621 - val_loss: 3214.4248\n",
            "Epoch 2207/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.8701 - val_loss: 3208.5793\n",
            "Epoch 2208/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2498.5588 - val_loss: 3205.4968\n",
            "Epoch 2209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.6572 - val_loss: 3206.7603\n",
            "Epoch 2210/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2498.6633 - val_loss: 3215.0249\n",
            "Epoch 2211/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2499.0833 - val_loss: 3213.0234\n",
            "Epoch 2212/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2498.5562 - val_loss: 3214.6987\n",
            "Epoch 2213/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.9026 - val_loss: 3211.0410\n",
            "Epoch 2214/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2497.7471 - val_loss: 3210.5273\n",
            "Epoch 2215/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2500.4275 - val_loss: 3207.4265\n",
            "Epoch 2216/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2497.9971 - val_loss: 3211.2866\n",
            "Epoch 2217/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.3123 - val_loss: 3209.4390\n",
            "Epoch 2218/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.8672 - val_loss: 3207.3582\n",
            "Epoch 2219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.8940 - val_loss: 3207.6721\n",
            "Epoch 2220/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2497.6692 - val_loss: 3207.7668\n",
            "Epoch 2221/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2497.1240 - val_loss: 3209.2822\n",
            "Epoch 2222/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.4722 - val_loss: 3211.7642\n",
            "Epoch 2223/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.1313 - val_loss: 3213.5498\n",
            "Epoch 2224/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2497.3455 - val_loss: 3216.9927\n",
            "Epoch 2225/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2496.9856 - val_loss: 3219.5210\n",
            "Epoch 2226/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2496.9014 - val_loss: 3222.8083\n",
            "Epoch 2227/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.3364 - val_loss: 3218.9346\n",
            "Epoch 2228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2498.3469 - val_loss: 3225.3948\n",
            "Epoch 2229/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2497.3606 - val_loss: 3226.8547\n",
            "Epoch 2230/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.6885 - val_loss: 3228.0281\n",
            "Epoch 2231/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.3787 - val_loss: 3226.9824\n",
            "Epoch 2232/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2497.0068 - val_loss: 3226.8501\n",
            "Epoch 2233/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.7046 - val_loss: 3226.3159\n",
            "Epoch 2234/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2498.2092 - val_loss: 3230.9343\n",
            "Epoch 2235/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2498.6570 - val_loss: 3228.9961\n",
            "Epoch 2236/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2497.4978 - val_loss: 3230.2234\n",
            "Epoch 2237/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2497.5088 - val_loss: 3222.3054\n",
            "Epoch 2238/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.3376 - val_loss: 3222.2163\n",
            "Epoch 2239/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.6409 - val_loss: 3225.2917\n",
            "Epoch 2240/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2498.5776 - val_loss: 3229.1221\n",
            "Epoch 2241/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2496.6411 - val_loss: 3225.7158\n",
            "Epoch 2242/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2497.5073 - val_loss: 3223.4297\n",
            "Epoch 2243/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.9746 - val_loss: 3220.7507\n",
            "Epoch 2244/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.5208 - val_loss: 3218.1721\n",
            "Epoch 2245/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.3083 - val_loss: 3217.3123\n",
            "Epoch 2246/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.6873 - val_loss: 3218.1819\n",
            "Epoch 2247/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2495.3042 - val_loss: 3219.4016\n",
            "Epoch 2248/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.0085 - val_loss: 3221.4392\n",
            "Epoch 2249/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.3716 - val_loss: 3221.1355\n",
            "Epoch 2250/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2496.1816 - val_loss: 3218.8933\n",
            "Epoch 2251/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.8328 - val_loss: 3218.1897\n",
            "Epoch 2252/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2495.2048 - val_loss: 3218.8325\n",
            "Epoch 2253/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.7314 - val_loss: 3220.5291\n",
            "Epoch 2254/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.6455 - val_loss: 3221.9038\n",
            "Epoch 2255/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.3464 - val_loss: 3222.5203\n",
            "Epoch 2256/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.5029 - val_loss: 3223.7598\n",
            "Epoch 2257/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.1858 - val_loss: 3221.9346\n",
            "Epoch 2258/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2497.5442 - val_loss: 3226.6636\n",
            "Epoch 2259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.7947 - val_loss: 3228.8374\n",
            "Epoch 2260/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2499.6357 - val_loss: 3236.3638\n",
            "Epoch 2261/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2499.3303 - val_loss: 3230.6433\n",
            "Epoch 2262/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.1833 - val_loss: 3230.6018\n",
            "Epoch 2263/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2496.3296 - val_loss: 3235.2634\n",
            "Epoch 2264/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2497.1924 - val_loss: 3232.3335\n",
            "Epoch 2265/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.3508 - val_loss: 3230.1890\n",
            "Epoch 2266/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.2683 - val_loss: 3228.4297\n",
            "Epoch 2267/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2494.2915 - val_loss: 3223.8865\n",
            "Epoch 2268/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2493.7866 - val_loss: 3218.7722\n",
            "Epoch 2269/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2493.8555 - val_loss: 3216.4470\n",
            "Epoch 2270/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2494.4268 - val_loss: 3215.7485\n",
            "Epoch 2271/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2494.2314 - val_loss: 3217.9316\n",
            "Epoch 2272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2494.0105 - val_loss: 3221.6506\n",
            "Epoch 2273/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2495.1055 - val_loss: 3222.1121\n",
            "Epoch 2274/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.1602 - val_loss: 3216.6240\n",
            "Epoch 2275/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2493.6863 - val_loss: 3215.0928\n",
            "Epoch 2276/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2499.6606 - val_loss: 3211.3567\n",
            "Epoch 2277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2495.5894 - val_loss: 3211.8950\n",
            "Epoch 2278/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2493.5078 - val_loss: 3215.0303\n",
            "Epoch 2279/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.5686 - val_loss: 3215.9944\n",
            "Epoch 2280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.9797 - val_loss: 3216.7510\n",
            "Epoch 2281/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2492.9595 - val_loss: 3218.6411\n",
            "Epoch 2282/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.9883 - val_loss: 3217.4946\n",
            "Epoch 2283/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.4636 - val_loss: 3215.8662\n",
            "Epoch 2284/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.5479 - val_loss: 3217.0872\n",
            "Epoch 2285/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.2888 - val_loss: 3215.2549\n",
            "Epoch 2286/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.3098 - val_loss: 3217.1282\n",
            "Epoch 2287/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2492.1978 - val_loss: 3215.9421\n",
            "Epoch 2288/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.6567 - val_loss: 3214.8384\n",
            "Epoch 2289/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.3208 - val_loss: 3216.0808\n",
            "Epoch 2290/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.0662 - val_loss: 3215.4033\n",
            "Epoch 2291/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2491.7903 - val_loss: 3212.2634\n",
            "Epoch 2292/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2491.9766 - val_loss: 3212.1714\n",
            "Epoch 2293/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.1504 - val_loss: 3212.6309\n",
            "Epoch 2294/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2491.8052 - val_loss: 3213.1509\n",
            "Epoch 2295/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.9080 - val_loss: 3216.9712\n",
            "Epoch 2296/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.6851 - val_loss: 3211.7900\n",
            "Epoch 2297/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2491.7209 - val_loss: 3210.9922\n",
            "Epoch 2298/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2491.0312 - val_loss: 3215.2043\n",
            "Epoch 2299/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2491.0647 - val_loss: 3217.7219\n",
            "Epoch 2300/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.8721 - val_loss: 3215.2458\n",
            "Epoch 2301/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2491.9910 - val_loss: 3212.6091\n",
            "Epoch 2302/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2491.7397 - val_loss: 3206.3040\n",
            "Epoch 2303/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2494.8733 - val_loss: 3202.2466\n",
            "Epoch 2304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2494.2861 - val_loss: 3202.8367\n",
            "Epoch 2305/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2493.4778 - val_loss: 3204.3445\n",
            "Epoch 2306/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.2859 - val_loss: 3204.8984\n",
            "Epoch 2307/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2491.3425 - val_loss: 3209.4849\n",
            "Epoch 2308/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.9519 - val_loss: 3211.6128\n",
            "Epoch 2309/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2490.7407 - val_loss: 3210.7563\n",
            "Epoch 2310/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2491.5137 - val_loss: 3211.6321\n",
            "Epoch 2311/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2486.2900 - val_loss: 3222.4285\n",
            "Epoch 2312/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2493.3154 - val_loss: 3227.4231\n",
            "Epoch 2313/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2496.8418 - val_loss: 3230.9993\n",
            "Epoch 2314/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2494.9448 - val_loss: 3226.0659\n",
            "Epoch 2315/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2496.8162 - val_loss: 3232.1204\n",
            "Epoch 2316/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2496.8088 - val_loss: 3226.9817\n",
            "Epoch 2317/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2493.5840 - val_loss: 3220.4839\n",
            "Epoch 2318/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2492.8650 - val_loss: 3215.1865\n",
            "Epoch 2319/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.7805 - val_loss: 3210.6753\n",
            "Epoch 2320/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2492.2769 - val_loss: 3210.7588\n",
            "Epoch 2321/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.2107 - val_loss: 3210.7085\n",
            "Epoch 2322/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.5950 - val_loss: 3207.8887\n",
            "Epoch 2323/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.8909 - val_loss: 3210.2131\n",
            "Epoch 2324/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.6873 - val_loss: 3209.7117\n",
            "Epoch 2325/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.4072 - val_loss: 3208.5254\n",
            "Epoch 2326/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.2341 - val_loss: 3208.1216\n",
            "Epoch 2327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2491.8347 - val_loss: 3205.9172\n",
            "Epoch 2328/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2492.2231 - val_loss: 3205.5417\n",
            "Epoch 2329/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2491.3320 - val_loss: 3207.1516\n",
            "Epoch 2330/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2493.3496 - val_loss: 3214.0659\n",
            "Epoch 2331/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2492.2827 - val_loss: 3215.3838\n",
            "Epoch 2332/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.3132 - val_loss: 3216.2197\n",
            "Epoch 2333/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.6780 - val_loss: 3215.5979\n",
            "Epoch 2334/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.7920 - val_loss: 3218.8933\n",
            "Epoch 2335/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.6580 - val_loss: 3218.7219\n",
            "Epoch 2336/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.7751 - val_loss: 3213.6506\n",
            "Epoch 2337/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2491.2283 - val_loss: 3213.1460\n",
            "Epoch 2338/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.4495 - val_loss: 3215.9087\n",
            "Epoch 2339/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2490.8870 - val_loss: 3211.8269\n",
            "Epoch 2340/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.8545 - val_loss: 3206.8958\n",
            "Epoch 2341/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2490.4036 - val_loss: 3204.9409\n",
            "Epoch 2342/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.3179 - val_loss: 3208.5759\n",
            "Epoch 2343/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2489.6653 - val_loss: 3210.4929\n",
            "Epoch 2344/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2487.1697 - val_loss: 3214.4529\n",
            "Epoch 2345/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2490.6401 - val_loss: 3218.7566\n",
            "Epoch 2346/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2491.8860 - val_loss: 3220.4758\n",
            "Epoch 2347/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.4414 - val_loss: 3218.4233\n",
            "Epoch 2348/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2491.4094 - val_loss: 3219.6716\n",
            "Epoch 2349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2489.7231 - val_loss: 3209.7227\n",
            "Epoch 2350/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2494.0112 - val_loss: 3201.2612\n",
            "Epoch 2351/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2489.0293 - val_loss: 3200.9062\n",
            "Epoch 2352/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.2258 - val_loss: 3201.7781\n",
            "Epoch 2353/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.2678 - val_loss: 3205.8672\n",
            "Epoch 2354/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.8657 - val_loss: 3208.0962\n",
            "Epoch 2355/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2489.0913 - val_loss: 3207.2563\n",
            "Epoch 2356/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.9597 - val_loss: 3202.9858\n",
            "Epoch 2357/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2489.0273 - val_loss: 3197.9165\n",
            "Epoch 2358/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.5903 - val_loss: 3196.6421\n",
            "Epoch 2359/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2491.7622 - val_loss: 3198.8484\n",
            "Epoch 2360/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2490.5820 - val_loss: 3200.5828\n",
            "Epoch 2361/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2489.5474 - val_loss: 3201.0408\n",
            "Epoch 2362/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2490.7563 - val_loss: 3204.8279\n",
            "Epoch 2363/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2488.7832 - val_loss: 3205.7900\n",
            "Epoch 2364/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.7029 - val_loss: 3206.4502\n",
            "Epoch 2365/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2490.1016 - val_loss: 3203.5190\n",
            "Epoch 2366/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.2837 - val_loss: 3203.7202\n",
            "Epoch 2367/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2492.4443 - val_loss: 3200.8010\n",
            "Epoch 2368/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.7771 - val_loss: 3203.8591\n",
            "Epoch 2369/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.0845 - val_loss: 3206.1560\n",
            "Epoch 2370/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2489.6904 - val_loss: 3208.7854\n",
            "Epoch 2371/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2488.4946 - val_loss: 3209.4568\n",
            "Epoch 2372/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2488.0776 - val_loss: 3208.2947\n",
            "Epoch 2373/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2489.3804 - val_loss: 3206.8416\n",
            "Epoch 2374/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2489.8638 - val_loss: 3208.6226\n",
            "Epoch 2375/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2488.3225 - val_loss: 3209.1177\n",
            "Epoch 2376/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.3508 - val_loss: 3212.4871\n",
            "Epoch 2377/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2492.6614 - val_loss: 3220.1538\n",
            "Epoch 2378/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.4207 - val_loss: 3216.7085\n",
            "Epoch 2379/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.1716 - val_loss: 3219.2756\n",
            "Epoch 2380/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.8584 - val_loss: 3217.9578\n",
            "Epoch 2381/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2487.9702 - val_loss: 3218.4846\n",
            "Epoch 2382/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2488.6243 - val_loss: 3217.0298\n",
            "Epoch 2383/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.6714 - val_loss: 3217.0098\n",
            "Epoch 2384/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2487.4106 - val_loss: 3215.3069\n",
            "Epoch 2385/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2488.0815 - val_loss: 3207.9958\n",
            "Epoch 2386/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2488.1753 - val_loss: 3207.0239\n",
            "Epoch 2387/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2486.6104 - val_loss: 3210.9277\n",
            "Epoch 2388/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.9219 - val_loss: 3217.8110\n",
            "Epoch 2389/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.0889 - val_loss: 3218.7388\n",
            "Epoch 2390/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2487.5498 - val_loss: 3218.9438\n",
            "Epoch 2391/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.0891 - val_loss: 3217.0530\n",
            "Epoch 2392/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.6858 - val_loss: 3218.9302\n",
            "Epoch 2393/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2486.1575 - val_loss: 3211.3799\n",
            "Epoch 2394/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2486.1816 - val_loss: 3209.8508\n",
            "Epoch 2395/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2486.1311 - val_loss: 3210.4109\n",
            "Epoch 2396/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2486.0901 - val_loss: 3213.7178\n",
            "Epoch 2397/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2486.2351 - val_loss: 3219.3518\n",
            "Epoch 2398/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2486.0364 - val_loss: 3228.6172\n",
            "Epoch 2399/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2490.9353 - val_loss: 3230.9004\n",
            "Epoch 2400/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2490.0239 - val_loss: 3229.4431\n",
            "Epoch 2401/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2489.1362 - val_loss: 3219.1116\n",
            "Epoch 2402/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.9553 - val_loss: 3214.9971\n",
            "Epoch 2403/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2491.8259 - val_loss: 3211.3523\n",
            "Epoch 2404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.3174 - val_loss: 3211.4460\n",
            "Epoch 2405/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2490.2607 - val_loss: 3209.3367\n",
            "Epoch 2406/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.2700 - val_loss: 3211.4863\n",
            "Epoch 2407/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2488.0251 - val_loss: 3212.0601\n",
            "Epoch 2408/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.1121 - val_loss: 3215.1284\n",
            "Epoch 2409/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.1582 - val_loss: 3212.2866\n",
            "Epoch 2410/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2487.2686 - val_loss: 3213.0232\n",
            "Epoch 2411/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2487.8892 - val_loss: 3216.9065\n",
            "Epoch 2412/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.4233 - val_loss: 3219.0085\n",
            "Epoch 2413/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.9270 - val_loss: 3220.6721\n",
            "Epoch 2414/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2485.7876 - val_loss: 3226.7654\n",
            "Epoch 2415/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.5854 - val_loss: 3222.5090\n",
            "Epoch 2416/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.7939 - val_loss: 3220.9297\n",
            "Epoch 2417/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2484.7498 - val_loss: 3220.8623\n",
            "Epoch 2418/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2484.6050 - val_loss: 3222.6226\n",
            "Epoch 2419/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.4768 - val_loss: 3227.8101\n",
            "Epoch 2420/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2486.7373 - val_loss: 3229.3284\n",
            "Epoch 2421/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.7356 - val_loss: 3224.0940\n",
            "Epoch 2422/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.2485 - val_loss: 3220.0159\n",
            "Epoch 2423/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.6245 - val_loss: 3218.6853\n",
            "Epoch 2424/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.8262 - val_loss: 3219.0852\n",
            "Epoch 2425/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.8547 - val_loss: 3219.3635\n",
            "Epoch 2426/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.4724 - val_loss: 3220.0920\n",
            "Epoch 2427/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.7664 - val_loss: 3214.7866\n",
            "Epoch 2428/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.5454 - val_loss: 3213.9744\n",
            "Epoch 2429/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.0210 - val_loss: 3214.9954\n",
            "Epoch 2430/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.2896 - val_loss: 3219.6729\n",
            "Epoch 2431/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2482.4604 - val_loss: 3216.0203\n",
            "Epoch 2432/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.6011 - val_loss: 3215.6709\n",
            "Epoch 2433/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.7136 - val_loss: 3216.1907\n",
            "Epoch 2434/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.6069 - val_loss: 3217.1909\n",
            "Epoch 2435/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2482.4219 - val_loss: 3220.2085\n",
            "Epoch 2436/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.7974 - val_loss: 3227.2876\n",
            "Epoch 2437/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.4751 - val_loss: 3222.6008\n",
            "Epoch 2438/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2483.6914 - val_loss: 3221.1858\n",
            "Epoch 2439/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2483.8113 - val_loss: 3220.9192\n",
            "Epoch 2440/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.3123 - val_loss: 3214.2822\n",
            "Epoch 2441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.1606 - val_loss: 3213.2354\n",
            "Epoch 2442/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.9209 - val_loss: 3213.6504\n",
            "Epoch 2443/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.4084 - val_loss: 3214.2246\n",
            "Epoch 2444/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.2290 - val_loss: 3211.5483\n",
            "Epoch 2445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.1260 - val_loss: 3209.4434\n",
            "Epoch 2446/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.1423 - val_loss: 3206.8860\n",
            "Epoch 2447/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.7368 - val_loss: 3207.1882\n",
            "Epoch 2448/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.3660 - val_loss: 3206.9272\n",
            "Epoch 2449/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.8425 - val_loss: 3205.8027\n",
            "Epoch 2450/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.3774 - val_loss: 3207.2803\n",
            "Epoch 2451/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.5139 - val_loss: 3209.3708\n",
            "Epoch 2452/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2482.8235 - val_loss: 3210.4609\n",
            "Epoch 2453/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2482.7241 - val_loss: 3210.1553\n",
            "Epoch 2454/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2482.4500 - val_loss: 3212.7148\n",
            "Epoch 2455/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.0120 - val_loss: 3218.6882\n",
            "Epoch 2456/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.1677 - val_loss: 3222.4128\n",
            "Epoch 2457/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.7742 - val_loss: 3224.9744\n",
            "Epoch 2458/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.7988 - val_loss: 3220.0793\n",
            "Epoch 2459/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2482.7559 - val_loss: 3223.2717\n",
            "Epoch 2460/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2485.1624 - val_loss: 3230.5642\n",
            "Epoch 2461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2487.3708 - val_loss: 3234.8940\n",
            "Epoch 2462/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2488.1309 - val_loss: 3232.7332\n",
            "Epoch 2463/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.7231 - val_loss: 3230.0105\n",
            "Epoch 2464/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2485.4888 - val_loss: 3227.8010\n",
            "Epoch 2465/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.4265 - val_loss: 3229.5535\n",
            "Epoch 2466/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.3660 - val_loss: 3228.2253\n",
            "Epoch 2467/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2485.0820 - val_loss: 3231.9341\n",
            "Epoch 2468/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2486.2407 - val_loss: 3229.6372\n",
            "Epoch 2469/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2485.3340 - val_loss: 3227.9333\n",
            "Epoch 2470/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2484.2690 - val_loss: 3221.5872\n",
            "Epoch 2471/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.1089 - val_loss: 3216.4824\n",
            "Epoch 2472/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.3748 - val_loss: 3214.4102\n",
            "Epoch 2473/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2480.3081 - val_loss: 3208.7827\n",
            "Epoch 2474/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.6895 - val_loss: 3204.1531\n",
            "Epoch 2475/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2482.4294 - val_loss: 3206.4182\n",
            "Epoch 2476/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.9001 - val_loss: 3207.7546\n",
            "Epoch 2477/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.1455 - val_loss: 3205.4758\n",
            "Epoch 2478/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2484.2285 - val_loss: 3204.6665\n",
            "Epoch 2479/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2482.9849 - val_loss: 3204.9045\n",
            "Epoch 2480/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.6404 - val_loss: 3207.1489\n",
            "Epoch 2481/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2482.8425 - val_loss: 3207.5022\n",
            "Epoch 2482/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2482.0271 - val_loss: 3209.7283\n",
            "Epoch 2483/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.5198 - val_loss: 3213.5151\n",
            "Epoch 2484/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.3794 - val_loss: 3217.5435\n",
            "Epoch 2485/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.5596 - val_loss: 3217.8743\n",
            "Epoch 2486/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2481.5566 - val_loss: 3222.3083\n",
            "Epoch 2487/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.6125 - val_loss: 3221.5635\n",
            "Epoch 2488/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2480.3162 - val_loss: 3226.8379\n",
            "Epoch 2489/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2482.9275 - val_loss: 3235.1116\n",
            "Epoch 2490/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2485.6775 - val_loss: 3227.0906\n",
            "Epoch 2491/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.4822 - val_loss: 3225.1074\n",
            "Epoch 2492/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.2388 - val_loss: 3230.6819\n",
            "Epoch 2493/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2483.2019 - val_loss: 3234.6379\n",
            "Epoch 2494/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.5325 - val_loss: 3231.1611\n",
            "Epoch 2495/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2483.2644 - val_loss: 3225.3809\n",
            "Epoch 2496/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2480.9031 - val_loss: 3219.3440\n",
            "Epoch 2497/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2480.4302 - val_loss: 3217.5664\n",
            "Epoch 2498/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.8501 - val_loss: 3218.2097\n",
            "Epoch 2499/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2479.2854 - val_loss: 3222.3247\n",
            "Epoch 2500/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2481.2080 - val_loss: 3225.6335\n",
            "Epoch 2501/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2480.4790 - val_loss: 3227.4214\n",
            "Epoch 2502/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.0627 - val_loss: 3227.8933\n",
            "Epoch 2503/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.7644 - val_loss: 3220.8516\n",
            "Epoch 2504/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.5266 - val_loss: 3217.7092\n",
            "Epoch 2505/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2479.7324 - val_loss: 3218.9629\n",
            "Epoch 2506/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2478.8740 - val_loss: 3221.0071\n",
            "Epoch 2507/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.6995 - val_loss: 3227.3147\n",
            "Epoch 2508/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.4534 - val_loss: 3228.0007\n",
            "Epoch 2509/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2476.8677 - val_loss: 3223.0342\n",
            "Epoch 2510/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2478.6902 - val_loss: 3221.4773\n",
            "Epoch 2511/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.6782 - val_loss: 3220.4158\n",
            "Epoch 2512/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.0957 - val_loss: 3222.9583\n",
            "Epoch 2513/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2478.6079 - val_loss: 3223.4971\n",
            "Epoch 2514/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.6484 - val_loss: 3224.6240\n",
            "Epoch 2515/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.4456 - val_loss: 3230.8999\n",
            "Epoch 2516/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.9204 - val_loss: 3234.3870\n",
            "Epoch 2517/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.3083 - val_loss: 3233.9873\n",
            "Epoch 2518/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.9517 - val_loss: 3228.4080\n",
            "Epoch 2519/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2478.3057 - val_loss: 3233.4529\n",
            "Epoch 2520/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2482.4951 - val_loss: 3243.1023\n",
            "Epoch 2521/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.1423 - val_loss: 3250.8542\n",
            "Epoch 2522/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2485.0032 - val_loss: 3246.7195\n",
            "Epoch 2523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2482.9929 - val_loss: 3240.8540\n",
            "Epoch 2524/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.2922 - val_loss: 3233.0247\n",
            "Epoch 2525/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.5178 - val_loss: 3231.3616\n",
            "Epoch 2526/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.4250 - val_loss: 3231.0166\n",
            "Epoch 2527/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.4141 - val_loss: 3231.9727\n",
            "Epoch 2528/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2476.4934 - val_loss: 3224.7717\n",
            "Epoch 2529/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.7820 - val_loss: 3220.5259\n",
            "Epoch 2530/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2478.8914 - val_loss: 3220.5291\n",
            "Epoch 2531/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2479.8831 - val_loss: 3218.3652\n",
            "Epoch 2532/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.0859 - val_loss: 3218.2532\n",
            "Epoch 2533/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2481.6404 - val_loss: 3221.2915\n",
            "Epoch 2534/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2477.6389 - val_loss: 3220.3821\n",
            "Epoch 2535/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.4814 - val_loss: 3220.7434\n",
            "Epoch 2536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.7192 - val_loss: 3221.4238\n",
            "Epoch 2537/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.2410 - val_loss: 3221.7163\n",
            "Epoch 2538/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2478.1233 - val_loss: 3222.1841\n",
            "Epoch 2539/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.8701 - val_loss: 3224.0344\n",
            "Epoch 2540/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2476.9514 - val_loss: 3227.3511\n",
            "Epoch 2541/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.2146 - val_loss: 3233.2988\n",
            "Epoch 2542/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.6270 - val_loss: 3236.2073\n",
            "Epoch 2543/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.3174 - val_loss: 3243.8428\n",
            "Epoch 2544/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2484.9399 - val_loss: 3245.8696\n",
            "Epoch 2545/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2486.0867 - val_loss: 3247.5071\n",
            "Epoch 2546/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2485.2087 - val_loss: 3234.7366\n",
            "Epoch 2547/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2480.4512 - val_loss: 3228.4719\n",
            "Epoch 2548/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.7322 - val_loss: 3226.0505\n",
            "Epoch 2549/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.3442 - val_loss: 3227.7195\n",
            "Epoch 2550/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2477.4475 - val_loss: 3224.7166\n",
            "Epoch 2551/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2477.4534 - val_loss: 3225.9568\n",
            "Epoch 2552/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2478.4644 - val_loss: 3227.1538\n",
            "Epoch 2553/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2478.8501 - val_loss: 3226.3623\n",
            "Epoch 2554/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.5789 - val_loss: 3218.5771\n",
            "Epoch 2555/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2476.5745 - val_loss: 3218.1145\n",
            "Epoch 2556/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.5308 - val_loss: 3217.3333\n",
            "Epoch 2557/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2477.0042 - val_loss: 3217.8728\n",
            "Epoch 2558/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.2732 - val_loss: 3217.6531\n",
            "Epoch 2559/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.1060 - val_loss: 3215.3718\n",
            "Epoch 2560/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2476.2537 - val_loss: 3212.8901\n",
            "Epoch 2561/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.3599 - val_loss: 3211.0386\n",
            "Epoch 2562/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.9597 - val_loss: 3210.7922\n",
            "Epoch 2563/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.0378 - val_loss: 3214.8413\n",
            "Epoch 2564/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2478.0012 - val_loss: 3219.5833\n",
            "Epoch 2565/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2477.3911 - val_loss: 3222.9729\n",
            "Epoch 2566/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479.0129 - val_loss: 3223.9041\n",
            "Epoch 2567/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.0403 - val_loss: 3223.1360\n",
            "Epoch 2568/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2478.2068 - val_loss: 3223.1650\n",
            "Epoch 2569/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2477.9229 - val_loss: 3223.1560\n",
            "Epoch 2570/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2477.7493 - val_loss: 3225.1035\n",
            "Epoch 2571/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.4429 - val_loss: 3224.0049\n",
            "Epoch 2572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.1016 - val_loss: 3221.7959\n",
            "Epoch 2573/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2476.5144 - val_loss: 3222.5767\n",
            "Epoch 2574/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.7292 - val_loss: 3223.9602\n",
            "Epoch 2575/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.9812 - val_loss: 3223.9290\n",
            "Epoch 2576/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.7356 - val_loss: 3224.1953\n",
            "Epoch 2577/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.7510 - val_loss: 3223.3347\n",
            "Epoch 2578/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2476.6233 - val_loss: 3220.2959\n",
            "Epoch 2579/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2475.3940 - val_loss: 3218.7612\n",
            "Epoch 2580/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.1765 - val_loss: 3218.1504\n",
            "Epoch 2581/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.6301 - val_loss: 3214.5554\n",
            "Epoch 2582/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2475.1643 - val_loss: 3214.3167\n",
            "Epoch 2583/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.4055 - val_loss: 3218.6340\n",
            "Epoch 2584/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.1409 - val_loss: 3223.6560\n",
            "Epoch 2585/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.4075 - val_loss: 3222.8125\n",
            "Epoch 2586/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.1885 - val_loss: 3219.4768\n",
            "Epoch 2587/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.3250 - val_loss: 3221.5608\n",
            "Epoch 2588/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.6099 - val_loss: 3217.3037\n",
            "Epoch 2589/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2476.9780 - val_loss: 3221.6404\n",
            "Epoch 2590/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2475.8267 - val_loss: 3221.2100\n",
            "Epoch 2591/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.2129 - val_loss: 3223.5471\n",
            "Epoch 2592/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.8433 - val_loss: 3221.9055\n",
            "Epoch 2593/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2475.0696 - val_loss: 3221.1909\n",
            "Epoch 2594/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2475.1770 - val_loss: 3221.6707\n",
            "Epoch 2595/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.4570 - val_loss: 3219.6265\n",
            "Epoch 2596/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2476.0566 - val_loss: 3226.1267\n",
            "Epoch 2597/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2481.6021 - val_loss: 3237.8672\n",
            "Epoch 2598/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2480.5242 - val_loss: 3236.4941\n",
            "Epoch 2599/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2478.0393 - val_loss: 3230.1956\n",
            "Epoch 2600/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2477.1335 - val_loss: 3219.8103\n",
            "Epoch 2601/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2475.7449 - val_loss: 3215.4060\n",
            "Epoch 2602/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2474.6606 - val_loss: 3214.8301\n",
            "Epoch 2603/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.9531 - val_loss: 3216.2471\n",
            "Epoch 2604/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2474.8223 - val_loss: 3215.9839\n",
            "Epoch 2605/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.1421 - val_loss: 3219.2097\n",
            "Epoch 2606/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2472.2844 - val_loss: 3223.5815\n",
            "Epoch 2607/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2475.2051 - val_loss: 3232.8477\n",
            "Epoch 2608/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2478.1284 - val_loss: 3240.9111\n",
            "Epoch 2609/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2481.9868 - val_loss: 3244.4971\n",
            "Epoch 2610/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2480.9055 - val_loss: 3238.1868\n",
            "Epoch 2611/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2478.5388 - val_loss: 3239.0674\n",
            "Epoch 2612/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2477.6953 - val_loss: 3236.7148\n",
            "Epoch 2613/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2473.9312 - val_loss: 3229.2405\n",
            "Epoch 2614/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.1602 - val_loss: 3227.5208\n",
            "Epoch 2615/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.7156 - val_loss: 3227.6616\n",
            "Epoch 2616/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.9448 - val_loss: 3227.9353\n",
            "Epoch 2617/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.3960 - val_loss: 3232.5271\n",
            "Epoch 2618/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2473.8455 - val_loss: 3232.7844\n",
            "Epoch 2619/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2473.8130 - val_loss: 3231.7229\n",
            "Epoch 2620/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.3462 - val_loss: 3233.6257\n",
            "Epoch 2621/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.9224 - val_loss: 3233.6162\n",
            "Epoch 2622/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.1079 - val_loss: 3233.2009\n",
            "Epoch 2623/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.4553 - val_loss: 3232.8135\n",
            "Epoch 2624/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.1091 - val_loss: 3230.1497\n",
            "Epoch 2625/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2472.8276 - val_loss: 3230.0540\n",
            "Epoch 2626/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.2478 - val_loss: 3235.6257\n",
            "Epoch 2627/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2473.9316 - val_loss: 3233.6917\n",
            "Epoch 2628/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.4436 - val_loss: 3232.3293\n",
            "Epoch 2629/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.2498 - val_loss: 3227.6602\n",
            "Epoch 2630/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.8005 - val_loss: 3225.6619\n",
            "Epoch 2631/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.6582 - val_loss: 3224.6008\n",
            "Epoch 2632/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2472.0972 - val_loss: 3222.4197\n",
            "Epoch 2633/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2472.1155 - val_loss: 3218.9753\n",
            "Epoch 2634/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2472.2310 - val_loss: 3221.9502\n",
            "Epoch 2635/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2471.6953 - val_loss: 3225.5190\n",
            "Epoch 2636/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.7598 - val_loss: 3226.2852\n",
            "Epoch 2637/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2471.6694 - val_loss: 3226.4062\n",
            "Epoch 2638/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.9517 - val_loss: 3228.0154\n",
            "Epoch 2639/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.3047 - val_loss: 3225.7275\n",
            "Epoch 2640/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.5547 - val_loss: 3219.4656\n",
            "Epoch 2641/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.2056 - val_loss: 3217.4160\n",
            "Epoch 2642/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.5415 - val_loss: 3216.9719\n",
            "Epoch 2643/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.7612 - val_loss: 3212.5239\n",
            "Epoch 2644/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.1968 - val_loss: 3212.4485\n",
            "Epoch 2645/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.0879 - val_loss: 3213.9385\n",
            "Epoch 2646/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2472.2271 - val_loss: 3218.2065\n",
            "Epoch 2647/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.6050 - val_loss: 3219.2595\n",
            "Epoch 2648/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.4309 - val_loss: 3221.6909\n",
            "Epoch 2649/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2474.5930 - val_loss: 3228.6636\n",
            "Epoch 2650/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.8501 - val_loss: 3229.3110\n",
            "Epoch 2651/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.5339 - val_loss: 3229.0342\n",
            "Epoch 2652/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.5374 - val_loss: 3233.6082\n",
            "Epoch 2653/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.7036 - val_loss: 3229.4958\n",
            "Epoch 2654/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2471.7061 - val_loss: 3230.7427\n",
            "Epoch 2655/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.2659 - val_loss: 3235.6160\n",
            "Epoch 2656/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2473.6550 - val_loss: 3231.7283\n",
            "Epoch 2657/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.8430 - val_loss: 3232.9253\n",
            "Epoch 2658/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.6694 - val_loss: 3227.7878\n",
            "Epoch 2659/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2468.9956 - val_loss: 3219.8892\n",
            "Epoch 2660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.0156 - val_loss: 3217.6074\n",
            "Epoch 2661/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.8320 - val_loss: 3218.1917\n",
            "Epoch 2662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.2683 - val_loss: 3219.6763\n",
            "Epoch 2663/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.2617 - val_loss: 3222.8296\n",
            "Epoch 2664/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2470.4126 - val_loss: 3218.3704\n",
            "Epoch 2665/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2472.8789 - val_loss: 3212.0095\n",
            "Epoch 2666/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.7891 - val_loss: 3211.3347\n",
            "Epoch 2667/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.8484 - val_loss: 3213.1626\n",
            "Epoch 2668/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2467.3645 - val_loss: 3220.5312\n",
            "Epoch 2669/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.4941 - val_loss: 3224.5029\n",
            "Epoch 2670/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.8770 - val_loss: 3223.1892\n",
            "Epoch 2671/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2470.4172 - val_loss: 3220.7510\n",
            "Epoch 2672/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2469.2734 - val_loss: 3217.0127\n",
            "Epoch 2673/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2470.3933 - val_loss: 3213.3325\n",
            "Epoch 2674/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.7266 - val_loss: 3213.7732\n",
            "Epoch 2675/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2468.7305 - val_loss: 3216.9246\n",
            "Epoch 2676/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.0652 - val_loss: 3217.5603\n",
            "Epoch 2677/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.8508 - val_loss: 3213.1396\n",
            "Epoch 2678/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.1619 - val_loss: 3219.3284\n",
            "Epoch 2679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.1365 - val_loss: 3223.8044\n",
            "Epoch 2680/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.9370 - val_loss: 3223.7859\n",
            "Epoch 2681/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.0986 - val_loss: 3221.7668\n",
            "Epoch 2682/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.2104 - val_loss: 3227.4065\n",
            "Epoch 2683/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2471.6899 - val_loss: 3230.3684\n",
            "Epoch 2684/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.2134 - val_loss: 3230.3640\n",
            "Epoch 2685/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.0544 - val_loss: 3227.8181\n",
            "Epoch 2686/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2472.6318 - val_loss: 3231.1169\n",
            "Epoch 2687/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.8135 - val_loss: 3231.4734\n",
            "Epoch 2688/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.2732 - val_loss: 3232.0640\n",
            "Epoch 2689/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2475.3110 - val_loss: 3226.9648\n",
            "Epoch 2690/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.2468 - val_loss: 3226.8735\n",
            "Epoch 2691/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.9880 - val_loss: 3227.7332\n",
            "Epoch 2692/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2470.0681 - val_loss: 3223.6892\n",
            "Epoch 2693/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.9448 - val_loss: 3228.2749\n",
            "Epoch 2694/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.8972 - val_loss: 3234.4663\n",
            "Epoch 2695/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.2700 - val_loss: 3230.2595\n",
            "Epoch 2696/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2474.4651 - val_loss: 3224.4585\n",
            "Epoch 2697/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.1909 - val_loss: 3224.0667\n",
            "Epoch 2698/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.8591 - val_loss: 3227.0986\n",
            "Epoch 2699/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2471.1672 - val_loss: 3229.9902\n",
            "Epoch 2700/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.9980 - val_loss: 3228.7764\n",
            "Epoch 2701/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.9082 - val_loss: 3227.1272\n",
            "Epoch 2702/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.3105 - val_loss: 3225.7275\n",
            "Epoch 2703/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2470.3687 - val_loss: 3225.6328\n",
            "Epoch 2704/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2472.1765 - val_loss: 3231.5471\n",
            "Epoch 2705/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.2412 - val_loss: 3230.8335\n",
            "Epoch 2706/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.9370 - val_loss: 3224.9792\n",
            "Epoch 2707/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2471.0815 - val_loss: 3220.5864\n",
            "Epoch 2708/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.3394 - val_loss: 3221.8865\n",
            "Epoch 2709/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.9868 - val_loss: 3224.0288\n",
            "Epoch 2710/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2470.9553 - val_loss: 3223.9092\n",
            "Epoch 2711/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.9092 - val_loss: 3224.8318\n",
            "Epoch 2712/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2470.1113 - val_loss: 3223.1516\n",
            "Epoch 2713/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.0112 - val_loss: 3225.0022\n",
            "Epoch 2714/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2471.0469 - val_loss: 3223.3054\n",
            "Epoch 2715/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2469.3347 - val_loss: 3219.8315\n",
            "Epoch 2716/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2469.1848 - val_loss: 3220.9373\n",
            "Epoch 2717/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2469.1672 - val_loss: 3221.2156\n",
            "Epoch 2718/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2469.1406 - val_loss: 3221.4133\n",
            "Epoch 2719/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2469.3586 - val_loss: 3221.0303\n",
            "Epoch 2720/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2468.7788 - val_loss: 3221.7114\n",
            "Epoch 2721/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.3464 - val_loss: 3232.1392\n",
            "Epoch 2722/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2475.3516 - val_loss: 3239.0903\n",
            "Epoch 2723/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2476.2673 - val_loss: 3237.1692\n",
            "Epoch 2724/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.3186 - val_loss: 3225.2378\n",
            "Epoch 2725/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2468.6157 - val_loss: 3216.6123\n",
            "Epoch 2726/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2470.6484 - val_loss: 3211.7747\n",
            "Epoch 2727/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2468.2146 - val_loss: 3211.7966\n",
            "Epoch 2728/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2468.2542 - val_loss: 3212.6047\n",
            "Epoch 2729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.9182 - val_loss: 3212.3210\n",
            "Epoch 2730/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2469.3079 - val_loss: 3214.0884\n",
            "Epoch 2731/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2468.0867 - val_loss: 3212.5874\n",
            "Epoch 2732/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2468.1130 - val_loss: 3217.1653\n",
            "Epoch 2733/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2468.2112 - val_loss: 3218.2620\n",
            "Epoch 2734/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.0034 - val_loss: 3216.4197\n",
            "Epoch 2735/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2469.1226 - val_loss: 3214.6841\n",
            "Epoch 2736/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2469.8757 - val_loss: 3213.0105\n",
            "Epoch 2737/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.2593 - val_loss: 3212.7493\n",
            "Epoch 2738/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2473.3799 - val_loss: 3214.1619\n",
            "Epoch 2739/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2473.5720 - val_loss: 3215.1011\n",
            "Epoch 2740/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2471.2549 - val_loss: 3216.0134\n",
            "Epoch 2741/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2471.5029 - val_loss: 3215.7397\n",
            "Epoch 2742/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2468.2327 - val_loss: 3219.4590\n",
            "Epoch 2743/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.4927 - val_loss: 3223.3062\n",
            "Epoch 2744/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.4104 - val_loss: 3225.5247\n",
            "Epoch 2745/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.0684 - val_loss: 3223.0835\n",
            "Epoch 2746/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2467.3894 - val_loss: 3226.9373\n",
            "Epoch 2747/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.4011 - val_loss: 3225.5200\n",
            "Epoch 2748/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.8704 - val_loss: 3224.7178\n",
            "Epoch 2749/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2466.8462 - val_loss: 3225.6082\n",
            "Epoch 2750/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.0234 - val_loss: 3225.5393\n",
            "Epoch 2751/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2467.2185 - val_loss: 3226.0127\n",
            "Epoch 2752/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.5867 - val_loss: 3225.3533\n",
            "Epoch 2753/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.3430 - val_loss: 3226.1160\n",
            "Epoch 2754/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2467.7830 - val_loss: 3227.6594\n",
            "Epoch 2755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.7024 - val_loss: 3226.8398\n",
            "Epoch 2756/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.3044 - val_loss: 3221.0320\n",
            "Epoch 2757/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.5344 - val_loss: 3218.1489\n",
            "Epoch 2758/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.8711 - val_loss: 3217.1938\n",
            "Epoch 2759/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.0083 - val_loss: 3216.9304\n",
            "Epoch 2760/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2466.1150 - val_loss: 3215.7991\n",
            "Epoch 2761/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2467.2073 - val_loss: 3219.9734\n",
            "Epoch 2762/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.9766 - val_loss: 3220.0752\n",
            "Epoch 2763/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.3472 - val_loss: 3217.5923\n",
            "Epoch 2764/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.1758 - val_loss: 3218.3379\n",
            "Epoch 2765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.0166 - val_loss: 3216.3735\n",
            "Epoch 2766/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2466.2712 - val_loss: 3217.1028\n",
            "Epoch 2767/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.0530 - val_loss: 3216.8323\n",
            "Epoch 2768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.2400 - val_loss: 3216.9453\n",
            "Epoch 2769/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.6118 - val_loss: 3212.0898\n",
            "Epoch 2770/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2468.0532 - val_loss: 3211.2878\n",
            "Epoch 2771/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.6074 - val_loss: 3214.5544\n",
            "Epoch 2772/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.3350 - val_loss: 3217.3147\n",
            "Epoch 2773/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.3193 - val_loss: 3219.3157\n",
            "Epoch 2774/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.5142 - val_loss: 3213.0710\n",
            "Epoch 2775/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2467.0190 - val_loss: 3210.4111\n",
            "Epoch 2776/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.4910 - val_loss: 3210.3301\n",
            "Epoch 2777/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.3450 - val_loss: 3212.5127\n",
            "Epoch 2778/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.0056 - val_loss: 3213.0283\n",
            "Epoch 2779/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2465.0996 - val_loss: 3215.2307\n",
            "Epoch 2780/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2465.3928 - val_loss: 3215.5095\n",
            "Epoch 2781/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.5376 - val_loss: 3218.0137\n",
            "Epoch 2782/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2465.6177 - val_loss: 3217.8440\n",
            "Epoch 2783/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.1318 - val_loss: 3220.0217\n",
            "Epoch 2784/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.7512 - val_loss: 3222.9783\n",
            "Epoch 2785/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.3394 - val_loss: 3225.7898\n",
            "Epoch 2786/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2464.9758 - val_loss: 3224.3328\n",
            "Epoch 2787/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.0588 - val_loss: 3223.7847\n",
            "Epoch 2788/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2464.9966 - val_loss: 3223.3835\n",
            "Epoch 2789/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2464.6326 - val_loss: 3222.5903\n",
            "Epoch 2790/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2464.9661 - val_loss: 3220.8040\n",
            "Epoch 2791/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.8284 - val_loss: 3220.9070\n",
            "Epoch 2792/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.1499 - val_loss: 3221.6392\n",
            "Epoch 2793/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.4827 - val_loss: 3226.6306\n",
            "Epoch 2794/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2464.4778 - val_loss: 3232.1465\n",
            "Epoch 2795/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.8911 - val_loss: 3237.9080\n",
            "Epoch 2796/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.8350 - val_loss: 3236.3174\n",
            "Epoch 2797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.0879 - val_loss: 3234.6990\n",
            "Epoch 2798/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.5830 - val_loss: 3228.7971\n",
            "Epoch 2799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.8831 - val_loss: 3226.1741\n",
            "Epoch 2800/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2466.3606 - val_loss: 3219.0808\n",
            "Epoch 2801/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.5820 - val_loss: 3220.2612\n",
            "Epoch 2802/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.6399 - val_loss: 3219.5938\n",
            "Epoch 2803/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.1311 - val_loss: 3215.5159\n",
            "Epoch 2804/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.2537 - val_loss: 3216.0315\n",
            "Epoch 2805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.2388 - val_loss: 3219.6733\n",
            "Epoch 2806/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.6902 - val_loss: 3220.4624\n",
            "Epoch 2807/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.5754 - val_loss: 3217.1658\n",
            "Epoch 2808/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2466.7598 - val_loss: 3216.3540\n",
            "Epoch 2809/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2472.1389 - val_loss: 3214.8735\n",
            "Epoch 2810/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2472.5376 - val_loss: 3214.9165\n",
            "Epoch 2811/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2469.7607 - val_loss: 3215.9341\n",
            "Epoch 2812/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2467.1924 - val_loss: 3216.8325\n",
            "Epoch 2813/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2467.1641 - val_loss: 3217.6929\n",
            "Epoch 2814/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.4814 - val_loss: 3217.9536\n",
            "Epoch 2815/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2467.8811 - val_loss: 3215.7124\n",
            "Epoch 2816/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2468.1985 - val_loss: 3216.4229\n",
            "Epoch 2817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2466.1047 - val_loss: 3220.3428\n",
            "Epoch 2818/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2465.3372 - val_loss: 3223.1060\n",
            "Epoch 2819/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.7012 - val_loss: 3221.4697\n",
            "Epoch 2820/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.0964 - val_loss: 3222.8582\n",
            "Epoch 2821/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2463.9551 - val_loss: 3224.3315\n",
            "Epoch 2822/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2462.5730 - val_loss: 3222.5808\n",
            "Epoch 2823/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.4570 - val_loss: 3224.8853\n",
            "Epoch 2824/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.2451 - val_loss: 3228.2620\n",
            "Epoch 2825/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2464.0305 - val_loss: 3227.3237\n",
            "Epoch 2826/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.0659 - val_loss: 3230.4502\n",
            "Epoch 2827/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.2925 - val_loss: 3231.0740\n",
            "Epoch 2828/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2463.1621 - val_loss: 3231.2043\n",
            "Epoch 2829/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.2178 - val_loss: 3231.1621\n",
            "Epoch 2830/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2462.8594 - val_loss: 3231.2795\n",
            "Epoch 2831/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2462.7290 - val_loss: 3230.3452\n",
            "Epoch 2832/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2462.6731 - val_loss: 3229.5247\n",
            "Epoch 2833/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.3340 - val_loss: 3228.1797\n",
            "Epoch 2834/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.2834 - val_loss: 3228.6379\n",
            "Epoch 2835/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.4651 - val_loss: 3227.8171\n",
            "Epoch 2836/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.1567 - val_loss: 3224.0952\n",
            "Epoch 2837/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.0789 - val_loss: 3223.5400\n",
            "Epoch 2838/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.2625 - val_loss: 3224.2317\n",
            "Epoch 2839/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.3103 - val_loss: 3227.2827\n",
            "Epoch 2840/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2461.6311 - val_loss: 3226.8574\n",
            "Epoch 2841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2461.9146 - val_loss: 3229.1401\n",
            "Epoch 2842/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.5894 - val_loss: 3230.9478\n",
            "Epoch 2843/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.0830 - val_loss: 3230.3772\n",
            "Epoch 2844/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.0432 - val_loss: 3228.3252\n",
            "Epoch 2845/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.2461 - val_loss: 3228.5872\n",
            "Epoch 2846/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.9946 - val_loss: 3227.5410\n",
            "Epoch 2847/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2462.0996 - val_loss: 3225.2205\n",
            "Epoch 2848/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.0935 - val_loss: 3224.6104\n",
            "Epoch 2849/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2462.7151 - val_loss: 3224.8328\n",
            "Epoch 2850/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.4868 - val_loss: 3224.5591\n",
            "Epoch 2851/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.6201 - val_loss: 3223.0747\n",
            "Epoch 2852/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2463.3552 - val_loss: 3223.8384\n",
            "Epoch 2853/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2464.0474 - val_loss: 3229.2678\n",
            "Epoch 2854/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.3772 - val_loss: 3230.3589\n",
            "Epoch 2855/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2461.2798 - val_loss: 3231.6650\n",
            "Epoch 2856/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.7524 - val_loss: 3239.5417\n",
            "Epoch 2857/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.9705 - val_loss: 3244.2156\n",
            "Epoch 2858/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2465.4790 - val_loss: 3248.8091\n",
            "Epoch 2859/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2466.7422 - val_loss: 3250.0840\n",
            "Epoch 2860/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2469.2097 - val_loss: 3254.6604\n",
            "Epoch 2861/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2465.4753 - val_loss: 3246.5146\n",
            "Epoch 2862/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.6716 - val_loss: 3246.8660\n",
            "Epoch 2863/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2464.7439 - val_loss: 3247.6770\n",
            "Epoch 2864/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.9368 - val_loss: 3246.4585\n",
            "Epoch 2865/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.5208 - val_loss: 3242.6116\n",
            "Epoch 2866/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.1650 - val_loss: 3243.2925\n",
            "Epoch 2867/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.6824 - val_loss: 3235.5398\n",
            "Epoch 2868/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2462.4590 - val_loss: 3238.4121\n",
            "Epoch 2869/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.3599 - val_loss: 3235.1538\n",
            "Epoch 2870/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2462.2244 - val_loss: 3233.0991\n",
            "Epoch 2871/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.8181 - val_loss: 3234.2141\n",
            "Epoch 2872/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2461.0598 - val_loss: 3235.1882\n",
            "Epoch 2873/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.7192 - val_loss: 3237.7485\n",
            "Epoch 2874/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.2712 - val_loss: 3241.5715\n",
            "Epoch 2875/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.9314 - val_loss: 3243.8428\n",
            "Epoch 2876/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.3542 - val_loss: 3243.4224\n",
            "Epoch 2877/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2462.1863 - val_loss: 3242.5867\n",
            "Epoch 2878/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.8535 - val_loss: 3241.4854\n",
            "Epoch 2879/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.2747 - val_loss: 3236.6899\n",
            "Epoch 2880/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.4126 - val_loss: 3235.8286\n",
            "Epoch 2881/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2463.8215 - val_loss: 3243.7788\n",
            "Epoch 2882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.8394 - val_loss: 3246.7871\n",
            "Epoch 2883/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2464.0698 - val_loss: 3245.6677\n",
            "Epoch 2884/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2463.1892 - val_loss: 3245.5889\n",
            "Epoch 2885/10000\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2462.9893 - val_loss: 3242.7405\n",
            "Epoch 2886/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.9146 - val_loss: 3238.5344\n",
            "Epoch 2887/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.9099 - val_loss: 3237.8831\n",
            "Epoch 2888/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.0813 - val_loss: 3237.5527\n",
            "Epoch 2889/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2460.5574 - val_loss: 3239.3494\n",
            "Epoch 2890/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.7222 - val_loss: 3237.8574\n",
            "Epoch 2891/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2461.4248 - val_loss: 3239.7542\n",
            "Epoch 2892/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2461.5383 - val_loss: 3239.8062\n",
            "Epoch 2893/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.0505 - val_loss: 3236.9519\n",
            "Epoch 2894/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2461.1550 - val_loss: 3237.1128\n",
            "Epoch 2895/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.5894 - val_loss: 3236.6160\n",
            "Epoch 2896/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.5623 - val_loss: 3237.1411\n",
            "Epoch 2897/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.5222 - val_loss: 3237.3948\n",
            "Epoch 2898/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.5481 - val_loss: 3236.7510\n",
            "Epoch 2899/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.2837 - val_loss: 3232.3210\n",
            "Epoch 2900/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.7456 - val_loss: 3229.9609\n",
            "Epoch 2901/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.1401 - val_loss: 3232.0884\n",
            "Epoch 2902/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.4333 - val_loss: 3236.2981\n",
            "Epoch 2903/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2460.7910 - val_loss: 3237.7334\n",
            "Epoch 2904/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.1597 - val_loss: 3234.7598\n",
            "Epoch 2905/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.6526 - val_loss: 3233.2036\n",
            "Epoch 2906/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.9668 - val_loss: 3235.5632\n",
            "Epoch 2907/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.8184 - val_loss: 3240.6643\n",
            "Epoch 2908/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2461.4609 - val_loss: 3250.4653\n",
            "Epoch 2909/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.8481 - val_loss: 3245.2085\n",
            "Epoch 2910/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.0647 - val_loss: 3240.7324\n",
            "Epoch 2911/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.9771 - val_loss: 3239.1685\n",
            "Epoch 2912/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2459.9192 - val_loss: 3236.7830\n",
            "Epoch 2913/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.9346 - val_loss: 3241.0427\n",
            "Epoch 2914/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2459.2551 - val_loss: 3243.4375\n",
            "Epoch 2915/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.4846 - val_loss: 3243.6135\n",
            "Epoch 2916/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.6213 - val_loss: 3246.9854\n",
            "Epoch 2917/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2461.6426 - val_loss: 3253.9473\n",
            "Epoch 2918/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.3921 - val_loss: 3257.9685\n",
            "Epoch 2919/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.9192 - val_loss: 3255.9385\n",
            "Epoch 2920/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2463.0151 - val_loss: 3255.1023\n",
            "Epoch 2921/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2461.9429 - val_loss: 3251.2502\n",
            "Epoch 2922/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.3804 - val_loss: 3246.1306\n",
            "Epoch 2923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.4084 - val_loss: 3244.4263\n",
            "Epoch 2924/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.1853 - val_loss: 3240.9136\n",
            "Epoch 2925/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.1863 - val_loss: 3239.6841\n",
            "Epoch 2926/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.1570 - val_loss: 3242.1860\n",
            "Epoch 2927/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.7053 - val_loss: 3244.4004\n",
            "Epoch 2928/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2459.4971 - val_loss: 3245.5259\n",
            "Epoch 2929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.3044 - val_loss: 3244.8704\n",
            "Epoch 2930/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.4392 - val_loss: 3244.3118\n",
            "Epoch 2931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.6382 - val_loss: 3240.8579\n",
            "Epoch 2932/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.6067 - val_loss: 3239.4680\n",
            "Epoch 2933/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.8662 - val_loss: 3234.0178\n",
            "Epoch 2934/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.6533 - val_loss: 3232.7397\n",
            "Epoch 2935/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.4963 - val_loss: 3233.1812\n",
            "Epoch 2936/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.3005 - val_loss: 3234.3462\n",
            "Epoch 2937/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2457.8037 - val_loss: 3234.7805\n",
            "Epoch 2938/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.4358 - val_loss: 3232.6128\n",
            "Epoch 2939/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2457.9358 - val_loss: 3232.2859\n",
            "Epoch 2940/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2458.0894 - val_loss: 3232.1416\n",
            "Epoch 2941/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.9026 - val_loss: 3232.0625\n",
            "Epoch 2942/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.8372 - val_loss: 3232.3789\n",
            "Epoch 2943/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.9790 - val_loss: 3232.2991\n",
            "Epoch 2944/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.4570 - val_loss: 3231.1025\n",
            "Epoch 2945/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2457.2734 - val_loss: 3227.2957\n",
            "Epoch 2946/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2458.4688 - val_loss: 3227.6729\n",
            "Epoch 2947/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2458.3245 - val_loss: 3229.6553\n",
            "Epoch 2948/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2457.8137 - val_loss: 3231.7278\n",
            "Epoch 2949/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2457.4385 - val_loss: 3233.1851\n",
            "Epoch 2950/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.0090 - val_loss: 3241.3784\n",
            "Epoch 2951/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.8123 - val_loss: 3247.2378\n",
            "Epoch 2952/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2463.2820 - val_loss: 3247.8752\n",
            "Epoch 2953/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2462.5251 - val_loss: 3244.9705\n",
            "Epoch 2954/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2460.9326 - val_loss: 3242.3040\n",
            "Epoch 2955/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.1062 - val_loss: 3237.8118\n",
            "Epoch 2956/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.0825 - val_loss: 3239.0105\n",
            "Epoch 2957/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2459.2466 - val_loss: 3238.3252\n",
            "Epoch 2958/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.7207 - val_loss: 3239.4006\n",
            "Epoch 2959/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.3857 - val_loss: 3243.2354\n",
            "Epoch 2960/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.3501 - val_loss: 3244.8933\n",
            "Epoch 2961/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.1465 - val_loss: 3247.0879\n",
            "Epoch 2962/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2457.3472 - val_loss: 3239.6523\n",
            "Epoch 2963/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.0159 - val_loss: 3240.0896\n",
            "Epoch 2964/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.9670 - val_loss: 3238.7097\n",
            "Epoch 2965/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.7043 - val_loss: 3241.6909\n",
            "Epoch 2966/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.3469 - val_loss: 3238.5659\n",
            "Epoch 2967/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2457.3181 - val_loss: 3235.2742\n",
            "Epoch 2968/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.5742 - val_loss: 3231.5640\n",
            "Epoch 2969/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.1550 - val_loss: 3235.6643\n",
            "Epoch 2970/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.3887 - val_loss: 3234.5850\n",
            "Epoch 2971/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2457.6106 - val_loss: 3233.0032\n",
            "Epoch 2972/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2457.2759 - val_loss: 3230.0007\n",
            "Epoch 2973/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.8171 - val_loss: 3230.3428\n",
            "Epoch 2974/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.4937 - val_loss: 3230.4126\n",
            "Epoch 2975/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.1724 - val_loss: 3237.8445\n",
            "Epoch 2976/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2460.2690 - val_loss: 3240.0679\n",
            "Epoch 2977/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.9846 - val_loss: 3234.9829\n",
            "Epoch 2978/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.6199 - val_loss: 3232.1396\n",
            "Epoch 2979/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.8225 - val_loss: 3230.0696\n",
            "Epoch 2980/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2455.9014 - val_loss: 3226.9502\n",
            "Epoch 2981/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2456.5647 - val_loss: 3224.6594\n",
            "Epoch 2982/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.5679 - val_loss: 3223.7461\n",
            "Epoch 2983/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.6987 - val_loss: 3223.6396\n",
            "Epoch 2984/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2457.2058 - val_loss: 3223.9277\n",
            "Epoch 2985/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.8762 - val_loss: 3224.2217\n",
            "Epoch 2986/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.3765 - val_loss: 3225.5049\n",
            "Epoch 2987/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.2795 - val_loss: 3226.7297\n",
            "Epoch 2988/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2455.9817 - val_loss: 3227.0752\n",
            "Epoch 2989/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2455.4080 - val_loss: 3227.9390\n",
            "Epoch 2990/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2454.8464 - val_loss: 3225.3220\n",
            "Epoch 2991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.0610 - val_loss: 3224.5940\n",
            "Epoch 2992/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.5586 - val_loss: 3227.4160\n",
            "Epoch 2993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.2244 - val_loss: 3224.9189\n",
            "Epoch 2994/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.5774 - val_loss: 3228.0378\n",
            "Epoch 2995/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.4358 - val_loss: 3228.0815\n",
            "Epoch 2996/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.5100 - val_loss: 3227.9941\n",
            "Epoch 2997/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.6052 - val_loss: 3225.9209\n",
            "Epoch 2998/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.7458 - val_loss: 3225.6042\n",
            "Epoch 2999/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.6738 - val_loss: 3227.5081\n",
            "Epoch 3000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.8606 - val_loss: 3231.4424\n",
            "Epoch 3001/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2454.9424 - val_loss: 3234.8828\n",
            "Epoch 3002/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.7278 - val_loss: 3238.1665\n",
            "Epoch 3003/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.1008 - val_loss: 3239.0225\n",
            "Epoch 3004/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.4993 - val_loss: 3239.8459\n",
            "Epoch 3005/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.8213 - val_loss: 3243.4783\n",
            "Epoch 3006/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2455.9126 - val_loss: 3239.8916\n",
            "Epoch 3007/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2455.3611 - val_loss: 3236.9031\n",
            "Epoch 3008/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.6865 - val_loss: 3239.8279\n",
            "Epoch 3009/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.8560 - val_loss: 3236.8462\n",
            "Epoch 3010/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2455.3291 - val_loss: 3233.1208\n",
            "Epoch 3011/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.4153 - val_loss: 3233.0852\n",
            "Epoch 3012/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.0696 - val_loss: 3231.4470\n",
            "Epoch 3013/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.5283 - val_loss: 3231.5535\n",
            "Epoch 3014/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2454.5762 - val_loss: 3231.9656\n",
            "Epoch 3015/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.3450 - val_loss: 3230.9573\n",
            "Epoch 3016/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2455.7708 - val_loss: 3231.6460\n",
            "Epoch 3017/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.9932 - val_loss: 3234.1321\n",
            "Epoch 3018/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.9502 - val_loss: 3234.7451\n",
            "Epoch 3019/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2456.1885 - val_loss: 3233.2107\n",
            "Epoch 3020/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.6912 - val_loss: 3232.0696\n",
            "Epoch 3021/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2461.2424 - val_loss: 3231.4990\n",
            "Epoch 3022/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.7695 - val_loss: 3234.3010\n",
            "Epoch 3023/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2457.4675 - val_loss: 3236.2747\n",
            "Epoch 3024/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.2979 - val_loss: 3237.5891\n",
            "Epoch 3025/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.4153 - val_loss: 3240.0503\n",
            "Epoch 3026/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.1008 - val_loss: 3246.8391\n",
            "Epoch 3027/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2454.1028 - val_loss: 3248.0962\n",
            "Epoch 3028/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.1658 - val_loss: 3255.6040\n",
            "Epoch 3029/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.3723 - val_loss: 3265.0442\n",
            "Epoch 3030/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2464.2612 - val_loss: 3260.7998\n",
            "Epoch 3031/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2457.0669 - val_loss: 3250.1309\n",
            "Epoch 3032/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2456.5955 - val_loss: 3242.3003\n",
            "Epoch 3033/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.1055 - val_loss: 3239.4929\n",
            "Epoch 3034/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2453.7368 - val_loss: 3237.3604\n",
            "Epoch 3035/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2454.5042 - val_loss: 3236.0713\n",
            "Epoch 3036/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2452.2024 - val_loss: 3240.0520\n",
            "Epoch 3037/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.8418 - val_loss: 3242.6763\n",
            "Epoch 3038/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2453.5403 - val_loss: 3243.3640\n",
            "Epoch 3039/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2453.4980 - val_loss: 3242.9944\n",
            "Epoch 3040/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.5962 - val_loss: 3241.3677\n",
            "Epoch 3041/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.6055 - val_loss: 3247.8933\n",
            "Epoch 3042/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.8274 - val_loss: 3251.2068\n",
            "Epoch 3043/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2454.6672 - val_loss: 3251.1172\n",
            "Epoch 3044/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.5947 - val_loss: 3248.0649\n",
            "Epoch 3045/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2453.7812 - val_loss: 3249.5828\n",
            "Epoch 3046/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2456.8855 - val_loss: 3257.5857\n",
            "Epoch 3047/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2460.0608 - val_loss: 3261.1602\n",
            "Epoch 3048/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2459.1885 - val_loss: 3260.0242\n",
            "Epoch 3049/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2460.4868 - val_loss: 3264.9172\n",
            "Epoch 3050/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2461.2544 - val_loss: 3264.8044\n",
            "Epoch 3051/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2458.6279 - val_loss: 3257.5085\n",
            "Epoch 3052/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2456.5688 - val_loss: 3253.8621\n",
            "Epoch 3053/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.3782 - val_loss: 3245.1829\n",
            "Epoch 3054/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.7644 - val_loss: 3244.4033\n",
            "Epoch 3055/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.4900 - val_loss: 3245.0059\n",
            "Epoch 3056/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.7798 - val_loss: 3248.1052\n",
            "Epoch 3057/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.3562 - val_loss: 3244.2122\n",
            "Epoch 3058/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2453.1274 - val_loss: 3242.9695\n",
            "Epoch 3059/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.5750 - val_loss: 3241.8123\n",
            "Epoch 3060/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2452.7729 - val_loss: 3241.9880\n",
            "Epoch 3061/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2452.0596 - val_loss: 3246.0823\n",
            "Epoch 3062/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.8767 - val_loss: 3247.5337\n",
            "Epoch 3063/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.3357 - val_loss: 3255.9736\n",
            "Epoch 3064/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2457.2146 - val_loss: 3257.1545\n",
            "Epoch 3065/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2455.5396 - val_loss: 3252.9087\n",
            "Epoch 3066/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.9185 - val_loss: 3249.8184\n",
            "Epoch 3067/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2453.0520 - val_loss: 3247.2725\n",
            "Epoch 3068/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.4277 - val_loss: 3239.7458\n",
            "Epoch 3069/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.3127 - val_loss: 3237.9377\n",
            "Epoch 3070/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2453.9663 - val_loss: 3235.1650\n",
            "Epoch 3071/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.1914 - val_loss: 3232.8865\n",
            "Epoch 3072/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.5239 - val_loss: 3233.7964\n",
            "Epoch 3073/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.8669 - val_loss: 3237.6921\n",
            "Epoch 3074/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.1660 - val_loss: 3245.9070\n",
            "Epoch 3075/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.0205 - val_loss: 3247.5144\n",
            "Epoch 3076/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.0771 - val_loss: 3247.4839\n",
            "Epoch 3077/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2451.5159 - val_loss: 3246.4199\n",
            "Epoch 3078/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2451.9570 - val_loss: 3246.4602\n",
            "Epoch 3079/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2452.2468 - val_loss: 3244.6033\n",
            "Epoch 3080/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2452.9629 - val_loss: 3240.5483\n",
            "Epoch 3081/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.9919 - val_loss: 3240.4800\n",
            "Epoch 3082/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2453.5222 - val_loss: 3239.0154\n",
            "Epoch 3083/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2453.8042 - val_loss: 3240.2219\n",
            "Epoch 3084/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2452.2722 - val_loss: 3240.3333\n",
            "Epoch 3085/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.3630 - val_loss: 3242.8989\n",
            "Epoch 3086/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.7368 - val_loss: 3245.6218\n",
            "Epoch 3087/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2451.8618 - val_loss: 3246.4375\n",
            "Epoch 3088/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2452.4172 - val_loss: 3248.3821\n",
            "Epoch 3089/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2455.6785 - val_loss: 3253.3083\n",
            "Epoch 3090/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.8967 - val_loss: 3247.2339\n",
            "Epoch 3091/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2452.2856 - val_loss: 3245.0298\n",
            "Epoch 3092/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.9121 - val_loss: 3241.4333\n",
            "Epoch 3093/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2456.1011 - val_loss: 3239.5889\n",
            "Epoch 3094/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.4114 - val_loss: 3238.3406\n",
            "Epoch 3095/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2458.4617 - val_loss: 3237.5952\n",
            "Epoch 3096/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2457.1257 - val_loss: 3237.9087\n",
            "Epoch 3097/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2457.1355 - val_loss: 3238.6948\n",
            "Epoch 3098/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.2773 - val_loss: 3238.8496\n",
            "Epoch 3099/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2454.3096 - val_loss: 3240.0066\n",
            "Epoch 3100/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2456.6733 - val_loss: 3237.5403\n",
            "Epoch 3101/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2456.8875 - val_loss: 3236.9800\n",
            "Epoch 3102/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2455.7100 - val_loss: 3237.7563\n",
            "Epoch 3103/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.9226 - val_loss: 3237.5159\n",
            "Epoch 3104/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2453.9368 - val_loss: 3234.8525\n",
            "Epoch 3105/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2454.5540 - val_loss: 3234.3364\n",
            "Epoch 3106/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2454.1331 - val_loss: 3235.4536\n",
            "Epoch 3107/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.5894 - val_loss: 3235.5872\n",
            "Epoch 3108/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2452.3337 - val_loss: 3236.4253\n",
            "Epoch 3109/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.7744 - val_loss: 3239.4038\n",
            "Epoch 3110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.7417 - val_loss: 3240.2859\n",
            "Epoch 3111/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.2585 - val_loss: 3244.0056\n",
            "Epoch 3112/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2450.6560 - val_loss: 3245.9727\n",
            "Epoch 3113/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.7161 - val_loss: 3252.3870\n",
            "Epoch 3114/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.8411 - val_loss: 3253.6594\n",
            "Epoch 3115/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.5713 - val_loss: 3255.9622\n",
            "Epoch 3116/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.7683 - val_loss: 3256.5110\n",
            "Epoch 3117/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2451.1990 - val_loss: 3252.4092\n",
            "Epoch 3118/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.6182 - val_loss: 3254.6541\n",
            "Epoch 3119/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2451.5576 - val_loss: 3251.2893\n",
            "Epoch 3120/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.8994 - val_loss: 3249.7322\n",
            "Epoch 3121/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.8066 - val_loss: 3247.2109\n",
            "Epoch 3122/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.2834 - val_loss: 3245.5352\n",
            "Epoch 3123/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.6775 - val_loss: 3248.6348\n",
            "Epoch 3124/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.6072 - val_loss: 3254.5298\n",
            "Epoch 3125/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2452.0613 - val_loss: 3249.0972\n",
            "Epoch 3126/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2450.8682 - val_loss: 3248.6875\n",
            "Epoch 3127/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.2993 - val_loss: 3242.2971\n",
            "Epoch 3128/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.5662 - val_loss: 3240.7354\n",
            "Epoch 3129/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.3157 - val_loss: 3238.8977\n",
            "Epoch 3130/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2452.0359 - val_loss: 3236.0344\n",
            "Epoch 3131/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.3127 - val_loss: 3236.1907\n",
            "Epoch 3132/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.0784 - val_loss: 3239.0208\n",
            "Epoch 3133/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.0698 - val_loss: 3244.7283\n",
            "Epoch 3134/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2451.3789 - val_loss: 3250.6218\n",
            "Epoch 3135/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2451.6792 - val_loss: 3247.3579\n",
            "Epoch 3136/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.1321 - val_loss: 3246.2205\n",
            "Epoch 3137/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.1641 - val_loss: 3245.3066\n",
            "Epoch 3138/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2451.2549 - val_loss: 3246.3123\n",
            "Epoch 3139/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.3882 - val_loss: 3245.5112\n",
            "Epoch 3140/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2450.6885 - val_loss: 3240.1760\n",
            "Epoch 3141/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2451.6262 - val_loss: 3238.7358\n",
            "Epoch 3142/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.5583 - val_loss: 3244.4424\n",
            "Epoch 3143/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.6987 - val_loss: 3251.4597\n",
            "Epoch 3144/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2449.3562 - val_loss: 3250.8877\n",
            "Epoch 3145/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.2515 - val_loss: 3248.0747\n",
            "Epoch 3146/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2449.0085 - val_loss: 3246.3013\n",
            "Epoch 3147/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2449.0391 - val_loss: 3242.9346\n",
            "Epoch 3148/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.2087 - val_loss: 3242.7202\n",
            "Epoch 3149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.7683 - val_loss: 3242.9404\n",
            "Epoch 3150/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.6382 - val_loss: 3243.0059\n",
            "Epoch 3151/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.1772 - val_loss: 3243.7429\n",
            "Epoch 3152/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.0886 - val_loss: 3244.9133\n",
            "Epoch 3153/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2449.1946 - val_loss: 3243.7444\n",
            "Epoch 3154/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.9277 - val_loss: 3242.8379\n",
            "Epoch 3155/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.2585 - val_loss: 3242.7234\n",
            "Epoch 3156/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.7307 - val_loss: 3245.8652\n",
            "Epoch 3157/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.2534 - val_loss: 3246.1909\n",
            "Epoch 3158/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2448.4143 - val_loss: 3246.3848\n",
            "Epoch 3159/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.6086 - val_loss: 3246.9878\n",
            "Epoch 3160/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.9514 - val_loss: 3248.9143\n",
            "Epoch 3161/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.2695 - val_loss: 3250.0042\n",
            "Epoch 3162/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.4841 - val_loss: 3250.8254\n",
            "Epoch 3163/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2451.3174 - val_loss: 3252.9158\n",
            "Epoch 3164/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.1919 - val_loss: 3243.6545\n",
            "Epoch 3165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2451.7175 - val_loss: 3239.4890\n",
            "Epoch 3166/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.3818 - val_loss: 3239.7009\n",
            "Epoch 3167/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2451.4751 - val_loss: 3241.1790\n",
            "Epoch 3168/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.3218 - val_loss: 3243.8459\n",
            "Epoch 3169/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.1514 - val_loss: 3248.9341\n",
            "Epoch 3170/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.6284 - val_loss: 3251.6116\n",
            "Epoch 3171/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.0400 - val_loss: 3253.7646\n",
            "Epoch 3172/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2448.5996 - val_loss: 3252.1721\n",
            "Epoch 3173/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.6794 - val_loss: 3253.0259\n",
            "Epoch 3174/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.2419 - val_loss: 3245.6604\n",
            "Epoch 3175/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.7515 - val_loss: 3246.2205\n",
            "Epoch 3176/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.4436 - val_loss: 3242.9702\n",
            "Epoch 3177/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2448.4521 - val_loss: 3242.3953\n",
            "Epoch 3178/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.9932 - val_loss: 3242.8647\n",
            "Epoch 3179/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.3621 - val_loss: 3243.4360\n",
            "Epoch 3180/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.1257 - val_loss: 3243.3083\n",
            "Epoch 3181/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.2754 - val_loss: 3241.2051\n",
            "Epoch 3182/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2449.1147 - val_loss: 3241.4753\n",
            "Epoch 3183/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.9688 - val_loss: 3242.0464\n",
            "Epoch 3184/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.0212 - val_loss: 3241.0283\n",
            "Epoch 3185/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.3044 - val_loss: 3240.7859\n",
            "Epoch 3186/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.8096 - val_loss: 3241.5640\n",
            "Epoch 3187/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2450.2568 - val_loss: 3243.1760\n",
            "Epoch 3188/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.5825 - val_loss: 3242.7683\n",
            "Epoch 3189/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.1965 - val_loss: 3242.9978\n",
            "Epoch 3190/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.1594 - val_loss: 3242.1289\n",
            "Epoch 3191/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.8127 - val_loss: 3241.7805\n",
            "Epoch 3192/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2448.0823 - val_loss: 3240.5144\n",
            "Epoch 3193/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.7056 - val_loss: 3241.5635\n",
            "Epoch 3194/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.3657 - val_loss: 3244.9814\n",
            "Epoch 3195/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.6125 - val_loss: 3245.7869\n",
            "Epoch 3196/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.5754 - val_loss: 3245.6748\n",
            "Epoch 3197/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2446.7954 - val_loss: 3246.4509\n",
            "Epoch 3198/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.6567 - val_loss: 3246.2964\n",
            "Epoch 3199/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2446.8140 - val_loss: 3245.9280\n",
            "Epoch 3200/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.0215 - val_loss: 3246.7058\n",
            "Epoch 3201/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.3547 - val_loss: 3247.5383\n",
            "Epoch 3202/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2447.4812 - val_loss: 3247.4084\n",
            "Epoch 3203/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.9736 - val_loss: 3246.4414\n",
            "Epoch 3204/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.8882 - val_loss: 3246.6216\n",
            "Epoch 3205/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.7798 - val_loss: 3247.5625\n",
            "Epoch 3206/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2447.5300 - val_loss: 3246.7686\n",
            "Epoch 3207/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.6802 - val_loss: 3247.6956\n",
            "Epoch 3208/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.4241 - val_loss: 3249.2908\n",
            "Epoch 3209/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.9304 - val_loss: 3252.9360\n",
            "Epoch 3210/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2446.7607 - val_loss: 3252.4993\n",
            "Epoch 3211/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2448.3433 - val_loss: 3247.5916\n",
            "Epoch 3212/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2446.8225 - val_loss: 3250.0952\n",
            "Epoch 3213/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.7090 - val_loss: 3250.8040\n",
            "Epoch 3214/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.3271 - val_loss: 3250.8757\n",
            "Epoch 3215/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.0996 - val_loss: 3250.2419\n",
            "Epoch 3216/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.4146 - val_loss: 3248.4148\n",
            "Epoch 3217/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2447.9578 - val_loss: 3250.4841\n",
            "Epoch 3218/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.5713 - val_loss: 3250.6121\n",
            "Epoch 3219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.3870 - val_loss: 3245.0393\n",
            "Epoch 3220/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.7708 - val_loss: 3244.1995\n",
            "Epoch 3221/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.0027 - val_loss: 3244.0898\n",
            "Epoch 3222/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.7227 - val_loss: 3243.7234\n",
            "Epoch 3223/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.0740 - val_loss: 3244.5459\n",
            "Epoch 3224/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.4834 - val_loss: 3241.8259\n",
            "Epoch 3225/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2447.6084 - val_loss: 3243.6917\n",
            "Epoch 3226/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.1758 - val_loss: 3243.4045\n",
            "Epoch 3227/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.5085 - val_loss: 3245.3975\n",
            "Epoch 3228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.8843 - val_loss: 3244.0891\n",
            "Epoch 3229/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.8628 - val_loss: 3244.0723\n",
            "Epoch 3230/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.9294 - val_loss: 3243.9207\n",
            "Epoch 3231/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.9316 - val_loss: 3243.8870\n",
            "Epoch 3232/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2445.9255 - val_loss: 3245.9517\n",
            "Epoch 3233/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.2048 - val_loss: 3245.1653\n",
            "Epoch 3234/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2446.3535 - val_loss: 3243.2627\n",
            "Epoch 3235/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.1299 - val_loss: 3245.4585\n",
            "Epoch 3236/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2445.7864 - val_loss: 3243.9006\n",
            "Epoch 3237/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.5989 - val_loss: 3242.6626\n",
            "Epoch 3238/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.5544 - val_loss: 3240.6516\n",
            "Epoch 3239/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.7231 - val_loss: 3243.7852\n",
            "Epoch 3240/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2444.9583 - val_loss: 3240.6677\n",
            "Epoch 3241/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.8181 - val_loss: 3239.3342\n",
            "Epoch 3242/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.7310 - val_loss: 3238.9966\n",
            "Epoch 3243/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.3879 - val_loss: 3238.5176\n",
            "Epoch 3244/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.3064 - val_loss: 3237.3752\n",
            "Epoch 3245/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2447.5283 - val_loss: 3238.6643\n",
            "Epoch 3246/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2447.0325 - val_loss: 3238.9104\n",
            "Epoch 3247/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2446.6199 - val_loss: 3240.4309\n",
            "Epoch 3248/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.5359 - val_loss: 3242.0579\n",
            "Epoch 3249/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.0964 - val_loss: 3242.2964\n",
            "Epoch 3250/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.4668 - val_loss: 3240.7910\n",
            "Epoch 3251/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.2654 - val_loss: 3248.1621\n",
            "Epoch 3252/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2444.7791 - val_loss: 3255.8293\n",
            "Epoch 3253/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2450.7600 - val_loss: 3262.8181\n",
            "Epoch 3254/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2450.7112 - val_loss: 3259.8948\n",
            "Epoch 3255/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.4836 - val_loss: 3256.9006\n",
            "Epoch 3256/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.0408 - val_loss: 3255.2109\n",
            "Epoch 3257/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.5308 - val_loss: 3253.1221\n",
            "Epoch 3258/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.5503 - val_loss: 3253.0371\n",
            "Epoch 3259/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.0862 - val_loss: 3247.0449\n",
            "Epoch 3260/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.8188 - val_loss: 3250.8110\n",
            "Epoch 3261/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.9424 - val_loss: 3253.7380\n",
            "Epoch 3262/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2444.8821 - val_loss: 3251.7559\n",
            "Epoch 3263/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.9861 - val_loss: 3252.7983\n",
            "Epoch 3264/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.4631 - val_loss: 3258.0706\n",
            "Epoch 3265/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.7805 - val_loss: 3262.4456\n",
            "Epoch 3266/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.1533 - val_loss: 3268.5242\n",
            "Epoch 3267/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.6201 - val_loss: 3261.4929\n",
            "Epoch 3268/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2447.8667 - val_loss: 3256.0320\n",
            "Epoch 3269/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2445.9597 - val_loss: 3254.5344\n",
            "Epoch 3270/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.7612 - val_loss: 3249.9133\n",
            "Epoch 3271/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.1519 - val_loss: 3247.5366\n",
            "Epoch 3272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.8130 - val_loss: 3245.1812\n",
            "Epoch 3273/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.3481 - val_loss: 3244.5891\n",
            "Epoch 3274/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.7065 - val_loss: 3246.5393\n",
            "Epoch 3275/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.8367 - val_loss: 3250.6677\n",
            "Epoch 3276/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.7400 - val_loss: 3245.0618\n",
            "Epoch 3277/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2444.4719 - val_loss: 3244.9375\n",
            "Epoch 3278/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.4192 - val_loss: 3251.4958\n",
            "Epoch 3279/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.6519 - val_loss: 3257.3992\n",
            "Epoch 3280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.5713 - val_loss: 3264.4080\n",
            "Epoch 3281/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.2495 - val_loss: 3261.2197\n",
            "Epoch 3282/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.1348 - val_loss: 3265.2815\n",
            "Epoch 3283/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2449.9368 - val_loss: 3265.7859\n",
            "Epoch 3284/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2449.4668 - val_loss: 3263.4934\n",
            "Epoch 3285/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2448.0847 - val_loss: 3260.3435\n",
            "Epoch 3286/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.2180 - val_loss: 3258.5642\n",
            "Epoch 3287/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.0037 - val_loss: 3254.7524\n",
            "Epoch 3288/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2445.6338 - val_loss: 3251.0623\n",
            "Epoch 3289/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.4043 - val_loss: 3251.0186\n",
            "Epoch 3290/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.3032 - val_loss: 3249.7310\n",
            "Epoch 3291/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.1782 - val_loss: 3250.9038\n",
            "Epoch 3292/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.2759 - val_loss: 3251.7732\n",
            "Epoch 3293/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2445.0454 - val_loss: 3256.8604\n",
            "Epoch 3294/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.6328 - val_loss: 3252.4841\n",
            "Epoch 3295/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.9170 - val_loss: 3250.9053\n",
            "Epoch 3296/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2444.1272 - val_loss: 3248.7859\n",
            "Epoch 3297/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.0881 - val_loss: 3246.5479\n",
            "Epoch 3298/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2443.2180 - val_loss: 3245.5898\n",
            "Epoch 3299/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.4629 - val_loss: 3245.7957\n",
            "Epoch 3300/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.6223 - val_loss: 3245.3943\n",
            "Epoch 3301/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2446.2380 - val_loss: 3247.8701\n",
            "Epoch 3302/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.1433 - val_loss: 3247.5779\n",
            "Epoch 3303/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.3748 - val_loss: 3246.4746\n",
            "Epoch 3304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.0789 - val_loss: 3247.7329\n",
            "Epoch 3305/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.0071 - val_loss: 3252.0359\n",
            "Epoch 3306/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.9189 - val_loss: 3254.7651\n",
            "Epoch 3307/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2446.0674 - val_loss: 3249.9998\n",
            "Epoch 3308/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.6409 - val_loss: 3249.0747\n",
            "Epoch 3309/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2447.9448 - val_loss: 3255.1660\n",
            "Epoch 3310/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.5339 - val_loss: 3254.6545\n",
            "Epoch 3311/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.4412 - val_loss: 3260.1973\n",
            "Epoch 3312/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.3994 - val_loss: 3260.6484\n",
            "Epoch 3313/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.8276 - val_loss: 3260.1130\n",
            "Epoch 3314/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2448.3198 - val_loss: 3258.2979\n",
            "Epoch 3315/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2447.6282 - val_loss: 3253.1008\n",
            "Epoch 3316/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.7600 - val_loss: 3254.6047\n",
            "Epoch 3317/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2444.2625 - val_loss: 3252.2844\n",
            "Epoch 3318/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2443.3960 - val_loss: 3249.0176\n",
            "Epoch 3319/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.7717 - val_loss: 3247.6763\n",
            "Epoch 3320/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2446.3748 - val_loss: 3246.5759\n",
            "Epoch 3321/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2444.1226 - val_loss: 3246.2188\n",
            "Epoch 3322/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2443.4363 - val_loss: 3246.4797\n",
            "Epoch 3323/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.6086 - val_loss: 3243.2415\n",
            "Epoch 3324/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.1575 - val_loss: 3243.8918\n",
            "Epoch 3325/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2443.4546 - val_loss: 3242.5386\n",
            "Epoch 3326/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.1123 - val_loss: 3242.3423\n",
            "Epoch 3327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2443.4255 - val_loss: 3242.3296\n",
            "Epoch 3328/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.8201 - val_loss: 3246.1973\n",
            "Epoch 3329/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2442.5447 - val_loss: 3247.0667\n",
            "Epoch 3330/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.6091 - val_loss: 3248.4265\n",
            "Epoch 3331/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.5234 - val_loss: 3250.0310\n",
            "Epoch 3332/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2442.0764 - val_loss: 3248.7205\n",
            "Epoch 3333/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2443.4404 - val_loss: 3246.3093\n",
            "Epoch 3334/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2442.2070 - val_loss: 3248.5801\n",
            "Epoch 3335/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.7852 - val_loss: 3249.7041\n",
            "Epoch 3336/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.8708 - val_loss: 3250.3279\n",
            "Epoch 3337/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.4272 - val_loss: 3255.9265\n",
            "Epoch 3338/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.1248 - val_loss: 3253.9009\n",
            "Epoch 3339/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2443.9602 - val_loss: 3252.0647\n",
            "Epoch 3340/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2443.5496 - val_loss: 3251.1553\n",
            "Epoch 3341/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2443.3308 - val_loss: 3258.1440\n",
            "Epoch 3342/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.8628 - val_loss: 3268.4536\n",
            "Epoch 3343/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2452.0493 - val_loss: 3272.8071\n",
            "Epoch 3344/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2450.9326 - val_loss: 3263.7051\n",
            "Epoch 3345/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2448.3328 - val_loss: 3258.4399\n",
            "Epoch 3346/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2444.9468 - val_loss: 3257.2866\n",
            "Epoch 3347/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.6514 - val_loss: 3260.9839\n",
            "Epoch 3348/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2446.3047 - val_loss: 3261.8259\n",
            "Epoch 3349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.9805 - val_loss: 3265.7922\n",
            "Epoch 3350/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.7188 - val_loss: 3269.2859\n",
            "Epoch 3351/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2450.9592 - val_loss: 3263.4353\n",
            "Epoch 3352/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.9604 - val_loss: 3258.9746\n",
            "Epoch 3353/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.9939 - val_loss: 3257.2563\n",
            "Epoch 3354/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2441.1475 - val_loss: 3249.4712\n",
            "Epoch 3355/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2442.2729 - val_loss: 3247.0190\n",
            "Epoch 3356/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.5854 - val_loss: 3246.6204\n",
            "Epoch 3357/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.5923 - val_loss: 3248.1365\n",
            "Epoch 3358/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2443.1943 - val_loss: 3253.1355\n",
            "Epoch 3359/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2443.9746 - val_loss: 3254.1299\n",
            "Epoch 3360/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.1370 - val_loss: 3252.5085\n",
            "Epoch 3361/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.1614 - val_loss: 3245.6648\n",
            "Epoch 3362/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.0264 - val_loss: 3243.5103\n",
            "Epoch 3363/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2439.9209 - val_loss: 3238.9456\n",
            "Epoch 3364/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.0229 - val_loss: 3238.5022\n",
            "Epoch 3365/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.4954 - val_loss: 3242.6797\n",
            "Epoch 3366/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2443.2922 - val_loss: 3248.0610\n",
            "Epoch 3367/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2445.5305 - val_loss: 3256.9810\n",
            "Epoch 3368/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2448.6631 - val_loss: 3261.6018\n",
            "Epoch 3369/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2451.3257 - val_loss: 3262.3708\n",
            "Epoch 3370/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2448.8787 - val_loss: 3256.3003\n",
            "Epoch 3371/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2447.9119 - val_loss: 3243.7783\n",
            "Epoch 3372/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.7644 - val_loss: 3238.8552\n",
            "Epoch 3373/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2440.7317 - val_loss: 3240.5884\n",
            "Epoch 3374/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.8411 - val_loss: 3238.3135\n",
            "Epoch 3375/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.4365 - val_loss: 3237.1169\n",
            "Epoch 3376/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.8000 - val_loss: 3235.3879\n",
            "Epoch 3377/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2440.7949 - val_loss: 3236.8750\n",
            "Epoch 3378/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2439.4141 - val_loss: 3241.7773\n",
            "Epoch 3379/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.0957 - val_loss: 3244.1648\n",
            "Epoch 3380/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.3813 - val_loss: 3244.3494\n",
            "Epoch 3381/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.2388 - val_loss: 3243.1096\n",
            "Epoch 3382/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.4539 - val_loss: 3241.6191\n",
            "Epoch 3383/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2440.9153 - val_loss: 3241.9326\n",
            "Epoch 3384/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2440.1416 - val_loss: 3242.4065\n",
            "Epoch 3385/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.9841 - val_loss: 3239.7871\n",
            "Epoch 3386/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.1743 - val_loss: 3235.3982\n",
            "Epoch 3387/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2440.8547 - val_loss: 3235.6191\n",
            "Epoch 3388/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2440.5737 - val_loss: 3235.2783\n",
            "Epoch 3389/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.7473 - val_loss: 3235.9761\n",
            "Epoch 3390/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.0784 - val_loss: 3241.5891\n",
            "Epoch 3391/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2439.7913 - val_loss: 3245.1172\n",
            "Epoch 3392/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.0396 - val_loss: 3239.1924\n",
            "Epoch 3393/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.2932 - val_loss: 3237.8174\n",
            "Epoch 3394/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.8521 - val_loss: 3235.4397\n",
            "Epoch 3395/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2440.9905 - val_loss: 3237.4690\n",
            "Epoch 3396/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.2971 - val_loss: 3237.6467\n",
            "Epoch 3397/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.7122 - val_loss: 3239.8423\n",
            "Epoch 3398/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.3845 - val_loss: 3243.6987\n",
            "Epoch 3399/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2441.9834 - val_loss: 3240.1165\n",
            "Epoch 3400/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2445.2859 - val_loss: 3247.9878\n",
            "Epoch 3401/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.2583 - val_loss: 3248.3381\n",
            "Epoch 3402/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2444.6536 - val_loss: 3250.4104\n",
            "Epoch 3403/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2444.2368 - val_loss: 3248.1211\n",
            "Epoch 3404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.9819 - val_loss: 3248.0730\n",
            "Epoch 3405/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2442.5925 - val_loss: 3247.1016\n",
            "Epoch 3406/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2441.7019 - val_loss: 3244.6082\n",
            "Epoch 3407/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.6885 - val_loss: 3242.6050\n",
            "Epoch 3408/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2440.4377 - val_loss: 3240.8101\n",
            "Epoch 3409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2440.0779 - val_loss: 3241.0688\n",
            "Epoch 3410/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2439.3872 - val_loss: 3240.1658\n",
            "Epoch 3411/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.7583 - val_loss: 3238.6553\n",
            "Epoch 3412/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.3848 - val_loss: 3234.8804\n",
            "Epoch 3413/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.3921 - val_loss: 3235.2871\n",
            "Epoch 3414/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2439.5042 - val_loss: 3236.3230\n",
            "Epoch 3415/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.2676 - val_loss: 3236.8040\n",
            "Epoch 3416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.1704 - val_loss: 3237.1987\n",
            "Epoch 3417/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2439.4871 - val_loss: 3237.7773\n",
            "Epoch 3418/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2440.3242 - val_loss: 3246.0906\n",
            "Epoch 3419/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2440.9404 - val_loss: 3246.2468\n",
            "Epoch 3420/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2440.1299 - val_loss: 3245.6096\n",
            "Epoch 3421/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.6982 - val_loss: 3244.9807\n",
            "Epoch 3422/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2441.8022 - val_loss: 3248.8933\n",
            "Epoch 3423/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.4675 - val_loss: 3248.5544\n",
            "Epoch 3424/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.2148 - val_loss: 3246.0303\n",
            "Epoch 3425/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.1262 - val_loss: 3244.2815\n",
            "Epoch 3426/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2439.4033 - val_loss: 3241.9673\n",
            "Epoch 3427/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2439.0962 - val_loss: 3239.6970\n",
            "Epoch 3428/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2437.8340 - val_loss: 3236.5566\n",
            "Epoch 3429/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.7781 - val_loss: 3235.8694\n",
            "Epoch 3430/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2438.5129 - val_loss: 3235.5527\n",
            "Epoch 3431/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.2573 - val_loss: 3235.8296\n",
            "Epoch 3432/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.6199 - val_loss: 3235.4294\n",
            "Epoch 3433/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.9927 - val_loss: 3238.6643\n",
            "Epoch 3434/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2437.7820 - val_loss: 3238.1533\n",
            "Epoch 3435/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2437.9580 - val_loss: 3238.1533\n",
            "Epoch 3436/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2437.6541 - val_loss: 3238.1433\n",
            "Epoch 3437/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.3682 - val_loss: 3241.6987\n",
            "Epoch 3438/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2437.3123 - val_loss: 3245.7837\n",
            "Epoch 3439/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.0662 - val_loss: 3242.2805\n",
            "Epoch 3440/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.5852 - val_loss: 3239.8054\n",
            "Epoch 3441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.2134 - val_loss: 3238.7122\n",
            "Epoch 3442/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2438.2664 - val_loss: 3234.9998\n",
            "Epoch 3443/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.6118 - val_loss: 3234.7788\n",
            "Epoch 3444/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2440.6895 - val_loss: 3235.2131\n",
            "Epoch 3445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.7881 - val_loss: 3235.5879\n",
            "Epoch 3446/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2438.9343 - val_loss: 3236.8215\n",
            "Epoch 3447/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.0635 - val_loss: 3239.1201\n",
            "Epoch 3448/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.2212 - val_loss: 3241.2900\n",
            "Epoch 3449/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2437.8884 - val_loss: 3243.3267\n",
            "Epoch 3450/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2437.9824 - val_loss: 3246.2412\n",
            "Epoch 3451/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.7498 - val_loss: 3246.9912\n",
            "Epoch 3452/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.2966 - val_loss: 3245.8196\n",
            "Epoch 3453/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.5835 - val_loss: 3245.2458\n",
            "Epoch 3454/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2437.8838 - val_loss: 3242.6990\n",
            "Epoch 3455/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.8245 - val_loss: 3237.4316\n",
            "Epoch 3456/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.9968 - val_loss: 3236.2573\n",
            "Epoch 3457/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.4141 - val_loss: 3232.2532\n",
            "Epoch 3458/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2441.5454 - val_loss: 3229.9246\n",
            "Epoch 3459/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2442.0015 - val_loss: 3230.1221\n",
            "Epoch 3460/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2441.3596 - val_loss: 3229.9849\n",
            "Epoch 3461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.9600 - val_loss: 3230.3701\n",
            "Epoch 3462/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2441.0959 - val_loss: 3231.1548\n",
            "Epoch 3463/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.9597 - val_loss: 3232.4597\n",
            "Epoch 3464/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.7781 - val_loss: 3231.8823\n",
            "Epoch 3465/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2440.5378 - val_loss: 3233.3589\n",
            "Epoch 3466/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2437.5564 - val_loss: 3234.1990\n",
            "Epoch 3467/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.4717 - val_loss: 3232.5530\n",
            "Epoch 3468/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.7549 - val_loss: 3233.6409\n",
            "Epoch 3469/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2437.1499 - val_loss: 3234.3604\n",
            "Epoch 3470/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2436.7253 - val_loss: 3237.9502\n",
            "Epoch 3471/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.4719 - val_loss: 3235.0654\n",
            "Epoch 3472/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.6030 - val_loss: 3234.4839\n",
            "Epoch 3473/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2437.2400 - val_loss: 3235.3567\n",
            "Epoch 3474/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2436.5564 - val_loss: 3236.3540\n",
            "Epoch 3475/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.1609 - val_loss: 3238.7773\n",
            "Epoch 3476/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.0813 - val_loss: 3239.8342\n",
            "Epoch 3477/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.7825 - val_loss: 3239.9373\n",
            "Epoch 3478/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.4731 - val_loss: 3243.8965\n",
            "Epoch 3479/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.9414 - val_loss: 3247.9622\n",
            "Epoch 3480/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.8188 - val_loss: 3248.6753\n",
            "Epoch 3481/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.1897 - val_loss: 3254.7371\n",
            "Epoch 3482/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2439.3303 - val_loss: 3250.7373\n",
            "Epoch 3483/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.5159 - val_loss: 3251.8101\n",
            "Epoch 3484/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2438.4719 - val_loss: 3251.0024\n",
            "Epoch 3485/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.4980 - val_loss: 3246.9158\n",
            "Epoch 3486/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2439.7502 - val_loss: 3248.7373\n",
            "Epoch 3487/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.3301 - val_loss: 3245.0103\n",
            "Epoch 3488/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2437.4141 - val_loss: 3246.8303\n",
            "Epoch 3489/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2436.2363 - val_loss: 3243.6707\n",
            "Epoch 3490/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.7483 - val_loss: 3241.4653\n",
            "Epoch 3491/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2435.8218 - val_loss: 3233.7197\n",
            "Epoch 3492/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.8176 - val_loss: 3231.3296\n",
            "Epoch 3493/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.1704 - val_loss: 3228.2893\n",
            "Epoch 3494/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2438.2161 - val_loss: 3229.1980\n",
            "Epoch 3495/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.3232 - val_loss: 3229.1675\n",
            "Epoch 3496/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.4502 - val_loss: 3228.8445\n",
            "Epoch 3497/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.1699 - val_loss: 3229.6621\n",
            "Epoch 3498/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.2856 - val_loss: 3233.0217\n",
            "Epoch 3499/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.5796 - val_loss: 3233.5874\n",
            "Epoch 3500/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.3672 - val_loss: 3233.1433\n",
            "Epoch 3501/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2437.3572 - val_loss: 3233.4592\n",
            "Epoch 3502/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2438.4746 - val_loss: 3234.0432\n",
            "Epoch 3503/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.8979 - val_loss: 3237.1191\n",
            "Epoch 3504/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.2234 - val_loss: 3236.9424\n",
            "Epoch 3505/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.1709 - val_loss: 3236.3608\n",
            "Epoch 3506/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2435.5461 - val_loss: 3236.7644\n",
            "Epoch 3507/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.3853 - val_loss: 3238.3423\n",
            "Epoch 3508/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.3792 - val_loss: 3241.1682\n",
            "Epoch 3509/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2435.1804 - val_loss: 3245.8677\n",
            "Epoch 3510/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2435.4507 - val_loss: 3244.3262\n",
            "Epoch 3511/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2435.4626 - val_loss: 3246.0515\n",
            "Epoch 3512/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.9480 - val_loss: 3247.0303\n",
            "Epoch 3513/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.7419 - val_loss: 3244.1204\n",
            "Epoch 3514/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.7673 - val_loss: 3244.6008\n",
            "Epoch 3515/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.8953 - val_loss: 3244.3936\n",
            "Epoch 3516/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2434.4526 - val_loss: 3242.1997\n",
            "Epoch 3517/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2434.1248 - val_loss: 3240.4966\n",
            "Epoch 3518/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2435.7959 - val_loss: 3238.1777\n",
            "Epoch 3519/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.3591 - val_loss: 3236.9204\n",
            "Epoch 3520/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.1917 - val_loss: 3235.6228\n",
            "Epoch 3521/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.4397 - val_loss: 3235.5095\n",
            "Epoch 3522/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2440.6816 - val_loss: 3232.4768\n",
            "Epoch 3523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2441.1016 - val_loss: 3232.4705\n",
            "Epoch 3524/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2437.6816 - val_loss: 3235.5642\n",
            "Epoch 3525/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2434.1968 - val_loss: 3240.9353\n",
            "Epoch 3526/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.9834 - val_loss: 3243.7471\n",
            "Epoch 3527/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.9673 - val_loss: 3244.3918\n",
            "Epoch 3528/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.7214 - val_loss: 3245.4685\n",
            "Epoch 3529/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.6851 - val_loss: 3246.4023\n",
            "Epoch 3530/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.8354 - val_loss: 3245.3198\n",
            "Epoch 3531/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.0303 - val_loss: 3241.9028\n",
            "Epoch 3532/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.6196 - val_loss: 3242.4456\n",
            "Epoch 3533/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2433.6633 - val_loss: 3241.8552\n",
            "Epoch 3534/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.6270 - val_loss: 3241.9453\n",
            "Epoch 3535/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.7434 - val_loss: 3246.8179\n",
            "Epoch 3536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.7493 - val_loss: 3251.4878\n",
            "Epoch 3537/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.6890 - val_loss: 3252.6819\n",
            "Epoch 3538/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.1421 - val_loss: 3253.6362\n",
            "Epoch 3539/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.5298 - val_loss: 3253.8552\n",
            "Epoch 3540/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2436.0232 - val_loss: 3252.7742\n",
            "Epoch 3541/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.5898 - val_loss: 3251.4102\n",
            "Epoch 3542/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2435.4658 - val_loss: 3251.7009\n",
            "Epoch 3543/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.8760 - val_loss: 3251.3438\n",
            "Epoch 3544/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2435.5281 - val_loss: 3250.2549\n",
            "Epoch 3545/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.5405 - val_loss: 3251.6594\n",
            "Epoch 3546/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.7969 - val_loss: 3249.2749\n",
            "Epoch 3547/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.8882 - val_loss: 3250.2075\n",
            "Epoch 3548/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2434.1431 - val_loss: 3250.5691\n",
            "Epoch 3549/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.2380 - val_loss: 3250.0491\n",
            "Epoch 3550/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.9534 - val_loss: 3244.0400\n",
            "Epoch 3551/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.4170 - val_loss: 3241.3518\n",
            "Epoch 3552/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.9419 - val_loss: 3238.6545\n",
            "Epoch 3553/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.0376 - val_loss: 3239.4705\n",
            "Epoch 3554/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.3777 - val_loss: 3240.3579\n",
            "Epoch 3555/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.8052 - val_loss: 3242.7478\n",
            "Epoch 3556/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.4441 - val_loss: 3246.0410\n",
            "Epoch 3557/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.3000 - val_loss: 3246.9121\n",
            "Epoch 3558/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.6851 - val_loss: 3242.5627\n",
            "Epoch 3559/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2432.9250 - val_loss: 3241.3584\n",
            "Epoch 3560/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.9114 - val_loss: 3246.2002\n",
            "Epoch 3561/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.5645 - val_loss: 3253.4277\n",
            "Epoch 3562/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2436.0977 - val_loss: 3255.9690\n",
            "Epoch 3563/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.9583 - val_loss: 3252.7556\n",
            "Epoch 3564/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2434.5720 - val_loss: 3253.3428\n",
            "Epoch 3565/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.6667 - val_loss: 3254.5710\n",
            "Epoch 3566/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.8708 - val_loss: 3247.4136\n",
            "Epoch 3567/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2431.6260 - val_loss: 3245.5654\n",
            "Epoch 3568/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.3132 - val_loss: 3246.6284\n",
            "Epoch 3569/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.7402 - val_loss: 3251.4360\n",
            "Epoch 3570/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.3884 - val_loss: 3247.1841\n",
            "Epoch 3571/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2434.6506 - val_loss: 3247.5190\n",
            "Epoch 3572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.0083 - val_loss: 3246.8054\n",
            "Epoch 3573/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.7329 - val_loss: 3246.6023\n",
            "Epoch 3574/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.8391 - val_loss: 3243.7915\n",
            "Epoch 3575/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2439.6038 - val_loss: 3243.8132\n",
            "Epoch 3576/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.7109 - val_loss: 3244.7297\n",
            "Epoch 3577/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2438.8123 - val_loss: 3242.6064\n",
            "Epoch 3578/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2439.0081 - val_loss: 3242.2461\n",
            "Epoch 3579/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2438.3003 - val_loss: 3241.3440\n",
            "Epoch 3580/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2436.7673 - val_loss: 3242.8943\n",
            "Epoch 3581/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.9529 - val_loss: 3243.1257\n",
            "Epoch 3582/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.2961 - val_loss: 3246.3560\n",
            "Epoch 3583/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.6401 - val_loss: 3250.9502\n",
            "Epoch 3584/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.0293 - val_loss: 3251.7666\n",
            "Epoch 3585/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2433.6987 - val_loss: 3253.5632\n",
            "Epoch 3586/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.2446 - val_loss: 3253.2922\n",
            "Epoch 3587/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.5740 - val_loss: 3252.0603\n",
            "Epoch 3588/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2433.9673 - val_loss: 3252.4438\n",
            "Epoch 3589/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.6794 - val_loss: 3251.5786\n",
            "Epoch 3590/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.7639 - val_loss: 3251.7822\n",
            "Epoch 3591/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2433.8760 - val_loss: 3251.6523\n",
            "Epoch 3592/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.8237 - val_loss: 3246.3779\n",
            "Epoch 3593/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2431.8230 - val_loss: 3243.5491\n",
            "Epoch 3594/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2434.9927 - val_loss: 3244.4829\n",
            "Epoch 3595/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.5623 - val_loss: 3246.4910\n",
            "Epoch 3596/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.5256 - val_loss: 3249.4414\n",
            "Epoch 3597/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.1121 - val_loss: 3248.9768\n",
            "Epoch 3598/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2432.8660 - val_loss: 3249.1965\n",
            "Epoch 3599/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.4893 - val_loss: 3248.2979\n",
            "Epoch 3600/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.7715 - val_loss: 3249.1050\n",
            "Epoch 3601/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.3914 - val_loss: 3249.8870\n",
            "Epoch 3602/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2433.6543 - val_loss: 3253.3975\n",
            "Epoch 3603/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2432.3794 - val_loss: 3253.7605\n",
            "Epoch 3604/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2431.3984 - val_loss: 3254.8010\n",
            "Epoch 3605/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2431.1238 - val_loss: 3256.0142\n",
            "Epoch 3606/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2431.1382 - val_loss: 3256.2317\n",
            "Epoch 3607/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2431.1873 - val_loss: 3257.6123\n",
            "Epoch 3608/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.0823 - val_loss: 3261.9326\n",
            "Epoch 3609/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2431.1616 - val_loss: 3260.5361\n",
            "Epoch 3610/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2431.0750 - val_loss: 3259.6360\n",
            "Epoch 3611/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.1482 - val_loss: 3257.5684\n",
            "Epoch 3612/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2430.3721 - val_loss: 3260.8071\n",
            "Epoch 3613/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.7188 - val_loss: 3269.2141\n",
            "Epoch 3614/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.5449 - val_loss: 3268.6541\n",
            "Epoch 3615/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2432.1504 - val_loss: 3266.2478\n",
            "Epoch 3616/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2431.8828 - val_loss: 3263.6011\n",
            "Epoch 3617/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2431.0564 - val_loss: 3263.6370\n",
            "Epoch 3618/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2430.4121 - val_loss: 3266.0471\n",
            "Epoch 3619/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2430.7942 - val_loss: 3261.1218\n",
            "Epoch 3620/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2431.6775 - val_loss: 3262.8909\n",
            "Epoch 3621/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.6028 - val_loss: 3267.5513\n",
            "Epoch 3622/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2432.5815 - val_loss: 3260.3721\n",
            "Epoch 3623/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.8823 - val_loss: 3257.1699\n",
            "Epoch 3624/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.6472 - val_loss: 3251.5786\n",
            "Epoch 3625/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.2451 - val_loss: 3250.7478\n",
            "Epoch 3626/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2431.7085 - val_loss: 3252.3684\n",
            "Epoch 3627/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2429.8889 - val_loss: 3251.9810\n",
            "Epoch 3628/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2432.0137 - val_loss: 3255.5818\n",
            "Epoch 3629/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.8743 - val_loss: 3255.8469\n",
            "Epoch 3630/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2430.5930 - val_loss: 3252.6594\n",
            "Epoch 3631/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.0840 - val_loss: 3252.9446\n",
            "Epoch 3632/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.0110 - val_loss: 3254.1033\n",
            "Epoch 3633/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2429.3098 - val_loss: 3255.2422\n",
            "Epoch 3634/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2429.2349 - val_loss: 3255.9912\n",
            "Epoch 3635/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.1519 - val_loss: 3254.9424\n",
            "Epoch 3636/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.3018 - val_loss: 3255.2131\n",
            "Epoch 3637/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.2695 - val_loss: 3257.3816\n",
            "Epoch 3638/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2429.1067 - val_loss: 3256.3608\n",
            "Epoch 3639/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2428.7966 - val_loss: 3255.5054\n",
            "Epoch 3640/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.5017 - val_loss: 3253.4309\n",
            "Epoch 3641/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2430.0442 - val_loss: 3255.1577\n",
            "Epoch 3642/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2428.9326 - val_loss: 3256.4976\n",
            "Epoch 3643/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.0803 - val_loss: 3255.3413\n",
            "Epoch 3644/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.4695 - val_loss: 3254.5544\n",
            "Epoch 3645/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.0588 - val_loss: 3251.2610\n",
            "Epoch 3646/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.3232 - val_loss: 3250.3467\n",
            "Epoch 3647/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.3943 - val_loss: 3251.7910\n",
            "Epoch 3648/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.1384 - val_loss: 3252.1770\n",
            "Epoch 3649/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2427.6414 - val_loss: 3251.2556\n",
            "Epoch 3650/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.7410 - val_loss: 3253.6819\n",
            "Epoch 3651/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.0964 - val_loss: 3252.8438\n",
            "Epoch 3652/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.8733 - val_loss: 3256.0903\n",
            "Epoch 3653/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.4680 - val_loss: 3256.8118\n",
            "Epoch 3654/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.4109 - val_loss: 3257.1138\n",
            "Epoch 3655/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.6816 - val_loss: 3257.0610\n",
            "Epoch 3656/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2428.9004 - val_loss: 3257.4622\n",
            "Epoch 3657/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.3894 - val_loss: 3255.3020\n",
            "Epoch 3658/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.3945 - val_loss: 3251.2410\n",
            "Epoch 3659/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.0874 - val_loss: 3251.4885\n",
            "Epoch 3660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.9294 - val_loss: 3254.3252\n",
            "Epoch 3661/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.2463 - val_loss: 3257.1240\n",
            "Epoch 3662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.5493 - val_loss: 3253.9304\n",
            "Epoch 3663/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2431.8164 - val_loss: 3247.5898\n",
            "Epoch 3664/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.4785 - val_loss: 3246.0081\n",
            "Epoch 3665/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2431.4333 - val_loss: 3242.6729\n",
            "Epoch 3666/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2432.9092 - val_loss: 3242.2908\n",
            "Epoch 3667/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2434.5056 - val_loss: 3242.6011\n",
            "Epoch 3668/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2434.6006 - val_loss: 3243.1846\n",
            "Epoch 3669/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2432.5757 - val_loss: 3243.7180\n",
            "Epoch 3670/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2435.5076 - val_loss: 3243.0098\n",
            "Epoch 3671/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2435.3210 - val_loss: 3241.2524\n",
            "Epoch 3672/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2440.2427 - val_loss: 3241.6853\n",
            "Epoch 3673/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2439.1089 - val_loss: 3239.6145\n",
            "Epoch 3674/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2433.4849 - val_loss: 3239.0979\n",
            "Epoch 3675/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2434.7705 - val_loss: 3242.6907\n",
            "Epoch 3676/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2429.1494 - val_loss: 3245.9761\n",
            "Epoch 3677/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2429.0493 - val_loss: 3244.6211\n",
            "Epoch 3678/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2427.9685 - val_loss: 3244.0398\n",
            "Epoch 3679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.3645 - val_loss: 3248.1917\n",
            "Epoch 3680/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.4431 - val_loss: 3246.7297\n",
            "Epoch 3681/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.5183 - val_loss: 3244.3357\n",
            "Epoch 3682/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.0813 - val_loss: 3243.9465\n",
            "Epoch 3683/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.6589 - val_loss: 3241.5715\n",
            "Epoch 3684/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2428.6106 - val_loss: 3240.3220\n",
            "Epoch 3685/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.3828 - val_loss: 3243.1436\n",
            "Epoch 3686/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.3601 - val_loss: 3247.0090\n",
            "Epoch 3687/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.6792 - val_loss: 3245.6370\n",
            "Epoch 3688/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.8547 - val_loss: 3243.1970\n",
            "Epoch 3689/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2427.2864 - val_loss: 3242.3792\n",
            "Epoch 3690/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.0222 - val_loss: 3246.2998\n",
            "Epoch 3691/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2428.1982 - val_loss: 3246.4895\n",
            "Epoch 3692/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.8191 - val_loss: 3249.5801\n",
            "Epoch 3693/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.8906 - val_loss: 3252.5081\n",
            "Epoch 3694/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2433.4990 - val_loss: 3253.5969\n",
            "Epoch 3695/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.9629 - val_loss: 3249.5337\n",
            "Epoch 3696/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.9548 - val_loss: 3247.6611\n",
            "Epoch 3697/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.3638 - val_loss: 3246.7078\n",
            "Epoch 3698/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2427.9255 - val_loss: 3244.3870\n",
            "Epoch 3699/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.1892 - val_loss: 3242.6960\n",
            "Epoch 3700/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2427.4163 - val_loss: 3244.4622\n",
            "Epoch 3701/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2430.6265 - val_loss: 3241.8311\n",
            "Epoch 3702/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2427.8875 - val_loss: 3241.5007\n",
            "Epoch 3703/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.6306 - val_loss: 3243.6172\n",
            "Epoch 3704/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2427.1162 - val_loss: 3244.2515\n",
            "Epoch 3705/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2427.6458 - val_loss: 3245.0562\n",
            "Epoch 3706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.5989 - val_loss: 3240.3364\n",
            "Epoch 3707/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.3777 - val_loss: 3239.5857\n",
            "Epoch 3708/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.7463 - val_loss: 3241.4150\n",
            "Epoch 3709/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2427.2478 - val_loss: 3243.1335\n",
            "Epoch 3710/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.1807 - val_loss: 3245.8203\n",
            "Epoch 3711/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.9470 - val_loss: 3245.5569\n",
            "Epoch 3712/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2424.5200 - val_loss: 3243.5193\n",
            "Epoch 3713/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2426.2561 - val_loss: 3246.0449\n",
            "Epoch 3714/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2426.1758 - val_loss: 3246.4871\n",
            "Epoch 3715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.6794 - val_loss: 3245.4607\n",
            "Epoch 3716/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.7349 - val_loss: 3241.9072\n",
            "Epoch 3717/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2427.4377 - val_loss: 3240.7461\n",
            "Epoch 3718/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2427.5559 - val_loss: 3242.1873\n",
            "Epoch 3719/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2427.0984 - val_loss: 3245.8696\n",
            "Epoch 3720/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.5759 - val_loss: 3244.4854\n",
            "Epoch 3721/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2427.0984 - val_loss: 3244.0247\n",
            "Epoch 3722/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.9092 - val_loss: 3242.1516\n",
            "Epoch 3723/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.0708 - val_loss: 3241.1497\n",
            "Epoch 3724/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2432.5798 - val_loss: 3241.0007\n",
            "Epoch 3725/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2431.6838 - val_loss: 3241.6057\n",
            "Epoch 3726/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2427.0569 - val_loss: 3244.5684\n",
            "Epoch 3727/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.3770 - val_loss: 3250.8574\n",
            "Epoch 3728/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.6118 - val_loss: 3259.4575\n",
            "Epoch 3729/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2427.9355 - val_loss: 3264.8301\n",
            "Epoch 3730/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.9221 - val_loss: 3269.3462\n",
            "Epoch 3731/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2430.4185 - val_loss: 3267.5024\n",
            "Epoch 3732/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.6814 - val_loss: 3261.6355\n",
            "Epoch 3733/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2427.4722 - val_loss: 3259.0105\n",
            "Epoch 3734/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.1934 - val_loss: 3259.4634\n",
            "Epoch 3735/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.0386 - val_loss: 3259.4160\n",
            "Epoch 3736/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.8879 - val_loss: 3256.4902\n",
            "Epoch 3737/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.1057 - val_loss: 3255.7585\n",
            "Epoch 3738/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.8657 - val_loss: 3256.3630\n",
            "Epoch 3739/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.1543 - val_loss: 3260.0667\n",
            "Epoch 3740/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.8223 - val_loss: 3261.5022\n",
            "Epoch 3741/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.9048 - val_loss: 3260.0977\n",
            "Epoch 3742/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.5334 - val_loss: 3256.4346\n",
            "Epoch 3743/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2425.0481 - val_loss: 3255.1868\n",
            "Epoch 3744/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.2659 - val_loss: 3256.1963\n",
            "Epoch 3745/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.1250 - val_loss: 3263.8511\n",
            "Epoch 3746/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.8843 - val_loss: 3267.9597\n",
            "Epoch 3747/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2426.3762 - val_loss: 3261.1833\n",
            "Epoch 3748/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.4419 - val_loss: 3255.7019\n",
            "Epoch 3749/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.5107 - val_loss: 3254.5081\n",
            "Epoch 3750/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.6030 - val_loss: 3254.3467\n",
            "Epoch 3751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.4424 - val_loss: 3256.3115\n",
            "Epoch 3752/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.8889 - val_loss: 3255.6597\n",
            "Epoch 3753/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.9275 - val_loss: 3254.1868\n",
            "Epoch 3754/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2426.4609 - val_loss: 3254.5081\n",
            "Epoch 3755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.7180 - val_loss: 3255.2358\n",
            "Epoch 3756/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.9160 - val_loss: 3253.1875\n",
            "Epoch 3757/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2428.6453 - val_loss: 3259.5767\n",
            "Epoch 3758/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2424.6716 - val_loss: 3259.0889\n",
            "Epoch 3759/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.8655 - val_loss: 3258.1489\n",
            "Epoch 3760/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.8486 - val_loss: 3264.2866\n",
            "Epoch 3761/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2425.3367 - val_loss: 3266.3103\n",
            "Epoch 3762/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2425.3206 - val_loss: 3265.4521\n",
            "Epoch 3763/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.4158 - val_loss: 3262.8022\n",
            "Epoch 3764/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.4155 - val_loss: 3261.0596\n",
            "Epoch 3765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.3037 - val_loss: 3261.4241\n",
            "Epoch 3766/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.8914 - val_loss: 3267.9919\n",
            "Epoch 3767/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2428.8721 - val_loss: 3272.6421\n",
            "Epoch 3768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.4272 - val_loss: 3273.9375\n",
            "Epoch 3769/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.2432 - val_loss: 3271.6218\n",
            "Epoch 3770/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.5242 - val_loss: 3266.5610\n",
            "Epoch 3771/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2424.7136 - val_loss: 3265.1833\n",
            "Epoch 3772/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.8159 - val_loss: 3267.2241\n",
            "Epoch 3773/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.3525 - val_loss: 3266.7886\n",
            "Epoch 3774/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2425.8228 - val_loss: 3267.8679\n",
            "Epoch 3775/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2426.5403 - val_loss: 3267.4858\n",
            "Epoch 3776/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2424.0303 - val_loss: 3257.7546\n",
            "Epoch 3777/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2423.2007 - val_loss: 3252.1216\n",
            "Epoch 3778/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.5547 - val_loss: 3251.7573\n",
            "Epoch 3779/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.7285 - val_loss: 3253.1001\n",
            "Epoch 3780/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2423.0000 - val_loss: 3248.3757\n",
            "Epoch 3781/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.3982 - val_loss: 3246.6633\n",
            "Epoch 3782/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.2412 - val_loss: 3245.0154\n",
            "Epoch 3783/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.5322 - val_loss: 3245.0674\n",
            "Epoch 3784/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.3569 - val_loss: 3245.7437\n",
            "Epoch 3785/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2423.1375 - val_loss: 3244.0298\n",
            "Epoch 3786/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2424.4004 - val_loss: 3244.6882\n",
            "Epoch 3787/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2423.7852 - val_loss: 3244.5527\n",
            "Epoch 3788/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.5581 - val_loss: 3245.0691\n",
            "Epoch 3789/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.2729 - val_loss: 3243.3384\n",
            "Epoch 3790/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.1292 - val_loss: 3241.0884\n",
            "Epoch 3791/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.4414 - val_loss: 3240.0947\n",
            "Epoch 3792/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2426.3582 - val_loss: 3240.5366\n",
            "Epoch 3793/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2425.1919 - val_loss: 3241.3647\n",
            "Epoch 3794/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.0498 - val_loss: 3242.6292\n",
            "Epoch 3795/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.5249 - val_loss: 3242.9136\n",
            "Epoch 3796/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.2515 - val_loss: 3244.3391\n",
            "Epoch 3797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.1733 - val_loss: 3251.3958\n",
            "Epoch 3798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.8396 - val_loss: 3258.8477\n",
            "Epoch 3799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.6338 - val_loss: 3261.2083\n",
            "Epoch 3800/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2427.2708 - val_loss: 3263.3879\n",
            "Epoch 3801/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2426.0505 - val_loss: 3260.5715\n",
            "Epoch 3802/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2424.6394 - val_loss: 3257.5352\n",
            "Epoch 3803/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.7832 - val_loss: 3255.1797\n",
            "Epoch 3804/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.3477 - val_loss: 3248.1067\n",
            "Epoch 3805/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.9365 - val_loss: 3246.1641\n",
            "Epoch 3806/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2422.5562 - val_loss: 3248.8728\n",
            "Epoch 3807/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.9045 - val_loss: 3247.8318\n",
            "Epoch 3808/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.2053 - val_loss: 3242.3096\n",
            "Epoch 3809/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.8425 - val_loss: 3240.8711\n",
            "Epoch 3810/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.9412 - val_loss: 3241.1011\n",
            "Epoch 3811/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.9736 - val_loss: 3240.7188\n",
            "Epoch 3812/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.3953 - val_loss: 3242.8599\n",
            "Epoch 3813/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2423.2104 - val_loss: 3243.8311\n",
            "Epoch 3814/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.3438 - val_loss: 3246.4690\n",
            "Epoch 3815/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.2786 - val_loss: 3244.6802\n",
            "Epoch 3816/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.6406 - val_loss: 3242.5625\n",
            "Epoch 3817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.7161 - val_loss: 3241.0691\n",
            "Epoch 3818/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.6887 - val_loss: 3240.4553\n",
            "Epoch 3819/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2423.4597 - val_loss: 3242.7717\n",
            "Epoch 3820/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2424.2600 - val_loss: 3250.8584\n",
            "Epoch 3821/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2427.5881 - val_loss: 3258.4910\n",
            "Epoch 3822/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2427.3594 - val_loss: 3258.0540\n",
            "Epoch 3823/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.4744 - val_loss: 3256.6516\n",
            "Epoch 3824/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.2073 - val_loss: 3253.4023\n",
            "Epoch 3825/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.5227 - val_loss: 3248.6929\n",
            "Epoch 3826/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2422.8000 - val_loss: 3246.8977\n",
            "Epoch 3827/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.9146 - val_loss: 3246.4246\n",
            "Epoch 3828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.8477 - val_loss: 3244.9128\n",
            "Epoch 3829/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.6731 - val_loss: 3241.1509\n",
            "Epoch 3830/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.9211 - val_loss: 3240.7703\n",
            "Epoch 3831/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.1035 - val_loss: 3243.3638\n",
            "Epoch 3832/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2422.2571 - val_loss: 3240.3174\n",
            "Epoch 3833/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2422.1746 - val_loss: 3239.9529\n",
            "Epoch 3834/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2422.7449 - val_loss: 3239.4009\n",
            "Epoch 3835/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.3306 - val_loss: 3239.7842\n",
            "Epoch 3836/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.2942 - val_loss: 3239.4690\n",
            "Epoch 3837/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.3662 - val_loss: 3236.3613\n",
            "Epoch 3838/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2422.9338 - val_loss: 3237.7883\n",
            "Epoch 3839/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.5325 - val_loss: 3237.9578\n",
            "Epoch 3840/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2423.0552 - val_loss: 3236.1365\n",
            "Epoch 3841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.7734 - val_loss: 3237.3196\n",
            "Epoch 3842/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.1133 - val_loss: 3237.5166\n",
            "Epoch 3843/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.5603 - val_loss: 3238.1338\n",
            "Epoch 3844/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.3953 - val_loss: 3238.1580\n",
            "Epoch 3845/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2422.1047 - val_loss: 3237.6790\n",
            "Epoch 3846/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.7168 - val_loss: 3237.8154\n",
            "Epoch 3847/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2423.3679 - val_loss: 3234.8428\n",
            "Epoch 3848/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.7593 - val_loss: 3233.8997\n",
            "Epoch 3849/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2429.0095 - val_loss: 3235.3013\n",
            "Epoch 3850/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2429.3457 - val_loss: 3235.2310\n",
            "Epoch 3851/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.2966 - val_loss: 3235.5422\n",
            "Epoch 3852/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2424.9707 - val_loss: 3235.2036\n",
            "Epoch 3853/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.1589 - val_loss: 3234.8909\n",
            "Epoch 3854/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2424.8521 - val_loss: 3234.6091\n",
            "Epoch 3855/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.9453 - val_loss: 3236.4502\n",
            "Epoch 3856/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.6155 - val_loss: 3240.9998\n",
            "Epoch 3857/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.6748 - val_loss: 3241.9438\n",
            "Epoch 3858/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2422.4578 - val_loss: 3243.1482\n",
            "Epoch 3859/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.2996 - val_loss: 3242.7061\n",
            "Epoch 3860/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.5537 - val_loss: 3245.2021\n",
            "Epoch 3861/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2422.9482 - val_loss: 3247.5923\n",
            "Epoch 3862/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.6021 - val_loss: 3246.4297\n",
            "Epoch 3863/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.2629 - val_loss: 3241.0562\n",
            "Epoch 3864/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2421.7485 - val_loss: 3238.2078\n",
            "Epoch 3865/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.1687 - val_loss: 3237.1787\n",
            "Epoch 3866/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.4590 - val_loss: 3237.1243\n",
            "Epoch 3867/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.7461 - val_loss: 3233.7524\n",
            "Epoch 3868/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.3438 - val_loss: 3234.0962\n",
            "Epoch 3869/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.7966 - val_loss: 3234.6650\n",
            "Epoch 3870/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2422.2061 - val_loss: 3235.1123\n",
            "Epoch 3871/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2421.8677 - val_loss: 3235.9309\n",
            "Epoch 3872/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.0308 - val_loss: 3236.2397\n",
            "Epoch 3873/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.9519 - val_loss: 3237.4680\n",
            "Epoch 3874/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2422.3599 - val_loss: 3239.9229\n",
            "Epoch 3875/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.7122 - val_loss: 3243.9961\n",
            "Epoch 3876/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.8127 - val_loss: 3242.7222\n",
            "Epoch 3877/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2422.1040 - val_loss: 3238.7373\n",
            "Epoch 3878/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.6660 - val_loss: 3238.6616\n",
            "Epoch 3879/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.1821 - val_loss: 3239.5767\n",
            "Epoch 3880/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.4353 - val_loss: 3241.6841\n",
            "Epoch 3881/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.5215 - val_loss: 3243.9368\n",
            "Epoch 3882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.6462 - val_loss: 3249.1746\n",
            "Epoch 3883/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2421.9888 - val_loss: 3248.6458\n",
            "Epoch 3884/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2422.1208 - val_loss: 3249.9919\n",
            "Epoch 3885/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.3411 - val_loss: 3249.7471\n",
            "Epoch 3886/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.9050 - val_loss: 3248.2356\n",
            "Epoch 3887/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.3794 - val_loss: 3245.4429\n",
            "Epoch 3888/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2420.6792 - val_loss: 3244.5190\n",
            "Epoch 3889/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.8374 - val_loss: 3243.9922\n",
            "Epoch 3890/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.0273 - val_loss: 3243.3743\n",
            "Epoch 3891/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2420.6736 - val_loss: 3242.8396\n",
            "Epoch 3892/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.0530 - val_loss: 3246.2388\n",
            "Epoch 3893/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.3237 - val_loss: 3252.5366\n",
            "Epoch 3894/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.0481 - val_loss: 3254.8911\n",
            "Epoch 3895/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.3875 - val_loss: 3257.8245\n",
            "Epoch 3896/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.0891 - val_loss: 3265.7773\n",
            "Epoch 3897/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.4802 - val_loss: 3261.6987\n",
            "Epoch 3898/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2425.2893 - val_loss: 3257.8789\n",
            "Epoch 3899/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2421.2439 - val_loss: 3250.4141\n",
            "Epoch 3900/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.7773 - val_loss: 3247.3252\n",
            "Epoch 3901/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.3245 - val_loss: 3244.1611\n",
            "Epoch 3902/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.9358 - val_loss: 3247.2053\n",
            "Epoch 3903/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.0576 - val_loss: 3248.5271\n",
            "Epoch 3904/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.3625 - val_loss: 3251.3936\n",
            "Epoch 3905/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2421.9565 - val_loss: 3254.3999\n",
            "Epoch 3906/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.1599 - val_loss: 3249.3630\n",
            "Epoch 3907/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.9609 - val_loss: 3248.1509\n",
            "Epoch 3908/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.0322 - val_loss: 3249.6875\n",
            "Epoch 3909/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.7256 - val_loss: 3250.6394\n",
            "Epoch 3910/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.4805 - val_loss: 3250.6790\n",
            "Epoch 3911/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.0525 - val_loss: 3249.9641\n",
            "Epoch 3912/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2419.4910 - val_loss: 3254.4751\n",
            "Epoch 3913/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.5010 - val_loss: 3261.2197\n",
            "Epoch 3914/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.9612 - val_loss: 3255.9998\n",
            "Epoch 3915/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.3281 - val_loss: 3252.1313\n",
            "Epoch 3916/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.9475 - val_loss: 3250.8806\n",
            "Epoch 3917/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.5303 - val_loss: 3251.3367\n",
            "Epoch 3918/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.2786 - val_loss: 3252.5544\n",
            "Epoch 3919/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2418.7598 - val_loss: 3253.9790\n",
            "Epoch 3920/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.5095 - val_loss: 3251.8494\n",
            "Epoch 3921/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.7236 - val_loss: 3250.6577\n",
            "Epoch 3922/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.6440 - val_loss: 3250.9172\n",
            "Epoch 3923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.0974 - val_loss: 3253.9502\n",
            "Epoch 3924/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2418.1531 - val_loss: 3257.3271\n",
            "Epoch 3925/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2419.7661 - val_loss: 3257.0859\n",
            "Epoch 3926/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2421.2290 - val_loss: 3253.9309\n",
            "Epoch 3927/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.9624 - val_loss: 3255.0210\n",
            "Epoch 3928/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.6133 - val_loss: 3254.7642\n",
            "Epoch 3929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.0984 - val_loss: 3253.9434\n",
            "Epoch 3930/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.0615 - val_loss: 3248.7549\n",
            "Epoch 3931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.4636 - val_loss: 3248.0095\n",
            "Epoch 3932/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2419.7961 - val_loss: 3248.8540\n",
            "Epoch 3933/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2419.8105 - val_loss: 3250.1067\n",
            "Epoch 3934/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.8115 - val_loss: 3255.5366\n",
            "Epoch 3935/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2419.3225 - val_loss: 3257.6067\n",
            "Epoch 3936/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.5996 - val_loss: 3261.3953\n",
            "Epoch 3937/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.5715 - val_loss: 3265.9343\n",
            "Epoch 3938/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.0525 - val_loss: 3265.9006\n",
            "Epoch 3939/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2424.0332 - val_loss: 3269.0491\n",
            "Epoch 3940/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2427.7917 - val_loss: 3275.3792\n",
            "Epoch 3941/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2429.8740 - val_loss: 3274.0857\n",
            "Epoch 3942/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.0784 - val_loss: 3270.2461\n",
            "Epoch 3943/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2425.4456 - val_loss: 3265.1826\n",
            "Epoch 3944/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2423.9763 - val_loss: 3263.4480\n",
            "Epoch 3945/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2423.6519 - val_loss: 3259.4880\n",
            "Epoch 3946/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.2825 - val_loss: 3262.0664\n",
            "Epoch 3947/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2425.6455 - val_loss: 3266.5647\n",
            "Epoch 3948/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.2239 - val_loss: 3262.5266\n",
            "Epoch 3949/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2423.6238 - val_loss: 3262.0779\n",
            "Epoch 3950/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.4624 - val_loss: 3260.1152\n",
            "Epoch 3951/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.1587 - val_loss: 3260.2891\n",
            "Epoch 3952/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2422.6194 - val_loss: 3263.3796\n",
            "Epoch 3953/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2428.2075 - val_loss: 3268.4600\n",
            "Epoch 3954/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2426.4854 - val_loss: 3266.5850\n",
            "Epoch 3955/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2422.6521 - val_loss: 3261.0740\n",
            "Epoch 3956/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.4604 - val_loss: 3258.3408\n",
            "Epoch 3957/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.6602 - val_loss: 3258.0032\n",
            "Epoch 3958/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.3901 - val_loss: 3258.7395\n",
            "Epoch 3959/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.1155 - val_loss: 3257.2349\n",
            "Epoch 3960/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.9731 - val_loss: 3256.6313\n",
            "Epoch 3961/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2420.4971 - val_loss: 3255.4717\n",
            "Epoch 3962/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2419.8010 - val_loss: 3256.8596\n",
            "Epoch 3963/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.2400 - val_loss: 3258.3743\n",
            "Epoch 3964/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.1982 - val_loss: 3256.5273\n",
            "Epoch 3965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.3677 - val_loss: 3254.6392\n",
            "Epoch 3966/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.7642 - val_loss: 3259.7195\n",
            "Epoch 3967/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2423.5154 - val_loss: 3270.8079\n",
            "Epoch 3968/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2426.7751 - val_loss: 3272.1833\n",
            "Epoch 3969/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2426.8857 - val_loss: 3269.6917\n",
            "Epoch 3970/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2425.1672 - val_loss: 3266.1040\n",
            "Epoch 3971/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2422.3728 - val_loss: 3261.0059\n",
            "Epoch 3972/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.0166 - val_loss: 3257.3188\n",
            "Epoch 3973/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.4729 - val_loss: 3249.9546\n",
            "Epoch 3974/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2416.8796 - val_loss: 3247.7034\n",
            "Epoch 3975/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.7812 - val_loss: 3246.2983\n",
            "Epoch 3976/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.4834 - val_loss: 3246.0103\n",
            "Epoch 3977/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.1853 - val_loss: 3243.9631\n",
            "Epoch 3978/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.4004 - val_loss: 3244.1572\n",
            "Epoch 3979/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.9907 - val_loss: 3247.5203\n",
            "Epoch 3980/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.7961 - val_loss: 3248.6660\n",
            "Epoch 3981/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2418.5977 - val_loss: 3249.1216\n",
            "Epoch 3982/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2418.6257 - val_loss: 3251.6692\n",
            "Epoch 3983/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.9617 - val_loss: 3247.2661\n",
            "Epoch 3984/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.4958 - val_loss: 3246.9065\n",
            "Epoch 3985/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.3191 - val_loss: 3249.5872\n",
            "Epoch 3986/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2421.4739 - val_loss: 3254.5679\n",
            "Epoch 3987/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2420.5415 - val_loss: 3256.9504\n",
            "Epoch 3988/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2419.3130 - val_loss: 3249.6858\n",
            "Epoch 3989/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.1841 - val_loss: 3246.0864\n",
            "Epoch 3990/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2417.9148 - val_loss: 3246.0828\n",
            "Epoch 3991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.1008 - val_loss: 3242.5479\n",
            "Epoch 3992/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2417.6973 - val_loss: 3242.5979\n",
            "Epoch 3993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.4680 - val_loss: 3243.3235\n",
            "Epoch 3994/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.3621 - val_loss: 3242.7900\n",
            "Epoch 3995/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2418.4688 - val_loss: 3247.0654\n",
            "Epoch 3996/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.2407 - val_loss: 3248.3132\n",
            "Epoch 3997/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2417.6580 - val_loss: 3248.2419\n",
            "Epoch 3998/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2417.1365 - val_loss: 3247.8071\n",
            "Epoch 3999/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.6987 - val_loss: 3247.8157\n",
            "Epoch 4000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.0237 - val_loss: 3244.1677\n",
            "Epoch 4001/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.1365 - val_loss: 3246.1897\n",
            "Epoch 4002/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2416.9785 - val_loss: 3246.3862\n",
            "Epoch 4003/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2418.3594 - val_loss: 3246.1169\n",
            "Epoch 4004/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.2368 - val_loss: 3247.4543\n",
            "Epoch 4005/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.2786 - val_loss: 3246.9790\n",
            "Epoch 4006/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2419.3284 - val_loss: 3245.9648\n",
            "Epoch 4007/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.7861 - val_loss: 3245.9072\n",
            "Epoch 4008/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.3403 - val_loss: 3246.0955\n",
            "Epoch 4009/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2417.8894 - val_loss: 3247.8540\n",
            "Epoch 4010/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2416.2603 - val_loss: 3248.2429\n",
            "Epoch 4011/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.6006 - val_loss: 3245.7573\n",
            "Epoch 4012/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2418.2656 - val_loss: 3248.7766\n",
            "Epoch 4013/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.2986 - val_loss: 3248.2966\n",
            "Epoch 4014/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2416.6746 - val_loss: 3246.3384\n",
            "Epoch 4015/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2416.9680 - val_loss: 3245.9966\n",
            "Epoch 4016/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2416.7217 - val_loss: 3245.9375\n",
            "Epoch 4017/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.0623 - val_loss: 3247.8079\n",
            "Epoch 4018/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.8096 - val_loss: 3248.4229\n",
            "Epoch 4019/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.5559 - val_loss: 3247.5547\n",
            "Epoch 4020/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2416.7974 - val_loss: 3246.0488\n",
            "Epoch 4021/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2416.7170 - val_loss: 3245.2507\n",
            "Epoch 4022/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2416.9814 - val_loss: 3245.7766\n",
            "Epoch 4023/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.9011 - val_loss: 3247.6953\n",
            "Epoch 4024/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.7664 - val_loss: 3251.1633\n",
            "Epoch 4025/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.9255 - val_loss: 3251.0962\n",
            "Epoch 4026/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2416.2327 - val_loss: 3249.6843\n",
            "Epoch 4027/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2415.8079 - val_loss: 3249.2595\n",
            "Epoch 4028/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2420.1111 - val_loss: 3248.3357\n",
            "Epoch 4029/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.3960 - val_loss: 3249.4429\n",
            "Epoch 4030/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2418.0466 - val_loss: 3249.2170\n",
            "Epoch 4031/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2418.5447 - val_loss: 3250.8965\n",
            "Epoch 4032/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.2729 - val_loss: 3252.8147\n",
            "Epoch 4033/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.9463 - val_loss: 3253.4768\n",
            "Epoch 4034/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.6650 - val_loss: 3254.5784\n",
            "Epoch 4035/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.7161 - val_loss: 3255.4287\n",
            "Epoch 4036/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.9031 - val_loss: 3257.9417\n",
            "Epoch 4037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.8999 - val_loss: 3257.2395\n",
            "Epoch 4038/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.4968 - val_loss: 3255.8708\n",
            "Epoch 4039/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2415.6851 - val_loss: 3257.0891\n",
            "Epoch 4040/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.4243 - val_loss: 3255.7131\n",
            "Epoch 4041/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.3513 - val_loss: 3255.1194\n",
            "Epoch 4042/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.9226 - val_loss: 3255.6787\n",
            "Epoch 4043/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.6355 - val_loss: 3255.9558\n",
            "Epoch 4044/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.4980 - val_loss: 3256.2654\n",
            "Epoch 4045/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.1626 - val_loss: 3257.2959\n",
            "Epoch 4046/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.7739 - val_loss: 3257.0217\n",
            "Epoch 4047/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2412.7515 - val_loss: 3253.9841\n",
            "Epoch 4048/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.1226 - val_loss: 3253.5127\n",
            "Epoch 4049/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.5422 - val_loss: 3253.9810\n",
            "Epoch 4050/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2416.6924 - val_loss: 3259.5439\n",
            "Epoch 4051/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.3142 - val_loss: 3256.8599\n",
            "Epoch 4052/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.1938 - val_loss: 3257.2041\n",
            "Epoch 4053/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.9316 - val_loss: 3257.2065\n",
            "Epoch 4054/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2414.4126 - val_loss: 3256.6401\n",
            "Epoch 4055/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.1011 - val_loss: 3257.4753\n",
            "Epoch 4056/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.9680 - val_loss: 3256.3904\n",
            "Epoch 4057/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.0703 - val_loss: 3254.9873\n",
            "Epoch 4058/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.1570 - val_loss: 3253.3855\n",
            "Epoch 4059/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.4443 - val_loss: 3253.4663\n",
            "Epoch 4060/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.6206 - val_loss: 3254.5342\n",
            "Epoch 4061/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.7710 - val_loss: 3258.9470\n",
            "Epoch 4062/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.7529 - val_loss: 3261.7378\n",
            "Epoch 4063/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.0728 - val_loss: 3264.9246\n",
            "Epoch 4064/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.6475 - val_loss: 3264.8076\n",
            "Epoch 4065/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2417.8010 - val_loss: 3270.8293\n",
            "Epoch 4066/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2415.6028 - val_loss: 3269.5322\n",
            "Epoch 4067/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2415.2087 - val_loss: 3268.4465\n",
            "Epoch 4068/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.6658 - val_loss: 3266.4634\n",
            "Epoch 4069/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.9934 - val_loss: 3264.6860\n",
            "Epoch 4070/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.1531 - val_loss: 3264.6023\n",
            "Epoch 4071/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.0159 - val_loss: 3263.4915\n",
            "Epoch 4072/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.1782 - val_loss: 3262.8157\n",
            "Epoch 4073/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.1768 - val_loss: 3262.3413\n",
            "Epoch 4074/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2415.3054 - val_loss: 3266.8035\n",
            "Epoch 4075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.1462 - val_loss: 3267.9404\n",
            "Epoch 4076/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.6208 - val_loss: 3269.0320\n",
            "Epoch 4077/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.3992 - val_loss: 3274.5984\n",
            "Epoch 4078/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.9041 - val_loss: 3275.0347\n",
            "Epoch 4079/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2416.4834 - val_loss: 3277.5249\n",
            "Epoch 4080/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.3110 - val_loss: 3276.3743\n",
            "Epoch 4081/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.8826 - val_loss: 3275.5947\n",
            "Epoch 4082/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2416.1599 - val_loss: 3272.6946\n",
            "Epoch 4083/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.8672 - val_loss: 3270.1289\n",
            "Epoch 4084/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.5276 - val_loss: 3263.9978\n",
            "Epoch 4085/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.2822 - val_loss: 3261.8953\n",
            "Epoch 4086/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.0891 - val_loss: 3261.7297\n",
            "Epoch 4087/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2420.3599 - val_loss: 3259.9602\n",
            "Epoch 4088/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2422.8931 - val_loss: 3259.5825\n",
            "Epoch 4089/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.7441 - val_loss: 3259.4937\n",
            "Epoch 4090/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2417.6035 - val_loss: 3262.0811\n",
            "Epoch 4091/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.1145 - val_loss: 3262.1658\n",
            "Epoch 4092/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.4255 - val_loss: 3263.1851\n",
            "Epoch 4093/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.9590 - val_loss: 3264.2471\n",
            "Epoch 4094/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.8101 - val_loss: 3265.2290\n",
            "Epoch 4095/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2413.7930 - val_loss: 3264.6882\n",
            "Epoch 4096/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.3857 - val_loss: 3263.6140\n",
            "Epoch 4097/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.4009 - val_loss: 3263.2346\n",
            "Epoch 4098/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.7578 - val_loss: 3263.7595\n",
            "Epoch 4099/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.9243 - val_loss: 3263.5903\n",
            "Epoch 4100/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.9783 - val_loss: 3266.3413\n",
            "Epoch 4101/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2414.7771 - val_loss: 3268.7559\n",
            "Epoch 4102/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2415.0332 - val_loss: 3266.3806\n",
            "Epoch 4103/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2412.8655 - val_loss: 3264.4189\n",
            "Epoch 4104/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.9185 - val_loss: 3263.7410\n",
            "Epoch 4105/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.8308 - val_loss: 3265.8076\n",
            "Epoch 4106/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.3723 - val_loss: 3266.3071\n",
            "Epoch 4107/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.9944 - val_loss: 3265.4951\n",
            "Epoch 4108/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.5471 - val_loss: 3262.9297\n",
            "Epoch 4109/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.9680 - val_loss: 3262.8059\n",
            "Epoch 4110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.0242 - val_loss: 3264.9800\n",
            "Epoch 4111/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2413.4634 - val_loss: 3266.2134\n",
            "Epoch 4112/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2413.2549 - val_loss: 3267.0471\n",
            "Epoch 4113/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.5078 - val_loss: 3265.0281\n",
            "Epoch 4114/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.4746 - val_loss: 3266.6116\n",
            "Epoch 4115/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.1196 - val_loss: 3267.7522\n",
            "Epoch 4116/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2413.5156 - val_loss: 3268.8276\n",
            "Epoch 4117/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.0688 - val_loss: 3271.8245\n",
            "Epoch 4118/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.5889 - val_loss: 3271.0737\n",
            "Epoch 4119/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.3279 - val_loss: 3271.4404\n",
            "Epoch 4120/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2416.1460 - val_loss: 3269.2703\n",
            "Epoch 4121/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.7070 - val_loss: 3269.9912\n",
            "Epoch 4122/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.7632 - val_loss: 3272.9128\n",
            "Epoch 4123/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2412.9822 - val_loss: 3274.4680\n",
            "Epoch 4124/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.0647 - val_loss: 3276.5698\n",
            "Epoch 4125/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.2678 - val_loss: 3275.8005\n",
            "Epoch 4126/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.1240 - val_loss: 3273.7693\n",
            "Epoch 4127/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2412.6270 - val_loss: 3273.4150\n",
            "Epoch 4128/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2413.0491 - val_loss: 3274.3054\n",
            "Epoch 4129/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.6028 - val_loss: 3273.7852\n",
            "Epoch 4130/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.9163 - val_loss: 3274.7461\n",
            "Epoch 4131/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.0903 - val_loss: 3266.7983\n",
            "Epoch 4132/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.2095 - val_loss: 3269.6609\n",
            "Epoch 4133/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2412.8418 - val_loss: 3268.5098\n",
            "Epoch 4134/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.4495 - val_loss: 3268.5706\n",
            "Epoch 4135/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2414.6633 - val_loss: 3266.2363\n",
            "Epoch 4136/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2412.5269 - val_loss: 3268.6042\n",
            "Epoch 4137/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.7495 - val_loss: 3271.3005\n",
            "Epoch 4138/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.1145 - val_loss: 3271.3469\n",
            "Epoch 4139/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2412.0955 - val_loss: 3271.7747\n",
            "Epoch 4140/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2412.3386 - val_loss: 3273.4229\n",
            "Epoch 4141/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.2505 - val_loss: 3272.5608\n",
            "Epoch 4142/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.1060 - val_loss: 3273.2747\n",
            "Epoch 4143/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2412.1145 - val_loss: 3273.9417\n",
            "Epoch 4144/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.9966 - val_loss: 3273.4172\n",
            "Epoch 4145/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.5146 - val_loss: 3271.6306\n",
            "Epoch 4146/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2411.7092 - val_loss: 3271.6995\n",
            "Epoch 4147/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.7683 - val_loss: 3270.8196\n",
            "Epoch 4148/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.5552 - val_loss: 3268.7725\n",
            "Epoch 4149/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.7068 - val_loss: 3267.4546\n",
            "Epoch 4150/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.0820 - val_loss: 3270.0295\n",
            "Epoch 4151/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2411.6711 - val_loss: 3269.8059\n",
            "Epoch 4152/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2412.0474 - val_loss: 3270.2166\n",
            "Epoch 4153/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.5525 - val_loss: 3269.3574\n",
            "Epoch 4154/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.7444 - val_loss: 3267.8823\n",
            "Epoch 4155/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2412.0195 - val_loss: 3267.0842\n",
            "Epoch 4156/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2411.2095 - val_loss: 3265.6050\n",
            "Epoch 4157/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2411.9287 - val_loss: 3263.3752\n",
            "Epoch 4158/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.7537 - val_loss: 3260.8772\n",
            "Epoch 4159/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.9160 - val_loss: 3260.3035\n",
            "Epoch 4160/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2415.0081 - val_loss: 3260.0559\n",
            "Epoch 4161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.6545 - val_loss: 3261.9983\n",
            "Epoch 4162/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2412.3572 - val_loss: 3263.9631\n",
            "Epoch 4163/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.2026 - val_loss: 3260.7261\n",
            "Epoch 4164/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.8501 - val_loss: 3258.6201\n",
            "Epoch 4165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.5093 - val_loss: 3258.0576\n",
            "Epoch 4166/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2416.5171 - val_loss: 3254.2053\n",
            "Epoch 4167/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2416.6143 - val_loss: 3254.2534\n",
            "Epoch 4168/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.4548 - val_loss: 3254.3398\n",
            "Epoch 4169/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.5195 - val_loss: 3255.0596\n",
            "Epoch 4170/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2413.3872 - val_loss: 3257.8989\n",
            "Epoch 4171/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.4514 - val_loss: 3258.7869\n",
            "Epoch 4172/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.5596 - val_loss: 3259.6819\n",
            "Epoch 4173/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2411.8799 - val_loss: 3259.5232\n",
            "Epoch 4174/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.4888 - val_loss: 3260.5457\n",
            "Epoch 4175/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.5154 - val_loss: 3262.5088\n",
            "Epoch 4176/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2410.9568 - val_loss: 3262.2683\n",
            "Epoch 4177/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.9597 - val_loss: 3262.7241\n",
            "Epoch 4178/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2410.8994 - val_loss: 3262.6907\n",
            "Epoch 4179/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2410.7078 - val_loss: 3264.0408\n",
            "Epoch 4180/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2410.1733 - val_loss: 3263.7590\n",
            "Epoch 4181/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2411.7502 - val_loss: 3264.4614\n",
            "Epoch 4182/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2413.2510 - val_loss: 3269.4241\n",
            "Epoch 4183/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.6382 - val_loss: 3269.8198\n",
            "Epoch 4184/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.2202 - val_loss: 3268.7571\n",
            "Epoch 4185/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.9219 - val_loss: 3266.7402\n",
            "Epoch 4186/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.1511 - val_loss: 3266.1296\n",
            "Epoch 4187/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.9878 - val_loss: 3266.2844\n",
            "Epoch 4188/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2411.1294 - val_loss: 3268.6106\n",
            "Epoch 4189/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.5513 - val_loss: 3270.5786\n",
            "Epoch 4190/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.3342 - val_loss: 3272.9031\n",
            "Epoch 4191/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2410.9507 - val_loss: 3276.8757\n",
            "Epoch 4192/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2410.9038 - val_loss: 3282.6570\n",
            "Epoch 4193/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.8154 - val_loss: 3282.9192\n",
            "Epoch 4194/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.8518 - val_loss: 3282.5928\n",
            "Epoch 4195/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2413.6614 - val_loss: 3282.6299\n",
            "Epoch 4196/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.9868 - val_loss: 3287.3838\n",
            "Epoch 4197/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2417.2246 - val_loss: 3289.6057\n",
            "Epoch 4198/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2414.7566 - val_loss: 3282.7178\n",
            "Epoch 4199/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2412.9893 - val_loss: 3278.6248\n",
            "Epoch 4200/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.3159 - val_loss: 3274.1023\n",
            "Epoch 4201/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.5906 - val_loss: 3272.1123\n",
            "Epoch 4202/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.9004 - val_loss: 3272.6011\n",
            "Epoch 4203/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.3586 - val_loss: 3272.2173\n",
            "Epoch 4204/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.7678 - val_loss: 3272.9673\n",
            "Epoch 4205/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.9050 - val_loss: 3277.8132\n",
            "Epoch 4206/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.2324 - val_loss: 3286.1699\n",
            "Epoch 4207/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2415.5725 - val_loss: 3285.6973\n",
            "Epoch 4208/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2413.9236 - val_loss: 3282.1428\n",
            "Epoch 4209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.0481 - val_loss: 3271.8984\n",
            "Epoch 4210/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.2349 - val_loss: 3269.2117\n",
            "Epoch 4211/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2411.3591 - val_loss: 3266.5337\n",
            "Epoch 4212/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.9231 - val_loss: 3263.2573\n",
            "Epoch 4213/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.4155 - val_loss: 3264.9622\n",
            "Epoch 4214/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2409.7795 - val_loss: 3265.4126\n",
            "Epoch 4215/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2412.7148 - val_loss: 3269.1467\n",
            "Epoch 4216/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.6169 - val_loss: 3265.8789\n",
            "Epoch 4217/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.6499 - val_loss: 3263.3010\n",
            "Epoch 4218/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.7766 - val_loss: 3262.6187\n",
            "Epoch 4219/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2409.3142 - val_loss: 3263.1040\n",
            "Epoch 4220/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.0286 - val_loss: 3264.4910\n",
            "Epoch 4221/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.5718 - val_loss: 3268.1858\n",
            "Epoch 4222/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.0034 - val_loss: 3269.9102\n",
            "Epoch 4223/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2411.3755 - val_loss: 3270.0266\n",
            "Epoch 4224/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.3025 - val_loss: 3266.4094\n",
            "Epoch 4225/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.7690 - val_loss: 3261.9871\n",
            "Epoch 4226/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2410.3882 - val_loss: 3262.9817\n",
            "Epoch 4227/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.3340 - val_loss: 3264.2078\n",
            "Epoch 4228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.3225 - val_loss: 3271.0471\n",
            "Epoch 4229/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.3640 - val_loss: 3280.4709\n",
            "Epoch 4230/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2413.9617 - val_loss: 3277.9897\n",
            "Epoch 4231/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2413.3577 - val_loss: 3277.9192\n",
            "Epoch 4232/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.4783 - val_loss: 3278.3928\n",
            "Epoch 4233/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.0417 - val_loss: 3284.3040\n",
            "Epoch 4234/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2416.6851 - val_loss: 3284.6650\n",
            "Epoch 4235/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2416.4055 - val_loss: 3281.6741\n",
            "Epoch 4236/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2415.3164 - val_loss: 3282.1160\n",
            "Epoch 4237/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.6572 - val_loss: 3280.2959\n",
            "Epoch 4238/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2414.9006 - val_loss: 3280.6411\n",
            "Epoch 4239/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.2935 - val_loss: 3274.7114\n",
            "Epoch 4240/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.8198 - val_loss: 3274.7739\n",
            "Epoch 4241/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2413.0674 - val_loss: 3282.2354\n",
            "Epoch 4242/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2414.9387 - val_loss: 3282.3838\n",
            "Epoch 4243/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2414.9287 - val_loss: 3279.4993\n",
            "Epoch 4244/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2412.0159 - val_loss: 3271.8352\n",
            "Epoch 4245/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.0151 - val_loss: 3270.3179\n",
            "Epoch 4246/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2409.3262 - val_loss: 3269.0562\n",
            "Epoch 4247/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.3296 - val_loss: 3268.9529\n",
            "Epoch 4248/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.8887 - val_loss: 3274.0247\n",
            "Epoch 4249/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.4895 - val_loss: 3277.1328\n",
            "Epoch 4250/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.0007 - val_loss: 3280.8420\n",
            "Epoch 4251/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2411.7502 - val_loss: 3283.1484\n",
            "Epoch 4252/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.4634 - val_loss: 3286.6790\n",
            "Epoch 4253/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.3308 - val_loss: 3281.6956\n",
            "Epoch 4254/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2409.4932 - val_loss: 3277.2646\n",
            "Epoch 4255/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2408.2324 - val_loss: 3275.1196\n",
            "Epoch 4256/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.5728 - val_loss: 3273.0415\n",
            "Epoch 4257/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.1274 - val_loss: 3270.0920\n",
            "Epoch 4258/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2410.4668 - val_loss: 3269.0120\n",
            "Epoch 4259/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.5972 - val_loss: 3273.4565\n",
            "Epoch 4260/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.2900 - val_loss: 3274.5828\n",
            "Epoch 4261/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2409.1025 - val_loss: 3273.3501\n",
            "Epoch 4262/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.9146 - val_loss: 3272.6548\n",
            "Epoch 4263/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2417.7368 - val_loss: 3275.4951\n",
            "Epoch 4264/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2417.0525 - val_loss: 3276.4453\n",
            "Epoch 4265/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2415.1418 - val_loss: 3276.4260\n",
            "Epoch 4266/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.0164 - val_loss: 3278.3645\n",
            "Epoch 4267/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2412.7212 - val_loss: 3279.8779\n",
            "Epoch 4268/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.2329 - val_loss: 3279.9854\n",
            "Epoch 4269/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.1379 - val_loss: 3281.6802\n",
            "Epoch 4270/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.9666 - val_loss: 3282.0505\n",
            "Epoch 4271/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.8931 - val_loss: 3279.0386\n",
            "Epoch 4272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.2654 - val_loss: 3277.2754\n",
            "Epoch 4273/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2410.1025 - val_loss: 3273.7085\n",
            "Epoch 4274/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2411.4639 - val_loss: 3273.0615\n",
            "Epoch 4275/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.7312 - val_loss: 3272.9871\n",
            "Epoch 4276/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.8206 - val_loss: 3273.5154\n",
            "Epoch 4277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.6086 - val_loss: 3273.0183\n",
            "Epoch 4278/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.0857 - val_loss: 3273.7075\n",
            "Epoch 4279/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.3005 - val_loss: 3272.9109\n",
            "Epoch 4280/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2409.6448 - val_loss: 3273.8372\n",
            "Epoch 4281/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.5098 - val_loss: 3273.8716\n",
            "Epoch 4282/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.3147 - val_loss: 3277.1960\n",
            "Epoch 4283/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.2297 - val_loss: 3277.2061\n",
            "Epoch 4284/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.4880 - val_loss: 3271.8149\n",
            "Epoch 4285/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.3198 - val_loss: 3270.5176\n",
            "Epoch 4286/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2410.5635 - val_loss: 3270.4312\n",
            "Epoch 4287/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.1272 - val_loss: 3269.2676\n",
            "Epoch 4288/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2412.6318 - val_loss: 3268.5427\n",
            "Epoch 4289/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2411.4333 - val_loss: 3267.1377\n",
            "Epoch 4290/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.6987 - val_loss: 3267.4158\n",
            "Epoch 4291/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2413.1428 - val_loss: 3268.3496\n",
            "Epoch 4292/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2407.6167 - val_loss: 3272.1987\n",
            "Epoch 4293/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.9446 - val_loss: 3278.3440\n",
            "Epoch 4294/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.9521 - val_loss: 3278.7283\n",
            "Epoch 4295/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.4592 - val_loss: 3277.6465\n",
            "Epoch 4296/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.6353 - val_loss: 3280.9353\n",
            "Epoch 4297/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.2510 - val_loss: 3286.5427\n",
            "Epoch 4298/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2409.3882 - val_loss: 3283.8328\n",
            "Epoch 4299/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.9700 - val_loss: 3281.1345\n",
            "Epoch 4300/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.3203 - val_loss: 3281.4929\n",
            "Epoch 4301/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.3606 - val_loss: 3282.3552\n",
            "Epoch 4302/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.7251 - val_loss: 3282.3083\n",
            "Epoch 4303/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.0005 - val_loss: 3277.2678\n",
            "Epoch 4304/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2406.5596 - val_loss: 3273.9473\n",
            "Epoch 4305/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.0603 - val_loss: 3272.2854\n",
            "Epoch 4306/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.9890 - val_loss: 3271.4971\n",
            "Epoch 4307/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.9653 - val_loss: 3272.0427\n",
            "Epoch 4308/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.9990 - val_loss: 3272.5583\n",
            "Epoch 4309/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.3069 - val_loss: 3273.5154\n",
            "Epoch 4310/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.0537 - val_loss: 3271.0032\n",
            "Epoch 4311/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.4595 - val_loss: 3272.3501\n",
            "Epoch 4312/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2407.7090 - val_loss: 3272.6201\n",
            "Epoch 4313/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.0535 - val_loss: 3272.5527\n",
            "Epoch 4314/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.5203 - val_loss: 3274.8477\n",
            "Epoch 4315/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.2466 - val_loss: 3276.2556\n",
            "Epoch 4316/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2408.2832 - val_loss: 3278.3181\n",
            "Epoch 4317/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2406.7954 - val_loss: 3277.1836\n",
            "Epoch 4318/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.7715 - val_loss: 3277.0994\n",
            "Epoch 4319/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.6814 - val_loss: 3276.6172\n",
            "Epoch 4320/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2406.6182 - val_loss: 3277.5320\n",
            "Epoch 4321/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.3689 - val_loss: 3279.5759\n",
            "Epoch 4322/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2407.9446 - val_loss: 3287.1636\n",
            "Epoch 4323/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.6682 - val_loss: 3287.4358\n",
            "Epoch 4324/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2407.3174 - val_loss: 3281.4846\n",
            "Epoch 4325/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2411.0544 - val_loss: 3284.8574\n",
            "Epoch 4326/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.9463 - val_loss: 3284.9758\n",
            "Epoch 4327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.3167 - val_loss: 3286.8669\n",
            "Epoch 4328/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2409.2700 - val_loss: 3285.2686\n",
            "Epoch 4329/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.5269 - val_loss: 3284.4944\n",
            "Epoch 4330/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.6807 - val_loss: 3282.2441\n",
            "Epoch 4331/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.9219 - val_loss: 3281.5562\n",
            "Epoch 4332/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.6711 - val_loss: 3276.4902\n",
            "Epoch 4333/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.8838 - val_loss: 3281.0945\n",
            "Epoch 4334/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2406.9639 - val_loss: 3282.0872\n",
            "Epoch 4335/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.5977 - val_loss: 3285.9438\n",
            "Epoch 4336/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.7756 - val_loss: 3281.4172\n",
            "Epoch 4337/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.9631 - val_loss: 3272.2148\n",
            "Epoch 4338/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.6331 - val_loss: 3269.4702\n",
            "Epoch 4339/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.9817 - val_loss: 3268.1299\n",
            "Epoch 4340/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2406.5266 - val_loss: 3265.5142\n",
            "Epoch 4341/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2405.9417 - val_loss: 3263.3757\n",
            "Epoch 4342/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.5840 - val_loss: 3264.0623\n",
            "Epoch 4343/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.9739 - val_loss: 3265.0474\n",
            "Epoch 4344/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.9761 - val_loss: 3267.1465\n",
            "Epoch 4345/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.7874 - val_loss: 3264.3928\n",
            "Epoch 4346/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2410.8914 - val_loss: 3263.4629\n",
            "Epoch 4347/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.9966 - val_loss: 3263.8591\n",
            "Epoch 4348/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.6736 - val_loss: 3266.2515\n",
            "Epoch 4349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.9004 - val_loss: 3272.1514\n",
            "Epoch 4350/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2406.7659 - val_loss: 3275.5315\n",
            "Epoch 4351/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.7856 - val_loss: 3274.2822\n",
            "Epoch 4352/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2406.4602 - val_loss: 3274.2612\n",
            "Epoch 4353/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.4766 - val_loss: 3276.9526\n",
            "Epoch 4354/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.4365 - val_loss: 3274.9102\n",
            "Epoch 4355/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.0322 - val_loss: 3279.3279\n",
            "Epoch 4356/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2407.2661 - val_loss: 3278.4397\n",
            "Epoch 4357/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.3311 - val_loss: 3275.9434\n",
            "Epoch 4358/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2406.4880 - val_loss: 3277.1707\n",
            "Epoch 4359/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.7031 - val_loss: 3271.3728\n",
            "Epoch 4360/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.4111 - val_loss: 3267.9617\n",
            "Epoch 4361/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.8572 - val_loss: 3266.5566\n",
            "Epoch 4362/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.2795 - val_loss: 3266.9656\n",
            "Epoch 4363/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.4099 - val_loss: 3267.5159\n",
            "Epoch 4364/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2406.3708 - val_loss: 3270.2141\n",
            "Epoch 4365/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.0591 - val_loss: 3271.5928\n",
            "Epoch 4366/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.9124 - val_loss: 3271.9502\n",
            "Epoch 4367/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.0161 - val_loss: 3271.4736\n",
            "Epoch 4368/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.9819 - val_loss: 3271.3660\n",
            "Epoch 4369/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.6736 - val_loss: 3271.8704\n",
            "Epoch 4370/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2405.1455 - val_loss: 3273.2942\n",
            "Epoch 4371/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.5430 - val_loss: 3274.8918\n",
            "Epoch 4372/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2404.8977 - val_loss: 3280.4951\n",
            "Epoch 4373/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.4541 - val_loss: 3284.2380\n",
            "Epoch 4374/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.6614 - val_loss: 3282.6682\n",
            "Epoch 4375/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.5234 - val_loss: 3281.5715\n",
            "Epoch 4376/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2405.2549 - val_loss: 3280.1731\n",
            "Epoch 4377/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2405.3022 - val_loss: 3280.9895\n",
            "Epoch 4378/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.9199 - val_loss: 3287.2258\n",
            "Epoch 4379/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.9797 - val_loss: 3286.6187\n",
            "Epoch 4380/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.8811 - val_loss: 3291.3879\n",
            "Epoch 4381/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2410.8560 - val_loss: 3290.7117\n",
            "Epoch 4382/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.0229 - val_loss: 3288.6819\n",
            "Epoch 4383/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.5530 - val_loss: 3287.1528\n",
            "Epoch 4384/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.1121 - val_loss: 3287.0112\n",
            "Epoch 4385/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.6079 - val_loss: 3284.6731\n",
            "Epoch 4386/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.8850 - val_loss: 3282.8977\n",
            "Epoch 4387/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2406.7500 - val_loss: 3284.2307\n",
            "Epoch 4388/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2406.8647 - val_loss: 3288.4185\n",
            "Epoch 4389/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.7751 - val_loss: 3291.4009\n",
            "Epoch 4390/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.8860 - val_loss: 3291.2043\n",
            "Epoch 4391/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.3167 - val_loss: 3293.9453\n",
            "Epoch 4392/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.4255 - val_loss: 3292.9341\n",
            "Epoch 4393/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.5803 - val_loss: 3289.0969\n",
            "Epoch 4394/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2408.3386 - val_loss: 3286.8196\n",
            "Epoch 4395/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.0120 - val_loss: 3278.4648\n",
            "Epoch 4396/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.1128 - val_loss: 3275.8806\n",
            "Epoch 4397/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.8174 - val_loss: 3273.0283\n",
            "Epoch 4398/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.0745 - val_loss: 3270.4685\n",
            "Epoch 4399/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.2896 - val_loss: 3270.2766\n",
            "Epoch 4400/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2408.4858 - val_loss: 3267.2610\n",
            "Epoch 4401/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.6807 - val_loss: 3267.2478\n",
            "Epoch 4402/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.5222 - val_loss: 3269.0544\n",
            "Epoch 4403/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.9546 - val_loss: 3269.5803\n",
            "Epoch 4404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.0164 - val_loss: 3269.4961\n",
            "Epoch 4405/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.9607 - val_loss: 3270.0664\n",
            "Epoch 4406/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2407.7480 - val_loss: 3270.3694\n",
            "Epoch 4407/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2404.6025 - val_loss: 3270.7742\n",
            "Epoch 4408/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2405.8882 - val_loss: 3268.2202\n",
            "Epoch 4409/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2408.4226 - val_loss: 3268.0583\n",
            "Epoch 4410/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2409.7305 - val_loss: 3268.4478\n",
            "Epoch 4411/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.7703 - val_loss: 3268.8828\n",
            "Epoch 4412/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2406.6265 - val_loss: 3269.3171\n",
            "Epoch 4413/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.0803 - val_loss: 3269.9712\n",
            "Epoch 4414/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.1873 - val_loss: 3271.5513\n",
            "Epoch 4415/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.0396 - val_loss: 3274.4778\n",
            "Epoch 4416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.8496 - val_loss: 3277.0955\n",
            "Epoch 4417/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.1755 - val_loss: 3279.5642\n",
            "Epoch 4418/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2404.7754 - val_loss: 3275.4607\n",
            "Epoch 4419/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.0068 - val_loss: 3273.7197\n",
            "Epoch 4420/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.8586 - val_loss: 3268.6416\n",
            "Epoch 4421/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.9497 - val_loss: 3267.9541\n",
            "Epoch 4422/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2403.7732 - val_loss: 3268.7244\n",
            "Epoch 4423/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.7100 - val_loss: 3269.5210\n",
            "Epoch 4424/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2403.6033 - val_loss: 3270.2395\n",
            "Epoch 4425/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.7510 - val_loss: 3273.2754\n",
            "Epoch 4426/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.8142 - val_loss: 3275.2620\n",
            "Epoch 4427/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.6030 - val_loss: 3271.1465\n",
            "Epoch 4428/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.5254 - val_loss: 3268.4495\n",
            "Epoch 4429/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.4233 - val_loss: 3269.0737\n",
            "Epoch 4430/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2405.2617 - val_loss: 3273.6570\n",
            "Epoch 4431/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.5000 - val_loss: 3275.2964\n",
            "Epoch 4432/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2405.0278 - val_loss: 3279.0627\n",
            "Epoch 4433/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.2761 - val_loss: 3280.7212\n",
            "Epoch 4434/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2408.6343 - val_loss: 3284.0771\n",
            "Epoch 4435/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2409.2437 - val_loss: 3283.6274\n",
            "Epoch 4436/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2409.3354 - val_loss: 3291.4031\n",
            "Epoch 4437/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2416.5789 - val_loss: 3300.7002\n",
            "Epoch 4438/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2421.0325 - val_loss: 3301.4978\n",
            "Epoch 4439/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2417.0828 - val_loss: 3295.2891\n",
            "Epoch 4440/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2419.3425 - val_loss: 3297.2671\n",
            "Epoch 4441/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.1843 - val_loss: 3292.2371\n",
            "Epoch 4442/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2413.6987 - val_loss: 3287.5850\n",
            "Epoch 4443/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2410.7380 - val_loss: 3286.7129\n",
            "Epoch 4444/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2409.1868 - val_loss: 3283.4167\n",
            "Epoch 4445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2407.9265 - val_loss: 3278.3069\n",
            "Epoch 4446/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.7612 - val_loss: 3277.2854\n",
            "Epoch 4447/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.2998 - val_loss: 3283.4773\n",
            "Epoch 4448/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2408.0022 - val_loss: 3282.4434\n",
            "Epoch 4449/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2405.1953 - val_loss: 3273.4238\n",
            "Epoch 4450/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2403.0632 - val_loss: 3269.8623\n",
            "Epoch 4451/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2404.4006 - val_loss: 3266.7051\n",
            "Epoch 4452/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.7151 - val_loss: 3266.3613\n",
            "Epoch 4453/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.4795 - val_loss: 3266.4866\n",
            "Epoch 4454/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2405.6689 - val_loss: 3266.7046\n",
            "Epoch 4455/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.9238 - val_loss: 3269.4658\n",
            "Epoch 4456/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.1892 - val_loss: 3270.1787\n",
            "Epoch 4457/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.8865 - val_loss: 3269.3701\n",
            "Epoch 4458/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.3098 - val_loss: 3270.8459\n",
            "Epoch 4459/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.6599 - val_loss: 3266.2917\n",
            "Epoch 4460/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2404.1860 - val_loss: 3267.1106\n",
            "Epoch 4461/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2403.5203 - val_loss: 3266.3918\n",
            "Epoch 4462/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.2119 - val_loss: 3266.0344\n",
            "Epoch 4463/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.7856 - val_loss: 3265.4792\n",
            "Epoch 4464/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.6519 - val_loss: 3265.0615\n",
            "Epoch 4465/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2404.2876 - val_loss: 3264.3127\n",
            "Epoch 4466/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2404.4492 - val_loss: 3264.8093\n",
            "Epoch 4467/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.9275 - val_loss: 3265.6392\n",
            "Epoch 4468/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2408.2822 - val_loss: 3265.8809\n",
            "Epoch 4469/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.7383 - val_loss: 3266.3701\n",
            "Epoch 4470/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.3521 - val_loss: 3268.2627\n",
            "Epoch 4471/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.0579 - val_loss: 3267.6355\n",
            "Epoch 4472/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2405.1960 - val_loss: 3269.3027\n",
            "Epoch 4473/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.3015 - val_loss: 3271.6565\n",
            "Epoch 4474/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.3804 - val_loss: 3272.2917\n",
            "Epoch 4475/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.9456 - val_loss: 3273.4927\n",
            "Epoch 4476/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.3313 - val_loss: 3273.7578\n",
            "Epoch 4477/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.1831 - val_loss: 3274.4321\n",
            "Epoch 4478/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2400.8396 - val_loss: 3273.6960\n",
            "Epoch 4479/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.1587 - val_loss: 3273.1570\n",
            "Epoch 4480/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2401.8760 - val_loss: 3273.8303\n",
            "Epoch 4481/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.8640 - val_loss: 3270.2498\n",
            "Epoch 4482/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.2068 - val_loss: 3268.3223\n",
            "Epoch 4483/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.1829 - val_loss: 3268.2739\n",
            "Epoch 4484/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2404.5273 - val_loss: 3268.8491\n",
            "Epoch 4485/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.9939 - val_loss: 3269.4417\n",
            "Epoch 4486/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.0461 - val_loss: 3269.3181\n",
            "Epoch 4487/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2401.9482 - val_loss: 3270.6313\n",
            "Epoch 4488/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.3262 - val_loss: 3271.3228\n",
            "Epoch 4489/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.1355 - val_loss: 3271.7290\n",
            "Epoch 4490/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2402.7124 - val_loss: 3269.9761\n",
            "Epoch 4491/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.5676 - val_loss: 3270.0378\n",
            "Epoch 4492/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2401.7983 - val_loss: 3271.6597\n",
            "Epoch 4493/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.0957 - val_loss: 3273.9417\n",
            "Epoch 4494/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.5496 - val_loss: 3272.2385\n",
            "Epoch 4495/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2402.7615 - val_loss: 3269.6448\n",
            "Epoch 4496/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2404.8677 - val_loss: 3267.6528\n",
            "Epoch 4497/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.4036 - val_loss: 3265.9624\n",
            "Epoch 4498/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2408.9104 - val_loss: 3266.3445\n",
            "Epoch 4499/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.8096 - val_loss: 3268.5610\n",
            "Epoch 4500/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.3430 - val_loss: 3270.0740\n",
            "Epoch 4501/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.1853 - val_loss: 3269.8188\n",
            "Epoch 4502/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2403.5601 - val_loss: 3270.7949\n",
            "Epoch 4503/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.2290 - val_loss: 3269.5825\n",
            "Epoch 4504/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.3870 - val_loss: 3267.0056\n",
            "Epoch 4505/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.6458 - val_loss: 3266.8687\n",
            "Epoch 4506/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.5447 - val_loss: 3266.4768\n",
            "Epoch 4507/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.2402 - val_loss: 3265.9873\n",
            "Epoch 4508/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2404.4224 - val_loss: 3265.2759\n",
            "Epoch 4509/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2407.0198 - val_loss: 3264.8982\n",
            "Epoch 4510/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2408.4053 - val_loss: 3265.0110\n",
            "Epoch 4511/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.9709 - val_loss: 3265.2781\n",
            "Epoch 4512/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.1318 - val_loss: 3267.3198\n",
            "Epoch 4513/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.8831 - val_loss: 3270.0239\n",
            "Epoch 4514/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.7830 - val_loss: 3271.6770\n",
            "Epoch 4515/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.1487 - val_loss: 3271.5203\n",
            "Epoch 4516/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.1189 - val_loss: 3275.3735\n",
            "Epoch 4517/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.6521 - val_loss: 3276.3757\n",
            "Epoch 4518/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.3103 - val_loss: 3274.6001\n",
            "Epoch 4519/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2401.1460 - val_loss: 3273.8896\n",
            "Epoch 4520/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.6897 - val_loss: 3277.6528\n",
            "Epoch 4521/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2402.5242 - val_loss: 3273.4231\n",
            "Epoch 4522/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.2236 - val_loss: 3278.2866\n",
            "Epoch 4523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.6665 - val_loss: 3283.6580\n",
            "Epoch 4524/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2405.2593 - val_loss: 3285.1714\n",
            "Epoch 4525/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.2236 - val_loss: 3286.1018\n",
            "Epoch 4526/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2405.6665 - val_loss: 3284.1541\n",
            "Epoch 4527/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2405.1885 - val_loss: 3282.0078\n",
            "Epoch 4528/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2401.1274 - val_loss: 3274.5354\n",
            "Epoch 4529/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.6902 - val_loss: 3274.9783\n",
            "Epoch 4530/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.8049 - val_loss: 3276.7781\n",
            "Epoch 4531/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2400.1704 - val_loss: 3271.6785\n",
            "Epoch 4532/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.2898 - val_loss: 3272.3696\n",
            "Epoch 4533/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.2432 - val_loss: 3273.3108\n",
            "Epoch 4534/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.1240 - val_loss: 3272.0315\n",
            "Epoch 4535/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.4880 - val_loss: 3272.2627\n",
            "Epoch 4536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.0601 - val_loss: 3272.1265\n",
            "Epoch 4537/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2401.5806 - val_loss: 3273.2195\n",
            "Epoch 4538/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2400.3252 - val_loss: 3273.6135\n",
            "Epoch 4539/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2405.0635 - val_loss: 3272.7942\n",
            "Epoch 4540/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.6772 - val_loss: 3272.9839\n",
            "Epoch 4541/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.1755 - val_loss: 3273.2502\n",
            "Epoch 4542/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.9841 - val_loss: 3275.0591\n",
            "Epoch 4543/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2401.5007 - val_loss: 3275.0930\n",
            "Epoch 4544/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2401.5383 - val_loss: 3276.9165\n",
            "Epoch 4545/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2400.1716 - val_loss: 3277.0049\n",
            "Epoch 4546/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.1143 - val_loss: 3278.1748\n",
            "Epoch 4547/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.0730 - val_loss: 3280.0835\n",
            "Epoch 4548/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.8057 - val_loss: 3280.7834\n",
            "Epoch 4549/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2399.7209 - val_loss: 3280.4646\n",
            "Epoch 4550/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.4719 - val_loss: 3276.8701\n",
            "Epoch 4551/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.3521 - val_loss: 3277.0710\n",
            "Epoch 4552/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2401.9609 - val_loss: 3281.9424\n",
            "Epoch 4553/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2401.4602 - val_loss: 3284.5828\n",
            "Epoch 4554/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.9866 - val_loss: 3283.7139\n",
            "Epoch 4555/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2399.9160 - val_loss: 3278.2693\n",
            "Epoch 4556/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2401.2400 - val_loss: 3278.0168\n",
            "Epoch 4557/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.4758 - val_loss: 3275.2227\n",
            "Epoch 4558/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.4404 - val_loss: 3275.7458\n",
            "Epoch 4559/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.8625 - val_loss: 3275.1172\n",
            "Epoch 4560/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.2524 - val_loss: 3274.3271\n",
            "Epoch 4561/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2398.0496 - val_loss: 3272.0962\n",
            "Epoch 4562/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2402.6633 - val_loss: 3269.3188\n",
            "Epoch 4563/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2411.4495 - val_loss: 3268.7478\n",
            "Epoch 4564/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2409.6768 - val_loss: 3268.1235\n",
            "Epoch 4565/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2411.2397 - val_loss: 3268.9937\n",
            "Epoch 4566/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.5017 - val_loss: 3269.9495\n",
            "Epoch 4567/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2408.8652 - val_loss: 3269.9910\n",
            "Epoch 4568/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2406.6904 - val_loss: 3272.5198\n",
            "Epoch 4569/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.2812 - val_loss: 3274.9009\n",
            "Epoch 4570/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2404.6826 - val_loss: 3277.7485\n",
            "Epoch 4571/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.0430 - val_loss: 3278.4688\n",
            "Epoch 4572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.2341 - val_loss: 3279.8303\n",
            "Epoch 4573/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2398.5576 - val_loss: 3282.1946\n",
            "Epoch 4574/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.1885 - val_loss: 3277.4944\n",
            "Epoch 4575/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.0288 - val_loss: 3275.2036\n",
            "Epoch 4576/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.3254 - val_loss: 3277.2939\n",
            "Epoch 4577/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.1550 - val_loss: 3277.8896\n",
            "Epoch 4578/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.3333 - val_loss: 3277.8640\n",
            "Epoch 4579/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2399.4268 - val_loss: 3278.0984\n",
            "Epoch 4580/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.4331 - val_loss: 3278.8604\n",
            "Epoch 4581/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.8972 - val_loss: 3279.6404\n",
            "Epoch 4582/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.8586 - val_loss: 3286.5962\n",
            "Epoch 4583/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.7690 - val_loss: 3293.4141\n",
            "Epoch 4584/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.5210 - val_loss: 3290.8062\n",
            "Epoch 4585/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2403.1389 - val_loss: 3290.2434\n",
            "Epoch 4586/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2405.1245 - val_loss: 3295.7202\n",
            "Epoch 4587/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2406.3633 - val_loss: 3298.5034\n",
            "Epoch 4588/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2408.1665 - val_loss: 3302.0120\n",
            "Epoch 4589/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2414.3179 - val_loss: 3305.2588\n",
            "Epoch 4590/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2415.6453 - val_loss: 3303.1763\n",
            "Epoch 4591/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2411.4531 - val_loss: 3295.3831\n",
            "Epoch 4592/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2407.0933 - val_loss: 3287.7700\n",
            "Epoch 4593/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.1079 - val_loss: 3281.5544\n",
            "Epoch 4594/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.7769 - val_loss: 3281.7654\n",
            "Epoch 4595/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.1479 - val_loss: 3278.2622\n",
            "Epoch 4596/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.6575 - val_loss: 3276.8767\n",
            "Epoch 4597/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2401.2039 - val_loss: 3280.8147\n",
            "Epoch 4598/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.2910 - val_loss: 3284.0940\n",
            "Epoch 4599/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.4617 - val_loss: 3290.7827\n",
            "Epoch 4600/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2403.9492 - val_loss: 3292.0857\n",
            "Epoch 4601/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2404.3777 - val_loss: 3288.7395\n",
            "Epoch 4602/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.1829 - val_loss: 3287.1099\n",
            "Epoch 4603/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2400.9109 - val_loss: 3284.6509\n",
            "Epoch 4604/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.6343 - val_loss: 3282.9192\n",
            "Epoch 4605/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.5017 - val_loss: 3281.2693\n",
            "Epoch 4606/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.0078 - val_loss: 3277.6531\n",
            "Epoch 4607/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2402.7661 - val_loss: 3273.5442\n",
            "Epoch 4608/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.5381 - val_loss: 3272.4927\n",
            "Epoch 4609/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2400.7009 - val_loss: 3270.9150\n",
            "Epoch 4610/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.4050 - val_loss: 3271.1404\n",
            "Epoch 4611/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.4956 - val_loss: 3273.0283\n",
            "Epoch 4612/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.6692 - val_loss: 3273.9866\n",
            "Epoch 4613/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.6187 - val_loss: 3276.1899\n",
            "Epoch 4614/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.8374 - val_loss: 3277.0664\n",
            "Epoch 4615/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2399.3760 - val_loss: 3273.5227\n",
            "Epoch 4616/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.1846 - val_loss: 3271.1760\n",
            "Epoch 4617/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2397.3564 - val_loss: 3271.3184\n",
            "Epoch 4618/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.6675 - val_loss: 3271.5610\n",
            "Epoch 4619/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2397.9844 - val_loss: 3274.2983\n",
            "Epoch 4620/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.6150 - val_loss: 3277.6448\n",
            "Epoch 4621/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2399.1853 - val_loss: 3278.3269\n",
            "Epoch 4622/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2398.1550 - val_loss: 3275.9790\n",
            "Epoch 4623/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.7664 - val_loss: 3274.9961\n",
            "Epoch 4624/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.9858 - val_loss: 3275.6868\n",
            "Epoch 4625/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2401.1826 - val_loss: 3275.2036\n",
            "Epoch 4626/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2401.1755 - val_loss: 3273.3862\n",
            "Epoch 4627/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2401.9482 - val_loss: 3272.8159\n",
            "Epoch 4628/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2401.9316 - val_loss: 3272.8220\n",
            "Epoch 4629/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2401.1523 - val_loss: 3271.9866\n",
            "Epoch 4630/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.8926 - val_loss: 3272.0049\n",
            "Epoch 4631/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.8413 - val_loss: 3272.2139\n",
            "Epoch 4632/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.5068 - val_loss: 3272.7988\n",
            "Epoch 4633/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2396.4526 - val_loss: 3277.5115\n",
            "Epoch 4634/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.1738 - val_loss: 3278.4629\n",
            "Epoch 4635/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.8760 - val_loss: 3278.9839\n",
            "Epoch 4636/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.6448 - val_loss: 3278.3865\n",
            "Epoch 4637/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2401.1252 - val_loss: 3279.9341\n",
            "Epoch 4638/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.5740 - val_loss: 3276.5078\n",
            "Epoch 4639/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2400.0659 - val_loss: 3275.1685\n",
            "Epoch 4640/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.8459 - val_loss: 3274.4409\n",
            "Epoch 4641/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.6423 - val_loss: 3274.6079\n",
            "Epoch 4642/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.9902 - val_loss: 3275.0098\n",
            "Epoch 4643/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.2615 - val_loss: 3275.8069\n",
            "Epoch 4644/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.5234 - val_loss: 3275.3528\n",
            "Epoch 4645/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2398.3040 - val_loss: 3273.8115\n",
            "Epoch 4646/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.3301 - val_loss: 3274.2354\n",
            "Epoch 4647/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.2400 - val_loss: 3274.1765\n",
            "Epoch 4648/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.7246 - val_loss: 3274.2483\n",
            "Epoch 4649/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.3013 - val_loss: 3279.6365\n",
            "Epoch 4650/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.7236 - val_loss: 3286.3616\n",
            "Epoch 4651/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2400.8547 - val_loss: 3287.3181\n",
            "Epoch 4652/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.0222 - val_loss: 3283.0442\n",
            "Epoch 4653/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.5269 - val_loss: 3277.2332\n",
            "Epoch 4654/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.1633 - val_loss: 3276.1309\n",
            "Epoch 4655/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.5525 - val_loss: 3274.7605\n",
            "Epoch 4656/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.4849 - val_loss: 3274.6602\n",
            "Epoch 4657/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2398.7583 - val_loss: 3275.8271\n",
            "Epoch 4658/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.8005 - val_loss: 3278.3728\n",
            "Epoch 4659/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.3708 - val_loss: 3279.0398\n",
            "Epoch 4660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.3179 - val_loss: 3277.4888\n",
            "Epoch 4661/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.8872 - val_loss: 3277.5955\n",
            "Epoch 4662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.1250 - val_loss: 3277.8613\n",
            "Epoch 4663/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2397.3320 - val_loss: 3275.4565\n",
            "Epoch 4664/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.0596 - val_loss: 3275.2871\n",
            "Epoch 4665/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.8083 - val_loss: 3275.1018\n",
            "Epoch 4666/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.0840 - val_loss: 3274.7285\n",
            "Epoch 4667/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.3198 - val_loss: 3275.0295\n",
            "Epoch 4668/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.5837 - val_loss: 3276.8767\n",
            "Epoch 4669/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2397.6257 - val_loss: 3278.0234\n",
            "Epoch 4670/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.1401 - val_loss: 3279.1130\n",
            "Epoch 4671/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.0552 - val_loss: 3278.1587\n",
            "Epoch 4672/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.4600 - val_loss: 3276.9902\n",
            "Epoch 4673/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.4844 - val_loss: 3277.1692\n",
            "Epoch 4674/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.1956 - val_loss: 3276.8816\n",
            "Epoch 4675/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2397.8955 - val_loss: 3277.0986\n",
            "Epoch 4676/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.4106 - val_loss: 3277.6323\n",
            "Epoch 4677/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.5374 - val_loss: 3279.2266\n",
            "Epoch 4678/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.5107 - val_loss: 3283.5422\n",
            "Epoch 4679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.1265 - val_loss: 3282.7461\n",
            "Epoch 4680/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.9475 - val_loss: 3284.1763\n",
            "Epoch 4681/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2398.2163 - val_loss: 3285.2822\n",
            "Epoch 4682/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.5728 - val_loss: 3285.7747\n",
            "Epoch 4683/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.6201 - val_loss: 3282.8191\n",
            "Epoch 4684/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.0732 - val_loss: 3282.2778\n",
            "Epoch 4685/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.3884 - val_loss: 3282.4382\n",
            "Epoch 4686/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2395.9172 - val_loss: 3281.8059\n",
            "Epoch 4687/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.0291 - val_loss: 3281.3047\n",
            "Epoch 4688/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.0952 - val_loss: 3281.4705\n",
            "Epoch 4689/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.1313 - val_loss: 3281.6323\n",
            "Epoch 4690/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.2070 - val_loss: 3281.8533\n",
            "Epoch 4691/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.7344 - val_loss: 3283.9543\n",
            "Epoch 4692/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2398.2766 - val_loss: 3285.7590\n",
            "Epoch 4693/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.7705 - val_loss: 3284.8872\n",
            "Epoch 4694/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.9231 - val_loss: 3285.9207\n",
            "Epoch 4695/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.0247 - val_loss: 3282.7742\n",
            "Epoch 4696/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.6018 - val_loss: 3280.7683\n",
            "Epoch 4697/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.4668 - val_loss: 3277.1309\n",
            "Epoch 4698/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2396.0679 - val_loss: 3277.8584\n",
            "Epoch 4699/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.8264 - val_loss: 3279.3047\n",
            "Epoch 4700/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2396.1157 - val_loss: 3281.8960\n",
            "Epoch 4701/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.1460 - val_loss: 3281.9397\n",
            "Epoch 4702/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.5625 - val_loss: 3283.0442\n",
            "Epoch 4703/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.7852 - val_loss: 3282.1409\n",
            "Epoch 4704/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2395.4622 - val_loss: 3278.7478\n",
            "Epoch 4705/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.5232 - val_loss: 3276.9021\n",
            "Epoch 4706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.1748 - val_loss: 3279.7085\n",
            "Epoch 4707/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.6580 - val_loss: 3279.0818\n",
            "Epoch 4708/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.8926 - val_loss: 3276.2410\n",
            "Epoch 4709/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.0227 - val_loss: 3275.9880\n",
            "Epoch 4710/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2396.8499 - val_loss: 3278.6660\n",
            "Epoch 4711/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.3970 - val_loss: 3278.2998\n",
            "Epoch 4712/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.8123 - val_loss: 3278.9534\n",
            "Epoch 4713/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2393.4604 - val_loss: 3281.0049\n",
            "Epoch 4714/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.8752 - val_loss: 3282.9070\n",
            "Epoch 4715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.1909 - val_loss: 3287.1082\n",
            "Epoch 4716/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2398.2200 - val_loss: 3285.7085\n",
            "Epoch 4717/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.9856 - val_loss: 3288.2578\n",
            "Epoch 4718/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.3606 - val_loss: 3286.7268\n",
            "Epoch 4719/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.3452 - val_loss: 3285.5596\n",
            "Epoch 4720/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.9363 - val_loss: 3289.6714\n",
            "Epoch 4721/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.3813 - val_loss: 3289.5889\n",
            "Epoch 4722/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2399.6479 - val_loss: 3291.4578\n",
            "Epoch 4723/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2401.2671 - val_loss: 3292.3396\n",
            "Epoch 4724/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2400.8850 - val_loss: 3290.3750\n",
            "Epoch 4725/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2400.2764 - val_loss: 3289.8596\n",
            "Epoch 4726/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2400.3728 - val_loss: 3286.6067\n",
            "Epoch 4727/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2397.5454 - val_loss: 3280.5371\n",
            "Epoch 4728/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2398.7908 - val_loss: 3276.0769\n",
            "Epoch 4729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.9790 - val_loss: 3275.4333\n",
            "Epoch 4730/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.5964 - val_loss: 3274.7493\n",
            "Epoch 4731/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.7737 - val_loss: 3274.0146\n",
            "Epoch 4732/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.8455 - val_loss: 3273.9802\n",
            "Epoch 4733/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2398.0234 - val_loss: 3273.1162\n",
            "Epoch 4734/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2399.6445 - val_loss: 3269.7698\n",
            "Epoch 4735/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2405.3342 - val_loss: 3268.5803\n",
            "Epoch 4736/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2403.6787 - val_loss: 3268.0049\n",
            "Epoch 4737/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2402.6995 - val_loss: 3267.5000\n",
            "Epoch 4738/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2402.4546 - val_loss: 3267.5994\n",
            "Epoch 4739/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2399.5803 - val_loss: 3268.0134\n",
            "Epoch 4740/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.5212 - val_loss: 3271.5024\n",
            "Epoch 4741/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.1582 - val_loss: 3273.0366\n",
            "Epoch 4742/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.3525 - val_loss: 3273.2124\n",
            "Epoch 4743/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.6382 - val_loss: 3274.5857\n",
            "Epoch 4744/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.0557 - val_loss: 3278.0896\n",
            "Epoch 4745/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2396.3652 - val_loss: 3275.8286\n",
            "Epoch 4746/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.0366 - val_loss: 3274.8486\n",
            "Epoch 4747/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.1562 - val_loss: 3273.4727\n",
            "Epoch 4748/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.7593 - val_loss: 3272.6467\n",
            "Epoch 4749/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.4758 - val_loss: 3271.7778\n",
            "Epoch 4750/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2395.9666 - val_loss: 3271.0823\n",
            "Epoch 4751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.9443 - val_loss: 3272.6541\n",
            "Epoch 4752/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.5281 - val_loss: 3273.0969\n",
            "Epoch 4753/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2399.5662 - val_loss: 3274.5745\n",
            "Epoch 4754/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2401.2114 - val_loss: 3275.2615\n",
            "Epoch 4755/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2399.3555 - val_loss: 3275.1748\n",
            "Epoch 4756/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2398.7139 - val_loss: 3275.1660\n",
            "Epoch 4757/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.4724 - val_loss: 3275.7805\n",
            "Epoch 4758/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.8364 - val_loss: 3276.1841\n",
            "Epoch 4759/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.0515 - val_loss: 3275.8013\n",
            "Epoch 4760/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.8430 - val_loss: 3275.7578\n",
            "Epoch 4761/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.8638 - val_loss: 3276.2517\n",
            "Epoch 4762/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2394.5730 - val_loss: 3277.5908\n",
            "Epoch 4763/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.9680 - val_loss: 3278.4375\n",
            "Epoch 4764/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.3918 - val_loss: 3276.1890\n",
            "Epoch 4765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.1355 - val_loss: 3274.9705\n",
            "Epoch 4766/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.6501 - val_loss: 3274.5442\n",
            "Epoch 4767/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.2500 - val_loss: 3274.7412\n",
            "Epoch 4768/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2396.0376 - val_loss: 3280.1587\n",
            "Epoch 4769/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.1377 - val_loss: 3276.9172\n",
            "Epoch 4770/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.6011 - val_loss: 3275.8428\n",
            "Epoch 4771/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.1372 - val_loss: 3274.7798\n",
            "Epoch 4772/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.2542 - val_loss: 3274.8596\n",
            "Epoch 4773/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.0195 - val_loss: 3275.1365\n",
            "Epoch 4774/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2394.6838 - val_loss: 3275.9136\n",
            "Epoch 4775/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.3567 - val_loss: 3277.1177\n",
            "Epoch 4776/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.7988 - val_loss: 3279.1577\n",
            "Epoch 4777/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2394.0615 - val_loss: 3280.4353\n",
            "Epoch 4778/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.8091 - val_loss: 3284.5085\n",
            "Epoch 4779/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2395.5017 - val_loss: 3285.6448\n",
            "Epoch 4780/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2396.2749 - val_loss: 3290.6716\n",
            "Epoch 4781/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.9985 - val_loss: 3291.6130\n",
            "Epoch 4782/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.3806 - val_loss: 3289.2451\n",
            "Epoch 4783/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.8535 - val_loss: 3288.2366\n",
            "Epoch 4784/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.4573 - val_loss: 3284.8638\n",
            "Epoch 4785/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2394.3928 - val_loss: 3285.2942\n",
            "Epoch 4786/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.8911 - val_loss: 3291.6538\n",
            "Epoch 4787/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2397.1211 - val_loss: 3292.9558\n",
            "Epoch 4788/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.4419 - val_loss: 3291.2422\n",
            "Epoch 4789/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.8533 - val_loss: 3286.1116\n",
            "Epoch 4790/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2394.2375 - val_loss: 3286.2444\n",
            "Epoch 4791/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.2097 - val_loss: 3290.3828\n",
            "Epoch 4792/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2396.5325 - val_loss: 3291.7844\n",
            "Epoch 4793/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2396.4153 - val_loss: 3287.2891\n",
            "Epoch 4794/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.5391 - val_loss: 3293.2156\n",
            "Epoch 4795/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.4343 - val_loss: 3292.7925\n",
            "Epoch 4796/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2396.7869 - val_loss: 3290.0073\n",
            "Epoch 4797/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.7847 - val_loss: 3289.8440\n",
            "Epoch 4798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.1672 - val_loss: 3287.5288\n",
            "Epoch 4799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.5024 - val_loss: 3287.7185\n",
            "Epoch 4800/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.7500 - val_loss: 3286.3237\n",
            "Epoch 4801/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2393.6462 - val_loss: 3285.7676\n",
            "Epoch 4802/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2393.8450 - val_loss: 3286.8977\n",
            "Epoch 4803/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2393.1372 - val_loss: 3289.2578\n",
            "Epoch 4804/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.3547 - val_loss: 3293.8862\n",
            "Epoch 4805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2397.4707 - val_loss: 3297.5278\n",
            "Epoch 4806/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2398.8870 - val_loss: 3297.9070\n",
            "Epoch 4807/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2398.0120 - val_loss: 3293.8054\n",
            "Epoch 4808/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2396.6504 - val_loss: 3293.2532\n",
            "Epoch 4809/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.2349 - val_loss: 3289.6909\n",
            "Epoch 4810/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.5378 - val_loss: 3284.2542\n",
            "Epoch 4811/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2393.9358 - val_loss: 3283.3865\n",
            "Epoch 4812/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2393.1882 - val_loss: 3283.8542\n",
            "Epoch 4813/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2395.8960 - val_loss: 3284.2563\n",
            "Epoch 4814/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.7947 - val_loss: 3283.5066\n",
            "Epoch 4815/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2394.2310 - val_loss: 3285.2610\n",
            "Epoch 4816/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.0527 - val_loss: 3287.0540\n",
            "Epoch 4817/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.9695 - val_loss: 3286.7324\n",
            "Epoch 4818/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.0312 - val_loss: 3286.5779\n",
            "Epoch 4819/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.3032 - val_loss: 3285.0415\n",
            "Epoch 4820/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.5559 - val_loss: 3283.7886\n",
            "Epoch 4821/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.6226 - val_loss: 3281.7957\n",
            "Epoch 4822/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.7876 - val_loss: 3281.1804\n",
            "Epoch 4823/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.0576 - val_loss: 3279.9761\n",
            "Epoch 4824/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2393.5281 - val_loss: 3280.6033\n",
            "Epoch 4825/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2393.9739 - val_loss: 3279.8110\n",
            "Epoch 4826/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2393.6265 - val_loss: 3278.5740\n",
            "Epoch 4827/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.9089 - val_loss: 3279.0322\n",
            "Epoch 4828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2392.1169 - val_loss: 3279.9048\n",
            "Epoch 4829/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2392.3403 - val_loss: 3281.4585\n",
            "Epoch 4830/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2391.5100 - val_loss: 3282.2949\n",
            "Epoch 4831/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.8521 - val_loss: 3282.8335\n",
            "Epoch 4832/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2392.2273 - val_loss: 3283.5474\n",
            "Epoch 4833/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.6694 - val_loss: 3283.6160\n",
            "Epoch 4834/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.9163 - val_loss: 3283.7358\n",
            "Epoch 4835/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2391.5840 - val_loss: 3282.1060\n",
            "Epoch 4836/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.8611 - val_loss: 3281.6323\n",
            "Epoch 4837/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2391.4888 - val_loss: 3282.6746\n",
            "Epoch 4838/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2391.6260 - val_loss: 3284.5266\n",
            "Epoch 4839/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.1101 - val_loss: 3282.4177\n",
            "Epoch 4840/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.2131 - val_loss: 3281.3926\n",
            "Epoch 4841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.1450 - val_loss: 3280.5183\n",
            "Epoch 4842/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2390.3816 - val_loss: 3281.1763\n",
            "Epoch 4843/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.9255 - val_loss: 3281.6929\n",
            "Epoch 4844/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.1736 - val_loss: 3281.2812\n",
            "Epoch 4845/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.7322 - val_loss: 3276.9128\n",
            "Epoch 4846/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.4758 - val_loss: 3273.3918\n",
            "Epoch 4847/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2393.9377 - val_loss: 3273.5256\n",
            "Epoch 4848/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2394.2639 - val_loss: 3273.0151\n",
            "Epoch 4849/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.8513 - val_loss: 3272.7754\n",
            "Epoch 4850/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.8999 - val_loss: 3271.9941\n",
            "Epoch 4851/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.9829 - val_loss: 3272.7178\n",
            "Epoch 4852/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2392.7117 - val_loss: 3275.3044\n",
            "Epoch 4853/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2391.5635 - val_loss: 3277.6394\n",
            "Epoch 4854/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.4976 - val_loss: 3278.5593\n",
            "Epoch 4855/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.7136 - val_loss: 3279.2427\n",
            "Epoch 4856/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2391.3625 - val_loss: 3279.7517\n",
            "Epoch 4857/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.4285 - val_loss: 3277.4512\n",
            "Epoch 4858/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.2825 - val_loss: 3277.3936\n",
            "Epoch 4859/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2391.9885 - val_loss: 3280.6917\n",
            "Epoch 4860/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.9705 - val_loss: 3280.7100\n",
            "Epoch 4861/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2390.7769 - val_loss: 3280.7654\n",
            "Epoch 4862/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.6936 - val_loss: 3281.7654\n",
            "Epoch 4863/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.8162 - val_loss: 3287.2686\n",
            "Epoch 4864/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.6448 - val_loss: 3286.8892\n",
            "Epoch 4865/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2390.8459 - val_loss: 3286.8472\n",
            "Epoch 4866/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.3840 - val_loss: 3287.9526\n",
            "Epoch 4867/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.5295 - val_loss: 3288.3271\n",
            "Epoch 4868/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.7251 - val_loss: 3291.2065\n",
            "Epoch 4869/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.1072 - val_loss: 3293.1147\n",
            "Epoch 4870/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.4771 - val_loss: 3291.3452\n",
            "Epoch 4871/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2389.1777 - val_loss: 3287.0850\n",
            "Epoch 4872/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.1895 - val_loss: 3286.4216\n",
            "Epoch 4873/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.4973 - val_loss: 3284.6377\n",
            "Epoch 4874/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.7068 - val_loss: 3284.8315\n",
            "Epoch 4875/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.2322 - val_loss: 3283.9937\n",
            "Epoch 4876/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.5615 - val_loss: 3284.5154\n",
            "Epoch 4877/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2388.3140 - val_loss: 3288.5408\n",
            "Epoch 4878/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.8748 - val_loss: 3292.8894\n",
            "Epoch 4879/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.4468 - val_loss: 3291.5703\n",
            "Epoch 4880/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.6333 - val_loss: 3294.0674\n",
            "Epoch 4881/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2391.1379 - val_loss: 3291.3159\n",
            "Epoch 4882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.5479 - val_loss: 3290.5801\n",
            "Epoch 4883/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.7461 - val_loss: 3288.4985\n",
            "Epoch 4884/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2391.3701 - val_loss: 3289.2971\n",
            "Epoch 4885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.2283 - val_loss: 3288.8518\n",
            "Epoch 4886/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2389.5447 - val_loss: 3286.7292\n",
            "Epoch 4887/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2392.0969 - val_loss: 3284.0884\n",
            "Epoch 4888/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.5300 - val_loss: 3284.5713\n",
            "Epoch 4889/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2389.7529 - val_loss: 3284.8647\n",
            "Epoch 4890/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.6787 - val_loss: 3288.8950\n",
            "Epoch 4891/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.1414 - val_loss: 3294.0425\n",
            "Epoch 4892/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2390.9534 - val_loss: 3294.6963\n",
            "Epoch 4893/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.5198 - val_loss: 3291.4753\n",
            "Epoch 4894/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.6047 - val_loss: 3291.3469\n",
            "Epoch 4895/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2389.7786 - val_loss: 3296.1072\n",
            "Epoch 4896/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.3083 - val_loss: 3299.9172\n",
            "Epoch 4897/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2392.8477 - val_loss: 3301.4319\n",
            "Epoch 4898/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2394.1665 - val_loss: 3301.8616\n",
            "Epoch 4899/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.7812 - val_loss: 3298.3660\n",
            "Epoch 4900/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2390.4111 - val_loss: 3289.3997\n",
            "Epoch 4901/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.7065 - val_loss: 3287.5496\n",
            "Epoch 4902/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.8540 - val_loss: 3288.9526\n",
            "Epoch 4903/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.7888 - val_loss: 3287.7302\n",
            "Epoch 4904/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.2412 - val_loss: 3288.8040\n",
            "Epoch 4905/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.0505 - val_loss: 3288.7502\n",
            "Epoch 4906/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2389.9619 - val_loss: 3292.3813\n",
            "Epoch 4907/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.2021 - val_loss: 3290.9915\n",
            "Epoch 4908/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.1548 - val_loss: 3287.5017\n",
            "Epoch 4909/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.9460 - val_loss: 3282.0134\n",
            "Epoch 4910/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.6550 - val_loss: 3281.0134\n",
            "Epoch 4911/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2389.5674 - val_loss: 3280.9929\n",
            "Epoch 4912/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2389.7131 - val_loss: 3282.2065\n",
            "Epoch 4913/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.9211 - val_loss: 3284.6208\n",
            "Epoch 4914/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.0784 - val_loss: 3284.0571\n",
            "Epoch 4915/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.7781 - val_loss: 3283.9041\n",
            "Epoch 4916/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.5740 - val_loss: 3282.7371\n",
            "Epoch 4917/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2391.4915 - val_loss: 3282.8540\n",
            "Epoch 4918/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.5754 - val_loss: 3283.2556\n",
            "Epoch 4919/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2395.2122 - val_loss: 3286.1084\n",
            "Epoch 4920/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2390.2263 - val_loss: 3288.1731\n",
            "Epoch 4921/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.2510 - val_loss: 3288.4854\n",
            "Epoch 4922/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2395.0034 - val_loss: 3293.8896\n",
            "Epoch 4923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2393.2058 - val_loss: 3293.2124\n",
            "Epoch 4924/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.5308 - val_loss: 3290.7466\n",
            "Epoch 4925/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.4927 - val_loss: 3287.7932\n",
            "Epoch 4926/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2390.4138 - val_loss: 3285.8579\n",
            "Epoch 4927/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.3572 - val_loss: 3282.5723\n",
            "Epoch 4928/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2389.4324 - val_loss: 3281.3752\n",
            "Epoch 4929/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.6636 - val_loss: 3281.7012\n",
            "Epoch 4930/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.4500 - val_loss: 3283.1787\n",
            "Epoch 4931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.3591 - val_loss: 3279.6370\n",
            "Epoch 4932/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.6655 - val_loss: 3276.0859\n",
            "Epoch 4933/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.7612 - val_loss: 3276.7065\n",
            "Epoch 4934/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2390.6624 - val_loss: 3279.8118\n",
            "Epoch 4935/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2389.4038 - val_loss: 3279.7334\n",
            "Epoch 4936/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.1924 - val_loss: 3280.8062\n",
            "Epoch 4937/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.3679 - val_loss: 3282.5259\n",
            "Epoch 4938/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.3823 - val_loss: 3283.7773\n",
            "Epoch 4939/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.5823 - val_loss: 3285.5688\n",
            "Epoch 4940/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2388.6472 - val_loss: 3285.1279\n",
            "Epoch 4941/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.7129 - val_loss: 3286.9690\n",
            "Epoch 4942/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.4871 - val_loss: 3287.4246\n",
            "Epoch 4943/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.2834 - val_loss: 3288.3870\n",
            "Epoch 4944/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.3411 - val_loss: 3287.3181\n",
            "Epoch 4945/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.4807 - val_loss: 3288.7605\n",
            "Epoch 4946/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2388.4309 - val_loss: 3289.7588\n",
            "Epoch 4947/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.9302 - val_loss: 3289.1523\n",
            "Epoch 4948/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.1628 - val_loss: 3289.1626\n",
            "Epoch 4949/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.9661 - val_loss: 3290.0049\n",
            "Epoch 4950/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.7695 - val_loss: 3290.2363\n",
            "Epoch 4951/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.5154 - val_loss: 3291.6860\n",
            "Epoch 4952/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2388.4487 - val_loss: 3289.7310\n",
            "Epoch 4953/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.4883 - val_loss: 3288.5400\n",
            "Epoch 4954/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.1016 - val_loss: 3288.9585\n",
            "Epoch 4955/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2388.5256 - val_loss: 3287.8892\n",
            "Epoch 4956/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.8940 - val_loss: 3286.5767\n",
            "Epoch 4957/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.7310 - val_loss: 3285.8447\n",
            "Epoch 4958/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2387.6970 - val_loss: 3283.8645\n",
            "Epoch 4959/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.7734 - val_loss: 3285.9231\n",
            "Epoch 4960/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2386.9539 - val_loss: 3288.6833\n",
            "Epoch 4961/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.8567 - val_loss: 3290.1072\n",
            "Epoch 4962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.2297 - val_loss: 3289.7085\n",
            "Epoch 4963/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2391.0918 - val_loss: 3287.5833\n",
            "Epoch 4964/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2389.4453 - val_loss: 3290.0737\n",
            "Epoch 4965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.1995 - val_loss: 3289.2058\n",
            "Epoch 4966/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2388.1484 - val_loss: 3289.0186\n",
            "Epoch 4967/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.9956 - val_loss: 3289.0168\n",
            "Epoch 4968/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.7302 - val_loss: 3288.8896\n",
            "Epoch 4969/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.6445 - val_loss: 3290.2739\n",
            "Epoch 4970/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2387.4946 - val_loss: 3293.0554\n",
            "Epoch 4971/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.0894 - val_loss: 3292.7610\n",
            "Epoch 4972/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.0371 - val_loss: 3290.3003\n",
            "Epoch 4973/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.6873 - val_loss: 3292.4092\n",
            "Epoch 4974/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.3787 - val_loss: 3293.2307\n",
            "Epoch 4975/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2386.8501 - val_loss: 3292.7317\n",
            "Epoch 4976/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.2537 - val_loss: 3293.1348\n",
            "Epoch 4977/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.1606 - val_loss: 3293.2107\n",
            "Epoch 4978/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.8186 - val_loss: 3293.1035\n",
            "Epoch 4979/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.7996 - val_loss: 3293.2627\n",
            "Epoch 4980/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.5308 - val_loss: 3292.7009\n",
            "Epoch 4981/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2388.0388 - val_loss: 3294.5696\n",
            "Epoch 4982/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2387.3179 - val_loss: 3292.4966\n",
            "Epoch 4983/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.0068 - val_loss: 3292.4365\n",
            "Epoch 4984/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.5930 - val_loss: 3293.5105\n",
            "Epoch 4985/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2386.2139 - val_loss: 3294.8447\n",
            "Epoch 4986/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.3752 - val_loss: 3299.5330\n",
            "Epoch 4987/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2390.2422 - val_loss: 3301.2102\n",
            "Epoch 4988/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.2817 - val_loss: 3298.6296\n",
            "Epoch 4989/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.4268 - val_loss: 3295.4358\n",
            "Epoch 4990/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.4868 - val_loss: 3295.5371\n",
            "Epoch 4991/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.3313 - val_loss: 3290.8630\n",
            "Epoch 4992/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.0483 - val_loss: 3290.0054\n",
            "Epoch 4993/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2387.4253 - val_loss: 3290.2781\n",
            "Epoch 4994/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.3193 - val_loss: 3294.0540\n",
            "Epoch 4995/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.6272 - val_loss: 3296.5337\n",
            "Epoch 4996/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.0254 - val_loss: 3296.0784\n",
            "Epoch 4997/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.2146 - val_loss: 3289.3420\n",
            "Epoch 4998/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2384.9961 - val_loss: 3284.2922\n",
            "Epoch 4999/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2387.6248 - val_loss: 3284.8223\n",
            "Epoch 5000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.3977 - val_loss: 3285.9128\n",
            "Epoch 5001/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2388.4724 - val_loss: 3289.6052\n",
            "Epoch 5002/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.6106 - val_loss: 3294.5457\n",
            "Epoch 5003/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.6443 - val_loss: 3297.5835\n",
            "Epoch 5004/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.1382 - val_loss: 3296.5713\n",
            "Epoch 5005/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.5911 - val_loss: 3295.6780\n",
            "Epoch 5006/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.6169 - val_loss: 3293.7566\n",
            "Epoch 5007/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.8430 - val_loss: 3291.3828\n",
            "Epoch 5008/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2386.1375 - val_loss: 3288.4746\n",
            "Epoch 5009/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.2996 - val_loss: 3287.2085\n",
            "Epoch 5010/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2385.2981 - val_loss: 3287.3953\n",
            "Epoch 5011/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.4829 - val_loss: 3288.3733\n",
            "Epoch 5012/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2386.1182 - val_loss: 3289.4497\n",
            "Epoch 5013/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2386.2273 - val_loss: 3292.1428\n",
            "Epoch 5014/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2384.6260 - val_loss: 3289.1948\n",
            "Epoch 5015/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2385.4514 - val_loss: 3292.0352\n",
            "Epoch 5016/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2385.9355 - val_loss: 3292.7139\n",
            "Epoch 5017/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.5898 - val_loss: 3288.4177\n",
            "Epoch 5018/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.8733 - val_loss: 3288.2034\n",
            "Epoch 5019/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.3147 - val_loss: 3288.0803\n",
            "Epoch 5020/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.8918 - val_loss: 3288.0657\n",
            "Epoch 5021/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2385.1912 - val_loss: 3287.8247\n",
            "Epoch 5022/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2386.0747 - val_loss: 3288.4092\n",
            "Epoch 5023/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.3811 - val_loss: 3290.0940\n",
            "Epoch 5024/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2386.1787 - val_loss: 3288.1819\n",
            "Epoch 5025/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.1587 - val_loss: 3290.1995\n",
            "Epoch 5026/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.7893 - val_loss: 3289.7500\n",
            "Epoch 5027/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2384.7998 - val_loss: 3288.7676\n",
            "Epoch 5028/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.5481 - val_loss: 3287.2329\n",
            "Epoch 5029/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.5396 - val_loss: 3286.1465\n",
            "Epoch 5030/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.0332 - val_loss: 3284.3286\n",
            "Epoch 5031/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.2373 - val_loss: 3285.5696\n",
            "Epoch 5032/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2387.5476 - val_loss: 3291.0200\n",
            "Epoch 5033/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2387.5278 - val_loss: 3294.2954\n",
            "Epoch 5034/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.5171 - val_loss: 3291.0503\n",
            "Epoch 5035/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.8271 - val_loss: 3290.2363\n",
            "Epoch 5036/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.0227 - val_loss: 3290.2578\n",
            "Epoch 5037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.8887 - val_loss: 3289.2002\n",
            "Epoch 5038/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.8083 - val_loss: 3288.1587\n",
            "Epoch 5039/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2385.0513 - val_loss: 3288.6362\n",
            "Epoch 5040/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.9944 - val_loss: 3289.4495\n",
            "Epoch 5041/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.2832 - val_loss: 3289.3188\n",
            "Epoch 5042/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.3391 - val_loss: 3288.0530\n",
            "Epoch 5043/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.9724 - val_loss: 3287.7903\n",
            "Epoch 5044/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2385.8147 - val_loss: 3284.1738\n",
            "Epoch 5045/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2385.8677 - val_loss: 3282.9263\n",
            "Epoch 5046/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2384.1384 - val_loss: 3283.6060\n",
            "Epoch 5047/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.3752 - val_loss: 3283.2908\n",
            "Epoch 5048/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2384.6196 - val_loss: 3284.6809\n",
            "Epoch 5049/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.3101 - val_loss: 3283.2058\n",
            "Epoch 5050/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.6260 - val_loss: 3285.7642\n",
            "Epoch 5051/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.4900 - val_loss: 3286.9360\n",
            "Epoch 5052/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.3403 - val_loss: 3288.6643\n",
            "Epoch 5053/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.6755 - val_loss: 3297.8840\n",
            "Epoch 5054/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2390.6055 - val_loss: 3301.8743\n",
            "Epoch 5055/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.9919 - val_loss: 3300.3894\n",
            "Epoch 5056/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2388.2957 - val_loss: 3295.5640\n",
            "Epoch 5057/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2387.3281 - val_loss: 3297.4517\n",
            "Epoch 5058/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2388.4006 - val_loss: 3293.1843\n",
            "Epoch 5059/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.4116 - val_loss: 3293.6665\n",
            "Epoch 5060/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2385.2915 - val_loss: 3293.6365\n",
            "Epoch 5061/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2384.3708 - val_loss: 3292.5254\n",
            "Epoch 5062/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.8770 - val_loss: 3290.9004\n",
            "Epoch 5063/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2383.9424 - val_loss: 3290.2595\n",
            "Epoch 5064/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2383.4651 - val_loss: 3290.1184\n",
            "Epoch 5065/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2383.3145 - val_loss: 3292.0583\n",
            "Epoch 5066/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.5935 - val_loss: 3292.6611\n",
            "Epoch 5067/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2384.3940 - val_loss: 3293.3708\n",
            "Epoch 5068/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.2822 - val_loss: 3297.4233\n",
            "Epoch 5069/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2384.6213 - val_loss: 3299.2581\n",
            "Epoch 5070/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2385.2329 - val_loss: 3302.6929\n",
            "Epoch 5071/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2390.3853 - val_loss: 3309.5232\n",
            "Epoch 5072/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2392.8450 - val_loss: 3310.2834\n",
            "Epoch 5073/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2391.0894 - val_loss: 3304.8784\n",
            "Epoch 5074/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2388.1814 - val_loss: 3302.7927\n",
            "Epoch 5075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.0942 - val_loss: 3300.0054\n",
            "Epoch 5076/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.5864 - val_loss: 3298.7021\n",
            "Epoch 5077/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.6472 - val_loss: 3300.2246\n",
            "Epoch 5078/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2385.2451 - val_loss: 3299.6780\n",
            "Epoch 5079/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2385.4526 - val_loss: 3298.3691\n",
            "Epoch 5080/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2383.5645 - val_loss: 3296.1169\n",
            "Epoch 5081/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.7412 - val_loss: 3292.7959\n",
            "Epoch 5082/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.3137 - val_loss: 3291.9490\n",
            "Epoch 5083/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.2915 - val_loss: 3293.0171\n",
            "Epoch 5084/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2383.6436 - val_loss: 3292.7341\n",
            "Epoch 5085/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2384.6035 - val_loss: 3289.5610\n",
            "Epoch 5086/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.0894 - val_loss: 3289.3406\n",
            "Epoch 5087/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.5969 - val_loss: 3291.1628\n",
            "Epoch 5088/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.2397 - val_loss: 3292.0808\n",
            "Epoch 5089/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.2690 - val_loss: 3298.0283\n",
            "Epoch 5090/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.7812 - val_loss: 3299.2402\n",
            "Epoch 5091/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2382.8955 - val_loss: 3299.0571\n",
            "Epoch 5092/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.2500 - val_loss: 3299.1899\n",
            "Epoch 5093/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.9429 - val_loss: 3298.5835\n",
            "Epoch 5094/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.6077 - val_loss: 3297.0857\n",
            "Epoch 5095/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.3650 - val_loss: 3299.4648\n",
            "Epoch 5096/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.7371 - val_loss: 3299.4551\n",
            "Epoch 5097/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2382.5645 - val_loss: 3300.9016\n",
            "Epoch 5098/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.6538 - val_loss: 3300.9641\n",
            "Epoch 5099/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.5957 - val_loss: 3302.8936\n",
            "Epoch 5100/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.9607 - val_loss: 3304.4431\n",
            "Epoch 5101/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.5881 - val_loss: 3304.7422\n",
            "Epoch 5102/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.1716 - val_loss: 3308.2234\n",
            "Epoch 5103/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2384.4004 - val_loss: 3303.5471\n",
            "Epoch 5104/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2384.1838 - val_loss: 3302.0168\n",
            "Epoch 5105/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.6409 - val_loss: 3300.6580\n",
            "Epoch 5106/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.3027 - val_loss: 3299.4429\n",
            "Epoch 5107/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.3394 - val_loss: 3304.2415\n",
            "Epoch 5108/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2384.3535 - val_loss: 3301.2222\n",
            "Epoch 5109/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.7322 - val_loss: 3300.8413\n",
            "Epoch 5110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.1975 - val_loss: 3299.0642\n",
            "Epoch 5111/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.0701 - val_loss: 3301.0266\n",
            "Epoch 5112/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.0798 - val_loss: 3301.3469\n",
            "Epoch 5113/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2383.0615 - val_loss: 3305.9551\n",
            "Epoch 5114/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.3269 - val_loss: 3302.3352\n",
            "Epoch 5115/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.0552 - val_loss: 3297.9441\n",
            "Epoch 5116/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.9399 - val_loss: 3296.2085\n",
            "Epoch 5117/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.5012 - val_loss: 3296.7441\n",
            "Epoch 5118/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2382.2607 - val_loss: 3298.7349\n",
            "Epoch 5119/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2386.3245 - val_loss: 3304.0635\n",
            "Epoch 5120/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2386.0088 - val_loss: 3304.9397\n",
            "Epoch 5121/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.3528 - val_loss: 3306.1257\n",
            "Epoch 5122/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2387.2397 - val_loss: 3304.9158\n",
            "Epoch 5123/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2386.9624 - val_loss: 3304.0474\n",
            "Epoch 5124/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2387.7471 - val_loss: 3299.8853\n",
            "Epoch 5125/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.4565 - val_loss: 3298.1704\n",
            "Epoch 5126/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.6968 - val_loss: 3298.4690\n",
            "Epoch 5127/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2386.9880 - val_loss: 3303.8879\n",
            "Epoch 5128/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2385.3779 - val_loss: 3302.6821\n",
            "Epoch 5129/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.5256 - val_loss: 3306.7861\n",
            "Epoch 5130/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2386.4231 - val_loss: 3315.5146\n",
            "Epoch 5131/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2390.9983 - val_loss: 3317.3477\n",
            "Epoch 5132/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2389.6741 - val_loss: 3312.0010\n",
            "Epoch 5133/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.8689 - val_loss: 3309.9353\n",
            "Epoch 5134/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2384.3276 - val_loss: 3304.5288\n",
            "Epoch 5135/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2382.1028 - val_loss: 3299.1921\n",
            "Epoch 5136/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.0613 - val_loss: 3297.1265\n",
            "Epoch 5137/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.1489 - val_loss: 3293.8511\n",
            "Epoch 5138/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.8997 - val_loss: 3294.7524\n",
            "Epoch 5139/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.3574 - val_loss: 3294.3564\n",
            "Epoch 5140/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2381.5925 - val_loss: 3294.8579\n",
            "Epoch 5141/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.9832 - val_loss: 3295.2410\n",
            "Epoch 5142/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.3682 - val_loss: 3295.3567\n",
            "Epoch 5143/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.2498 - val_loss: 3297.0295\n",
            "Epoch 5144/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.4680 - val_loss: 3298.3728\n",
            "Epoch 5145/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2380.8591 - val_loss: 3302.5046\n",
            "Epoch 5146/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.1221 - val_loss: 3304.5166\n",
            "Epoch 5147/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.0408 - val_loss: 3308.4983\n",
            "Epoch 5148/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.4900 - val_loss: 3307.7102\n",
            "Epoch 5149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.7861 - val_loss: 3304.2991\n",
            "Epoch 5150/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.3298 - val_loss: 3302.1106\n",
            "Epoch 5151/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.6326 - val_loss: 3301.1265\n",
            "Epoch 5152/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.0288 - val_loss: 3304.7205\n",
            "Epoch 5153/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2385.4702 - val_loss: 3303.7141\n",
            "Epoch 5154/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.9429 - val_loss: 3304.9465\n",
            "Epoch 5155/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2386.8167 - val_loss: 3307.6953\n",
            "Epoch 5156/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.8352 - val_loss: 3301.8799\n",
            "Epoch 5157/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2387.7439 - val_loss: 3297.5112\n",
            "Epoch 5158/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2392.3494 - val_loss: 3305.7317\n",
            "Epoch 5159/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2389.5076 - val_loss: 3303.5032\n",
            "Epoch 5160/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2388.8735 - val_loss: 3304.2798\n",
            "Epoch 5161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2388.5891 - val_loss: 3301.5898\n",
            "Epoch 5162/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2385.9302 - val_loss: 3294.1008\n",
            "Epoch 5163/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.9307 - val_loss: 3289.6042\n",
            "Epoch 5164/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.7620 - val_loss: 3287.1292\n",
            "Epoch 5165/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2380.7368 - val_loss: 3287.1675\n",
            "Epoch 5166/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.1697 - val_loss: 3291.7107\n",
            "Epoch 5167/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2381.2273 - val_loss: 3291.3174\n",
            "Epoch 5168/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.2881 - val_loss: 3291.5366\n",
            "Epoch 5169/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.5737 - val_loss: 3288.9399\n",
            "Epoch 5170/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2379.7000 - val_loss: 3287.6172\n",
            "Epoch 5171/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.4680 - val_loss: 3291.3428\n",
            "Epoch 5172/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.3210 - val_loss: 3291.5417\n",
            "Epoch 5173/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.5896 - val_loss: 3291.9346\n",
            "Epoch 5174/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2380.3174 - val_loss: 3293.4473\n",
            "Epoch 5175/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2380.3267 - val_loss: 3295.2698\n",
            "Epoch 5176/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.5105 - val_loss: 3293.9858\n",
            "Epoch 5177/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2381.0242 - val_loss: 3291.3005\n",
            "Epoch 5178/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.1094 - val_loss: 3289.3062\n",
            "Epoch 5179/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2380.3840 - val_loss: 3293.1221\n",
            "Epoch 5180/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2381.4189 - val_loss: 3294.4670\n",
            "Epoch 5181/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.3701 - val_loss: 3291.9111\n",
            "Epoch 5182/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.5984 - val_loss: 3291.5142\n",
            "Epoch 5183/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.2129 - val_loss: 3296.2009\n",
            "Epoch 5184/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2380.2112 - val_loss: 3297.9961\n",
            "Epoch 5185/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2379.5994 - val_loss: 3296.0835\n",
            "Epoch 5186/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.9458 - val_loss: 3296.4541\n",
            "Epoch 5187/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.7224 - val_loss: 3296.5359\n",
            "Epoch 5188/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.5645 - val_loss: 3301.6106\n",
            "Epoch 5189/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.8711 - val_loss: 3305.1934\n",
            "Epoch 5190/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2379.5457 - val_loss: 3309.9165\n",
            "Epoch 5191/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.7439 - val_loss: 3308.4744\n",
            "Epoch 5192/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.9028 - val_loss: 3307.1104\n",
            "Epoch 5193/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.8679 - val_loss: 3308.8635\n",
            "Epoch 5194/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.6924 - val_loss: 3307.5491\n",
            "Epoch 5195/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2381.4590 - val_loss: 3307.1724\n",
            "Epoch 5196/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.2944 - val_loss: 3306.4055\n",
            "Epoch 5197/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.0815 - val_loss: 3306.1941\n",
            "Epoch 5198/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.5854 - val_loss: 3307.6338\n",
            "Epoch 5199/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.2253 - val_loss: 3305.1201\n",
            "Epoch 5200/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2377.5254 - val_loss: 3300.7979\n",
            "Epoch 5201/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2379.5139 - val_loss: 3302.3989\n",
            "Epoch 5202/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.3630 - val_loss: 3302.0183\n",
            "Epoch 5203/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.5557 - val_loss: 3297.2380\n",
            "Epoch 5204/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.6013 - val_loss: 3296.3708\n",
            "Epoch 5205/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.1709 - val_loss: 3293.4983\n",
            "Epoch 5206/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.0032 - val_loss: 3292.9316\n",
            "Epoch 5207/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.5474 - val_loss: 3292.9016\n",
            "Epoch 5208/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.5496 - val_loss: 3293.6995\n",
            "Epoch 5209/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2382.4475 - val_loss: 3293.1025\n",
            "Epoch 5210/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2382.4729 - val_loss: 3292.7642\n",
            "Epoch 5211/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2386.3538 - val_loss: 3292.6934\n",
            "Epoch 5212/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2386.6611 - val_loss: 3290.3328\n",
            "Epoch 5213/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.7258 - val_loss: 3290.0225\n",
            "Epoch 5214/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.5593 - val_loss: 3289.7102\n",
            "Epoch 5215/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.3193 - val_loss: 3290.2588\n",
            "Epoch 5216/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.1672 - val_loss: 3292.7097\n",
            "Epoch 5217/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.1362 - val_loss: 3293.1675\n",
            "Epoch 5218/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2382.1306 - val_loss: 3293.9280\n",
            "Epoch 5219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.9565 - val_loss: 3294.5054\n",
            "Epoch 5220/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.6987 - val_loss: 3295.9521\n",
            "Epoch 5221/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2380.8665 - val_loss: 3295.9839\n",
            "Epoch 5222/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.0742 - val_loss: 3296.2683\n",
            "Epoch 5223/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.1162 - val_loss: 3300.1931\n",
            "Epoch 5224/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.2227 - val_loss: 3299.5784\n",
            "Epoch 5225/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.0908 - val_loss: 3299.5972\n",
            "Epoch 5226/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2379.3247 - val_loss: 3299.0168\n",
            "Epoch 5227/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.1819 - val_loss: 3298.0938\n",
            "Epoch 5228/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.1780 - val_loss: 3294.5427\n",
            "Epoch 5229/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.9146 - val_loss: 3295.1621\n",
            "Epoch 5230/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2384.4570 - val_loss: 3295.6208\n",
            "Epoch 5231/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.6528 - val_loss: 3295.6370\n",
            "Epoch 5232/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.2803 - val_loss: 3295.5466\n",
            "Epoch 5233/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.5833 - val_loss: 3295.1594\n",
            "Epoch 5234/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.9763 - val_loss: 3294.6050\n",
            "Epoch 5235/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.9553 - val_loss: 3289.1145\n",
            "Epoch 5236/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2379.2019 - val_loss: 3287.6577\n",
            "Epoch 5237/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.2378 - val_loss: 3286.5193\n",
            "Epoch 5238/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.0071 - val_loss: 3284.3550\n",
            "Epoch 5239/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2381.4341 - val_loss: 3284.1001\n",
            "Epoch 5240/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.5034 - val_loss: 3285.3726\n",
            "Epoch 5241/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2379.0515 - val_loss: 3285.6733\n",
            "Epoch 5242/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.1379 - val_loss: 3286.3525\n",
            "Epoch 5243/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.5193 - val_loss: 3286.5610\n",
            "Epoch 5244/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.5312 - val_loss: 3288.0200\n",
            "Epoch 5245/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.4480 - val_loss: 3288.9404\n",
            "Epoch 5246/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2379.8110 - val_loss: 3293.1008\n",
            "Epoch 5247/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.8352 - val_loss: 3294.5344\n",
            "Epoch 5248/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2383.3604 - val_loss: 3299.8638\n",
            "Epoch 5249/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2382.5994 - val_loss: 3299.0034\n",
            "Epoch 5250/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.7126 - val_loss: 3302.4414\n",
            "Epoch 5251/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2383.0459 - val_loss: 3303.4578\n",
            "Epoch 5252/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.2993 - val_loss: 3302.1135\n",
            "Epoch 5253/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.9050 - val_loss: 3302.2275\n",
            "Epoch 5254/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.6565 - val_loss: 3301.3718\n",
            "Epoch 5255/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2383.7349 - val_loss: 3303.6243\n",
            "Epoch 5256/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2382.6040 - val_loss: 3304.5710\n",
            "Epoch 5257/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2382.4446 - val_loss: 3304.9404\n",
            "Epoch 5258/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2383.0518 - val_loss: 3305.0874\n",
            "Epoch 5259/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2383.1570 - val_loss: 3303.8926\n",
            "Epoch 5260/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2382.9858 - val_loss: 3302.0479\n",
            "Epoch 5261/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2381.4253 - val_loss: 3294.0198\n",
            "Epoch 5262/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.5344 - val_loss: 3292.4912\n",
            "Epoch 5263/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.4785 - val_loss: 3297.3831\n",
            "Epoch 5264/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.5708 - val_loss: 3302.6257\n",
            "Epoch 5265/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.0088 - val_loss: 3302.4360\n",
            "Epoch 5266/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2380.1465 - val_loss: 3300.1323\n",
            "Epoch 5267/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.2336 - val_loss: 3292.4670\n",
            "Epoch 5268/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.4490 - val_loss: 3289.5842\n",
            "Epoch 5269/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2377.0803 - val_loss: 3288.3877\n",
            "Epoch 5270/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.5054 - val_loss: 3287.8252\n",
            "Epoch 5271/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2378.1123 - val_loss: 3289.1274\n",
            "Epoch 5272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.2495 - val_loss: 3289.2703\n",
            "Epoch 5273/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.8501 - val_loss: 3285.4221\n",
            "Epoch 5274/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.2603 - val_loss: 3284.3679\n",
            "Epoch 5275/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.4736 - val_loss: 3284.3372\n",
            "Epoch 5276/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2377.9417 - val_loss: 3283.0029\n",
            "Epoch 5277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.6602 - val_loss: 3283.9976\n",
            "Epoch 5278/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.7437 - val_loss: 3283.0454\n",
            "Epoch 5279/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.4492 - val_loss: 3280.6772\n",
            "Epoch 5280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.6885 - val_loss: 3279.6082\n",
            "Epoch 5281/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2378.7739 - val_loss: 3281.8694\n",
            "Epoch 5282/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.5454 - val_loss: 3284.9910\n",
            "Epoch 5283/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.4272 - val_loss: 3286.1746\n",
            "Epoch 5284/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.7173 - val_loss: 3285.7739\n",
            "Epoch 5285/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2377.6304 - val_loss: 3283.0322\n",
            "Epoch 5286/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2377.3728 - val_loss: 3283.2434\n",
            "Epoch 5287/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.5327 - val_loss: 3288.3484\n",
            "Epoch 5288/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.6033 - val_loss: 3290.3687\n",
            "Epoch 5289/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.7590 - val_loss: 3285.4573\n",
            "Epoch 5290/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.4810 - val_loss: 3284.4490\n",
            "Epoch 5291/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.2312 - val_loss: 3288.4717\n",
            "Epoch 5292/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.6111 - val_loss: 3290.2178\n",
            "Epoch 5293/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.0735 - val_loss: 3289.7358\n",
            "Epoch 5294/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.7468 - val_loss: 3290.1665\n",
            "Epoch 5295/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2377.5408 - val_loss: 3289.1467\n",
            "Epoch 5296/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2376.7769 - val_loss: 3289.2253\n",
            "Epoch 5297/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.3459 - val_loss: 3289.3259\n",
            "Epoch 5298/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.2922 - val_loss: 3287.8804\n",
            "Epoch 5299/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2376.7327 - val_loss: 3287.3147\n",
            "Epoch 5300/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.5107 - val_loss: 3287.5120\n",
            "Epoch 5301/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.0686 - val_loss: 3285.5940\n",
            "Epoch 5302/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2378.4414 - val_loss: 3285.2244\n",
            "Epoch 5303/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.6340 - val_loss: 3285.2190\n",
            "Epoch 5304/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2378.5139 - val_loss: 3285.6741\n",
            "Epoch 5305/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2379.0713 - val_loss: 3286.8735\n",
            "Epoch 5306/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.0632 - val_loss: 3287.6047\n",
            "Epoch 5307/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2377.1331 - val_loss: 3290.8928\n",
            "Epoch 5308/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.4348 - val_loss: 3291.1990\n",
            "Epoch 5309/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2376.2029 - val_loss: 3290.1089\n",
            "Epoch 5310/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.3862 - val_loss: 3292.5505\n",
            "Epoch 5311/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.8911 - val_loss: 3296.1448\n",
            "Epoch 5312/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.3608 - val_loss: 3297.5535\n",
            "Epoch 5313/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2376.2659 - val_loss: 3296.6809\n",
            "Epoch 5314/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.1045 - val_loss: 3295.8223\n",
            "Epoch 5315/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.1465 - val_loss: 3296.1499\n",
            "Epoch 5316/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2376.2136 - val_loss: 3296.2910\n",
            "Epoch 5317/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.4072 - val_loss: 3295.6416\n",
            "Epoch 5318/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2375.8542 - val_loss: 3295.4104\n",
            "Epoch 5319/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.7275 - val_loss: 3295.1091\n",
            "Epoch 5320/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.1125 - val_loss: 3297.2661\n",
            "Epoch 5321/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.7717 - val_loss: 3295.7637\n",
            "Epoch 5322/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2377.4663 - val_loss: 3294.9937\n",
            "Epoch 5323/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2376.5950 - val_loss: 3296.7009\n",
            "Epoch 5324/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.2515 - val_loss: 3297.1709\n",
            "Epoch 5325/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.8811 - val_loss: 3297.8940\n",
            "Epoch 5326/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.3149 - val_loss: 3300.2971\n",
            "Epoch 5327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.5376 - val_loss: 3303.6682\n",
            "Epoch 5328/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2376.1240 - val_loss: 3304.0603\n",
            "Epoch 5329/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.5469 - val_loss: 3300.6169\n",
            "Epoch 5330/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.5320 - val_loss: 3298.7563\n",
            "Epoch 5331/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.6982 - val_loss: 3297.9719\n",
            "Epoch 5332/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.2500 - val_loss: 3298.6819\n",
            "Epoch 5333/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2376.7715 - val_loss: 3299.3726\n",
            "Epoch 5334/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.2170 - val_loss: 3299.3687\n",
            "Epoch 5335/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.7090 - val_loss: 3299.3096\n",
            "Epoch 5336/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.2395 - val_loss: 3298.8254\n",
            "Epoch 5337/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.8894 - val_loss: 3300.2236\n",
            "Epoch 5338/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.8625 - val_loss: 3302.5706\n",
            "Epoch 5339/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.8367 - val_loss: 3301.2678\n",
            "Epoch 5340/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.1814 - val_loss: 3301.9646\n",
            "Epoch 5341/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2378.0718 - val_loss: 3301.7693\n",
            "Epoch 5342/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.9045 - val_loss: 3301.5928\n",
            "Epoch 5343/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2376.6062 - val_loss: 3298.5808\n",
            "Epoch 5344/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.3677 - val_loss: 3302.1106\n",
            "Epoch 5345/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.7288 - val_loss: 3306.6467\n",
            "Epoch 5346/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.9792 - val_loss: 3302.6846\n",
            "Epoch 5347/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2375.0818 - val_loss: 3297.2373\n",
            "Epoch 5348/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2374.5212 - val_loss: 3294.8911\n",
            "Epoch 5349/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.4224 - val_loss: 3295.0095\n",
            "Epoch 5350/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2375.8179 - val_loss: 3297.7891\n",
            "Epoch 5351/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.5447 - val_loss: 3297.1250\n",
            "Epoch 5352/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.4016 - val_loss: 3297.5696\n",
            "Epoch 5353/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2373.5977 - val_loss: 3301.4287\n",
            "Epoch 5354/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2376.3145 - val_loss: 3303.1685\n",
            "Epoch 5355/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.4207 - val_loss: 3302.3430\n",
            "Epoch 5356/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2375.9766 - val_loss: 3301.4297\n",
            "Epoch 5357/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.3879 - val_loss: 3300.9585\n",
            "Epoch 5358/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2374.8757 - val_loss: 3300.0186\n",
            "Epoch 5359/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.0325 - val_loss: 3299.4360\n",
            "Epoch 5360/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.0671 - val_loss: 3298.2710\n",
            "Epoch 5361/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.1663 - val_loss: 3298.1978\n",
            "Epoch 5362/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2375.4451 - val_loss: 3297.5208\n",
            "Epoch 5363/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2374.7852 - val_loss: 3296.9736\n",
            "Epoch 5364/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.9185 - val_loss: 3292.2190\n",
            "Epoch 5365/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.5945 - val_loss: 3291.0842\n",
            "Epoch 5366/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.4570 - val_loss: 3290.7217\n",
            "Epoch 5367/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.8162 - val_loss: 3290.1155\n",
            "Epoch 5368/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2374.9519 - val_loss: 3290.5969\n",
            "Epoch 5369/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.6750 - val_loss: 3287.9622\n",
            "Epoch 5370/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.6145 - val_loss: 3287.6523\n",
            "Epoch 5371/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.1658 - val_loss: 3287.6492\n",
            "Epoch 5372/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.2903 - val_loss: 3287.8542\n",
            "Epoch 5373/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2378.2871 - val_loss: 3287.6284\n",
            "Epoch 5374/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.5986 - val_loss: 3287.2747\n",
            "Epoch 5375/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2375.5093 - val_loss: 3290.1152\n",
            "Epoch 5376/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2373.9917 - val_loss: 3291.2966\n",
            "Epoch 5377/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.7444 - val_loss: 3292.7083\n",
            "Epoch 5378/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2373.7529 - val_loss: 3293.3469\n",
            "Epoch 5379/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.2954 - val_loss: 3295.8367\n",
            "Epoch 5380/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.9097 - val_loss: 3296.8035\n",
            "Epoch 5381/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.0212 - val_loss: 3295.8235\n",
            "Epoch 5382/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2375.6702 - val_loss: 3295.9390\n",
            "Epoch 5383/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2374.9792 - val_loss: 3298.2402\n",
            "Epoch 5384/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2381.7163 - val_loss: 3297.6946\n",
            "Epoch 5385/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2383.2034 - val_loss: 3297.4133\n",
            "Epoch 5386/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2381.1843 - val_loss: 3295.6685\n",
            "Epoch 5387/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.1951 - val_loss: 3298.3647\n",
            "Epoch 5388/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2374.7412 - val_loss: 3297.5603\n",
            "Epoch 5389/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.9893 - val_loss: 3294.9646\n",
            "Epoch 5390/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2375.7844 - val_loss: 3294.3096\n",
            "Epoch 5391/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.5435 - val_loss: 3297.3823\n",
            "Epoch 5392/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.3489 - val_loss: 3299.6833\n",
            "Epoch 5393/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2375.4253 - val_loss: 3300.3560\n",
            "Epoch 5394/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2375.9570 - val_loss: 3300.4927\n",
            "Epoch 5395/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.9131 - val_loss: 3301.3347\n",
            "Epoch 5396/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.3591 - val_loss: 3301.3838\n",
            "Epoch 5397/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2374.2864 - val_loss: 3301.5906\n",
            "Epoch 5398/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.5483 - val_loss: 3302.0415\n",
            "Epoch 5399/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.4604 - val_loss: 3302.9133\n",
            "Epoch 5400/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.3857 - val_loss: 3304.3828\n",
            "Epoch 5401/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.9219 - val_loss: 3304.7778\n",
            "Epoch 5402/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.3555 - val_loss: 3307.2273\n",
            "Epoch 5403/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2374.0403 - val_loss: 3306.2471\n",
            "Epoch 5404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.2432 - val_loss: 3305.8059\n",
            "Epoch 5405/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.8547 - val_loss: 3303.5771\n",
            "Epoch 5406/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2372.4231 - val_loss: 3308.3091\n",
            "Epoch 5407/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2376.2600 - val_loss: 3307.5916\n",
            "Epoch 5408/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2376.5740 - val_loss: 3312.3003\n",
            "Epoch 5409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.0525 - val_loss: 3315.9746\n",
            "Epoch 5410/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2377.1963 - val_loss: 3310.1453\n",
            "Epoch 5411/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.1589 - val_loss: 3311.1433\n",
            "Epoch 5412/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.7844 - val_loss: 3308.8328\n",
            "Epoch 5413/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2374.1440 - val_loss: 3308.5825\n",
            "Epoch 5414/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.2659 - val_loss: 3307.6162\n",
            "Epoch 5415/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.7097 - val_loss: 3306.8887\n",
            "Epoch 5416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.3877 - val_loss: 3307.1082\n",
            "Epoch 5417/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.3853 - val_loss: 3304.5271\n",
            "Epoch 5418/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2374.8923 - val_loss: 3305.5183\n",
            "Epoch 5419/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.5051 - val_loss: 3305.4634\n",
            "Epoch 5420/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2373.7104 - val_loss: 3307.8335\n",
            "Epoch 5421/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.9448 - val_loss: 3311.8557\n",
            "Epoch 5422/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.7036 - val_loss: 3313.4985\n",
            "Epoch 5423/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2377.0522 - val_loss: 3313.6755\n",
            "Epoch 5424/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.5356 - val_loss: 3309.8579\n",
            "Epoch 5425/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.1431 - val_loss: 3306.9978\n",
            "Epoch 5426/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.9377 - val_loss: 3310.3103\n",
            "Epoch 5427/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.2983 - val_loss: 3312.5422\n",
            "Epoch 5428/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.5571 - val_loss: 3311.7847\n",
            "Epoch 5429/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2375.4739 - val_loss: 3307.5586\n",
            "Epoch 5430/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2373.0735 - val_loss: 3304.5127\n",
            "Epoch 5431/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.1704 - val_loss: 3298.6580\n",
            "Epoch 5432/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.1868 - val_loss: 3297.5510\n",
            "Epoch 5433/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.7324 - val_loss: 3300.4858\n",
            "Epoch 5434/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.4067 - val_loss: 3301.3557\n",
            "Epoch 5435/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.0510 - val_loss: 3302.1040\n",
            "Epoch 5436/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.6875 - val_loss: 3303.1934\n",
            "Epoch 5437/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.6443 - val_loss: 3300.9697\n",
            "Epoch 5438/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.7966 - val_loss: 3300.6162\n",
            "Epoch 5439/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2372.9099 - val_loss: 3300.2605\n",
            "Epoch 5440/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.4343 - val_loss: 3299.1130\n",
            "Epoch 5441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.2847 - val_loss: 3297.6216\n",
            "Epoch 5442/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.2554 - val_loss: 3297.6025\n",
            "Epoch 5443/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.6987 - val_loss: 3296.3440\n",
            "Epoch 5444/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2371.8176 - val_loss: 3296.3230\n",
            "Epoch 5445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.3630 - val_loss: 3296.4065\n",
            "Epoch 5446/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.8960 - val_loss: 3297.3428\n",
            "Epoch 5447/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.3354 - val_loss: 3298.0168\n",
            "Epoch 5448/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.2456 - val_loss: 3299.3940\n",
            "Epoch 5449/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2372.3857 - val_loss: 3302.5142\n",
            "Epoch 5450/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.7451 - val_loss: 3298.1755\n",
            "Epoch 5451/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.9460 - val_loss: 3298.3953\n",
            "Epoch 5452/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.9573 - val_loss: 3304.2041\n",
            "Epoch 5453/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2373.7778 - val_loss: 3305.2356\n",
            "Epoch 5454/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2374.0652 - val_loss: 3305.8469\n",
            "Epoch 5455/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.0376 - val_loss: 3305.2163\n",
            "Epoch 5456/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.7258 - val_loss: 3302.3254\n",
            "Epoch 5457/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.5706 - val_loss: 3301.6523\n",
            "Epoch 5458/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2372.4263 - val_loss: 3301.0166\n",
            "Epoch 5459/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2372.3245 - val_loss: 3299.7524\n",
            "Epoch 5460/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.8115 - val_loss: 3299.9431\n",
            "Epoch 5461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.9011 - val_loss: 3297.8494\n",
            "Epoch 5462/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2372.5000 - val_loss: 3297.3093\n",
            "Epoch 5463/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.1536 - val_loss: 3296.8542\n",
            "Epoch 5464/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2372.3293 - val_loss: 3296.7659\n",
            "Epoch 5465/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.7766 - val_loss: 3298.1917\n",
            "Epoch 5466/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.9570 - val_loss: 3298.5698\n",
            "Epoch 5467/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.4302 - val_loss: 3297.8975\n",
            "Epoch 5468/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.6047 - val_loss: 3297.3215\n",
            "Epoch 5469/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.3076 - val_loss: 3301.3381\n",
            "Epoch 5470/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.5881 - val_loss: 3305.9214\n",
            "Epoch 5471/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.1575 - val_loss: 3306.9033\n",
            "Epoch 5472/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.4185 - val_loss: 3301.5591\n",
            "Epoch 5473/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.3816 - val_loss: 3298.8589\n",
            "Epoch 5474/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2372.7822 - val_loss: 3295.3352\n",
            "Epoch 5475/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2372.7852 - val_loss: 3294.4998\n",
            "Epoch 5476/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2372.4954 - val_loss: 3295.6685\n",
            "Epoch 5477/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.4282 - val_loss: 3296.2412\n",
            "Epoch 5478/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.9285 - val_loss: 3296.5288\n",
            "Epoch 5479/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2371.5520 - val_loss: 3297.6345\n",
            "Epoch 5480/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.8774 - val_loss: 3299.3767\n",
            "Epoch 5481/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2372.8320 - val_loss: 3299.9880\n",
            "Epoch 5482/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2373.6897 - val_loss: 3300.6716\n",
            "Epoch 5483/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.1357 - val_loss: 3301.1279\n",
            "Epoch 5484/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.1975 - val_loss: 3301.2891\n",
            "Epoch 5485/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.4204 - val_loss: 3300.7822\n",
            "Epoch 5486/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2372.1262 - val_loss: 3298.9224\n",
            "Epoch 5487/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2373.7578 - val_loss: 3297.7178\n",
            "Epoch 5488/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.3789 - val_loss: 3297.7114\n",
            "Epoch 5489/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.6138 - val_loss: 3301.0103\n",
            "Epoch 5490/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.2781 - val_loss: 3302.5535\n",
            "Epoch 5491/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2379.4800 - val_loss: 3303.3616\n",
            "Epoch 5492/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2378.8164 - val_loss: 3304.0994\n",
            "Epoch 5493/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2378.1294 - val_loss: 3304.2026\n",
            "Epoch 5494/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.0798 - val_loss: 3303.9858\n",
            "Epoch 5495/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.2551 - val_loss: 3303.8372\n",
            "Epoch 5496/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.1582 - val_loss: 3304.0266\n",
            "Epoch 5497/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2372.3784 - val_loss: 3302.7998\n",
            "Epoch 5498/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.6970 - val_loss: 3302.8550\n",
            "Epoch 5499/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.4614 - val_loss: 3302.8760\n",
            "Epoch 5500/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.8672 - val_loss: 3301.8083\n",
            "Epoch 5501/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2374.6865 - val_loss: 3300.3496\n",
            "Epoch 5502/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.2395 - val_loss: 3300.5745\n",
            "Epoch 5503/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.1646 - val_loss: 3302.9368\n",
            "Epoch 5504/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2370.5769 - val_loss: 3302.6846\n",
            "Epoch 5505/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.2703 - val_loss: 3301.7922\n",
            "Epoch 5506/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.8430 - val_loss: 3298.3079\n",
            "Epoch 5507/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2371.4766 - val_loss: 3297.9902\n",
            "Epoch 5508/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.5977 - val_loss: 3300.3911\n",
            "Epoch 5509/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.1802 - val_loss: 3300.6570\n",
            "Epoch 5510/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.5061 - val_loss: 3300.8733\n",
            "Epoch 5511/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.6833 - val_loss: 3296.4822\n",
            "Epoch 5512/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2373.4539 - val_loss: 3295.4265\n",
            "Epoch 5513/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.0647 - val_loss: 3294.9207\n",
            "Epoch 5514/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.4277 - val_loss: 3294.3809\n",
            "Epoch 5515/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.5371 - val_loss: 3297.8948\n",
            "Epoch 5516/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.0972 - val_loss: 3294.3247\n",
            "Epoch 5517/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.6943 - val_loss: 3295.6074\n",
            "Epoch 5518/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2372.4880 - val_loss: 3293.3792\n",
            "Epoch 5519/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.1233 - val_loss: 3293.7205\n",
            "Epoch 5520/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2371.3499 - val_loss: 3293.0088\n",
            "Epoch 5521/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.1218 - val_loss: 3291.8665\n",
            "Epoch 5522/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.8574 - val_loss: 3291.9902\n",
            "Epoch 5523/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.3511 - val_loss: 3291.2810\n",
            "Epoch 5524/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.1562 - val_loss: 3293.9717\n",
            "Epoch 5525/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.8057 - val_loss: 3294.9841\n",
            "Epoch 5526/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.9451 - val_loss: 3300.9800\n",
            "Epoch 5527/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.6460 - val_loss: 3302.9231\n",
            "Epoch 5528/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2370.6790 - val_loss: 3301.0994\n",
            "Epoch 5529/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.5725 - val_loss: 3298.5056\n",
            "Epoch 5530/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.5461 - val_loss: 3295.7107\n",
            "Epoch 5531/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.0959 - val_loss: 3294.9792\n",
            "Epoch 5532/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.7336 - val_loss: 3295.1475\n",
            "Epoch 5533/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2370.0073 - val_loss: 3295.0459\n",
            "Epoch 5534/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.9116 - val_loss: 3293.0256\n",
            "Epoch 5535/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.7400 - val_loss: 3290.0667\n",
            "Epoch 5536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.2891 - val_loss: 3289.7578\n",
            "Epoch 5537/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.9812 - val_loss: 3289.9270\n",
            "Epoch 5538/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2368.9807 - val_loss: 3289.7578\n",
            "Epoch 5539/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.4436 - val_loss: 3286.5042\n",
            "Epoch 5540/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2369.4629 - val_loss: 3286.6865\n",
            "Epoch 5541/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.0386 - val_loss: 3289.7942\n",
            "Epoch 5542/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.8857 - val_loss: 3286.9814\n",
            "Epoch 5543/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.8879 - val_loss: 3287.0266\n",
            "Epoch 5544/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.5515 - val_loss: 3287.3982\n",
            "Epoch 5545/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.8169 - val_loss: 3288.0383\n",
            "Epoch 5546/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.3713 - val_loss: 3290.9460\n",
            "Epoch 5547/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.6118 - val_loss: 3291.8511\n",
            "Epoch 5548/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2372.0840 - val_loss: 3292.4941\n",
            "Epoch 5549/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.3459 - val_loss: 3288.1660\n",
            "Epoch 5550/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.7278 - val_loss: 3292.3816\n",
            "Epoch 5551/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.3684 - val_loss: 3297.7412\n",
            "Epoch 5552/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.6836 - val_loss: 3300.0632\n",
            "Epoch 5553/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2371.9001 - val_loss: 3299.7131\n",
            "Epoch 5554/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.5803 - val_loss: 3299.7878\n",
            "Epoch 5555/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.6924 - val_loss: 3297.0857\n",
            "Epoch 5556/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.9434 - val_loss: 3293.2598\n",
            "Epoch 5557/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.0293 - val_loss: 3294.0923\n",
            "Epoch 5558/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2371.0203 - val_loss: 3297.6721\n",
            "Epoch 5559/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2370.4341 - val_loss: 3296.8552\n",
            "Epoch 5560/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2369.9546 - val_loss: 3296.0759\n",
            "Epoch 5561/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.5769 - val_loss: 3295.3181\n",
            "Epoch 5562/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2369.6760 - val_loss: 3294.2612\n",
            "Epoch 5563/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2370.0808 - val_loss: 3296.4116\n",
            "Epoch 5564/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.7041 - val_loss: 3295.3694\n",
            "Epoch 5565/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.2854 - val_loss: 3296.3438\n",
            "Epoch 5566/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2368.8262 - val_loss: 3295.7217\n",
            "Epoch 5567/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.7969 - val_loss: 3292.4712\n",
            "Epoch 5568/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.6040 - val_loss: 3292.5232\n",
            "Epoch 5569/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.3118 - val_loss: 3292.3013\n",
            "Epoch 5570/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.8643 - val_loss: 3289.8374\n",
            "Epoch 5571/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2371.1643 - val_loss: 3287.6963\n",
            "Epoch 5572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.6589 - val_loss: 3286.9993\n",
            "Epoch 5573/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2374.6343 - val_loss: 3287.0085\n",
            "Epoch 5574/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.7097 - val_loss: 3288.4031\n",
            "Epoch 5575/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2370.3928 - val_loss: 3290.4885\n",
            "Epoch 5576/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2368.5764 - val_loss: 3290.4497\n",
            "Epoch 5577/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.5879 - val_loss: 3290.0310\n",
            "Epoch 5578/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2369.1775 - val_loss: 3290.3174\n",
            "Epoch 5579/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.9722 - val_loss: 3290.4375\n",
            "Epoch 5580/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.4944 - val_loss: 3290.9502\n",
            "Epoch 5581/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2369.5681 - val_loss: 3291.6812\n",
            "Epoch 5582/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.0439 - val_loss: 3291.1772\n",
            "Epoch 5583/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.8499 - val_loss: 3292.2021\n",
            "Epoch 5584/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.1743 - val_loss: 3293.0327\n",
            "Epoch 5585/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.6755 - val_loss: 3293.9360\n",
            "Epoch 5586/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2367.9180 - val_loss: 3294.0938\n",
            "Epoch 5587/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2368.2532 - val_loss: 3293.6370\n",
            "Epoch 5588/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2367.6926 - val_loss: 3294.3086\n",
            "Epoch 5589/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.8972 - val_loss: 3294.8660\n",
            "Epoch 5590/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.5967 - val_loss: 3295.2307\n",
            "Epoch 5591/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2367.8960 - val_loss: 3295.1292\n",
            "Epoch 5592/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2368.0894 - val_loss: 3295.5120\n",
            "Epoch 5593/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.8997 - val_loss: 3294.6079\n",
            "Epoch 5594/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.8643 - val_loss: 3292.3647\n",
            "Epoch 5595/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.7361 - val_loss: 3294.0713\n",
            "Epoch 5596/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.6934 - val_loss: 3291.7566\n",
            "Epoch 5597/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2370.9817 - val_loss: 3291.5000\n",
            "Epoch 5598/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.0811 - val_loss: 3291.8799\n",
            "Epoch 5599/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.2551 - val_loss: 3292.7893\n",
            "Epoch 5600/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2368.1189 - val_loss: 3293.2766\n",
            "Epoch 5601/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2366.9358 - val_loss: 3290.5142\n",
            "Epoch 5602/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2369.3882 - val_loss: 3290.4810\n",
            "Epoch 5603/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2368.7197 - val_loss: 3290.6523\n",
            "Epoch 5604/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.8979 - val_loss: 3292.3174\n",
            "Epoch 5605/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.2224 - val_loss: 3287.9241\n",
            "Epoch 5606/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2367.7339 - val_loss: 3287.5591\n",
            "Epoch 5607/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.9348 - val_loss: 3288.1528\n",
            "Epoch 5608/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2368.0647 - val_loss: 3288.8799\n",
            "Epoch 5609/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.9136 - val_loss: 3288.4751\n",
            "Epoch 5610/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.6934 - val_loss: 3288.3765\n",
            "Epoch 5611/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2367.5989 - val_loss: 3285.9597\n",
            "Epoch 5612/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.3123 - val_loss: 3285.5679\n",
            "Epoch 5613/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2368.4990 - val_loss: 3284.7615\n",
            "Epoch 5614/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.5361 - val_loss: 3284.9543\n",
            "Epoch 5615/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.4114 - val_loss: 3282.9741\n",
            "Epoch 5616/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.7561 - val_loss: 3283.4070\n",
            "Epoch 5617/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2367.0442 - val_loss: 3286.3328\n",
            "Epoch 5618/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.8289 - val_loss: 3289.2612\n",
            "Epoch 5619/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2369.9773 - val_loss: 3290.4087\n",
            "Epoch 5620/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2369.5088 - val_loss: 3290.1418\n",
            "Epoch 5621/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.8557 - val_loss: 3290.2227\n",
            "Epoch 5622/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.1162 - val_loss: 3291.6790\n",
            "Epoch 5623/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.1848 - val_loss: 3291.2773\n",
            "Epoch 5624/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2370.6589 - val_loss: 3294.4976\n",
            "Epoch 5625/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2370.2434 - val_loss: 3297.9382\n",
            "Epoch 5626/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2368.5442 - val_loss: 3293.9685\n",
            "Epoch 5627/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2367.9260 - val_loss: 3292.3188\n",
            "Epoch 5628/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.2170 - val_loss: 3297.9009\n",
            "Epoch 5629/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.2786 - val_loss: 3300.3860\n",
            "Epoch 5630/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2372.4475 - val_loss: 3305.3484\n",
            "Epoch 5631/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2377.8650 - val_loss: 3309.9087\n",
            "Epoch 5632/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2376.9487 - val_loss: 3311.4465\n",
            "Epoch 5633/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2380.3496 - val_loss: 3318.6497\n",
            "Epoch 5634/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2382.1438 - val_loss: 3317.5063\n",
            "Epoch 5635/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2380.5813 - val_loss: 3312.5671\n",
            "Epoch 5636/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2374.9951 - val_loss: 3302.1169\n",
            "Epoch 5637/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.4558 - val_loss: 3298.2878\n",
            "Epoch 5638/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.4055 - val_loss: 3306.3789\n",
            "Epoch 5639/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2377.2212 - val_loss: 3309.1194\n",
            "Epoch 5640/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2379.0493 - val_loss: 3307.4177\n",
            "Epoch 5641/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2375.1604 - val_loss: 3299.1145\n",
            "Epoch 5642/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2370.1892 - val_loss: 3293.6257\n",
            "Epoch 5643/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.2539 - val_loss: 3287.0603\n",
            "Epoch 5644/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.4080 - val_loss: 3285.7366\n",
            "Epoch 5645/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.6323 - val_loss: 3283.5464\n",
            "Epoch 5646/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.1169 - val_loss: 3283.5505\n",
            "Epoch 5647/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2370.5398 - val_loss: 3284.0728\n",
            "Epoch 5648/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.9453 - val_loss: 3284.9060\n",
            "Epoch 5649/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2371.0156 - val_loss: 3284.9902\n",
            "Epoch 5650/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.0391 - val_loss: 3285.1348\n",
            "Epoch 5651/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2368.4929 - val_loss: 3285.4087\n",
            "Epoch 5652/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2368.7844 - val_loss: 3286.0857\n",
            "Epoch 5653/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.6465 - val_loss: 3286.3960\n",
            "Epoch 5654/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.2100 - val_loss: 3287.2595\n",
            "Epoch 5655/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.6963 - val_loss: 3287.9832\n",
            "Epoch 5656/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2369.6704 - val_loss: 3289.1965\n",
            "Epoch 5657/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.9180 - val_loss: 3289.7222\n",
            "Epoch 5658/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.2825 - val_loss: 3290.4971\n",
            "Epoch 5659/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.6562 - val_loss: 3290.5771\n",
            "Epoch 5660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.7029 - val_loss: 3291.4207\n",
            "Epoch 5661/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2365.9790 - val_loss: 3290.6731\n",
            "Epoch 5662/10000\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 2368.4556 - val_loss: 3288.1841\n",
            "Epoch 5663/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.9358 - val_loss: 3288.8687\n",
            "Epoch 5664/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.3455 - val_loss: 3288.6292\n",
            "Epoch 5665/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.5232 - val_loss: 3288.5527\n",
            "Epoch 5666/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.6206 - val_loss: 3287.6770\n",
            "Epoch 5667/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2368.8083 - val_loss: 3287.6777\n",
            "Epoch 5668/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2369.2800 - val_loss: 3287.9480\n",
            "Epoch 5669/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.5784 - val_loss: 3288.2935\n",
            "Epoch 5670/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2373.0186 - val_loss: 3288.6882\n",
            "Epoch 5671/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.6357 - val_loss: 3288.4304\n",
            "Epoch 5672/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2373.1531 - val_loss: 3287.9929\n",
            "Epoch 5673/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2373.3062 - val_loss: 3287.4126\n",
            "Epoch 5674/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2370.9802 - val_loss: 3288.0259\n",
            "Epoch 5675/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2368.9578 - val_loss: 3289.3179\n",
            "Epoch 5676/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.0024 - val_loss: 3290.0146\n",
            "Epoch 5677/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.9871 - val_loss: 3290.7163\n",
            "Epoch 5678/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2368.0730 - val_loss: 3288.7869\n",
            "Epoch 5679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.3840 - val_loss: 3286.2292\n",
            "Epoch 5680/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2368.0603 - val_loss: 3283.4382\n",
            "Epoch 5681/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.6907 - val_loss: 3284.2402\n",
            "Epoch 5682/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.2439 - val_loss: 3283.4382\n",
            "Epoch 5683/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.7698 - val_loss: 3281.0024\n",
            "Epoch 5684/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2371.1797 - val_loss: 3280.9546\n",
            "Epoch 5685/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2372.5015 - val_loss: 3281.7878\n",
            "Epoch 5686/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.4287 - val_loss: 3281.4697\n",
            "Epoch 5687/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.9680 - val_loss: 3281.2717\n",
            "Epoch 5688/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.1213 - val_loss: 3280.6018\n",
            "Epoch 5689/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2366.5312 - val_loss: 3281.4517\n",
            "Epoch 5690/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2364.9675 - val_loss: 3282.4746\n",
            "Epoch 5691/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.1494 - val_loss: 3282.4729\n",
            "Epoch 5692/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.1948 - val_loss: 3282.4634\n",
            "Epoch 5693/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2364.5488 - val_loss: 3283.1841\n",
            "Epoch 5694/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2365.9307 - val_loss: 3284.3953\n",
            "Epoch 5695/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2364.7046 - val_loss: 3284.4622\n",
            "Epoch 5696/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.5977 - val_loss: 3281.6514\n",
            "Epoch 5697/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.8010 - val_loss: 3281.8301\n",
            "Epoch 5698/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2371.0454 - val_loss: 3282.1458\n",
            "Epoch 5699/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2373.5422 - val_loss: 3280.7021\n",
            "Epoch 5700/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2370.2156 - val_loss: 3281.0142\n",
            "Epoch 5701/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.1150 - val_loss: 3281.2158\n",
            "Epoch 5702/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.9419 - val_loss: 3282.3318\n",
            "Epoch 5703/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2364.7500 - val_loss: 3283.5879\n",
            "Epoch 5704/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2364.7400 - val_loss: 3285.0737\n",
            "Epoch 5705/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.2144 - val_loss: 3284.4316\n",
            "Epoch 5706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2364.9778 - val_loss: 3286.1904\n",
            "Epoch 5707/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2365.3469 - val_loss: 3284.3523\n",
            "Epoch 5708/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2365.4502 - val_loss: 3283.1721\n",
            "Epoch 5709/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.5459 - val_loss: 3283.4197\n",
            "Epoch 5710/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.3374 - val_loss: 3286.0530\n",
            "Epoch 5711/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.7898 - val_loss: 3285.2275\n",
            "Epoch 5712/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.9946 - val_loss: 3284.6140\n",
            "Epoch 5713/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.9817 - val_loss: 3285.4871\n",
            "Epoch 5714/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2371.0754 - val_loss: 3285.1292\n",
            "Epoch 5715/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2370.3521 - val_loss: 3285.7756\n",
            "Epoch 5716/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2375.2354 - val_loss: 3286.8669\n",
            "Epoch 5717/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2375.4302 - val_loss: 3286.5151\n",
            "Epoch 5718/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2372.6199 - val_loss: 3286.9446\n",
            "Epoch 5719/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2371.1028 - val_loss: 3286.9944\n",
            "Epoch 5720/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2368.7905 - val_loss: 3286.8967\n",
            "Epoch 5721/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.2817 - val_loss: 3290.8950\n",
            "Epoch 5722/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2364.2329 - val_loss: 3293.0608\n",
            "Epoch 5723/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.5925 - val_loss: 3294.7324\n",
            "Epoch 5724/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2369.8105 - val_loss: 3302.3862\n",
            "Epoch 5725/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2368.6436 - val_loss: 3303.0415\n",
            "Epoch 5726/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2368.1982 - val_loss: 3300.2717\n",
            "Epoch 5727/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2367.3167 - val_loss: 3298.8267\n",
            "Epoch 5728/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.8940 - val_loss: 3296.0310\n",
            "Epoch 5729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.8232 - val_loss: 3295.4690\n",
            "Epoch 5730/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.0959 - val_loss: 3296.7490\n",
            "Epoch 5731/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.5195 - val_loss: 3293.6636\n",
            "Epoch 5732/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.6914 - val_loss: 3292.6060\n",
            "Epoch 5733/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.6106 - val_loss: 3292.6387\n",
            "Epoch 5734/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.2883 - val_loss: 3292.8596\n",
            "Epoch 5735/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2366.3674 - val_loss: 3293.6328\n",
            "Epoch 5736/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2367.0640 - val_loss: 3294.3472\n",
            "Epoch 5737/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2367.8755 - val_loss: 3296.6257\n",
            "Epoch 5738/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2366.0112 - val_loss: 3296.9690\n",
            "Epoch 5739/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.5640 - val_loss: 3298.0793\n",
            "Epoch 5740/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.4949 - val_loss: 3300.9529\n",
            "Epoch 5741/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2367.0222 - val_loss: 3303.1973\n",
            "Epoch 5742/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.8743 - val_loss: 3303.7947\n",
            "Epoch 5743/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.0593 - val_loss: 3302.1379\n",
            "Epoch 5744/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.6016 - val_loss: 3304.4214\n",
            "Epoch 5745/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.9185 - val_loss: 3303.3901\n",
            "Epoch 5746/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.3115 - val_loss: 3303.8511\n",
            "Epoch 5747/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.6055 - val_loss: 3306.3772\n",
            "Epoch 5748/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.5896 - val_loss: 3306.5034\n",
            "Epoch 5749/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2367.1331 - val_loss: 3308.4885\n",
            "Epoch 5750/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.7175 - val_loss: 3302.7834\n",
            "Epoch 5751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.6091 - val_loss: 3299.5752\n",
            "Epoch 5752/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.1990 - val_loss: 3294.6465\n",
            "Epoch 5753/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2363.1665 - val_loss: 3293.2644\n",
            "Epoch 5754/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.2600 - val_loss: 3292.3408\n",
            "Epoch 5755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2364.6897 - val_loss: 3295.4304\n",
            "Epoch 5756/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.0898 - val_loss: 3297.4290\n",
            "Epoch 5757/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2369.7266 - val_loss: 3298.1003\n",
            "Epoch 5758/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2368.6553 - val_loss: 3298.8215\n",
            "Epoch 5759/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2365.9626 - val_loss: 3299.5078\n",
            "Epoch 5760/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2364.5161 - val_loss: 3299.8940\n",
            "Epoch 5761/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2363.4302 - val_loss: 3301.8940\n",
            "Epoch 5762/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2363.1343 - val_loss: 3308.8525\n",
            "Epoch 5763/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.5718 - val_loss: 3313.8691\n",
            "Epoch 5764/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2368.8586 - val_loss: 3312.2380\n",
            "Epoch 5765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.7219 - val_loss: 3314.3142\n",
            "Epoch 5766/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2369.6836 - val_loss: 3316.5168\n",
            "Epoch 5767/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2370.7793 - val_loss: 3320.0176\n",
            "Epoch 5768/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2371.3899 - val_loss: 3324.9609\n",
            "Epoch 5769/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2372.5864 - val_loss: 3322.0801\n",
            "Epoch 5770/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2369.7966 - val_loss: 3317.3496\n",
            "Epoch 5771/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.3792 - val_loss: 3313.7358\n",
            "Epoch 5772/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2363.7217 - val_loss: 3310.6235\n",
            "Epoch 5773/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.7568 - val_loss: 3309.6450\n",
            "Epoch 5774/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.6233 - val_loss: 3309.4578\n",
            "Epoch 5775/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.9739 - val_loss: 3308.4277\n",
            "Epoch 5776/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.6394 - val_loss: 3308.5520\n",
            "Epoch 5777/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2362.5596 - val_loss: 3308.5247\n",
            "Epoch 5778/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2362.3748 - val_loss: 3308.4634\n",
            "Epoch 5779/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.3394 - val_loss: 3309.6965\n",
            "Epoch 5780/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2363.1763 - val_loss: 3313.7053\n",
            "Epoch 5781/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2363.5305 - val_loss: 3314.1011\n",
            "Epoch 5782/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2363.2361 - val_loss: 3312.5547\n",
            "Epoch 5783/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2363.1721 - val_loss: 3307.7009\n",
            "Epoch 5784/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.8174 - val_loss: 3306.1865\n",
            "Epoch 5785/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2361.9919 - val_loss: 3307.4280\n",
            "Epoch 5786/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2361.8540 - val_loss: 3306.7676\n",
            "Epoch 5787/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2361.8452 - val_loss: 3305.8984\n",
            "Epoch 5788/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.6123 - val_loss: 3305.0996\n",
            "Epoch 5789/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2362.3413 - val_loss: 3305.7222\n",
            "Epoch 5790/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.5930 - val_loss: 3308.5962\n",
            "Epoch 5791/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.2070 - val_loss: 3310.0315\n",
            "Epoch 5792/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2362.2651 - val_loss: 3312.8816\n",
            "Epoch 5793/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.1057 - val_loss: 3311.5190\n",
            "Epoch 5794/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2361.5364 - val_loss: 3312.4092\n",
            "Epoch 5795/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.7876 - val_loss: 3313.2036\n",
            "Epoch 5796/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2361.2529 - val_loss: 3311.1265\n",
            "Epoch 5797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.5112 - val_loss: 3309.3174\n",
            "Epoch 5798/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.9468 - val_loss: 3308.4648\n",
            "Epoch 5799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.7717 - val_loss: 3308.9343\n",
            "Epoch 5800/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2363.0454 - val_loss: 3310.3870\n",
            "Epoch 5801/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.9250 - val_loss: 3310.9790\n",
            "Epoch 5802/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.2839 - val_loss: 3308.5198\n",
            "Epoch 5803/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.4385 - val_loss: 3306.1145\n",
            "Epoch 5804/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2367.9229 - val_loss: 3305.3279\n",
            "Epoch 5805/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2367.0842 - val_loss: 3306.7268\n",
            "Epoch 5806/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.6919 - val_loss: 3307.5447\n",
            "Epoch 5807/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2363.1001 - val_loss: 3308.8013\n",
            "Epoch 5808/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2362.2366 - val_loss: 3307.4685\n",
            "Epoch 5809/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.9055 - val_loss: 3307.5203\n",
            "Epoch 5810/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.2063 - val_loss: 3306.6282\n",
            "Epoch 5811/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2362.6143 - val_loss: 3306.4966\n",
            "Epoch 5812/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.6130 - val_loss: 3304.0210\n",
            "Epoch 5813/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2363.1292 - val_loss: 3303.5112\n",
            "Epoch 5814/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2363.4255 - val_loss: 3302.7710\n",
            "Epoch 5815/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2364.1602 - val_loss: 3300.7412\n",
            "Epoch 5816/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2361.8582 - val_loss: 3300.6194\n",
            "Epoch 5817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.2034 - val_loss: 3301.1064\n",
            "Epoch 5818/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.3799 - val_loss: 3301.7690\n",
            "Epoch 5819/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.7144 - val_loss: 3302.5007\n",
            "Epoch 5820/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2365.0500 - val_loss: 3306.3525\n",
            "Epoch 5821/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.9133 - val_loss: 3304.2756\n",
            "Epoch 5822/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.7656 - val_loss: 3304.7678\n",
            "Epoch 5823/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.6975 - val_loss: 3304.8518\n",
            "Epoch 5824/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.9785 - val_loss: 3305.3718\n",
            "Epoch 5825/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.3335 - val_loss: 3305.6145\n",
            "Epoch 5826/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.1875 - val_loss: 3308.4497\n",
            "Epoch 5827/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2365.7117 - val_loss: 3311.2458\n",
            "Epoch 5828/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.9282 - val_loss: 3310.6265\n",
            "Epoch 5829/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.4924 - val_loss: 3309.3027\n",
            "Epoch 5830/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2364.5520 - val_loss: 3307.5337\n",
            "Epoch 5831/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.3940 - val_loss: 3308.7781\n",
            "Epoch 5832/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2363.8481 - val_loss: 3309.4592\n",
            "Epoch 5833/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2364.0269 - val_loss: 3309.3877\n",
            "Epoch 5834/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2365.4070 - val_loss: 3310.5491\n",
            "Epoch 5835/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.3625 - val_loss: 3308.1355\n",
            "Epoch 5836/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2363.5149 - val_loss: 3305.4465\n",
            "Epoch 5837/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2363.8750 - val_loss: 3307.0464\n",
            "Epoch 5838/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2366.9448 - val_loss: 3313.4421\n",
            "Epoch 5839/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2368.7119 - val_loss: 3312.2036\n",
            "Epoch 5840/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.2246 - val_loss: 3310.3809\n",
            "Epoch 5841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2364.7705 - val_loss: 3309.2869\n",
            "Epoch 5842/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.7471 - val_loss: 3310.0579\n",
            "Epoch 5843/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.1589 - val_loss: 3309.3467\n",
            "Epoch 5844/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2366.5635 - val_loss: 3303.2764\n",
            "Epoch 5845/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.6838 - val_loss: 3302.6267\n",
            "Epoch 5846/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.7317 - val_loss: 3303.4617\n",
            "Epoch 5847/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.6646 - val_loss: 3301.4741\n",
            "Epoch 5848/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.3960 - val_loss: 3300.8091\n",
            "Epoch 5849/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.8640 - val_loss: 3302.5608\n",
            "Epoch 5850/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2361.6716 - val_loss: 3302.6179\n",
            "Epoch 5851/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.5154 - val_loss: 3302.6653\n",
            "Epoch 5852/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.4302 - val_loss: 3301.9941\n",
            "Epoch 5853/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.2239 - val_loss: 3299.6892\n",
            "Epoch 5854/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.6917 - val_loss: 3297.8940\n",
            "Epoch 5855/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.9539 - val_loss: 3301.2490\n",
            "Epoch 5856/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2360.4009 - val_loss: 3300.6228\n",
            "Epoch 5857/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.4736 - val_loss: 3299.9873\n",
            "Epoch 5858/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.7581 - val_loss: 3302.2212\n",
            "Epoch 5859/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.5359 - val_loss: 3303.3809\n",
            "Epoch 5860/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2359.8018 - val_loss: 3305.3093\n",
            "Epoch 5861/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.6174 - val_loss: 3304.0393\n",
            "Epoch 5862/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.9895 - val_loss: 3306.0278\n",
            "Epoch 5863/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2359.7258 - val_loss: 3306.8230\n",
            "Epoch 5864/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.8857 - val_loss: 3307.2615\n",
            "Epoch 5865/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.6829 - val_loss: 3307.1047\n",
            "Epoch 5866/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2359.6382 - val_loss: 3306.3003\n",
            "Epoch 5867/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.0396 - val_loss: 3308.6123\n",
            "Epoch 5868/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.6902 - val_loss: 3306.6658\n",
            "Epoch 5869/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2359.8250 - val_loss: 3305.3147\n",
            "Epoch 5870/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.3223 - val_loss: 3307.8247\n",
            "Epoch 5871/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2361.5569 - val_loss: 3307.9685\n",
            "Epoch 5872/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.0679 - val_loss: 3307.5723\n",
            "Epoch 5873/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2361.6755 - val_loss: 3310.9465\n",
            "Epoch 5874/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2364.1929 - val_loss: 3311.7654\n",
            "Epoch 5875/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.4971 - val_loss: 3307.5481\n",
            "Epoch 5876/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2360.6067 - val_loss: 3306.1091\n",
            "Epoch 5877/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.5950 - val_loss: 3309.2197\n",
            "Epoch 5878/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.4468 - val_loss: 3304.9971\n",
            "Epoch 5879/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.3367 - val_loss: 3303.5920\n",
            "Epoch 5880/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.9500 - val_loss: 3304.2019\n",
            "Epoch 5881/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2359.6370 - val_loss: 3304.8860\n",
            "Epoch 5882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.4094 - val_loss: 3304.2446\n",
            "Epoch 5883/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.3586 - val_loss: 3302.5579\n",
            "Epoch 5884/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.9988 - val_loss: 3302.4978\n",
            "Epoch 5885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.9822 - val_loss: 3302.7581\n",
            "Epoch 5886/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2359.5273 - val_loss: 3302.9565\n",
            "Epoch 5887/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.2720 - val_loss: 3303.7222\n",
            "Epoch 5888/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.3208 - val_loss: 3302.0259\n",
            "Epoch 5889/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.4026 - val_loss: 3301.2219\n",
            "Epoch 5890/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.7007 - val_loss: 3299.9712\n",
            "Epoch 5891/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2360.4827 - val_loss: 3298.9624\n",
            "Epoch 5892/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.3511 - val_loss: 3298.3301\n",
            "Epoch 5893/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.4395 - val_loss: 3298.4407\n",
            "Epoch 5894/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.5820 - val_loss: 3298.8835\n",
            "Epoch 5895/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2364.4746 - val_loss: 3299.6489\n",
            "Epoch 5896/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2359.8557 - val_loss: 3301.1682\n",
            "Epoch 5897/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.7883 - val_loss: 3302.1145\n",
            "Epoch 5898/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2359.1929 - val_loss: 3302.7998\n",
            "Epoch 5899/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.7219 - val_loss: 3303.1777\n",
            "Epoch 5900/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.1003 - val_loss: 3303.9424\n",
            "Epoch 5901/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.3728 - val_loss: 3303.2090\n",
            "Epoch 5902/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2363.3560 - val_loss: 3303.2378\n",
            "Epoch 5903/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2365.2563 - val_loss: 3301.7524\n",
            "Epoch 5904/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.6035 - val_loss: 3301.1924\n",
            "Epoch 5905/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.3057 - val_loss: 3301.0154\n",
            "Epoch 5906/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2363.0747 - val_loss: 3302.3691\n",
            "Epoch 5907/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2363.5886 - val_loss: 3302.1602\n",
            "Epoch 5908/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.2820 - val_loss: 3303.6572\n",
            "Epoch 5909/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.8345 - val_loss: 3304.4634\n",
            "Epoch 5910/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2359.1328 - val_loss: 3310.0635\n",
            "Epoch 5911/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.1826 - val_loss: 3315.2332\n",
            "Epoch 5912/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2360.4556 - val_loss: 3316.5835\n",
            "Epoch 5913/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.4680 - val_loss: 3316.3896\n",
            "Epoch 5914/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.9268 - val_loss: 3315.3750\n",
            "Epoch 5915/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.3337 - val_loss: 3313.7297\n",
            "Epoch 5916/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.3667 - val_loss: 3311.3792\n",
            "Epoch 5917/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.1814 - val_loss: 3311.0190\n",
            "Epoch 5918/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.8147 - val_loss: 3311.5889\n",
            "Epoch 5919/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.0581 - val_loss: 3316.2798\n",
            "Epoch 5920/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2359.1086 - val_loss: 3318.2654\n",
            "Epoch 5921/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2358.8408 - val_loss: 3318.6404\n",
            "Epoch 5922/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.8542 - val_loss: 3318.4741\n",
            "Epoch 5923/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.3857 - val_loss: 3311.7300\n",
            "Epoch 5924/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.9578 - val_loss: 3307.8848\n",
            "Epoch 5925/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2361.2161 - val_loss: 3304.7046\n",
            "Epoch 5926/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2362.8892 - val_loss: 3303.4448\n",
            "Epoch 5927/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2363.6892 - val_loss: 3302.5759\n",
            "Epoch 5928/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.1174 - val_loss: 3302.3132\n",
            "Epoch 5929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.5842 - val_loss: 3302.2429\n",
            "Epoch 5930/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.0293 - val_loss: 3302.0916\n",
            "Epoch 5931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.8574 - val_loss: 3301.9509\n",
            "Epoch 5932/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.2710 - val_loss: 3302.1755\n",
            "Epoch 5933/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2364.3743 - val_loss: 3300.8223\n",
            "Epoch 5934/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2365.5598 - val_loss: 3300.5442\n",
            "Epoch 5935/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.8311 - val_loss: 3301.5994\n",
            "Epoch 5936/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2361.4463 - val_loss: 3303.3630\n",
            "Epoch 5937/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2361.7065 - val_loss: 3304.0481\n",
            "Epoch 5938/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.4438 - val_loss: 3304.7893\n",
            "Epoch 5939/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.7205 - val_loss: 3306.2634\n",
            "Epoch 5940/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2357.9832 - val_loss: 3304.9678\n",
            "Epoch 5941/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.3960 - val_loss: 3303.8879\n",
            "Epoch 5942/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2359.2424 - val_loss: 3303.0254\n",
            "Epoch 5943/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.2278 - val_loss: 3299.2366\n",
            "Epoch 5944/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.2710 - val_loss: 3299.7017\n",
            "Epoch 5945/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.4380 - val_loss: 3302.9602\n",
            "Epoch 5946/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2358.8826 - val_loss: 3303.9951\n",
            "Epoch 5947/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.2092 - val_loss: 3303.5273\n",
            "Epoch 5948/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.5427 - val_loss: 3306.1733\n",
            "Epoch 5949/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.7249 - val_loss: 3301.3916\n",
            "Epoch 5950/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2358.1980 - val_loss: 3302.9094\n",
            "Epoch 5951/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.8889 - val_loss: 3302.4038\n",
            "Epoch 5952/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.2117 - val_loss: 3302.3467\n",
            "Epoch 5953/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.2527 - val_loss: 3302.4736\n",
            "Epoch 5954/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2358.7180 - val_loss: 3301.2412\n",
            "Epoch 5955/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.7380 - val_loss: 3301.6108\n",
            "Epoch 5956/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.4146 - val_loss: 3301.1921\n",
            "Epoch 5957/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.0750 - val_loss: 3304.5481\n",
            "Epoch 5958/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2358.9868 - val_loss: 3308.7756\n",
            "Epoch 5959/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2359.8083 - val_loss: 3309.8918\n",
            "Epoch 5960/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.3005 - val_loss: 3306.9529\n",
            "Epoch 5961/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.7358 - val_loss: 3310.7810\n",
            "Epoch 5962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.6140 - val_loss: 3314.0288\n",
            "Epoch 5963/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.2024 - val_loss: 3318.8171\n",
            "Epoch 5964/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2361.2258 - val_loss: 3318.2476\n",
            "Epoch 5965/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.3591 - val_loss: 3316.3772\n",
            "Epoch 5966/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.3845 - val_loss: 3314.8079\n",
            "Epoch 5967/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2358.6946 - val_loss: 3312.9880\n",
            "Epoch 5968/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.5667 - val_loss: 3308.8591\n",
            "Epoch 5969/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2358.0964 - val_loss: 3307.3965\n",
            "Epoch 5970/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.3206 - val_loss: 3306.1833\n",
            "Epoch 5971/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.6943 - val_loss: 3306.5393\n",
            "Epoch 5972/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.5417 - val_loss: 3306.9094\n",
            "Epoch 5973/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.3323 - val_loss: 3307.3467\n",
            "Epoch 5974/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2357.7615 - val_loss: 3308.1204\n",
            "Epoch 5975/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2356.7937 - val_loss: 3308.9285\n",
            "Epoch 5976/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2357.8540 - val_loss: 3309.5818\n",
            "Epoch 5977/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.7446 - val_loss: 3305.5288\n",
            "Epoch 5978/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.4714 - val_loss: 3305.4333\n",
            "Epoch 5979/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.1912 - val_loss: 3304.9543\n",
            "Epoch 5980/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2359.1228 - val_loss: 3305.5200\n",
            "Epoch 5981/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.8315 - val_loss: 3305.0166\n",
            "Epoch 5982/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2359.1545 - val_loss: 3304.6460\n",
            "Epoch 5983/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2357.7729 - val_loss: 3303.7493\n",
            "Epoch 5984/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.1228 - val_loss: 3304.1978\n",
            "Epoch 5985/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.4495 - val_loss: 3303.7542\n",
            "Epoch 5986/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.0085 - val_loss: 3302.9023\n",
            "Epoch 5987/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2359.4458 - val_loss: 3303.4456\n",
            "Epoch 5988/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.9595 - val_loss: 3307.7822\n",
            "Epoch 5989/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.5144 - val_loss: 3310.2834\n",
            "Epoch 5990/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2358.8560 - val_loss: 3311.9080\n",
            "Epoch 5991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.5925 - val_loss: 3307.7771\n",
            "Epoch 5992/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2357.5767 - val_loss: 3305.9028\n",
            "Epoch 5993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.0715 - val_loss: 3303.5623\n",
            "Epoch 5994/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2357.8459 - val_loss: 3304.6091\n",
            "Epoch 5995/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.7749 - val_loss: 3303.8784\n",
            "Epoch 5996/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.2839 - val_loss: 3304.7092\n",
            "Epoch 5997/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2356.4148 - val_loss: 3306.3184\n",
            "Epoch 5998/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.0071 - val_loss: 3306.9910\n",
            "Epoch 5999/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2358.7588 - val_loss: 3306.6074\n",
            "Epoch 6000/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.8154 - val_loss: 3306.9697\n",
            "Epoch 6001/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.7983 - val_loss: 3306.8455\n",
            "Epoch 6002/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.2361 - val_loss: 3306.1106\n",
            "Epoch 6003/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.9192 - val_loss: 3307.3806\n",
            "Epoch 6004/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2356.3245 - val_loss: 3304.6931\n",
            "Epoch 6005/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.4568 - val_loss: 3304.0908\n",
            "Epoch 6006/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2356.0535 - val_loss: 3304.2847\n",
            "Epoch 6007/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.3354 - val_loss: 3305.0625\n",
            "Epoch 6008/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2357.2651 - val_loss: 3306.6709\n",
            "Epoch 6009/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2361.7585 - val_loss: 3308.2979\n",
            "Epoch 6010/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.8782 - val_loss: 3311.4270\n",
            "Epoch 6011/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.2026 - val_loss: 3312.9526\n",
            "Epoch 6012/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.2019 - val_loss: 3315.3645\n",
            "Epoch 6013/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.8467 - val_loss: 3311.9680\n",
            "Epoch 6014/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.1362 - val_loss: 3308.0796\n",
            "Epoch 6015/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.8596 - val_loss: 3306.9817\n",
            "Epoch 6016/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.9216 - val_loss: 3306.6147\n",
            "Epoch 6017/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.0916 - val_loss: 3306.9287\n",
            "Epoch 6018/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.3403 - val_loss: 3308.0203\n",
            "Epoch 6019/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.4902 - val_loss: 3311.8215\n",
            "Epoch 6020/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.0964 - val_loss: 3313.1753\n",
            "Epoch 6021/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.8567 - val_loss: 3313.2573\n",
            "Epoch 6022/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.0884 - val_loss: 3312.6555\n",
            "Epoch 6023/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.8987 - val_loss: 3309.5386\n",
            "Epoch 6024/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.7400 - val_loss: 3313.5723\n",
            "Epoch 6025/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.8220 - val_loss: 3312.7935\n",
            "Epoch 6026/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.0608 - val_loss: 3314.5569\n",
            "Epoch 6027/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.1428 - val_loss: 3313.2395\n",
            "Epoch 6028/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.1833 - val_loss: 3313.0422\n",
            "Epoch 6029/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2355.9932 - val_loss: 3314.0049\n",
            "Epoch 6030/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.3669 - val_loss: 3313.4509\n",
            "Epoch 6031/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.5308 - val_loss: 3313.1621\n",
            "Epoch 6032/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2356.2593 - val_loss: 3313.6340\n",
            "Epoch 6033/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.2864 - val_loss: 3312.1697\n",
            "Epoch 6034/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.7271 - val_loss: 3314.1169\n",
            "Epoch 6035/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.7664 - val_loss: 3312.6685\n",
            "Epoch 6036/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2355.9736 - val_loss: 3312.8154\n",
            "Epoch 6037/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2356.2930 - val_loss: 3313.1235\n",
            "Epoch 6038/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.3748 - val_loss: 3312.9207\n",
            "Epoch 6039/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.5630 - val_loss: 3309.6772\n",
            "Epoch 6040/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.3208 - val_loss: 3308.3154\n",
            "Epoch 6041/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.7612 - val_loss: 3310.1221\n",
            "Epoch 6042/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2361.2100 - val_loss: 3310.4951\n",
            "Epoch 6043/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2360.2043 - val_loss: 3311.2830\n",
            "Epoch 6044/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.3950 - val_loss: 3312.4165\n",
            "Epoch 6045/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.4668 - val_loss: 3312.1082\n",
            "Epoch 6046/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.7024 - val_loss: 3311.1685\n",
            "Epoch 6047/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2358.4277 - val_loss: 3309.1123\n",
            "Epoch 6048/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.4043 - val_loss: 3308.2219\n",
            "Epoch 6049/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2363.6492 - val_loss: 3307.7739\n",
            "Epoch 6050/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.7373 - val_loss: 3306.0295\n",
            "Epoch 6051/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.1182 - val_loss: 3306.5825\n",
            "Epoch 6052/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.9575 - val_loss: 3310.1572\n",
            "Epoch 6053/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2355.7966 - val_loss: 3311.0640\n",
            "Epoch 6054/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.5054 - val_loss: 3311.0166\n",
            "Epoch 6055/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.9895 - val_loss: 3313.7109\n",
            "Epoch 6056/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2355.3418 - val_loss: 3311.5442\n",
            "Epoch 6057/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2355.5930 - val_loss: 3311.5442\n",
            "Epoch 6058/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.8438 - val_loss: 3314.4207\n",
            "Epoch 6059/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.9224 - val_loss: 3314.9438\n",
            "Epoch 6060/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.8037 - val_loss: 3315.9360\n",
            "Epoch 6061/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.4998 - val_loss: 3316.2563\n",
            "Epoch 6062/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2355.4482 - val_loss: 3316.0247\n",
            "Epoch 6063/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.5061 - val_loss: 3314.8953\n",
            "Epoch 6064/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.3552 - val_loss: 3314.4961\n",
            "Epoch 6065/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2355.2791 - val_loss: 3313.4294\n",
            "Epoch 6066/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.6948 - val_loss: 3311.5632\n",
            "Epoch 6067/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2355.6130 - val_loss: 3311.3430\n",
            "Epoch 6068/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.6265 - val_loss: 3310.7085\n",
            "Epoch 6069/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.5686 - val_loss: 3310.7412\n",
            "Epoch 6070/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.9617 - val_loss: 3311.9241\n",
            "Epoch 6071/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.2205 - val_loss: 3314.5754\n",
            "Epoch 6072/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.7529 - val_loss: 3322.4404\n",
            "Epoch 6073/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2360.1953 - val_loss: 3324.6665\n",
            "Epoch 6074/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.3555 - val_loss: 3321.2146\n",
            "Epoch 6075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.7761 - val_loss: 3319.4602\n",
            "Epoch 6076/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.5159 - val_loss: 3316.1953\n",
            "Epoch 6077/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2355.2161 - val_loss: 3314.8701\n",
            "Epoch 6078/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.9856 - val_loss: 3315.8984\n",
            "Epoch 6079/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.9011 - val_loss: 3315.8167\n",
            "Epoch 6080/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2354.8984 - val_loss: 3317.1841\n",
            "Epoch 6081/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.0938 - val_loss: 3314.5625\n",
            "Epoch 6082/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2353.6392 - val_loss: 3313.0066\n",
            "Epoch 6083/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.3496 - val_loss: 3311.3486\n",
            "Epoch 6084/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.0483 - val_loss: 3309.9597\n",
            "Epoch 6085/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2360.2786 - val_loss: 3309.8564\n",
            "Epoch 6086/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2360.6125 - val_loss: 3309.6477\n",
            "Epoch 6087/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2356.4211 - val_loss: 3309.7268\n",
            "Epoch 6088/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.8064 - val_loss: 3310.0955\n",
            "Epoch 6089/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2355.5671 - val_loss: 3310.2876\n",
            "Epoch 6090/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2355.2185 - val_loss: 3309.8660\n",
            "Epoch 6091/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.1162 - val_loss: 3306.7427\n",
            "Epoch 6092/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.6350 - val_loss: 3306.5427\n",
            "Epoch 6093/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2359.1060 - val_loss: 3306.3813\n",
            "Epoch 6094/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2359.5115 - val_loss: 3307.0684\n",
            "Epoch 6095/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.9976 - val_loss: 3306.8352\n",
            "Epoch 6096/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2354.6067 - val_loss: 3306.2654\n",
            "Epoch 6097/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.5588 - val_loss: 3306.2339\n",
            "Epoch 6098/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.5615 - val_loss: 3304.6868\n",
            "Epoch 6099/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.3103 - val_loss: 3304.2766\n",
            "Epoch 6100/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2354.9797 - val_loss: 3304.0034\n",
            "Epoch 6101/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.6301 - val_loss: 3304.7893\n",
            "Epoch 6102/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.5264 - val_loss: 3304.2148\n",
            "Epoch 6103/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.8811 - val_loss: 3304.5537\n",
            "Epoch 6104/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2356.0718 - val_loss: 3303.9065\n",
            "Epoch 6105/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2355.7324 - val_loss: 3304.5410\n",
            "Epoch 6106/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.5667 - val_loss: 3307.5247\n",
            "Epoch 6107/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.7986 - val_loss: 3309.3293\n",
            "Epoch 6108/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.7102 - val_loss: 3312.9797\n",
            "Epoch 6109/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2354.3105 - val_loss: 3314.2141\n",
            "Epoch 6110/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.7637 - val_loss: 3313.5784\n",
            "Epoch 6111/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2358.0935 - val_loss: 3320.5520\n",
            "Epoch 6112/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.2014 - val_loss: 3324.4951\n",
            "Epoch 6113/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.4448 - val_loss: 3325.5015\n",
            "Epoch 6114/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2358.0352 - val_loss: 3323.6948\n",
            "Epoch 6115/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2358.0510 - val_loss: 3321.1733\n",
            "Epoch 6116/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.7590 - val_loss: 3321.3967\n",
            "Epoch 6117/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.6636 - val_loss: 3321.2522\n",
            "Epoch 6118/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.6401 - val_loss: 3321.1777\n",
            "Epoch 6119/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2354.6392 - val_loss: 3318.7346\n",
            "Epoch 6120/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2355.7104 - val_loss: 3317.5967\n",
            "Epoch 6121/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.1257 - val_loss: 3317.4390\n",
            "Epoch 6122/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.9243 - val_loss: 3316.8062\n",
            "Epoch 6123/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.8787 - val_loss: 3317.5688\n",
            "Epoch 6124/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2357.4998 - val_loss: 3318.4934\n",
            "Epoch 6125/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.5535 - val_loss: 3317.3525\n",
            "Epoch 6126/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.2734 - val_loss: 3318.0154\n",
            "Epoch 6127/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2355.9248 - val_loss: 3313.4297\n",
            "Epoch 6128/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.7593 - val_loss: 3313.0239\n",
            "Epoch 6129/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.6948 - val_loss: 3313.4646\n",
            "Epoch 6130/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.4861 - val_loss: 3315.3708\n",
            "Epoch 6131/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2356.2090 - val_loss: 3316.7205\n",
            "Epoch 6132/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.0034 - val_loss: 3321.6018\n",
            "Epoch 6133/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.1306 - val_loss: 3322.6089\n",
            "Epoch 6134/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2354.3391 - val_loss: 3323.8765\n",
            "Epoch 6135/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.0447 - val_loss: 3323.0029\n",
            "Epoch 6136/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.8057 - val_loss: 3321.8372\n",
            "Epoch 6137/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.3638 - val_loss: 3322.0562\n",
            "Epoch 6138/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2353.5322 - val_loss: 3321.1313\n",
            "Epoch 6139/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.8293 - val_loss: 3321.5540\n",
            "Epoch 6140/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.1338 - val_loss: 3322.9993\n",
            "Epoch 6141/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.3518 - val_loss: 3320.0347\n",
            "Epoch 6142/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.7273 - val_loss: 3321.3582\n",
            "Epoch 6143/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2352.7397 - val_loss: 3318.4473\n",
            "Epoch 6144/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.6145 - val_loss: 3316.3428\n",
            "Epoch 6145/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2357.2385 - val_loss: 3315.4722\n",
            "Epoch 6146/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.2451 - val_loss: 3315.6211\n",
            "Epoch 6147/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2355.1519 - val_loss: 3319.3484\n",
            "Epoch 6148/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.3044 - val_loss: 3323.3269\n",
            "Epoch 6149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.3567 - val_loss: 3325.1040\n",
            "Epoch 6150/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.3823 - val_loss: 3327.8022\n",
            "Epoch 6151/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2358.1562 - val_loss: 3321.2710\n",
            "Epoch 6152/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.7126 - val_loss: 3320.3132\n",
            "Epoch 6153/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.4460 - val_loss: 3321.5400\n",
            "Epoch 6154/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.9207 - val_loss: 3325.6396\n",
            "Epoch 6155/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2357.1833 - val_loss: 3330.2822\n",
            "Epoch 6156/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2359.2432 - val_loss: 3332.7957\n",
            "Epoch 6157/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.7031 - val_loss: 3337.2261\n",
            "Epoch 6158/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2367.3943 - val_loss: 3338.6860\n",
            "Epoch 6159/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.4741 - val_loss: 3334.7188\n",
            "Epoch 6160/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2359.9968 - val_loss: 3329.4480\n",
            "Epoch 6161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2357.1470 - val_loss: 3326.3997\n",
            "Epoch 6162/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.2261 - val_loss: 3323.5679\n",
            "Epoch 6163/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2355.2529 - val_loss: 3322.6335\n",
            "Epoch 6164/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.5376 - val_loss: 3321.2397\n",
            "Epoch 6165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.6372 - val_loss: 3316.4084\n",
            "Epoch 6166/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.1736 - val_loss: 3314.8052\n",
            "Epoch 6167/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.0918 - val_loss: 3313.3391\n",
            "Epoch 6168/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.8882 - val_loss: 3316.4734\n",
            "Epoch 6169/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2353.6350 - val_loss: 3318.1853\n",
            "Epoch 6170/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.8594 - val_loss: 3320.6731\n",
            "Epoch 6171/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.8652 - val_loss: 3320.2117\n",
            "Epoch 6172/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.8005 - val_loss: 3320.3704\n",
            "Epoch 6173/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2354.0647 - val_loss: 3319.1514\n",
            "Epoch 6174/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2352.9688 - val_loss: 3320.1321\n",
            "Epoch 6175/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.7896 - val_loss: 3319.4160\n",
            "Epoch 6176/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2357.4907 - val_loss: 3315.2466\n",
            "Epoch 6177/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2356.9797 - val_loss: 3314.4409\n",
            "Epoch 6178/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2355.1003 - val_loss: 3315.7141\n",
            "Epoch 6179/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.7903 - val_loss: 3319.2732\n",
            "Epoch 6180/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.3325 - val_loss: 3319.4065\n",
            "Epoch 6181/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2353.1504 - val_loss: 3319.3379\n",
            "Epoch 6182/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.2905 - val_loss: 3319.3804\n",
            "Epoch 6183/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2352.9885 - val_loss: 3319.6221\n",
            "Epoch 6184/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.9988 - val_loss: 3319.0088\n",
            "Epoch 6185/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2353.0518 - val_loss: 3320.7947\n",
            "Epoch 6186/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.2322 - val_loss: 3321.1531\n",
            "Epoch 6187/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.0266 - val_loss: 3320.5935\n",
            "Epoch 6188/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2354.3794 - val_loss: 3322.6709\n",
            "Epoch 6189/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.9897 - val_loss: 3321.8953\n",
            "Epoch 6190/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2354.5754 - val_loss: 3320.5435\n",
            "Epoch 6191/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.7612 - val_loss: 3321.3347\n",
            "Epoch 6192/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.3303 - val_loss: 3321.1506\n",
            "Epoch 6193/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.7600 - val_loss: 3320.2156\n",
            "Epoch 6194/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2353.1223 - val_loss: 3321.8340\n",
            "Epoch 6195/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.1411 - val_loss: 3321.7524\n",
            "Epoch 6196/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.6086 - val_loss: 3320.8171\n",
            "Epoch 6197/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2356.7424 - val_loss: 3319.8623\n",
            "Epoch 6198/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2360.9348 - val_loss: 3320.8154\n",
            "Epoch 6199/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2361.7219 - val_loss: 3319.7471\n",
            "Epoch 6200/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.8284 - val_loss: 3320.3950\n",
            "Epoch 6201/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.0532 - val_loss: 3320.3628\n",
            "Epoch 6202/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2360.1782 - val_loss: 3320.9172\n",
            "Epoch 6203/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2361.9341 - val_loss: 3321.2292\n",
            "Epoch 6204/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2359.3032 - val_loss: 3320.0747\n",
            "Epoch 6205/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2355.4805 - val_loss: 3319.8477\n",
            "Epoch 6206/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.0251 - val_loss: 3320.2332\n",
            "Epoch 6207/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.1633 - val_loss: 3321.1990\n",
            "Epoch 6208/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.8142 - val_loss: 3317.9678\n",
            "Epoch 6209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.8252 - val_loss: 3316.7405\n",
            "Epoch 6210/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.6702 - val_loss: 3317.0852\n",
            "Epoch 6211/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.4668 - val_loss: 3316.8523\n",
            "Epoch 6212/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2352.8926 - val_loss: 3318.6841\n",
            "Epoch 6213/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.0605 - val_loss: 3323.3716\n",
            "Epoch 6214/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.4592 - val_loss: 3324.4165\n",
            "Epoch 6215/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.9302 - val_loss: 3324.2710\n",
            "Epoch 6216/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.2378 - val_loss: 3325.1394\n",
            "Epoch 6217/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.1409 - val_loss: 3329.2437\n",
            "Epoch 6218/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.0757 - val_loss: 3327.2949\n",
            "Epoch 6219/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.7285 - val_loss: 3331.2971\n",
            "Epoch 6220/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.7942 - val_loss: 3331.2273\n",
            "Epoch 6221/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2352.4709 - val_loss: 3329.8821\n",
            "Epoch 6222/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.1221 - val_loss: 3333.1113\n",
            "Epoch 6223/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.1809 - val_loss: 3328.7598\n",
            "Epoch 6224/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2353.0718 - val_loss: 3323.6335\n",
            "Epoch 6225/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.0874 - val_loss: 3323.3684\n",
            "Epoch 6226/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2351.7993 - val_loss: 3322.8357\n",
            "Epoch 6227/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.9971 - val_loss: 3322.7615\n",
            "Epoch 6228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.0918 - val_loss: 3324.4460\n",
            "Epoch 6229/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.8403 - val_loss: 3323.3357\n",
            "Epoch 6230/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.6072 - val_loss: 3319.7092\n",
            "Epoch 6231/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2350.7854 - val_loss: 3318.2009\n",
            "Epoch 6232/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.9211 - val_loss: 3317.8440\n",
            "Epoch 6233/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.5884 - val_loss: 3320.0596\n",
            "Epoch 6234/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.5952 - val_loss: 3320.2148\n",
            "Epoch 6235/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2354.5310 - val_loss: 3319.2964\n",
            "Epoch 6236/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2355.6467 - val_loss: 3318.7227\n",
            "Epoch 6237/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2355.5166 - val_loss: 3318.7971\n",
            "Epoch 6238/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.8779 - val_loss: 3321.0540\n",
            "Epoch 6239/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.0598 - val_loss: 3321.5232\n",
            "Epoch 6240/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.9893 - val_loss: 3321.6135\n",
            "Epoch 6241/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2351.6814 - val_loss: 3321.0947\n",
            "Epoch 6242/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.7253 - val_loss: 3320.8083\n",
            "Epoch 6243/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.4690 - val_loss: 3321.0168\n",
            "Epoch 6244/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.0820 - val_loss: 3321.7141\n",
            "Epoch 6245/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.6484 - val_loss: 3324.4722\n",
            "Epoch 6246/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.5383 - val_loss: 3325.4944\n",
            "Epoch 6247/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.9268 - val_loss: 3325.2380\n",
            "Epoch 6248/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.9587 - val_loss: 3325.7227\n",
            "Epoch 6249/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.8540 - val_loss: 3324.5776\n",
            "Epoch 6250/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2351.4392 - val_loss: 3323.4346\n",
            "Epoch 6251/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.1401 - val_loss: 3324.3848\n",
            "Epoch 6252/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.0845 - val_loss: 3324.4121\n",
            "Epoch 6253/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.0432 - val_loss: 3323.4216\n",
            "Epoch 6254/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.1370 - val_loss: 3322.8845\n",
            "Epoch 6255/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.8323 - val_loss: 3322.5540\n",
            "Epoch 6256/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.5688 - val_loss: 3323.0442\n",
            "Epoch 6257/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.2510 - val_loss: 3324.4617\n",
            "Epoch 6258/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.8875 - val_loss: 3327.1689\n",
            "Epoch 6259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.1465 - val_loss: 3330.2861\n",
            "Epoch 6260/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.7083 - val_loss: 3334.2107\n",
            "Epoch 6261/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2356.3701 - val_loss: 3339.0503\n",
            "Epoch 6262/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2357.4314 - val_loss: 3343.5654\n",
            "Epoch 6263/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2362.0037 - val_loss: 3346.4224\n",
            "Epoch 6264/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363.6567 - val_loss: 3346.6257\n",
            "Epoch 6265/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2362.5125 - val_loss: 3342.3020\n",
            "Epoch 6266/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2354.3728 - val_loss: 3333.5852\n",
            "Epoch 6267/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.0408 - val_loss: 3329.5972\n",
            "Epoch 6268/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.9958 - val_loss: 3323.4915\n",
            "Epoch 6269/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2350.4912 - val_loss: 3322.6208\n",
            "Epoch 6270/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.5823 - val_loss: 3323.1931\n",
            "Epoch 6271/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.0886 - val_loss: 3324.5322\n",
            "Epoch 6272/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.4065 - val_loss: 3322.7009\n",
            "Epoch 6273/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.6401 - val_loss: 3322.6755\n",
            "Epoch 6274/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2351.4365 - val_loss: 3323.5579\n",
            "Epoch 6275/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.6716 - val_loss: 3323.8167\n",
            "Epoch 6276/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.3162 - val_loss: 3322.8486\n",
            "Epoch 6277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.2659 - val_loss: 3322.4172\n",
            "Epoch 6278/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2351.2085 - val_loss: 3321.6372\n",
            "Epoch 6279/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.1067 - val_loss: 3319.1089\n",
            "Epoch 6280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.6404 - val_loss: 3318.8992\n",
            "Epoch 6281/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.5957 - val_loss: 3323.5737\n",
            "Epoch 6282/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.3464 - val_loss: 3323.0710\n",
            "Epoch 6283/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2350.4585 - val_loss: 3321.2573\n",
            "Epoch 6284/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.8167 - val_loss: 3324.9592\n",
            "Epoch 6285/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.9893 - val_loss: 3319.7634\n",
            "Epoch 6286/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2349.3813 - val_loss: 3319.4441\n",
            "Epoch 6287/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2350.1548 - val_loss: 3319.5603\n",
            "Epoch 6288/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.9585 - val_loss: 3320.0815\n",
            "Epoch 6289/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2349.0254 - val_loss: 3319.6980\n",
            "Epoch 6290/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.3787 - val_loss: 3319.9536\n",
            "Epoch 6291/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2351.4028 - val_loss: 3322.5142\n",
            "Epoch 6292/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2356.7876 - val_loss: 3329.6707\n",
            "Epoch 6293/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2355.2793 - val_loss: 3328.9783\n",
            "Epoch 6294/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2355.7483 - val_loss: 3331.8772\n",
            "Epoch 6295/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.4534 - val_loss: 3330.1458\n",
            "Epoch 6296/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2354.3896 - val_loss: 3330.9705\n",
            "Epoch 6297/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2354.2542 - val_loss: 3329.6707\n",
            "Epoch 6298/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2352.7722 - val_loss: 3325.3293\n",
            "Epoch 6299/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2353.7180 - val_loss: 3325.3423\n",
            "Epoch 6300/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.4824 - val_loss: 3320.8975\n",
            "Epoch 6301/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.3035 - val_loss: 3320.7959\n",
            "Epoch 6302/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.7456 - val_loss: 3319.6504\n",
            "Epoch 6303/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.1375 - val_loss: 3313.6580\n",
            "Epoch 6304/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.7305 - val_loss: 3313.9937\n",
            "Epoch 6305/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.3276 - val_loss: 3316.7710\n",
            "Epoch 6306/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.7158 - val_loss: 3316.4839\n",
            "Epoch 6307/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.0715 - val_loss: 3315.7959\n",
            "Epoch 6308/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2350.4673 - val_loss: 3316.7566\n",
            "Epoch 6309/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2349.6892 - val_loss: 3313.6841\n",
            "Epoch 6310/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2347.9294 - val_loss: 3310.7429\n",
            "Epoch 6311/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.8643 - val_loss: 3311.5303\n",
            "Epoch 6312/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2349.0317 - val_loss: 3313.4365\n",
            "Epoch 6313/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2349.3052 - val_loss: 3314.3806\n",
            "Epoch 6314/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2349.2759 - val_loss: 3313.0786\n",
            "Epoch 6315/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2349.0339 - val_loss: 3312.6892\n",
            "Epoch 6316/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2347.7546 - val_loss: 3313.7051\n",
            "Epoch 6317/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.4558 - val_loss: 3312.5410\n",
            "Epoch 6318/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.0613 - val_loss: 3313.7991\n",
            "Epoch 6319/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2349.4224 - val_loss: 3312.7051\n",
            "Epoch 6320/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.9324 - val_loss: 3311.7036\n",
            "Epoch 6321/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.9111 - val_loss: 3309.2991\n",
            "Epoch 6322/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.3269 - val_loss: 3311.2852\n",
            "Epoch 6323/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.6414 - val_loss: 3314.7222\n",
            "Epoch 6324/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.5857 - val_loss: 3312.5386\n",
            "Epoch 6325/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.2424 - val_loss: 3312.2971\n",
            "Epoch 6326/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.9641 - val_loss: 3311.3438\n",
            "Epoch 6327/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2348.3713 - val_loss: 3310.1812\n",
            "Epoch 6328/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.7566 - val_loss: 3310.6821\n",
            "Epoch 6329/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.8584 - val_loss: 3310.4480\n",
            "Epoch 6330/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2352.0276 - val_loss: 3308.6235\n",
            "Epoch 6331/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.9565 - val_loss: 3308.7058\n",
            "Epoch 6332/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.4622 - val_loss: 3309.1965\n",
            "Epoch 6333/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.3767 - val_loss: 3309.9060\n",
            "Epoch 6334/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.2922 - val_loss: 3310.3933\n",
            "Epoch 6335/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.1694 - val_loss: 3311.7859\n",
            "Epoch 6336/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2348.7539 - val_loss: 3311.2805\n",
            "Epoch 6337/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.6453 - val_loss: 3311.7043\n",
            "Epoch 6338/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.0725 - val_loss: 3311.2822\n",
            "Epoch 6339/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.7175 - val_loss: 3310.0962\n",
            "Epoch 6340/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.8225 - val_loss: 3310.0498\n",
            "Epoch 6341/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.9199 - val_loss: 3313.6960\n",
            "Epoch 6342/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.0444 - val_loss: 3314.1082\n",
            "Epoch 6343/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.9294 - val_loss: 3314.8035\n",
            "Epoch 6344/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.2883 - val_loss: 3312.0183\n",
            "Epoch 6345/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.2361 - val_loss: 3307.4177\n",
            "Epoch 6346/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2353.9602 - val_loss: 3306.7134\n",
            "Epoch 6347/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.9197 - val_loss: 3305.4045\n",
            "Epoch 6348/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2349.6257 - val_loss: 3303.7354\n",
            "Epoch 6349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.0012 - val_loss: 3303.0337\n",
            "Epoch 6350/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2350.9807 - val_loss: 3304.4072\n",
            "Epoch 6351/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.1851 - val_loss: 3304.7310\n",
            "Epoch 6352/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.5359 - val_loss: 3305.9207\n",
            "Epoch 6353/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.8816 - val_loss: 3307.5681\n",
            "Epoch 6354/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.0803 - val_loss: 3308.3132\n",
            "Epoch 6355/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.9844 - val_loss: 3308.6497\n",
            "Epoch 6356/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.1082 - val_loss: 3309.8284\n",
            "Epoch 6357/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.7285 - val_loss: 3308.3838\n",
            "Epoch 6358/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.2607 - val_loss: 3306.1082\n",
            "Epoch 6359/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2348.6223 - val_loss: 3305.5535\n",
            "Epoch 6360/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2347.5120 - val_loss: 3305.4504\n",
            "Epoch 6361/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.2734 - val_loss: 3303.9915\n",
            "Epoch 6362/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.7690 - val_loss: 3304.4473\n",
            "Epoch 6363/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.9807 - val_loss: 3305.3845\n",
            "Epoch 6364/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.4543 - val_loss: 3305.4438\n",
            "Epoch 6365/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.7517 - val_loss: 3305.8645\n",
            "Epoch 6366/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.8281 - val_loss: 3307.3765\n",
            "Epoch 6367/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.5354 - val_loss: 3309.0547\n",
            "Epoch 6368/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.3972 - val_loss: 3308.8936\n",
            "Epoch 6369/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.4001 - val_loss: 3311.8992\n",
            "Epoch 6370/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.3379 - val_loss: 3311.3035\n",
            "Epoch 6371/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.2812 - val_loss: 3310.3501\n",
            "Epoch 6372/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2348.1924 - val_loss: 3308.7837\n",
            "Epoch 6373/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.7288 - val_loss: 3309.1628\n",
            "Epoch 6374/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.7109 - val_loss: 3308.4629\n",
            "Epoch 6375/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.6914 - val_loss: 3305.3462\n",
            "Epoch 6376/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2347.5776 - val_loss: 3305.3132\n",
            "Epoch 6377/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.4832 - val_loss: 3306.3350\n",
            "Epoch 6378/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.4253 - val_loss: 3307.1877\n",
            "Epoch 6379/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.8389 - val_loss: 3308.9136\n",
            "Epoch 6380/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2345.9473 - val_loss: 3309.9177\n",
            "Epoch 6381/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2347.0100 - val_loss: 3310.9609\n",
            "Epoch 6382/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2346.7959 - val_loss: 3313.0615\n",
            "Epoch 6383/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.1567 - val_loss: 3315.2083\n",
            "Epoch 6384/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2347.0786 - val_loss: 3315.8210\n",
            "Epoch 6385/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.1409 - val_loss: 3315.9158\n",
            "Epoch 6386/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2347.7563 - val_loss: 3315.6572\n",
            "Epoch 6387/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2347.1917 - val_loss: 3318.5278\n",
            "Epoch 6388/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2351.8425 - val_loss: 3324.6362\n",
            "Epoch 6389/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2358.6619 - val_loss: 3330.7102\n",
            "Epoch 6390/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2362.9392 - val_loss: 3326.8896\n",
            "Epoch 6391/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2356.0808 - val_loss: 3316.8140\n",
            "Epoch 6392/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2352.4758 - val_loss: 3315.1096\n",
            "Epoch 6393/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.2593 - val_loss: 3314.4270\n",
            "Epoch 6394/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2349.7600 - val_loss: 3313.2642\n",
            "Epoch 6395/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2349.6504 - val_loss: 3312.1416\n",
            "Epoch 6396/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2348.3215 - val_loss: 3311.4353\n",
            "Epoch 6397/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.5400 - val_loss: 3313.0720\n",
            "Epoch 6398/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2348.7742 - val_loss: 3305.7830\n",
            "Epoch 6399/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.4429 - val_loss: 3303.9768\n",
            "Epoch 6400/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.8252 - val_loss: 3303.7310\n",
            "Epoch 6401/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.7773 - val_loss: 3303.5210\n",
            "Epoch 6402/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2346.8779 - val_loss: 3302.4351\n",
            "Epoch 6403/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.9565 - val_loss: 3304.2092\n",
            "Epoch 6404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.9146 - val_loss: 3302.4541\n",
            "Epoch 6405/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.7981 - val_loss: 3301.5786\n",
            "Epoch 6406/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2346.3186 - val_loss: 3304.2302\n",
            "Epoch 6407/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2351.0366 - val_loss: 3304.1448\n",
            "Epoch 6408/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.0303 - val_loss: 3304.1841\n",
            "Epoch 6409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2349.1855 - val_loss: 3304.3877\n",
            "Epoch 6410/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.2236 - val_loss: 3304.7815\n",
            "Epoch 6411/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.6489 - val_loss: 3305.4890\n",
            "Epoch 6412/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.6040 - val_loss: 3306.0330\n",
            "Epoch 6413/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.1096 - val_loss: 3307.9392\n",
            "Epoch 6414/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2346.4343 - val_loss: 3308.5088\n",
            "Epoch 6415/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.4192 - val_loss: 3309.0271\n",
            "Epoch 6416/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2346.1775 - val_loss: 3310.0691\n",
            "Epoch 6417/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.8606 - val_loss: 3311.9397\n",
            "Epoch 6418/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.2278 - val_loss: 3311.3821\n",
            "Epoch 6419/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.6511 - val_loss: 3311.2290\n",
            "Epoch 6420/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.4431 - val_loss: 3311.4270\n",
            "Epoch 6421/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.7554 - val_loss: 3310.0337\n",
            "Epoch 6422/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.6050 - val_loss: 3310.0408\n",
            "Epoch 6423/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.4407 - val_loss: 3310.4197\n",
            "Epoch 6424/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.9849 - val_loss: 3313.6770\n",
            "Epoch 6425/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.8784 - val_loss: 3312.3687\n",
            "Epoch 6426/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.9622 - val_loss: 3313.4326\n",
            "Epoch 6427/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.6680 - val_loss: 3312.9849\n",
            "Epoch 6428/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2346.4426 - val_loss: 3313.1819\n",
            "Epoch 6429/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.8135 - val_loss: 3316.2236\n",
            "Epoch 6430/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.2981 - val_loss: 3318.0852\n",
            "Epoch 6431/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2347.4666 - val_loss: 3318.6018\n",
            "Epoch 6432/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.9680 - val_loss: 3317.4680\n",
            "Epoch 6433/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2351.4490 - val_loss: 3319.9617\n",
            "Epoch 6434/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.9224 - val_loss: 3310.3420\n",
            "Epoch 6435/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.2192 - val_loss: 3306.9998\n",
            "Epoch 6436/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2347.1626 - val_loss: 3306.3950\n",
            "Epoch 6437/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2347.2725 - val_loss: 3306.5383\n",
            "Epoch 6438/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.1594 - val_loss: 3306.7419\n",
            "Epoch 6439/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.3840 - val_loss: 3306.5889\n",
            "Epoch 6440/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2346.7493 - val_loss: 3306.8616\n",
            "Epoch 6441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.1724 - val_loss: 3311.1284\n",
            "Epoch 6442/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.2625 - val_loss: 3314.2524\n",
            "Epoch 6443/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2352.0376 - val_loss: 3318.4854\n",
            "Epoch 6444/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2350.6875 - val_loss: 3316.1160\n",
            "Epoch 6445/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.5764 - val_loss: 3314.1233\n",
            "Epoch 6446/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.4817 - val_loss: 3310.6243\n",
            "Epoch 6447/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.2605 - val_loss: 3308.1648\n",
            "Epoch 6448/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.1340 - val_loss: 3306.7317\n",
            "Epoch 6449/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2346.2498 - val_loss: 3306.3560\n",
            "Epoch 6450/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2345.8167 - val_loss: 3306.2419\n",
            "Epoch 6451/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2347.2666 - val_loss: 3303.5491\n",
            "Epoch 6452/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2347.7227 - val_loss: 3303.2053\n",
            "Epoch 6453/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.8391 - val_loss: 3303.8198\n",
            "Epoch 6454/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.4680 - val_loss: 3304.5344\n",
            "Epoch 6455/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.1633 - val_loss: 3307.4690\n",
            "Epoch 6456/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.8396 - val_loss: 3307.8003\n",
            "Epoch 6457/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2345.4268 - val_loss: 3310.6570\n",
            "Epoch 6458/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2348.3628 - val_loss: 3309.0007\n",
            "Epoch 6459/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.2776 - val_loss: 3315.7566\n",
            "Epoch 6460/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.2498 - val_loss: 3314.2844\n",
            "Epoch 6461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.8232 - val_loss: 3314.2083\n",
            "Epoch 6462/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.2976 - val_loss: 3314.7468\n",
            "Epoch 6463/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2346.3906 - val_loss: 3314.0610\n",
            "Epoch 6464/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2346.9636 - val_loss: 3313.1636\n",
            "Epoch 6465/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.6528 - val_loss: 3313.6724\n",
            "Epoch 6466/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.5566 - val_loss: 3313.8804\n",
            "Epoch 6467/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.5259 - val_loss: 3313.7273\n",
            "Epoch 6468/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.2139 - val_loss: 3314.1184\n",
            "Epoch 6469/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2345.2249 - val_loss: 3314.4312\n",
            "Epoch 6470/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.9287 - val_loss: 3311.1960\n",
            "Epoch 6471/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.1565 - val_loss: 3310.2019\n",
            "Epoch 6472/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.3105 - val_loss: 3312.4377\n",
            "Epoch 6473/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.8933 - val_loss: 3313.9590\n",
            "Epoch 6474/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.8572 - val_loss: 3315.0054\n",
            "Epoch 6475/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.1194 - val_loss: 3315.2229\n",
            "Epoch 6476/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.3555 - val_loss: 3316.4734\n",
            "Epoch 6477/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.3816 - val_loss: 3315.6018\n",
            "Epoch 6478/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.8872 - val_loss: 3315.3660\n",
            "Epoch 6479/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.7383 - val_loss: 3314.2302\n",
            "Epoch 6480/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.4814 - val_loss: 3315.2917\n",
            "Epoch 6481/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2344.8096 - val_loss: 3315.8557\n",
            "Epoch 6482/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.0386 - val_loss: 3314.6179\n",
            "Epoch 6483/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.5513 - val_loss: 3314.8066\n",
            "Epoch 6484/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.8391 - val_loss: 3314.1960\n",
            "Epoch 6485/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.6111 - val_loss: 3314.4136\n",
            "Epoch 6486/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2344.0820 - val_loss: 3312.4360\n",
            "Epoch 6487/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.6814 - val_loss: 3312.1995\n",
            "Epoch 6488/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.4653 - val_loss: 3313.4260\n",
            "Epoch 6489/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2347.2634 - val_loss: 3313.5315\n",
            "Epoch 6490/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2348.1440 - val_loss: 3314.9285\n",
            "Epoch 6491/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.7344 - val_loss: 3315.2910\n",
            "Epoch 6492/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.4463 - val_loss: 3315.6355\n",
            "Epoch 6493/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.0737 - val_loss: 3315.7097\n",
            "Epoch 6494/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2345.1692 - val_loss: 3316.8125\n",
            "Epoch 6495/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.6936 - val_loss: 3315.9473\n",
            "Epoch 6496/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2347.6453 - val_loss: 3316.3140\n",
            "Epoch 6497/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2350.2644 - val_loss: 3319.6765\n",
            "Epoch 6498/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.7317 - val_loss: 3322.5801\n",
            "Epoch 6499/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.4607 - val_loss: 3325.2156\n",
            "Epoch 6500/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.2981 - val_loss: 3326.4768\n",
            "Epoch 6501/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.8704 - val_loss: 3325.8728\n",
            "Epoch 6502/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2345.0276 - val_loss: 3323.1003\n",
            "Epoch 6503/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2349.0452 - val_loss: 3323.8967\n",
            "Epoch 6504/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.5298 - val_loss: 3323.8259\n",
            "Epoch 6505/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2347.8406 - val_loss: 3322.0566\n",
            "Epoch 6506/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.5891 - val_loss: 3321.8293\n",
            "Epoch 6507/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.6028 - val_loss: 3322.4077\n",
            "Epoch 6508/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2344.0447 - val_loss: 3326.3716\n",
            "Epoch 6509/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.2446 - val_loss: 3327.1091\n",
            "Epoch 6510/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2345.0649 - val_loss: 3327.8062\n",
            "Epoch 6511/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.2644 - val_loss: 3328.4177\n",
            "Epoch 6512/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.8215 - val_loss: 3328.9255\n",
            "Epoch 6513/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.9714 - val_loss: 3329.0359\n",
            "Epoch 6514/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2344.0371 - val_loss: 3328.8511\n",
            "Epoch 6515/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.1543 - val_loss: 3329.4080\n",
            "Epoch 6516/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.9756 - val_loss: 3329.7012\n",
            "Epoch 6517/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.0996 - val_loss: 3325.8220\n",
            "Epoch 6518/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.4194 - val_loss: 3324.0940\n",
            "Epoch 6519/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2344.9399 - val_loss: 3323.5576\n",
            "Epoch 6520/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.1436 - val_loss: 3322.8975\n",
            "Epoch 6521/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.1079 - val_loss: 3323.4255\n",
            "Epoch 6522/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.9482 - val_loss: 3324.1177\n",
            "Epoch 6523/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.6653 - val_loss: 3324.9424\n",
            "Epoch 6524/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2344.3142 - val_loss: 3326.3479\n",
            "Epoch 6525/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2345.1323 - val_loss: 3326.3203\n",
            "Epoch 6526/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.3857 - val_loss: 3325.6394\n",
            "Epoch 6527/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.0945 - val_loss: 3326.2922\n",
            "Epoch 6528/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2342.9573 - val_loss: 3322.7451\n",
            "Epoch 6529/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.5088 - val_loss: 3320.2170\n",
            "Epoch 6530/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.3899 - val_loss: 3318.9016\n",
            "Epoch 6531/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2349.8127 - val_loss: 3316.9622\n",
            "Epoch 6532/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.7761 - val_loss: 3316.5898\n",
            "Epoch 6533/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2345.8018 - val_loss: 3317.1426\n",
            "Epoch 6534/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.1948 - val_loss: 3318.5930\n",
            "Epoch 6535/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.7954 - val_loss: 3319.7102\n",
            "Epoch 6536/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.6262 - val_loss: 3319.7446\n",
            "Epoch 6537/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.1621 - val_loss: 3320.2454\n",
            "Epoch 6538/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2345.1055 - val_loss: 3321.9302\n",
            "Epoch 6539/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.1492 - val_loss: 3322.3210\n",
            "Epoch 6540/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2347.6384 - val_loss: 3326.9153\n",
            "Epoch 6541/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2345.1350 - val_loss: 3323.3665\n",
            "Epoch 6542/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.1272 - val_loss: 3320.5520\n",
            "Epoch 6543/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.0200 - val_loss: 3320.0771\n",
            "Epoch 6544/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.4463 - val_loss: 3320.8279\n",
            "Epoch 6545/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2342.9604 - val_loss: 3321.0540\n",
            "Epoch 6546/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2341.9849 - val_loss: 3324.0571\n",
            "Epoch 6547/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2344.2893 - val_loss: 3325.4678\n",
            "Epoch 6548/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.3447 - val_loss: 3325.3831\n",
            "Epoch 6549/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2345.8899 - val_loss: 3328.3999\n",
            "Epoch 6550/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.5564 - val_loss: 3328.2004\n",
            "Epoch 6551/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2346.0649 - val_loss: 3329.5715\n",
            "Epoch 6552/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.4956 - val_loss: 3327.8892\n",
            "Epoch 6553/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2345.0466 - val_loss: 3325.8118\n",
            "Epoch 6554/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.3489 - val_loss: 3320.9534\n",
            "Epoch 6555/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.0879 - val_loss: 3319.1533\n",
            "Epoch 6556/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.0811 - val_loss: 3319.3877\n",
            "Epoch 6557/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2344.0430 - val_loss: 3317.9497\n",
            "Epoch 6558/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.1472 - val_loss: 3317.7571\n",
            "Epoch 6559/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.7124 - val_loss: 3318.5029\n",
            "Epoch 6560/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.9736 - val_loss: 3322.3940\n",
            "Epoch 6561/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.1372 - val_loss: 3318.4585\n",
            "Epoch 6562/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.8696 - val_loss: 3316.9622\n",
            "Epoch 6563/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.5107 - val_loss: 3316.4624\n",
            "Epoch 6564/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.0146 - val_loss: 3315.6331\n",
            "Epoch 6565/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.8943 - val_loss: 3315.4109\n",
            "Epoch 6566/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.1995 - val_loss: 3315.7390\n",
            "Epoch 6567/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.5964 - val_loss: 3320.2603\n",
            "Epoch 6568/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.6350 - val_loss: 3319.7822\n",
            "Epoch 6569/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2344.1731 - val_loss: 3317.5740\n",
            "Epoch 6570/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2342.9062 - val_loss: 3318.3940\n",
            "Epoch 6571/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.0137 - val_loss: 3318.4614\n",
            "Epoch 6572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.7114 - val_loss: 3318.3210\n",
            "Epoch 6573/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2342.4468 - val_loss: 3316.7100\n",
            "Epoch 6574/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.8784 - val_loss: 3312.0867\n",
            "Epoch 6575/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.2185 - val_loss: 3311.9321\n",
            "Epoch 6576/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.6863 - val_loss: 3313.6729\n",
            "Epoch 6577/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2348.8855 - val_loss: 3315.1626\n",
            "Epoch 6578/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2349.9043 - val_loss: 3315.4543\n",
            "Epoch 6579/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.5029 - val_loss: 3316.0996\n",
            "Epoch 6580/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2350.7563 - val_loss: 3317.2754\n",
            "Epoch 6581/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2349.7852 - val_loss: 3317.0996\n",
            "Epoch 6582/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2347.9602 - val_loss: 3317.2842\n",
            "Epoch 6583/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2348.1582 - val_loss: 3316.3726\n",
            "Epoch 6584/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2347.3696 - val_loss: 3316.3115\n",
            "Epoch 6585/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.2800 - val_loss: 3316.0105\n",
            "Epoch 6586/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.8843 - val_loss: 3316.2454\n",
            "Epoch 6587/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.0464 - val_loss: 3318.1003\n",
            "Epoch 6588/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.4141 - val_loss: 3316.0386\n",
            "Epoch 6589/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.8201 - val_loss: 3316.9172\n",
            "Epoch 6590/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2341.4204 - val_loss: 3316.6602\n",
            "Epoch 6591/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.2551 - val_loss: 3314.9463\n",
            "Epoch 6592/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.8547 - val_loss: 3313.5571\n",
            "Epoch 6593/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.9407 - val_loss: 3313.5371\n",
            "Epoch 6594/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2344.9683 - val_loss: 3313.5090\n",
            "Epoch 6595/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.2834 - val_loss: 3314.0366\n",
            "Epoch 6596/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.5435 - val_loss: 3311.9329\n",
            "Epoch 6597/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2347.8120 - val_loss: 3311.9607\n",
            "Epoch 6598/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2346.9172 - val_loss: 3311.7979\n",
            "Epoch 6599/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.2991 - val_loss: 3312.7844\n",
            "Epoch 6600/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.2104 - val_loss: 3313.2932\n",
            "Epoch 6601/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2344.3018 - val_loss: 3314.5178\n",
            "Epoch 6602/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.7458 - val_loss: 3314.8911\n",
            "Epoch 6603/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2342.7896 - val_loss: 3315.5803\n",
            "Epoch 6604/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.2666 - val_loss: 3315.9583\n",
            "Epoch 6605/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2342.6145 - val_loss: 3316.8496\n",
            "Epoch 6606/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.1716 - val_loss: 3315.2957\n",
            "Epoch 6607/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.0769 - val_loss: 3314.4824\n",
            "Epoch 6608/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.9031 - val_loss: 3312.3728\n",
            "Epoch 6609/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.8284 - val_loss: 3311.9119\n",
            "Epoch 6610/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2341.9268 - val_loss: 3313.2493\n",
            "Epoch 6611/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.4314 - val_loss: 3313.8083\n",
            "Epoch 6612/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.1165 - val_loss: 3314.1531\n",
            "Epoch 6613/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.9272 - val_loss: 3316.8596\n",
            "Epoch 6614/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2341.3193 - val_loss: 3320.1914\n",
            "Epoch 6615/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.8342 - val_loss: 3322.5955\n",
            "Epoch 6616/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.6467 - val_loss: 3320.4004\n",
            "Epoch 6617/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.3662 - val_loss: 3317.0142\n",
            "Epoch 6618/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2341.5159 - val_loss: 3316.2542\n",
            "Epoch 6619/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.2920 - val_loss: 3314.5979\n",
            "Epoch 6620/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.9900 - val_loss: 3314.6321\n",
            "Epoch 6621/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.3279 - val_loss: 3314.9807\n",
            "Epoch 6622/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2341.6946 - val_loss: 3315.1370\n",
            "Epoch 6623/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2341.6833 - val_loss: 3315.2476\n",
            "Epoch 6624/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.0088 - val_loss: 3313.7302\n",
            "Epoch 6625/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.9265 - val_loss: 3313.2395\n",
            "Epoch 6626/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.9661 - val_loss: 3311.1284\n",
            "Epoch 6627/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.2537 - val_loss: 3310.5793\n",
            "Epoch 6628/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2345.0481 - val_loss: 3311.1584\n",
            "Epoch 6629/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.3975 - val_loss: 3311.6272\n",
            "Epoch 6630/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.4932 - val_loss: 3313.5703\n",
            "Epoch 6631/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.2310 - val_loss: 3315.2991\n",
            "Epoch 6632/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2341.6921 - val_loss: 3315.8062\n",
            "Epoch 6633/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.1924 - val_loss: 3316.5112\n",
            "Epoch 6634/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.5847 - val_loss: 3319.6177\n",
            "Epoch 6635/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.9861 - val_loss: 3318.8813\n",
            "Epoch 6636/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2341.0056 - val_loss: 3314.5310\n",
            "Epoch 6637/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.4875 - val_loss: 3312.3223\n",
            "Epoch 6638/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.9065 - val_loss: 3311.2124\n",
            "Epoch 6639/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.4758 - val_loss: 3311.3992\n",
            "Epoch 6640/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.6272 - val_loss: 3312.3271\n",
            "Epoch 6641/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2342.4556 - val_loss: 3312.9761\n",
            "Epoch 6642/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.3618 - val_loss: 3312.8301\n",
            "Epoch 6643/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.4011 - val_loss: 3314.7493\n",
            "Epoch 6644/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2345.3582 - val_loss: 3315.2139\n",
            "Epoch 6645/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.3232 - val_loss: 3315.1692\n",
            "Epoch 6646/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.4519 - val_loss: 3314.6418\n",
            "Epoch 6647/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.7800 - val_loss: 3314.2500\n",
            "Epoch 6648/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.8386 - val_loss: 3314.6265\n",
            "Epoch 6649/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2340.9082 - val_loss: 3316.6152\n",
            "Epoch 6650/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.1150 - val_loss: 3313.2461\n",
            "Epoch 6651/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.3984 - val_loss: 3313.1379\n",
            "Epoch 6652/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2341.4907 - val_loss: 3313.8384\n",
            "Epoch 6653/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2341.1589 - val_loss: 3313.2710\n",
            "Epoch 6654/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.1677 - val_loss: 3310.5369\n",
            "Epoch 6655/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.3374 - val_loss: 3312.0969\n",
            "Epoch 6656/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.2686 - val_loss: 3314.1033\n",
            "Epoch 6657/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2340.6421 - val_loss: 3317.7612\n",
            "Epoch 6658/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.9983 - val_loss: 3317.4141\n",
            "Epoch 6659/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2340.9976 - val_loss: 3314.3245\n",
            "Epoch 6660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.4875 - val_loss: 3310.9517\n",
            "Epoch 6661/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.8088 - val_loss: 3311.1987\n",
            "Epoch 6662/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.5518 - val_loss: 3311.9910\n",
            "Epoch 6663/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.6472 - val_loss: 3312.5759\n",
            "Epoch 6664/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.4546 - val_loss: 3312.7388\n",
            "Epoch 6665/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.2512 - val_loss: 3313.3679\n",
            "Epoch 6666/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.6831 - val_loss: 3313.3838\n",
            "Epoch 6667/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.9548 - val_loss: 3312.3679\n",
            "Epoch 6668/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.0732 - val_loss: 3313.1204\n",
            "Epoch 6669/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.5081 - val_loss: 3312.7859\n",
            "Epoch 6670/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2343.2927 - val_loss: 3312.2754\n",
            "Epoch 6671/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.6353 - val_loss: 3312.6392\n",
            "Epoch 6672/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.9253 - val_loss: 3311.5298\n",
            "Epoch 6673/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.8643 - val_loss: 3312.3462\n",
            "Epoch 6674/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2340.9136 - val_loss: 3313.8989\n",
            "Epoch 6675/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.8372 - val_loss: 3315.4021\n",
            "Epoch 6676/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.9089 - val_loss: 3314.8159\n",
            "Epoch 6677/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.0012 - val_loss: 3315.6545\n",
            "Epoch 6678/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.8340 - val_loss: 3315.5706\n",
            "Epoch 6679/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2341.1238 - val_loss: 3319.8252\n",
            "Epoch 6680/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.4443 - val_loss: 3326.1697\n",
            "Epoch 6681/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.8857 - val_loss: 3329.2708\n",
            "Epoch 6682/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2344.5320 - val_loss: 3334.8896\n",
            "Epoch 6683/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2345.7395 - val_loss: 3335.7378\n",
            "Epoch 6684/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.0024 - val_loss: 3335.2178\n",
            "Epoch 6685/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.6067 - val_loss: 3339.9885\n",
            "Epoch 6686/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2348.6487 - val_loss: 3339.1235\n",
            "Epoch 6687/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2348.8225 - val_loss: 3341.1140\n",
            "Epoch 6688/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2348.8013 - val_loss: 3339.9670\n",
            "Epoch 6689/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2346.5171 - val_loss: 3333.5771\n",
            "Epoch 6690/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2343.4307 - val_loss: 3331.2241\n",
            "Epoch 6691/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2343.3206 - val_loss: 3329.5776\n",
            "Epoch 6692/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.3889 - val_loss: 3328.7202\n",
            "Epoch 6693/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.7493 - val_loss: 3331.2114\n",
            "Epoch 6694/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.3843 - val_loss: 3328.9990\n",
            "Epoch 6695/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2341.8230 - val_loss: 3324.9487\n",
            "Epoch 6696/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.0869 - val_loss: 3324.4231\n",
            "Epoch 6697/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2340.8853 - val_loss: 3324.8503\n",
            "Epoch 6698/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.2371 - val_loss: 3324.9641\n",
            "Epoch 6699/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2340.8435 - val_loss: 3323.9133\n",
            "Epoch 6700/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2342.1550 - val_loss: 3320.8101\n",
            "Epoch 6701/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.3540 - val_loss: 3323.3518\n",
            "Epoch 6702/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.7454 - val_loss: 3325.1348\n",
            "Epoch 6703/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2341.2671 - val_loss: 3324.5615\n",
            "Epoch 6704/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2339.4678 - val_loss: 3325.1865\n",
            "Epoch 6705/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.5996 - val_loss: 3324.5857\n",
            "Epoch 6706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.1074 - val_loss: 3324.7588\n",
            "Epoch 6707/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.1118 - val_loss: 3324.4792\n",
            "Epoch 6708/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.5776 - val_loss: 3327.1284\n",
            "Epoch 6709/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2339.7156 - val_loss: 3327.5747\n",
            "Epoch 6710/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.7798 - val_loss: 3326.8054\n",
            "Epoch 6711/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.1189 - val_loss: 3331.0085\n",
            "Epoch 6712/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.6882 - val_loss: 3332.6353\n",
            "Epoch 6713/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2341.4253 - val_loss: 3331.5674\n",
            "Epoch 6714/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2341.7241 - val_loss: 3331.4463\n",
            "Epoch 6715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.7749 - val_loss: 3327.9553\n",
            "Epoch 6716/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.5898 - val_loss: 3327.5857\n",
            "Epoch 6717/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2340.7693 - val_loss: 3326.7666\n",
            "Epoch 6718/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2339.4365 - val_loss: 3325.4143\n",
            "Epoch 6719/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2338.8521 - val_loss: 3325.1292\n",
            "Epoch 6720/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.2510 - val_loss: 3324.2524\n",
            "Epoch 6721/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2339.0271 - val_loss: 3324.9241\n",
            "Epoch 6722/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.1885 - val_loss: 3328.6924\n",
            "Epoch 6723/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2339.7710 - val_loss: 3327.5085\n",
            "Epoch 6724/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.4453 - val_loss: 3330.3572\n",
            "Epoch 6725/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2339.5283 - val_loss: 3333.2275\n",
            "Epoch 6726/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.5334 - val_loss: 3339.1777\n",
            "Epoch 6727/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.9490 - val_loss: 3337.5815\n",
            "Epoch 6728/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2340.9626 - val_loss: 3338.3757\n",
            "Epoch 6729/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2340.8774 - val_loss: 3341.8877\n",
            "Epoch 6730/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2343.0776 - val_loss: 3342.6785\n",
            "Epoch 6731/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.0632 - val_loss: 3340.7490\n",
            "Epoch 6732/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2340.3872 - val_loss: 3337.6755\n",
            "Epoch 6733/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2342.3367 - val_loss: 3330.7185\n",
            "Epoch 6734/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2339.0457 - val_loss: 3331.4031\n",
            "Epoch 6735/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.3162 - val_loss: 3330.0273\n",
            "Epoch 6736/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.5486 - val_loss: 3329.6755\n",
            "Epoch 6737/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2339.4468 - val_loss: 3329.7805\n",
            "Epoch 6738/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2338.7522 - val_loss: 3331.4634\n",
            "Epoch 6739/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2339.4797 - val_loss: 3331.7356\n",
            "Epoch 6740/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2339.1133 - val_loss: 3332.5122\n",
            "Epoch 6741/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2339.7478 - val_loss: 3335.0640\n",
            "Epoch 6742/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.6919 - val_loss: 3335.5635\n",
            "Epoch 6743/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.2112 - val_loss: 3338.1851\n",
            "Epoch 6744/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.4661 - val_loss: 3339.2957\n",
            "Epoch 6745/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.2034 - val_loss: 3338.2490\n",
            "Epoch 6746/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2338.3198 - val_loss: 3337.6677\n",
            "Epoch 6747/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.4834 - val_loss: 3338.6741\n",
            "Epoch 6748/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.5081 - val_loss: 3340.1272\n",
            "Epoch 6749/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.6670 - val_loss: 3340.6018\n",
            "Epoch 6750/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2339.1787 - val_loss: 3340.2004\n",
            "Epoch 6751/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.9368 - val_loss: 3337.1704\n",
            "Epoch 6752/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.4561 - val_loss: 3336.1160\n",
            "Epoch 6753/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.4971 - val_loss: 3335.6885\n",
            "Epoch 6754/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2338.7087 - val_loss: 3335.9673\n",
            "Epoch 6755/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2338.5225 - val_loss: 3335.2410\n",
            "Epoch 6756/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.0144 - val_loss: 3334.1052\n",
            "Epoch 6757/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.9868 - val_loss: 3333.5403\n",
            "Epoch 6758/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2339.4172 - val_loss: 3331.8540\n",
            "Epoch 6759/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.4954 - val_loss: 3335.2612\n",
            "Epoch 6760/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2344.4514 - val_loss: 3336.4846\n",
            "Epoch 6761/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2344.7393 - val_loss: 3336.4866\n",
            "Epoch 6762/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.0586 - val_loss: 3337.3347\n",
            "Epoch 6763/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.9448 - val_loss: 3337.9990\n",
            "Epoch 6764/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.4885 - val_loss: 3338.4343\n",
            "Epoch 6765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.0017 - val_loss: 3339.6257\n",
            "Epoch 6766/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.7708 - val_loss: 3339.3335\n",
            "Epoch 6767/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.1543 - val_loss: 3337.8235\n",
            "Epoch 6768/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2339.2063 - val_loss: 3340.5923\n",
            "Epoch 6769/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.2356 - val_loss: 3341.3779\n",
            "Epoch 6770/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2339.2324 - val_loss: 3340.5703\n",
            "Epoch 6771/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.8369 - val_loss: 3343.1890\n",
            "Epoch 6772/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.0715 - val_loss: 3344.2332\n",
            "Epoch 6773/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2338.9775 - val_loss: 3341.4023\n",
            "Epoch 6774/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.4707 - val_loss: 3340.8848\n",
            "Epoch 6775/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.7639 - val_loss: 3341.9954\n",
            "Epoch 6776/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2338.2593 - val_loss: 3344.6072\n",
            "Epoch 6777/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2339.1125 - val_loss: 3345.7444\n",
            "Epoch 6778/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2338.1421 - val_loss: 3345.0098\n",
            "Epoch 6779/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2337.7046 - val_loss: 3340.1416\n",
            "Epoch 6780/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2341.6409 - val_loss: 3340.0698\n",
            "Epoch 6781/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.8645 - val_loss: 3340.6724\n",
            "Epoch 6782/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2344.5442 - val_loss: 3340.6729\n",
            "Epoch 6783/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2345.5552 - val_loss: 3340.8389\n",
            "Epoch 6784/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2344.2178 - val_loss: 3342.1873\n",
            "Epoch 6785/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2342.4026 - val_loss: 3342.4993\n",
            "Epoch 6786/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2339.2041 - val_loss: 3342.0459\n",
            "Epoch 6787/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2338.7451 - val_loss: 3341.7734\n",
            "Epoch 6788/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.3315 - val_loss: 3341.9670\n",
            "Epoch 6789/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.4333 - val_loss: 3343.4978\n",
            "Epoch 6790/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.2742 - val_loss: 3343.4961\n",
            "Epoch 6791/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.2749 - val_loss: 3347.5305\n",
            "Epoch 6792/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2339.5479 - val_loss: 3347.3652\n",
            "Epoch 6793/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2339.4346 - val_loss: 3345.8052\n",
            "Epoch 6794/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2337.6807 - val_loss: 3345.8511\n",
            "Epoch 6795/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2337.5874 - val_loss: 3346.6208\n",
            "Epoch 6796/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2339.0706 - val_loss: 3350.9456\n",
            "Epoch 6797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.3779 - val_loss: 3351.3315\n",
            "Epoch 6798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2341.4236 - val_loss: 3353.8267\n",
            "Epoch 6799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2342.9133 - val_loss: 3353.5178\n",
            "Epoch 6800/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2340.9502 - val_loss: 3344.9121\n",
            "Epoch 6801/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.3545 - val_loss: 3340.8835\n",
            "Epoch 6802/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2339.9966 - val_loss: 3334.4639\n",
            "Epoch 6803/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.1204 - val_loss: 3333.6953\n",
            "Epoch 6804/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2336.8945 - val_loss: 3333.8220\n",
            "Epoch 6805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.6021 - val_loss: 3334.0884\n",
            "Epoch 6806/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.4172 - val_loss: 3333.0479\n",
            "Epoch 6807/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2338.3508 - val_loss: 3330.3435\n",
            "Epoch 6808/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2337.3081 - val_loss: 3330.3108\n",
            "Epoch 6809/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.2144 - val_loss: 3330.8967\n",
            "Epoch 6810/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2337.0044 - val_loss: 3331.3501\n",
            "Epoch 6811/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.1646 - val_loss: 3331.2859\n",
            "Epoch 6812/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2336.4285 - val_loss: 3330.8203\n",
            "Epoch 6813/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2338.2026 - val_loss: 3334.7356\n",
            "Epoch 6814/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.6987 - val_loss: 3334.0649\n",
            "Epoch 6815/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2336.8284 - val_loss: 3335.1467\n",
            "Epoch 6816/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2335.1240 - val_loss: 3339.5466\n",
            "Epoch 6817/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2336.4055 - val_loss: 3340.5203\n",
            "Epoch 6818/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.2041 - val_loss: 3338.3052\n",
            "Epoch 6819/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.1279 - val_loss: 3339.2292\n",
            "Epoch 6820/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2339.6035 - val_loss: 3333.1909\n",
            "Epoch 6821/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2337.8757 - val_loss: 3331.2693\n",
            "Epoch 6822/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.2395 - val_loss: 3332.8022\n",
            "Epoch 6823/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.0212 - val_loss: 3334.1704\n",
            "Epoch 6824/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.1367 - val_loss: 3336.0667\n",
            "Epoch 6825/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.2693 - val_loss: 3337.0857\n",
            "Epoch 6826/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.0828 - val_loss: 3337.8877\n",
            "Epoch 6827/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.1770 - val_loss: 3340.0178\n",
            "Epoch 6828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.9333 - val_loss: 3343.9465\n",
            "Epoch 6829/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.3708 - val_loss: 3344.4856\n",
            "Epoch 6830/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2338.1787 - val_loss: 3343.7690\n",
            "Epoch 6831/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.9050 - val_loss: 3343.6052\n",
            "Epoch 6832/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2337.8604 - val_loss: 3344.7859\n",
            "Epoch 6833/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2338.4192 - val_loss: 3343.7285\n",
            "Epoch 6834/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2340.0889 - val_loss: 3337.0227\n",
            "Epoch 6835/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.3823 - val_loss: 3334.3230\n",
            "Epoch 6836/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2336.4448 - val_loss: 3333.5769\n",
            "Epoch 6837/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2337.4189 - val_loss: 3339.1057\n",
            "Epoch 6838/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2337.4548 - val_loss: 3340.8293\n",
            "Epoch 6839/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.9233 - val_loss: 3341.8406\n",
            "Epoch 6840/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.2393 - val_loss: 3341.7300\n",
            "Epoch 6841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2340.4568 - val_loss: 3348.9038\n",
            "Epoch 6842/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2343.8254 - val_loss: 3352.1665\n",
            "Epoch 6843/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2345.5178 - val_loss: 3355.5176\n",
            "Epoch 6844/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2346.3413 - val_loss: 3354.9512\n",
            "Epoch 6845/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2344.5574 - val_loss: 3352.1187\n",
            "Epoch 6846/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2342.9795 - val_loss: 3351.0793\n",
            "Epoch 6847/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2344.1704 - val_loss: 3351.9543\n",
            "Epoch 6848/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2340.3064 - val_loss: 3342.4976\n",
            "Epoch 6849/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2338.1814 - val_loss: 3338.5288\n",
            "Epoch 6850/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.3403 - val_loss: 3335.6140\n",
            "Epoch 6851/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2335.2898 - val_loss: 3333.4382\n",
            "Epoch 6852/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.4492 - val_loss: 3332.9368\n",
            "Epoch 6853/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2335.5715 - val_loss: 3333.0906\n",
            "Epoch 6854/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2337.2363 - val_loss: 3333.5505\n",
            "Epoch 6855/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.7896 - val_loss: 3333.2927\n",
            "Epoch 6856/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.3110 - val_loss: 3332.5864\n",
            "Epoch 6857/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2337.0815 - val_loss: 3330.8279\n",
            "Epoch 6858/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2335.0144 - val_loss: 3334.1667\n",
            "Epoch 6859/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.1582 - val_loss: 3339.1184\n",
            "Epoch 6860/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.2383 - val_loss: 3339.0098\n",
            "Epoch 6861/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2336.5093 - val_loss: 3337.7053\n",
            "Epoch 6862/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.3796 - val_loss: 3337.9062\n",
            "Epoch 6863/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.0735 - val_loss: 3337.3286\n",
            "Epoch 6864/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2337.2263 - val_loss: 3336.5186\n",
            "Epoch 6865/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.9258 - val_loss: 3333.3784\n",
            "Epoch 6866/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2335.3733 - val_loss: 3333.9490\n",
            "Epoch 6867/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.1162 - val_loss: 3334.6433\n",
            "Epoch 6868/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.7864 - val_loss: 3339.6411\n",
            "Epoch 6869/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2335.9946 - val_loss: 3339.1306\n",
            "Epoch 6870/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.2468 - val_loss: 3337.8708\n",
            "Epoch 6871/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.7039 - val_loss: 3337.9199\n",
            "Epoch 6872/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.7822 - val_loss: 3341.4277\n",
            "Epoch 6873/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2334.9822 - val_loss: 3340.6978\n",
            "Epoch 6874/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.1519 - val_loss: 3339.7454\n",
            "Epoch 6875/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.7097 - val_loss: 3340.5513\n",
            "Epoch 6876/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.6389 - val_loss: 3345.2229\n",
            "Epoch 6877/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.0273 - val_loss: 3344.7163\n",
            "Epoch 6878/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2335.9556 - val_loss: 3342.8301\n",
            "Epoch 6879/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.6233 - val_loss: 3340.1794\n",
            "Epoch 6880/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.6475 - val_loss: 3340.4209\n",
            "Epoch 6881/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.1475 - val_loss: 3345.5273\n",
            "Epoch 6882/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2337.8687 - val_loss: 3344.0146\n",
            "Epoch 6883/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.8318 - val_loss: 3341.8748\n",
            "Epoch 6884/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2335.0989 - val_loss: 3340.8188\n",
            "Epoch 6885/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2334.8459 - val_loss: 3341.1733\n",
            "Epoch 6886/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2335.4258 - val_loss: 3341.4304\n",
            "Epoch 6887/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.8059 - val_loss: 3341.9495\n",
            "Epoch 6888/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.0945 - val_loss: 3340.8096\n",
            "Epoch 6889/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.4729 - val_loss: 3341.6538\n",
            "Epoch 6890/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2336.3186 - val_loss: 3342.2036\n",
            "Epoch 6891/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.6606 - val_loss: 3342.6733\n",
            "Epoch 6892/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.6086 - val_loss: 3342.4810\n",
            "Epoch 6893/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.7068 - val_loss: 3344.3687\n",
            "Epoch 6894/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.8501 - val_loss: 3344.6321\n",
            "Epoch 6895/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2336.1594 - val_loss: 3345.0142\n",
            "Epoch 6896/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2336.4749 - val_loss: 3344.0330\n",
            "Epoch 6897/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.5686 - val_loss: 3340.3503\n",
            "Epoch 6898/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2334.4614 - val_loss: 3339.9229\n",
            "Epoch 6899/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.5166 - val_loss: 3339.2749\n",
            "Epoch 6900/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2333.4607 - val_loss: 3342.3579\n",
            "Epoch 6901/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2335.0752 - val_loss: 3341.8062\n",
            "Epoch 6902/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.6465 - val_loss: 3344.4521\n",
            "Epoch 6903/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2336.6592 - val_loss: 3351.9150\n",
            "Epoch 6904/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2338.3540 - val_loss: 3352.7612\n",
            "Epoch 6905/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.5374 - val_loss: 3350.7075\n",
            "Epoch 6906/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2337.5513 - val_loss: 3346.8110\n",
            "Epoch 6907/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2335.3679 - val_loss: 3345.3062\n",
            "Epoch 6908/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.7275 - val_loss: 3346.7693\n",
            "Epoch 6909/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.8831 - val_loss: 3342.8845\n",
            "Epoch 6910/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.2251 - val_loss: 3341.6128\n",
            "Epoch 6911/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2334.0891 - val_loss: 3339.5242\n",
            "Epoch 6912/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2338.4277 - val_loss: 3336.9446\n",
            "Epoch 6913/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2338.6052 - val_loss: 3336.0977\n",
            "Epoch 6914/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.8213 - val_loss: 3336.0315\n",
            "Epoch 6915/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.3037 - val_loss: 3336.7788\n",
            "Epoch 6916/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2334.4453 - val_loss: 3339.9465\n",
            "Epoch 6917/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.4546 - val_loss: 3341.0427\n",
            "Epoch 6918/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.2876 - val_loss: 3342.0312\n",
            "Epoch 6919/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.4817 - val_loss: 3340.5310\n",
            "Epoch 6920/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2335.9177 - val_loss: 3339.5122\n",
            "Epoch 6921/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.0706 - val_loss: 3342.0823\n",
            "Epoch 6922/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.8921 - val_loss: 3346.2710\n",
            "Epoch 6923/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2337.3137 - val_loss: 3346.7124\n",
            "Epoch 6924/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2337.4663 - val_loss: 3346.2842\n",
            "Epoch 6925/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2337.4507 - val_loss: 3346.6699\n",
            "Epoch 6926/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2336.7986 - val_loss: 3344.0938\n",
            "Epoch 6927/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2335.2905 - val_loss: 3342.4558\n",
            "Epoch 6928/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.3662 - val_loss: 3342.0208\n",
            "Epoch 6929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.2578 - val_loss: 3340.8918\n",
            "Epoch 6930/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2335.1079 - val_loss: 3339.5361\n",
            "Epoch 6931/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2334.1028 - val_loss: 3339.8950\n",
            "Epoch 6932/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.1680 - val_loss: 3343.4910\n",
            "Epoch 6933/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.0669 - val_loss: 3342.6187\n",
            "Epoch 6934/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.1677 - val_loss: 3341.7507\n",
            "Epoch 6935/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2333.7488 - val_loss: 3342.1370\n",
            "Epoch 6936/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.9011 - val_loss: 3340.9915\n",
            "Epoch 6937/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.6265 - val_loss: 3341.4641\n",
            "Epoch 6938/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.8501 - val_loss: 3342.5127\n",
            "Epoch 6939/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.6963 - val_loss: 3344.4087\n",
            "Epoch 6940/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2336.5518 - val_loss: 3343.0168\n",
            "Epoch 6941/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2335.2427 - val_loss: 3340.7571\n",
            "Epoch 6942/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.2412 - val_loss: 3336.9873\n",
            "Epoch 6943/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.3181 - val_loss: 3335.9177\n",
            "Epoch 6944/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2335.3232 - val_loss: 3333.5291\n",
            "Epoch 6945/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.2488 - val_loss: 3332.5322\n",
            "Epoch 6946/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2334.5891 - val_loss: 3332.9915\n",
            "Epoch 6947/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.5137 - val_loss: 3332.6858\n",
            "Epoch 6948/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2333.7429 - val_loss: 3331.9807\n",
            "Epoch 6949/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2333.2754 - val_loss: 3332.1917\n",
            "Epoch 6950/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2334.3484 - val_loss: 3333.0330\n",
            "Epoch 6951/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2333.5264 - val_loss: 3332.2034\n",
            "Epoch 6952/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.3445 - val_loss: 3331.9922\n",
            "Epoch 6953/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2333.9307 - val_loss: 3333.5723\n",
            "Epoch 6954/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2333.4905 - val_loss: 3335.8877\n",
            "Epoch 6955/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.5725 - val_loss: 3340.8784\n",
            "Epoch 6956/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.6699 - val_loss: 3342.0503\n",
            "Epoch 6957/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2333.1917 - val_loss: 3338.6489\n",
            "Epoch 6958/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.7151 - val_loss: 3338.1060\n",
            "Epoch 6959/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.6589 - val_loss: 3337.3230\n",
            "Epoch 6960/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.4558 - val_loss: 3337.3230\n",
            "Epoch 6961/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2331.5342 - val_loss: 3341.3516\n",
            "Epoch 6962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.4575 - val_loss: 3342.8789\n",
            "Epoch 6963/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.1001 - val_loss: 3340.2590\n",
            "Epoch 6964/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.1150 - val_loss: 3340.7915\n",
            "Epoch 6965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.8188 - val_loss: 3338.0417\n",
            "Epoch 6966/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2334.3921 - val_loss: 3337.8884\n",
            "Epoch 6967/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.6975 - val_loss: 3339.9529\n",
            "Epoch 6968/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.3259 - val_loss: 3340.6492\n",
            "Epoch 6969/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.3535 - val_loss: 3341.4077\n",
            "Epoch 6970/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.9102 - val_loss: 3343.4783\n",
            "Epoch 6971/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2332.1343 - val_loss: 3338.6165\n",
            "Epoch 6972/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.0781 - val_loss: 3336.6746\n",
            "Epoch 6973/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.2524 - val_loss: 3337.2991\n",
            "Epoch 6974/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.4766 - val_loss: 3337.3115\n",
            "Epoch 6975/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2332.4504 - val_loss: 3337.4470\n",
            "Epoch 6976/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.4263 - val_loss: 3337.6145\n",
            "Epoch 6977/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2332.8586 - val_loss: 3338.6426\n",
            "Epoch 6978/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.8523 - val_loss: 3340.2346\n",
            "Epoch 6979/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2335.1326 - val_loss: 3340.2866\n",
            "Epoch 6980/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.7061 - val_loss: 3338.8933\n",
            "Epoch 6981/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2339.1416 - val_loss: 3332.3030\n",
            "Epoch 6982/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.1252 - val_loss: 3327.7795\n",
            "Epoch 6983/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2333.0574 - val_loss: 3327.5090\n",
            "Epoch 6984/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.8982 - val_loss: 3328.4478\n",
            "Epoch 6985/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2332.8418 - val_loss: 3330.1379\n",
            "Epoch 6986/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.9578 - val_loss: 3330.4097\n",
            "Epoch 6987/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.5615 - val_loss: 3329.6672\n",
            "Epoch 6988/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2332.7097 - val_loss: 3330.3333\n",
            "Epoch 6989/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.9983 - val_loss: 3330.6265\n",
            "Epoch 6990/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.2000 - val_loss: 3330.1104\n",
            "Epoch 6991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.6270 - val_loss: 3330.0247\n",
            "Epoch 6992/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.9028 - val_loss: 3330.2803\n",
            "Epoch 6993/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2331.3745 - val_loss: 3330.9551\n",
            "Epoch 6994/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.8811 - val_loss: 3329.2429\n",
            "Epoch 6995/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2332.2856 - val_loss: 3329.1003\n",
            "Epoch 6996/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2333.2610 - val_loss: 3329.6047\n",
            "Epoch 6997/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.0364 - val_loss: 3331.0872\n",
            "Epoch 6998/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2332.2861 - val_loss: 3334.0603\n",
            "Epoch 6999/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.3672 - val_loss: 3335.7209\n",
            "Epoch 7000/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.9390 - val_loss: 3337.1509\n",
            "Epoch 7001/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.2839 - val_loss: 3335.6609\n",
            "Epoch 7002/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2330.6660 - val_loss: 3332.9775\n",
            "Epoch 7003/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2332.2305 - val_loss: 3330.3828\n",
            "Epoch 7004/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.1404 - val_loss: 3330.1104\n",
            "Epoch 7005/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.2183 - val_loss: 3328.8069\n",
            "Epoch 7006/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2331.2419 - val_loss: 3329.1597\n",
            "Epoch 7007/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2330.9668 - val_loss: 3329.7578\n",
            "Epoch 7008/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2331.5352 - val_loss: 3331.9143\n",
            "Epoch 7009/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.2795 - val_loss: 3334.9465\n",
            "Epoch 7010/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.3943 - val_loss: 3332.0679\n",
            "Epoch 7011/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2330.8040 - val_loss: 3332.3816\n",
            "Epoch 7012/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.9983 - val_loss: 3331.9744\n",
            "Epoch 7013/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.9697 - val_loss: 3329.2644\n",
            "Epoch 7014/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.7761 - val_loss: 3325.9600\n",
            "Epoch 7015/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.6050 - val_loss: 3324.9971\n",
            "Epoch 7016/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2332.9260 - val_loss: 3327.1067\n",
            "Epoch 7017/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2331.3352 - val_loss: 3328.0625\n",
            "Epoch 7018/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.4937 - val_loss: 3330.8259\n",
            "Epoch 7019/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.3606 - val_loss: 3331.6362\n",
            "Epoch 7020/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.7070 - val_loss: 3332.4561\n",
            "Epoch 7021/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2332.9573 - val_loss: 3332.1616\n",
            "Epoch 7022/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.9502 - val_loss: 3327.0591\n",
            "Epoch 7023/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.0020 - val_loss: 3325.6514\n",
            "Epoch 7024/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.5125 - val_loss: 3325.5664\n",
            "Epoch 7025/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.3645 - val_loss: 3325.5996\n",
            "Epoch 7026/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2332.8396 - val_loss: 3326.1521\n",
            "Epoch 7027/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.5964 - val_loss: 3326.0955\n",
            "Epoch 7028/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.8625 - val_loss: 3325.2546\n",
            "Epoch 7029/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.5122 - val_loss: 3323.6370\n",
            "Epoch 7030/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2331.3577 - val_loss: 3324.8765\n",
            "Epoch 7031/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.7144 - val_loss: 3325.6648\n",
            "Epoch 7032/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.2671 - val_loss: 3327.2097\n",
            "Epoch 7033/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2337.9583 - val_loss: 3326.8772\n",
            "Epoch 7034/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2335.6350 - val_loss: 3326.2844\n",
            "Epoch 7035/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2335.0483 - val_loss: 3325.6899\n",
            "Epoch 7036/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.4333 - val_loss: 3326.1467\n",
            "Epoch 7037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.6697 - val_loss: 3327.5190\n",
            "Epoch 7038/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2332.2061 - val_loss: 3333.2700\n",
            "Epoch 7039/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.1667 - val_loss: 3334.2878\n",
            "Epoch 7040/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.5125 - val_loss: 3338.3044\n",
            "Epoch 7041/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.4714 - val_loss: 3332.5186\n",
            "Epoch 7042/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.4946 - val_loss: 3330.5891\n",
            "Epoch 7043/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2332.5251 - val_loss: 3330.7659\n",
            "Epoch 7044/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2331.7629 - val_loss: 3330.4546\n",
            "Epoch 7045/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.7461 - val_loss: 3329.1763\n",
            "Epoch 7046/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.3457 - val_loss: 3329.0559\n",
            "Epoch 7047/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2331.1626 - val_loss: 3328.9690\n",
            "Epoch 7048/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.8015 - val_loss: 3329.2339\n",
            "Epoch 7049/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2331.0830 - val_loss: 3330.6023\n",
            "Epoch 7050/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.5879 - val_loss: 3332.4070\n",
            "Epoch 7051/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.7644 - val_loss: 3333.2588\n",
            "Epoch 7052/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2331.2844 - val_loss: 3337.0376\n",
            "Epoch 7053/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.3369 - val_loss: 3340.6553\n",
            "Epoch 7054/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2334.1589 - val_loss: 3341.1138\n",
            "Epoch 7055/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2334.6106 - val_loss: 3340.0554\n",
            "Epoch 7056/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2334.4749 - val_loss: 3341.1892\n",
            "Epoch 7057/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.6921 - val_loss: 3334.6755\n",
            "Epoch 7058/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.0479 - val_loss: 3332.6941\n",
            "Epoch 7059/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.9800 - val_loss: 3333.0046\n",
            "Epoch 7060/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.5447 - val_loss: 3333.2947\n",
            "Epoch 7061/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.0854 - val_loss: 3333.2798\n",
            "Epoch 7062/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2333.3701 - val_loss: 3335.2964\n",
            "Epoch 7063/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.8655 - val_loss: 3336.4229\n",
            "Epoch 7064/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.1614 - val_loss: 3335.8984\n",
            "Epoch 7065/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.6414 - val_loss: 3332.6523\n",
            "Epoch 7066/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2331.0134 - val_loss: 3333.1033\n",
            "Epoch 7067/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.1575 - val_loss: 3334.0432\n",
            "Epoch 7068/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.0107 - val_loss: 3334.2722\n",
            "Epoch 7069/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2332.1340 - val_loss: 3330.0134\n",
            "Epoch 7070/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2331.7034 - val_loss: 3331.0056\n",
            "Epoch 7071/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.2830 - val_loss: 3333.5029\n",
            "Epoch 7072/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.6746 - val_loss: 3333.2925\n",
            "Epoch 7073/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.4280 - val_loss: 3332.7356\n",
            "Epoch 7074/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.3713 - val_loss: 3333.8462\n",
            "Epoch 7075/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2331.8547 - val_loss: 3334.1243\n",
            "Epoch 7076/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2331.9661 - val_loss: 3338.9871\n",
            "Epoch 7077/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.3625 - val_loss: 3340.3767\n",
            "Epoch 7078/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.7754 - val_loss: 3339.9207\n",
            "Epoch 7079/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.9827 - val_loss: 3338.9639\n",
            "Epoch 7080/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.3005 - val_loss: 3335.0801\n",
            "Epoch 7081/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2332.1455 - val_loss: 3335.1179\n",
            "Epoch 7082/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.9060 - val_loss: 3330.0796\n",
            "Epoch 7083/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2336.9963 - val_loss: 3327.1812\n",
            "Epoch 7084/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.5955 - val_loss: 3327.8252\n",
            "Epoch 7085/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.2671 - val_loss: 3328.7534\n",
            "Epoch 7086/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2331.7131 - val_loss: 3326.1216\n",
            "Epoch 7087/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2331.5488 - val_loss: 3325.6636\n",
            "Epoch 7088/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2333.0337 - val_loss: 3324.8904\n",
            "Epoch 7089/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.5874 - val_loss: 3325.6892\n",
            "Epoch 7090/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.3208 - val_loss: 3325.8428\n",
            "Epoch 7091/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2334.5525 - val_loss: 3326.2388\n",
            "Epoch 7092/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.3535 - val_loss: 3329.2246\n",
            "Epoch 7093/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.6177 - val_loss: 3334.5095\n",
            "Epoch 7094/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2330.0061 - val_loss: 3334.2781\n",
            "Epoch 7095/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2329.9302 - val_loss: 3334.4573\n",
            "Epoch 7096/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.4705 - val_loss: 3334.3660\n",
            "Epoch 7097/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.9944 - val_loss: 3334.5400\n",
            "Epoch 7098/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2329.1055 - val_loss: 3335.3721\n",
            "Epoch 7099/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.8984 - val_loss: 3336.0647\n",
            "Epoch 7100/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.1421 - val_loss: 3336.3096\n",
            "Epoch 7101/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2329.5322 - val_loss: 3336.1233\n",
            "Epoch 7102/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.6038 - val_loss: 3336.3892\n",
            "Epoch 7103/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.0911 - val_loss: 3336.0085\n",
            "Epoch 7104/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2329.8264 - val_loss: 3336.7515\n",
            "Epoch 7105/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.1331 - val_loss: 3336.2212\n",
            "Epoch 7106/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.2520 - val_loss: 3336.3440\n",
            "Epoch 7107/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.0095 - val_loss: 3335.1621\n",
            "Epoch 7108/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.0847 - val_loss: 3335.8784\n",
            "Epoch 7109/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2328.9668 - val_loss: 3336.2202\n",
            "Epoch 7110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.8242 - val_loss: 3335.9377\n",
            "Epoch 7111/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.6589 - val_loss: 3336.3071\n",
            "Epoch 7112/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.9954 - val_loss: 3337.2595\n",
            "Epoch 7113/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.7437 - val_loss: 3335.9336\n",
            "Epoch 7114/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.8411 - val_loss: 3331.0583\n",
            "Epoch 7115/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2328.1895 - val_loss: 3330.2629\n",
            "Epoch 7116/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.1296 - val_loss: 3328.0857\n",
            "Epoch 7117/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.0098 - val_loss: 3327.9934\n",
            "Epoch 7118/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.2854 - val_loss: 3329.0679\n",
            "Epoch 7119/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.4490 - val_loss: 3334.6865\n",
            "Epoch 7120/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2328.9912 - val_loss: 3335.6619\n",
            "Epoch 7121/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.4062 - val_loss: 3332.8037\n",
            "Epoch 7122/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.8259 - val_loss: 3336.5776\n",
            "Epoch 7123/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.3035 - val_loss: 3334.9373\n",
            "Epoch 7124/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.9583 - val_loss: 3333.4558\n",
            "Epoch 7125/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.0537 - val_loss: 3332.3567\n",
            "Epoch 7126/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2328.9751 - val_loss: 3333.5249\n",
            "Epoch 7127/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2329.6770 - val_loss: 3335.8196\n",
            "Epoch 7128/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.1038 - val_loss: 3336.5176\n",
            "Epoch 7129/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.8772 - val_loss: 3334.1621\n",
            "Epoch 7130/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.3706 - val_loss: 3333.7493\n",
            "Epoch 7131/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.8438 - val_loss: 3336.1433\n",
            "Epoch 7132/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2329.6333 - val_loss: 3337.2654\n",
            "Epoch 7133/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2329.0256 - val_loss: 3336.8372\n",
            "Epoch 7134/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.1040 - val_loss: 3335.8733\n",
            "Epoch 7135/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.1226 - val_loss: 3335.9004\n",
            "Epoch 7136/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.6511 - val_loss: 3336.0945\n",
            "Epoch 7137/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.2539 - val_loss: 3334.4297\n",
            "Epoch 7138/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2333.3691 - val_loss: 3334.5786\n",
            "Epoch 7139/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.8208 - val_loss: 3334.7971\n",
            "Epoch 7140/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.7349 - val_loss: 3335.6917\n",
            "Epoch 7141/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.9617 - val_loss: 3335.1538\n",
            "Epoch 7142/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.2075 - val_loss: 3334.7422\n",
            "Epoch 7143/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2329.4160 - val_loss: 3335.2009\n",
            "Epoch 7144/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.7129 - val_loss: 3336.2810\n",
            "Epoch 7145/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.2334 - val_loss: 3336.0698\n",
            "Epoch 7146/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.5957 - val_loss: 3336.7380\n",
            "Epoch 7147/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2330.1907 - val_loss: 3338.8958\n",
            "Epoch 7148/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.6375 - val_loss: 3334.9312\n",
            "Epoch 7149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.1680 - val_loss: 3334.1099\n",
            "Epoch 7150/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.9697 - val_loss: 3334.0994\n",
            "Epoch 7151/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.8606 - val_loss: 3334.0330\n",
            "Epoch 7152/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.2168 - val_loss: 3334.7798\n",
            "Epoch 7153/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.9033 - val_loss: 3334.4519\n",
            "Epoch 7154/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.2307 - val_loss: 3334.6633\n",
            "Epoch 7155/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2328.7231 - val_loss: 3330.2227\n",
            "Epoch 7156/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.7041 - val_loss: 3331.2180\n",
            "Epoch 7157/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.9226 - val_loss: 3332.5049\n",
            "Epoch 7158/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.6216 - val_loss: 3332.9255\n",
            "Epoch 7159/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.8928 - val_loss: 3334.8728\n",
            "Epoch 7160/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2328.6211 - val_loss: 3335.3459\n",
            "Epoch 7161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.7742 - val_loss: 3335.5938\n",
            "Epoch 7162/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.5166 - val_loss: 3335.3420\n",
            "Epoch 7163/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.0164 - val_loss: 3337.0176\n",
            "Epoch 7164/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2327.6890 - val_loss: 3337.7451\n",
            "Epoch 7165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.3418 - val_loss: 3336.1113\n",
            "Epoch 7166/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.5132 - val_loss: 3336.6587\n",
            "Epoch 7167/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.9180 - val_loss: 3339.4746\n",
            "Epoch 7168/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.1240 - val_loss: 3342.1145\n",
            "Epoch 7169/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.8008 - val_loss: 3342.4810\n",
            "Epoch 7170/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.3809 - val_loss: 3341.4055\n",
            "Epoch 7171/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.2588 - val_loss: 3342.4902\n",
            "Epoch 7172/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2327.7144 - val_loss: 3348.1555\n",
            "Epoch 7173/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2333.6855 - val_loss: 3351.4888\n",
            "Epoch 7174/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.8025 - val_loss: 3351.4810\n",
            "Epoch 7175/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.5999 - val_loss: 3352.1528\n",
            "Epoch 7176/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2334.6670 - val_loss: 3349.5940\n",
            "Epoch 7177/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.7476 - val_loss: 3348.1240\n",
            "Epoch 7178/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2332.5898 - val_loss: 3343.9478\n",
            "Epoch 7179/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2331.7656 - val_loss: 3342.2307\n",
            "Epoch 7180/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.0894 - val_loss: 3345.9084\n",
            "Epoch 7181/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2330.4961 - val_loss: 3345.4238\n",
            "Epoch 7182/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.5107 - val_loss: 3343.4065\n",
            "Epoch 7183/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.1746 - val_loss: 3344.2073\n",
            "Epoch 7184/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2329.4880 - val_loss: 3344.1609\n",
            "Epoch 7185/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.7087 - val_loss: 3340.6909\n",
            "Epoch 7186/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.9351 - val_loss: 3339.8291\n",
            "Epoch 7187/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.0793 - val_loss: 3339.3367\n",
            "Epoch 7188/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2327.0454 - val_loss: 3339.6658\n",
            "Epoch 7189/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.1794 - val_loss: 3340.6235\n",
            "Epoch 7190/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2327.0850 - val_loss: 3340.1675\n",
            "Epoch 7191/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.2595 - val_loss: 3339.7861\n",
            "Epoch 7192/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.3618 - val_loss: 3343.9797\n",
            "Epoch 7193/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2328.5488 - val_loss: 3346.8230\n",
            "Epoch 7194/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.9912 - val_loss: 3346.9121\n",
            "Epoch 7195/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.3289 - val_loss: 3345.9990\n",
            "Epoch 7196/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2328.6267 - val_loss: 3341.0879\n",
            "Epoch 7197/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.1057 - val_loss: 3341.4702\n",
            "Epoch 7198/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2329.0493 - val_loss: 3342.0996\n",
            "Epoch 7199/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.3582 - val_loss: 3342.3125\n",
            "Epoch 7200/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.5012 - val_loss: 3341.4177\n",
            "Epoch 7201/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2328.5188 - val_loss: 3343.8718\n",
            "Epoch 7202/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.1287 - val_loss: 3344.8491\n",
            "Epoch 7203/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.5457 - val_loss: 3346.4390\n",
            "Epoch 7204/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2327.1873 - val_loss: 3347.2373\n",
            "Epoch 7205/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.7964 - val_loss: 3346.7642\n",
            "Epoch 7206/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.6233 - val_loss: 3346.5850\n",
            "Epoch 7207/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.7241 - val_loss: 3344.7598\n",
            "Epoch 7208/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.4050 - val_loss: 3344.9038\n",
            "Epoch 7209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.8606 - val_loss: 3343.7173\n",
            "Epoch 7210/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.6826 - val_loss: 3342.3840\n",
            "Epoch 7211/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.6653 - val_loss: 3342.8125\n",
            "Epoch 7212/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2330.1284 - val_loss: 3344.2188\n",
            "Epoch 7213/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2332.4143 - val_loss: 3345.4033\n",
            "Epoch 7214/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.4971 - val_loss: 3345.0952\n",
            "Epoch 7215/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.6660 - val_loss: 3344.9189\n",
            "Epoch 7216/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2330.7334 - val_loss: 3344.0427\n",
            "Epoch 7217/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.7407 - val_loss: 3345.9016\n",
            "Epoch 7218/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.4114 - val_loss: 3346.4573\n",
            "Epoch 7219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.0847 - val_loss: 3341.7397\n",
            "Epoch 7220/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2328.1519 - val_loss: 3341.2747\n",
            "Epoch 7221/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2327.9146 - val_loss: 3341.4905\n",
            "Epoch 7222/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.6064 - val_loss: 3343.3784\n",
            "Epoch 7223/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.1914 - val_loss: 3342.4832\n",
            "Epoch 7224/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.2290 - val_loss: 3340.9365\n",
            "Epoch 7225/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2328.3301 - val_loss: 3337.6091\n",
            "Epoch 7226/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.8286 - val_loss: 3337.9666\n",
            "Epoch 7227/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.8792 - val_loss: 3338.2371\n",
            "Epoch 7228/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2327.7273 - val_loss: 3337.4207\n",
            "Epoch 7229/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2327.2759 - val_loss: 3337.4929\n",
            "Epoch 7230/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2327.1865 - val_loss: 3337.8245\n",
            "Epoch 7231/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2326.2698 - val_loss: 3337.0959\n",
            "Epoch 7232/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.0491 - val_loss: 3336.6313\n",
            "Epoch 7233/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.9778 - val_loss: 3336.8389\n",
            "Epoch 7234/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2327.2021 - val_loss: 3337.5625\n",
            "Epoch 7235/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2331.5769 - val_loss: 3341.8621\n",
            "Epoch 7236/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.8198 - val_loss: 3341.1384\n",
            "Epoch 7237/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.1406 - val_loss: 3340.3806\n",
            "Epoch 7238/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.9272 - val_loss: 3341.1104\n",
            "Epoch 7239/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2330.0493 - val_loss: 3343.7522\n",
            "Epoch 7240/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.5364 - val_loss: 3343.6194\n",
            "Epoch 7241/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.5691 - val_loss: 3345.6140\n",
            "Epoch 7242/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.3293 - val_loss: 3345.8455\n",
            "Epoch 7243/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2327.4739 - val_loss: 3345.7419\n",
            "Epoch 7244/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.3396 - val_loss: 3344.5383\n",
            "Epoch 7245/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.0879 - val_loss: 3344.7129\n",
            "Epoch 7246/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.2549 - val_loss: 3348.1492\n",
            "Epoch 7247/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2328.8413 - val_loss: 3349.2810\n",
            "Epoch 7248/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.1130 - val_loss: 3347.0710\n",
            "Epoch 7249/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.2444 - val_loss: 3346.3665\n",
            "Epoch 7250/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.0950 - val_loss: 3346.3982\n",
            "Epoch 7251/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.8958 - val_loss: 3345.4583\n",
            "Epoch 7252/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2327.5076 - val_loss: 3348.4172\n",
            "Epoch 7253/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2329.2915 - val_loss: 3354.4873\n",
            "Epoch 7254/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2331.3315 - val_loss: 3360.9927\n",
            "Epoch 7255/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.1975 - val_loss: 3363.9944\n",
            "Epoch 7256/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.8799 - val_loss: 3362.1882\n",
            "Epoch 7257/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2331.0833 - val_loss: 3355.8311\n",
            "Epoch 7258/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.8594 - val_loss: 3352.4438\n",
            "Epoch 7259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.9351 - val_loss: 3351.3171\n",
            "Epoch 7260/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2327.4250 - val_loss: 3350.1052\n",
            "Epoch 7261/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.6814 - val_loss: 3355.2422\n",
            "Epoch 7262/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.6782 - val_loss: 3358.6160\n",
            "Epoch 7263/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2331.8889 - val_loss: 3359.2205\n",
            "Epoch 7264/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2332.6624 - val_loss: 3360.8892\n",
            "Epoch 7265/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2334.1631 - val_loss: 3360.9148\n",
            "Epoch 7266/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2329.1235 - val_loss: 3352.6763\n",
            "Epoch 7267/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2328.7637 - val_loss: 3346.1619\n",
            "Epoch 7268/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.9016 - val_loss: 3340.1187\n",
            "Epoch 7269/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2328.0850 - val_loss: 3338.2646\n",
            "Epoch 7270/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.1208 - val_loss: 3337.9434\n",
            "Epoch 7271/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.1479 - val_loss: 3338.2046\n",
            "Epoch 7272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2325.4409 - val_loss: 3339.2852\n",
            "Epoch 7273/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.7393 - val_loss: 3341.4719\n",
            "Epoch 7274/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.1101 - val_loss: 3336.2217\n",
            "Epoch 7275/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2326.9268 - val_loss: 3335.1240\n",
            "Epoch 7276/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.6621 - val_loss: 3332.2002\n",
            "Epoch 7277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.0759 - val_loss: 3331.1216\n",
            "Epoch 7278/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2330.5295 - val_loss: 3330.4890\n",
            "Epoch 7279/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.7090 - val_loss: 3330.5073\n",
            "Epoch 7280/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.2883 - val_loss: 3332.7812\n",
            "Epoch 7281/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.9758 - val_loss: 3334.2708\n",
            "Epoch 7282/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2330.5476 - val_loss: 3334.6250\n",
            "Epoch 7283/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2333.7000 - val_loss: 3333.5454\n",
            "Epoch 7284/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2334.6233 - val_loss: 3332.9585\n",
            "Epoch 7285/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2339.7375 - val_loss: 3332.6677\n",
            "Epoch 7286/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2335.4648 - val_loss: 3330.7935\n",
            "Epoch 7287/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2330.0566 - val_loss: 3330.3435\n",
            "Epoch 7288/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2330.2244 - val_loss: 3330.7781\n",
            "Epoch 7289/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.6833 - val_loss: 3331.0793\n",
            "Epoch 7290/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.9609 - val_loss: 3331.0513\n",
            "Epoch 7291/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.7310 - val_loss: 3330.6553\n",
            "Epoch 7292/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2328.1414 - val_loss: 3329.1448\n",
            "Epoch 7293/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2329.0901 - val_loss: 3327.4641\n",
            "Epoch 7294/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.8210 - val_loss: 3327.2363\n",
            "Epoch 7295/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.3909 - val_loss: 3326.8157\n",
            "Epoch 7296/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2329.8054 - val_loss: 3326.5415\n",
            "Epoch 7297/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2329.8748 - val_loss: 3326.2156\n",
            "Epoch 7298/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2328.8875 - val_loss: 3325.8828\n",
            "Epoch 7299/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.7539 - val_loss: 3325.9465\n",
            "Epoch 7300/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.2065 - val_loss: 3326.0376\n",
            "Epoch 7301/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.2915 - val_loss: 3325.5254\n",
            "Epoch 7302/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2326.8127 - val_loss: 3325.9600\n",
            "Epoch 7303/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2327.2581 - val_loss: 3326.2742\n",
            "Epoch 7304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.3132 - val_loss: 3328.1235\n",
            "Epoch 7305/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2325.1672 - val_loss: 3328.5046\n",
            "Epoch 7306/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.8921 - val_loss: 3327.6370\n",
            "Epoch 7307/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.2966 - val_loss: 3328.3027\n",
            "Epoch 7308/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.2776 - val_loss: 3330.3174\n",
            "Epoch 7309/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.6382 - val_loss: 3330.8572\n",
            "Epoch 7310/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2328.1775 - val_loss: 3331.5510\n",
            "Epoch 7311/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2326.3594 - val_loss: 3332.9185\n",
            "Epoch 7312/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.2949 - val_loss: 3333.0239\n",
            "Epoch 7313/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2325.1128 - val_loss: 3333.1960\n",
            "Epoch 7314/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.7903 - val_loss: 3336.5803\n",
            "Epoch 7315/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2327.9424 - val_loss: 3337.0222\n",
            "Epoch 7316/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.0352 - val_loss: 3336.7236\n",
            "Epoch 7317/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2325.7271 - val_loss: 3337.7891\n",
            "Epoch 7318/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.2463 - val_loss: 3337.6216\n",
            "Epoch 7319/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.3643 - val_loss: 3338.2844\n",
            "Epoch 7320/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.8901 - val_loss: 3337.6201\n",
            "Epoch 7321/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.2632 - val_loss: 3337.1008\n",
            "Epoch 7322/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2323.5676 - val_loss: 3336.7690\n",
            "Epoch 7323/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2325.2053 - val_loss: 3338.7876\n",
            "Epoch 7324/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2324.0503 - val_loss: 3342.1797\n",
            "Epoch 7325/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2323.5947 - val_loss: 3341.6797\n",
            "Epoch 7326/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2324.4324 - val_loss: 3341.9631\n",
            "Epoch 7327/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.9963 - val_loss: 3341.9460\n",
            "Epoch 7328/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.0320 - val_loss: 3340.3972\n",
            "Epoch 7329/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.9070 - val_loss: 3338.6980\n",
            "Epoch 7330/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2325.9050 - val_loss: 3335.3140\n",
            "Epoch 7331/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.2820 - val_loss: 3334.5137\n",
            "Epoch 7332/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.6086 - val_loss: 3334.6067\n",
            "Epoch 7333/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.9539 - val_loss: 3336.2837\n",
            "Epoch 7334/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2323.9272 - val_loss: 3337.8567\n",
            "Epoch 7335/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2324.7815 - val_loss: 3338.7100\n",
            "Epoch 7336/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.0710 - val_loss: 3338.3728\n",
            "Epoch 7337/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.2178 - val_loss: 3336.8677\n",
            "Epoch 7338/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.7776 - val_loss: 3334.6323\n",
            "Epoch 7339/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2326.1074 - val_loss: 3333.4260\n",
            "Epoch 7340/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.7361 - val_loss: 3332.9561\n",
            "Epoch 7341/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.6060 - val_loss: 3332.2468\n",
            "Epoch 7342/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.1084 - val_loss: 3332.0991\n",
            "Epoch 7343/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.3386 - val_loss: 3331.8135\n",
            "Epoch 7344/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2326.2581 - val_loss: 3333.2317\n",
            "Epoch 7345/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2320.3445 - val_loss: 3332.3086\n",
            "Epoch 7346/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.0361 - val_loss: 3331.9265\n",
            "Epoch 7347/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.3159 - val_loss: 3330.3020\n",
            "Epoch 7348/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.0505 - val_loss: 3329.8784\n",
            "Epoch 7349/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2323.5195 - val_loss: 3329.7644\n",
            "Epoch 7350/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2323.7563 - val_loss: 3328.4121\n",
            "Epoch 7351/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.7769 - val_loss: 3328.7578\n",
            "Epoch 7352/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2323.5496 - val_loss: 3328.8496\n",
            "Epoch 7353/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.3928 - val_loss: 3329.4797\n",
            "Epoch 7354/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.4580 - val_loss: 3330.7598\n",
            "Epoch 7355/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2323.1697 - val_loss: 3329.3203\n",
            "Epoch 7356/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.4141 - val_loss: 3327.9270\n",
            "Epoch 7357/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.9055 - val_loss: 3326.8359\n",
            "Epoch 7358/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2325.3208 - val_loss: 3329.6475\n",
            "Epoch 7359/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.6245 - val_loss: 3329.2766\n",
            "Epoch 7360/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.4155 - val_loss: 3332.7861\n",
            "Epoch 7361/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.6406 - val_loss: 3333.2749\n",
            "Epoch 7362/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.3838 - val_loss: 3328.1819\n",
            "Epoch 7363/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2324.6765 - val_loss: 3328.7268\n",
            "Epoch 7364/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.5417 - val_loss: 3329.3428\n",
            "Epoch 7365/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.5022 - val_loss: 3329.1047\n",
            "Epoch 7366/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.9397 - val_loss: 3328.1282\n",
            "Epoch 7367/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.3708 - val_loss: 3327.0828\n",
            "Epoch 7368/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.8896 - val_loss: 3325.9055\n",
            "Epoch 7369/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2323.6460 - val_loss: 3325.4712\n",
            "Epoch 7370/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.1785 - val_loss: 3326.0000\n",
            "Epoch 7371/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.4019 - val_loss: 3323.2275\n",
            "Epoch 7372/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.4578 - val_loss: 3322.5310\n",
            "Epoch 7373/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.8049 - val_loss: 3324.0969\n",
            "Epoch 7374/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2325.1851 - val_loss: 3324.5796\n",
            "Epoch 7375/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2325.6487 - val_loss: 3324.1636\n",
            "Epoch 7376/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2325.1326 - val_loss: 3325.8877\n",
            "Epoch 7377/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.5547 - val_loss: 3326.2212\n",
            "Epoch 7378/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2323.2012 - val_loss: 3326.1953\n",
            "Epoch 7379/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.5969 - val_loss: 3325.8010\n",
            "Epoch 7380/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.8154 - val_loss: 3324.5752\n",
            "Epoch 7381/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.3625 - val_loss: 3324.9397\n",
            "Epoch 7382/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2325.8457 - val_loss: 3325.3518\n",
            "Epoch 7383/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.3979 - val_loss: 3328.6804\n",
            "Epoch 7384/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.2778 - val_loss: 3331.4910\n",
            "Epoch 7385/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2324.5332 - val_loss: 3335.2471\n",
            "Epoch 7386/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.8096 - val_loss: 3339.2026\n",
            "Epoch 7387/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.3445 - val_loss: 3334.1514\n",
            "Epoch 7388/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.2600 - val_loss: 3331.7102\n",
            "Epoch 7389/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2323.1621 - val_loss: 3333.6653\n",
            "Epoch 7390/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.7761 - val_loss: 3335.1248\n",
            "Epoch 7391/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.6860 - val_loss: 3331.3315\n",
            "Epoch 7392/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.4539 - val_loss: 3328.8467\n",
            "Epoch 7393/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2323.4578 - val_loss: 3330.8879\n",
            "Epoch 7394/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.4788 - val_loss: 3332.2698\n",
            "Epoch 7395/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2323.9775 - val_loss: 3333.3374\n",
            "Epoch 7396/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2323.1326 - val_loss: 3334.0198\n",
            "Epoch 7397/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2323.3564 - val_loss: 3334.3679\n",
            "Epoch 7398/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2322.7776 - val_loss: 3334.3503\n",
            "Epoch 7399/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.0095 - val_loss: 3335.4839\n",
            "Epoch 7400/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.8748 - val_loss: 3335.8359\n",
            "Epoch 7401/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.7996 - val_loss: 3336.0415\n",
            "Epoch 7402/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.7358 - val_loss: 3335.7246\n",
            "Epoch 7403/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.6196 - val_loss: 3336.6748\n",
            "Epoch 7404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.3953 - val_loss: 3338.8628\n",
            "Epoch 7405/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.6987 - val_loss: 3338.1682\n",
            "Epoch 7406/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2323.0210 - val_loss: 3334.6267\n",
            "Epoch 7407/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.4346 - val_loss: 3337.1516\n",
            "Epoch 7408/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.7502 - val_loss: 3340.6418\n",
            "Epoch 7409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2326.5964 - val_loss: 3345.7429\n",
            "Epoch 7410/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2324.2529 - val_loss: 3347.1226\n",
            "Epoch 7411/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.8682 - val_loss: 3345.1851\n",
            "Epoch 7412/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.2039 - val_loss: 3344.8547\n",
            "Epoch 7413/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.8738 - val_loss: 3343.7927\n",
            "Epoch 7414/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.1257 - val_loss: 3343.7803\n",
            "Epoch 7415/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.7278 - val_loss: 3343.5432\n",
            "Epoch 7416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.5784 - val_loss: 3342.6765\n",
            "Epoch 7417/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.1985 - val_loss: 3341.0916\n",
            "Epoch 7418/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.5171 - val_loss: 3342.5166\n",
            "Epoch 7419/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.3586 - val_loss: 3343.2114\n",
            "Epoch 7420/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2321.5967 - val_loss: 3340.9993\n",
            "Epoch 7421/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.1750 - val_loss: 3340.6907\n",
            "Epoch 7422/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.4082 - val_loss: 3340.6692\n",
            "Epoch 7423/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.8728 - val_loss: 3340.5056\n",
            "Epoch 7424/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2323.6201 - val_loss: 3340.7566\n",
            "Epoch 7425/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2327.0007 - val_loss: 3344.1467\n",
            "Epoch 7426/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.7837 - val_loss: 3345.5952\n",
            "Epoch 7427/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.1328 - val_loss: 3345.2830\n",
            "Epoch 7428/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2322.7446 - val_loss: 3344.9165\n",
            "Epoch 7429/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.5708 - val_loss: 3343.9985\n",
            "Epoch 7430/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.3862 - val_loss: 3339.6003\n",
            "Epoch 7431/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.2217 - val_loss: 3337.9773\n",
            "Epoch 7432/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.2146 - val_loss: 3337.9622\n",
            "Epoch 7433/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2323.1011 - val_loss: 3337.4456\n",
            "Epoch 7434/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.4329 - val_loss: 3337.4922\n",
            "Epoch 7435/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.8015 - val_loss: 3337.9873\n",
            "Epoch 7436/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.9539 - val_loss: 3338.9985\n",
            "Epoch 7437/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2321.8813 - val_loss: 3339.3315\n",
            "Epoch 7438/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.3022 - val_loss: 3338.7346\n",
            "Epoch 7439/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2322.2722 - val_loss: 3338.4666\n",
            "Epoch 7440/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2321.9514 - val_loss: 3337.9487\n",
            "Epoch 7441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.0527 - val_loss: 3337.9534\n",
            "Epoch 7442/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.5925 - val_loss: 3338.8909\n",
            "Epoch 7443/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.4695 - val_loss: 3340.8855\n",
            "Epoch 7444/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2322.4563 - val_loss: 3340.4192\n",
            "Epoch 7445/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2324.5898 - val_loss: 3340.1602\n",
            "Epoch 7446/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.4832 - val_loss: 3339.7878\n",
            "Epoch 7447/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.1067 - val_loss: 3339.4192\n",
            "Epoch 7448/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.7085 - val_loss: 3339.8140\n",
            "Epoch 7449/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2323.0210 - val_loss: 3338.8809\n",
            "Epoch 7450/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2330.1948 - val_loss: 3338.6052\n",
            "Epoch 7451/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2331.0505 - val_loss: 3342.3579\n",
            "Epoch 7452/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2332.7681 - val_loss: 3341.6060\n",
            "Epoch 7453/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2326.2839 - val_loss: 3341.5520\n",
            "Epoch 7454/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.2427 - val_loss: 3341.9941\n",
            "Epoch 7455/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.5784 - val_loss: 3344.6147\n",
            "Epoch 7456/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.0730 - val_loss: 3346.0696\n",
            "Epoch 7457/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2325.9526 - val_loss: 3349.2710\n",
            "Epoch 7458/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.1838 - val_loss: 3348.9648\n",
            "Epoch 7459/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2324.6230 - val_loss: 3348.6748\n",
            "Epoch 7460/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2325.3013 - val_loss: 3344.8108\n",
            "Epoch 7461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.6897 - val_loss: 3344.6292\n",
            "Epoch 7462/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.1169 - val_loss: 3344.9817\n",
            "Epoch 7463/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.1008 - val_loss: 3343.4834\n",
            "Epoch 7464/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2322.6807 - val_loss: 3342.6858\n",
            "Epoch 7465/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.3767 - val_loss: 3342.5891\n",
            "Epoch 7466/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.3506 - val_loss: 3346.5247\n",
            "Epoch 7467/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2323.9504 - val_loss: 3346.5137\n",
            "Epoch 7468/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.4790 - val_loss: 3344.6060\n",
            "Epoch 7469/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.1460 - val_loss: 3344.2097\n",
            "Epoch 7470/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.3647 - val_loss: 3343.3118\n",
            "Epoch 7471/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.8591 - val_loss: 3342.4399\n",
            "Epoch 7472/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2321.5520 - val_loss: 3343.9255\n",
            "Epoch 7473/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.6987 - val_loss: 3343.1201\n",
            "Epoch 7474/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.7629 - val_loss: 3342.0281\n",
            "Epoch 7475/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.8420 - val_loss: 3342.3721\n",
            "Epoch 7476/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2321.0891 - val_loss: 3337.8357\n",
            "Epoch 7477/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.9636 - val_loss: 3336.8589\n",
            "Epoch 7478/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.5017 - val_loss: 3336.7302\n",
            "Epoch 7479/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.1785 - val_loss: 3337.7454\n",
            "Epoch 7480/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.2927 - val_loss: 3338.8613\n",
            "Epoch 7481/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2320.2910 - val_loss: 3342.9136\n",
            "Epoch 7482/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.1621 - val_loss: 3347.4722\n",
            "Epoch 7483/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.8689 - val_loss: 3347.9985\n",
            "Epoch 7484/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.9399 - val_loss: 3348.7124\n",
            "Epoch 7485/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.7324 - val_loss: 3351.9128\n",
            "Epoch 7486/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2326.2180 - val_loss: 3351.4880\n",
            "Epoch 7487/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.5100 - val_loss: 3350.2715\n",
            "Epoch 7488/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2326.4263 - val_loss: 3354.2627\n",
            "Epoch 7489/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2327.6279 - val_loss: 3353.6204\n",
            "Epoch 7490/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2327.1223 - val_loss: 3351.7498\n",
            "Epoch 7491/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.5857 - val_loss: 3346.5166\n",
            "Epoch 7492/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.5173 - val_loss: 3343.5898\n",
            "Epoch 7493/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.3530 - val_loss: 3341.5032\n",
            "Epoch 7494/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2321.0588 - val_loss: 3339.0803\n",
            "Epoch 7495/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.9978 - val_loss: 3341.1609\n",
            "Epoch 7496/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.3081 - val_loss: 3342.0879\n",
            "Epoch 7497/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.6907 - val_loss: 3339.3079\n",
            "Epoch 7498/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2325.6350 - val_loss: 3339.4502\n",
            "Epoch 7499/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.4656 - val_loss: 3339.5667\n",
            "Epoch 7500/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.7375 - val_loss: 3339.7261\n",
            "Epoch 7501/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.1436 - val_loss: 3339.6362\n",
            "Epoch 7502/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2322.9155 - val_loss: 3339.6477\n",
            "Epoch 7503/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.2773 - val_loss: 3339.9214\n",
            "Epoch 7504/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.8569 - val_loss: 3342.7668\n",
            "Epoch 7505/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.0984 - val_loss: 3346.7690\n",
            "Epoch 7506/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2326.2456 - val_loss: 3345.0520\n",
            "Epoch 7507/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2324.0989 - val_loss: 3345.9607\n",
            "Epoch 7508/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2322.8025 - val_loss: 3351.4087\n",
            "Epoch 7509/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2322.4736 - val_loss: 3354.4934\n",
            "Epoch 7510/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.7710 - val_loss: 3354.8655\n",
            "Epoch 7511/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2324.1404 - val_loss: 3354.0967\n",
            "Epoch 7512/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.1125 - val_loss: 3353.7251\n",
            "Epoch 7513/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2326.9714 - val_loss: 3355.4287\n",
            "Epoch 7514/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2327.4126 - val_loss: 3355.1240\n",
            "Epoch 7515/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2326.6606 - val_loss: 3355.1809\n",
            "Epoch 7516/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2325.7378 - val_loss: 3354.6184\n",
            "Epoch 7517/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.2407 - val_loss: 3354.6509\n",
            "Epoch 7518/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.7593 - val_loss: 3354.6252\n",
            "Epoch 7519/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.3025 - val_loss: 3354.2146\n",
            "Epoch 7520/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.8064 - val_loss: 3353.7378\n",
            "Epoch 7521/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.7759 - val_loss: 3352.6338\n",
            "Epoch 7522/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.5156 - val_loss: 3350.7388\n",
            "Epoch 7523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.0532 - val_loss: 3349.6365\n",
            "Epoch 7524/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.5051 - val_loss: 3349.4080\n",
            "Epoch 7525/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2321.2349 - val_loss: 3348.8835\n",
            "Epoch 7526/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.5244 - val_loss: 3348.7690\n",
            "Epoch 7527/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.7915 - val_loss: 3347.4351\n",
            "Epoch 7528/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2320.5125 - val_loss: 3346.9766\n",
            "Epoch 7529/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.0579 - val_loss: 3349.6279\n",
            "Epoch 7530/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.7476 - val_loss: 3353.3928\n",
            "Epoch 7531/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.8308 - val_loss: 3353.0828\n",
            "Epoch 7532/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.1597 - val_loss: 3348.6760\n",
            "Epoch 7533/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.4868 - val_loss: 3347.0688\n",
            "Epoch 7534/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.1394 - val_loss: 3343.7773\n",
            "Epoch 7535/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2321.8315 - val_loss: 3343.3430\n",
            "Epoch 7536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.8704 - val_loss: 3343.6836\n",
            "Epoch 7537/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2320.6565 - val_loss: 3343.7297\n",
            "Epoch 7538/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.5254 - val_loss: 3345.3772\n",
            "Epoch 7539/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.3303 - val_loss: 3348.6260\n",
            "Epoch 7540/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2319.3000 - val_loss: 3348.5696\n",
            "Epoch 7541/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.4404 - val_loss: 3349.3447\n",
            "Epoch 7542/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.1597 - val_loss: 3347.5273\n",
            "Epoch 7543/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.2832 - val_loss: 3347.1565\n",
            "Epoch 7544/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2318.8374 - val_loss: 3348.0217\n",
            "Epoch 7545/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.9829 - val_loss: 3349.5254\n",
            "Epoch 7546/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.3711 - val_loss: 3352.6001\n",
            "Epoch 7547/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.0435 - val_loss: 3353.2092\n",
            "Epoch 7548/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.9993 - val_loss: 3347.8628\n",
            "Epoch 7549/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2319.6523 - val_loss: 3345.5408\n",
            "Epoch 7550/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.6409 - val_loss: 3346.0305\n",
            "Epoch 7551/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.1228 - val_loss: 3349.1699\n",
            "Epoch 7552/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.8491 - val_loss: 3347.8054\n",
            "Epoch 7553/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.0527 - val_loss: 3348.2268\n",
            "Epoch 7554/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.7380 - val_loss: 3348.7219\n",
            "Epoch 7555/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2322.5320 - val_loss: 3349.4209\n",
            "Epoch 7556/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.7625 - val_loss: 3348.7644\n",
            "Epoch 7557/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.8384 - val_loss: 3348.8350\n",
            "Epoch 7558/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.2666 - val_loss: 3354.7598\n",
            "Epoch 7559/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2328.7317 - val_loss: 3356.2185\n",
            "Epoch 7560/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.4912 - val_loss: 3349.1794\n",
            "Epoch 7561/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.9731 - val_loss: 3345.6963\n",
            "Epoch 7562/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.7271 - val_loss: 3342.4192\n",
            "Epoch 7563/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.9463 - val_loss: 3339.2085\n",
            "Epoch 7564/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2320.1836 - val_loss: 3338.6748\n",
            "Epoch 7565/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.3782 - val_loss: 3337.6531\n",
            "Epoch 7566/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2318.1343 - val_loss: 3338.4519\n",
            "Epoch 7567/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.6777 - val_loss: 3339.1467\n",
            "Epoch 7568/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.1562 - val_loss: 3337.9695\n",
            "Epoch 7569/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2318.2026 - val_loss: 3335.7322\n",
            "Epoch 7570/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.5439 - val_loss: 3334.8533\n",
            "Epoch 7571/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.6560 - val_loss: 3334.7410\n",
            "Epoch 7572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.3894 - val_loss: 3335.2966\n",
            "Epoch 7573/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.9197 - val_loss: 3339.0488\n",
            "Epoch 7574/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.4758 - val_loss: 3341.8223\n",
            "Epoch 7575/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.6260 - val_loss: 3346.2346\n",
            "Epoch 7576/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.4351 - val_loss: 3346.9951\n",
            "Epoch 7577/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.0229 - val_loss: 3348.8982\n",
            "Epoch 7578/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.5693 - val_loss: 3346.1770\n",
            "Epoch 7579/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2318.4243 - val_loss: 3345.8455\n",
            "Epoch 7580/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.5830 - val_loss: 3343.0723\n",
            "Epoch 7581/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.3940 - val_loss: 3339.0640\n",
            "Epoch 7582/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.0278 - val_loss: 3337.4814\n",
            "Epoch 7583/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2321.3682 - val_loss: 3335.7659\n",
            "Epoch 7584/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.4175 - val_loss: 3335.5881\n",
            "Epoch 7585/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.2703 - val_loss: 3336.2542\n",
            "Epoch 7586/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.1458 - val_loss: 3336.8252\n",
            "Epoch 7587/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.8743 - val_loss: 3336.4004\n",
            "Epoch 7588/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.7219 - val_loss: 3338.8564\n",
            "Epoch 7589/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.7039 - val_loss: 3340.6721\n",
            "Epoch 7590/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2319.2910 - val_loss: 3342.3262\n",
            "Epoch 7591/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.0007 - val_loss: 3339.7402\n",
            "Epoch 7592/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2322.0032 - val_loss: 3339.4741\n",
            "Epoch 7593/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2324.9172 - val_loss: 3339.1404\n",
            "Epoch 7594/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.1445 - val_loss: 3338.0393\n",
            "Epoch 7595/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2321.1543 - val_loss: 3337.1274\n",
            "Epoch 7596/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.1172 - val_loss: 3337.0996\n",
            "Epoch 7597/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.8772 - val_loss: 3337.0879\n",
            "Epoch 7598/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.4460 - val_loss: 3337.3047\n",
            "Epoch 7599/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.0613 - val_loss: 3337.4985\n",
            "Epoch 7600/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.9158 - val_loss: 3341.2725\n",
            "Epoch 7601/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2319.2595 - val_loss: 3346.7766\n",
            "Epoch 7602/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.8445 - val_loss: 3354.4185\n",
            "Epoch 7603/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2324.6287 - val_loss: 3355.0730\n",
            "Epoch 7604/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2324.4270 - val_loss: 3354.6067\n",
            "Epoch 7605/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.9321 - val_loss: 3354.2017\n",
            "Epoch 7606/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.9307 - val_loss: 3354.0281\n",
            "Epoch 7607/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.8174 - val_loss: 3350.1289\n",
            "Epoch 7608/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2319.6868 - val_loss: 3343.9712\n",
            "Epoch 7609/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2319.6238 - val_loss: 3343.3035\n",
            "Epoch 7610/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.5261 - val_loss: 3340.2483\n",
            "Epoch 7611/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2317.4158 - val_loss: 3335.9375\n",
            "Epoch 7612/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.6555 - val_loss: 3336.2195\n",
            "Epoch 7613/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.6355 - val_loss: 3335.3892\n",
            "Epoch 7614/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.1238 - val_loss: 3334.3831\n",
            "Epoch 7615/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.6968 - val_loss: 3335.2283\n",
            "Epoch 7616/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2317.5986 - val_loss: 3335.6833\n",
            "Epoch 7617/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.0769 - val_loss: 3335.6804\n",
            "Epoch 7618/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.0718 - val_loss: 3336.5242\n",
            "Epoch 7619/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2318.6602 - val_loss: 3338.9390\n",
            "Epoch 7620/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.9175 - val_loss: 3339.8157\n",
            "Epoch 7621/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.3257 - val_loss: 3342.0078\n",
            "Epoch 7622/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2319.5854 - val_loss: 3340.9404\n",
            "Epoch 7623/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2320.0999 - val_loss: 3336.4678\n",
            "Epoch 7624/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2318.1482 - val_loss: 3335.3323\n",
            "Epoch 7625/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.6880 - val_loss: 3336.6565\n",
            "Epoch 7626/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.0251 - val_loss: 3336.5491\n",
            "Epoch 7627/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2320.6343 - val_loss: 3333.1155\n",
            "Epoch 7628/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2316.0383 - val_loss: 3334.4961\n",
            "Epoch 7629/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.2654 - val_loss: 3338.0571\n",
            "Epoch 7630/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.4775 - val_loss: 3339.6003\n",
            "Epoch 7631/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.0271 - val_loss: 3339.1907\n",
            "Epoch 7632/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2317.4763 - val_loss: 3340.4485\n",
            "Epoch 7633/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.7869 - val_loss: 3340.1833\n",
            "Epoch 7634/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.7314 - val_loss: 3339.7805\n",
            "Epoch 7635/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.4158 - val_loss: 3341.4634\n",
            "Epoch 7636/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.5686 - val_loss: 3342.2307\n",
            "Epoch 7637/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2317.9104 - val_loss: 3341.1338\n",
            "Epoch 7638/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.4292 - val_loss: 3338.1169\n",
            "Epoch 7639/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.2476 - val_loss: 3337.3271\n",
            "Epoch 7640/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.2583 - val_loss: 3338.4133\n",
            "Epoch 7641/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.1660 - val_loss: 3339.9585\n",
            "Epoch 7642/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2316.6357 - val_loss: 3339.9712\n",
            "Epoch 7643/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.1143 - val_loss: 3340.8879\n",
            "Epoch 7644/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.9146 - val_loss: 3341.4976\n",
            "Epoch 7645/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.2700 - val_loss: 3342.8547\n",
            "Epoch 7646/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2319.5608 - val_loss: 3345.4734\n",
            "Epoch 7647/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.6060 - val_loss: 3344.4409\n",
            "Epoch 7648/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.8606 - val_loss: 3339.9941\n",
            "Epoch 7649/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2315.7158 - val_loss: 3335.7190\n",
            "Epoch 7650/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.5095 - val_loss: 3334.5166\n",
            "Epoch 7651/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.5571 - val_loss: 3334.7302\n",
            "Epoch 7652/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.2632 - val_loss: 3333.3853\n",
            "Epoch 7653/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.3857 - val_loss: 3334.4177\n",
            "Epoch 7654/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.0376 - val_loss: 3334.3765\n",
            "Epoch 7655/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.1736 - val_loss: 3335.0552\n",
            "Epoch 7656/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.2925 - val_loss: 3335.4543\n",
            "Epoch 7657/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2316.4224 - val_loss: 3336.1284\n",
            "Epoch 7658/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.6523 - val_loss: 3336.9177\n",
            "Epoch 7659/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.6709 - val_loss: 3336.0591\n",
            "Epoch 7660/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2316.6423 - val_loss: 3334.1460\n",
            "Epoch 7661/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.4980 - val_loss: 3333.1084\n",
            "Epoch 7662/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.5300 - val_loss: 3333.4270\n",
            "Epoch 7663/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2316.7097 - val_loss: 3336.5049\n",
            "Epoch 7664/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2315.8076 - val_loss: 3337.7747\n",
            "Epoch 7665/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.8811 - val_loss: 3338.6733\n",
            "Epoch 7666/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.6482 - val_loss: 3340.7317\n",
            "Epoch 7667/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.8555 - val_loss: 3341.0234\n",
            "Epoch 7668/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2318.5837 - val_loss: 3339.0337\n",
            "Epoch 7669/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.7961 - val_loss: 3345.3147\n",
            "Epoch 7670/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.3967 - val_loss: 3346.6226\n",
            "Epoch 7671/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.0051 - val_loss: 3351.0828\n",
            "Epoch 7672/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2322.3110 - val_loss: 3347.3452\n",
            "Epoch 7673/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2318.7283 - val_loss: 3345.5815\n",
            "Epoch 7674/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2317.9990 - val_loss: 3344.6191\n",
            "Epoch 7675/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.6047 - val_loss: 3348.2410\n",
            "Epoch 7676/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2318.5833 - val_loss: 3343.8091\n",
            "Epoch 7677/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.9805 - val_loss: 3340.8525\n",
            "Epoch 7678/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.4976 - val_loss: 3341.3293\n",
            "Epoch 7679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.2339 - val_loss: 3342.1187\n",
            "Epoch 7680/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2315.7366 - val_loss: 3345.0371\n",
            "Epoch 7681/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.5435 - val_loss: 3345.1289\n",
            "Epoch 7682/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.7620 - val_loss: 3345.5818\n",
            "Epoch 7683/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.5654 - val_loss: 3345.9663\n",
            "Epoch 7684/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2319.7310 - val_loss: 3343.5161\n",
            "Epoch 7685/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2318.0391 - val_loss: 3342.5688\n",
            "Epoch 7686/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.9719 - val_loss: 3347.0403\n",
            "Epoch 7687/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2321.8708 - val_loss: 3350.4597\n",
            "Epoch 7688/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.6147 - val_loss: 3348.3455\n",
            "Epoch 7689/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2319.5745 - val_loss: 3346.8296\n",
            "Epoch 7690/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.4077 - val_loss: 3342.9434\n",
            "Epoch 7691/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2315.9739 - val_loss: 3341.2710\n",
            "Epoch 7692/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.6042 - val_loss: 3342.3525\n",
            "Epoch 7693/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2318.2896 - val_loss: 3348.1528\n",
            "Epoch 7694/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2317.9302 - val_loss: 3343.4214\n",
            "Epoch 7695/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2315.7458 - val_loss: 3339.7444\n",
            "Epoch 7696/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2315.6731 - val_loss: 3338.4873\n",
            "Epoch 7697/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.6152 - val_loss: 3338.4060\n",
            "Epoch 7698/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2316.7510 - val_loss: 3338.0776\n",
            "Epoch 7699/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.6428 - val_loss: 3338.0352\n",
            "Epoch 7700/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.4502 - val_loss: 3338.1165\n",
            "Epoch 7701/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2314.7732 - val_loss: 3336.2004\n",
            "Epoch 7702/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.4133 - val_loss: 3337.1653\n",
            "Epoch 7703/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.7886 - val_loss: 3338.4929\n",
            "Epoch 7704/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.5391 - val_loss: 3337.3916\n",
            "Epoch 7705/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2316.4617 - val_loss: 3337.9248\n",
            "Epoch 7706/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2315.7373 - val_loss: 3339.9565\n",
            "Epoch 7707/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.3062 - val_loss: 3340.9045\n",
            "Epoch 7708/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.7590 - val_loss: 3340.5481\n",
            "Epoch 7709/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.4224 - val_loss: 3341.0647\n",
            "Epoch 7710/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.0095 - val_loss: 3341.5483\n",
            "Epoch 7711/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.5215 - val_loss: 3341.0935\n",
            "Epoch 7712/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2315.3445 - val_loss: 3341.9517\n",
            "Epoch 7713/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.7539 - val_loss: 3342.3635\n",
            "Epoch 7714/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.1755 - val_loss: 3342.1707\n",
            "Epoch 7715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.5332 - val_loss: 3338.9841\n",
            "Epoch 7716/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2315.1606 - val_loss: 3338.0208\n",
            "Epoch 7717/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2315.1833 - val_loss: 3338.1184\n",
            "Epoch 7718/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.1562 - val_loss: 3337.4470\n",
            "Epoch 7719/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.0981 - val_loss: 3337.2261\n",
            "Epoch 7720/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.9897 - val_loss: 3337.8708\n",
            "Epoch 7721/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2323.6035 - val_loss: 3339.1672\n",
            "Epoch 7722/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2323.3501 - val_loss: 3339.0872\n",
            "Epoch 7723/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2320.3162 - val_loss: 3339.5583\n",
            "Epoch 7724/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.3899 - val_loss: 3342.5872\n",
            "Epoch 7725/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.6755 - val_loss: 3344.1072\n",
            "Epoch 7726/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.3608 - val_loss: 3344.8215\n",
            "Epoch 7727/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.8044 - val_loss: 3344.1089\n",
            "Epoch 7728/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.9431 - val_loss: 3341.1460\n",
            "Epoch 7729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.3120 - val_loss: 3340.5078\n",
            "Epoch 7730/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.8406 - val_loss: 3340.7700\n",
            "Epoch 7731/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2315.1401 - val_loss: 3340.5305\n",
            "Epoch 7732/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.5483 - val_loss: 3342.0039\n",
            "Epoch 7733/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.2319 - val_loss: 3342.1201\n",
            "Epoch 7734/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.2041 - val_loss: 3341.0530\n",
            "Epoch 7735/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.8237 - val_loss: 3339.3564\n",
            "Epoch 7736/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2314.6646 - val_loss: 3338.7317\n",
            "Epoch 7737/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2316.4817 - val_loss: 3338.7629\n",
            "Epoch 7738/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.8757 - val_loss: 3339.0681\n",
            "Epoch 7739/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.2222 - val_loss: 3339.2002\n",
            "Epoch 7740/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.7573 - val_loss: 3341.6794\n",
            "Epoch 7741/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.5325 - val_loss: 3343.3491\n",
            "Epoch 7742/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.3167 - val_loss: 3343.6362\n",
            "Epoch 7743/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2313.6467 - val_loss: 3341.7844\n",
            "Epoch 7744/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.1775 - val_loss: 3339.6257\n",
            "Epoch 7745/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.1304 - val_loss: 3342.0664\n",
            "Epoch 7746/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2316.8218 - val_loss: 3343.6340\n",
            "Epoch 7747/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.6460 - val_loss: 3343.8469\n",
            "Epoch 7748/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2315.6797 - val_loss: 3345.5342\n",
            "Epoch 7749/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.3018 - val_loss: 3345.3381\n",
            "Epoch 7750/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.0410 - val_loss: 3345.5723\n",
            "Epoch 7751/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2314.2000 - val_loss: 3346.4858\n",
            "Epoch 7752/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2314.2300 - val_loss: 3348.7922\n",
            "Epoch 7753/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.9912 - val_loss: 3349.0449\n",
            "Epoch 7754/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2313.9058 - val_loss: 3349.1196\n",
            "Epoch 7755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.1892 - val_loss: 3351.2878\n",
            "Epoch 7756/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.9421 - val_loss: 3348.2866\n",
            "Epoch 7757/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2315.8733 - val_loss: 3348.4109\n",
            "Epoch 7758/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2315.5571 - val_loss: 3345.5134\n",
            "Epoch 7759/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.9192 - val_loss: 3345.6597\n",
            "Epoch 7760/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2315.0369 - val_loss: 3343.2441\n",
            "Epoch 7761/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.5352 - val_loss: 3344.0315\n",
            "Epoch 7762/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.7974 - val_loss: 3350.0940\n",
            "Epoch 7763/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2318.2898 - val_loss: 3356.2666\n",
            "Epoch 7764/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2321.3179 - val_loss: 3358.2410\n",
            "Epoch 7765/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.8096 - val_loss: 3355.8455\n",
            "Epoch 7766/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2318.8220 - val_loss: 3358.5674\n",
            "Epoch 7767/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.6787 - val_loss: 3358.6770\n",
            "Epoch 7768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.1594 - val_loss: 3359.3635\n",
            "Epoch 7769/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.6189 - val_loss: 3357.7661\n",
            "Epoch 7770/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.8762 - val_loss: 3363.1516\n",
            "Epoch 7771/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.8435 - val_loss: 3358.9866\n",
            "Epoch 7772/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2319.3320 - val_loss: 3354.6489\n",
            "Epoch 7773/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2317.0332 - val_loss: 3352.1516\n",
            "Epoch 7774/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.0950 - val_loss: 3350.3752\n",
            "Epoch 7775/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2314.9709 - val_loss: 3349.9829\n",
            "Epoch 7776/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.9663 - val_loss: 3353.4216\n",
            "Epoch 7777/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2319.8914 - val_loss: 3352.6667\n",
            "Epoch 7778/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2317.0964 - val_loss: 3344.0090\n",
            "Epoch 7779/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.0332 - val_loss: 3340.6160\n",
            "Epoch 7780/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.3792 - val_loss: 3339.7932\n",
            "Epoch 7781/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2313.9043 - val_loss: 3339.6089\n",
            "Epoch 7782/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2313.5166 - val_loss: 3342.1196\n",
            "Epoch 7783/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.8787 - val_loss: 3344.2083\n",
            "Epoch 7784/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.2834 - val_loss: 3347.3997\n",
            "Epoch 7785/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2317.2068 - val_loss: 3346.6289\n",
            "Epoch 7786/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2314.5674 - val_loss: 3340.5833\n",
            "Epoch 7787/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2314.4221 - val_loss: 3338.8557\n",
            "Epoch 7788/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2313.7620 - val_loss: 3339.8687\n",
            "Epoch 7789/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.8799 - val_loss: 3340.0483\n",
            "Epoch 7790/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.7034 - val_loss: 3338.0674\n",
            "Epoch 7791/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.9812 - val_loss: 3337.3652\n",
            "Epoch 7792/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2312.3284 - val_loss: 3338.5278\n",
            "Epoch 7793/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.9338 - val_loss: 3340.5889\n",
            "Epoch 7794/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2313.1558 - val_loss: 3338.4746\n",
            "Epoch 7795/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2313.8306 - val_loss: 3337.1216\n",
            "Epoch 7796/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.2781 - val_loss: 3337.3806\n",
            "Epoch 7797/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.4771 - val_loss: 3337.8035\n",
            "Epoch 7798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.8110 - val_loss: 3339.7241\n",
            "Epoch 7799/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.3877 - val_loss: 3338.2356\n",
            "Epoch 7800/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2313.1311 - val_loss: 3336.8940\n",
            "Epoch 7801/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2313.0811 - val_loss: 3336.3535\n",
            "Epoch 7802/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.6594 - val_loss: 3335.9814\n",
            "Epoch 7803/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2311.8274 - val_loss: 3337.2683\n",
            "Epoch 7804/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.7166 - val_loss: 3338.3140\n",
            "Epoch 7805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.7788 - val_loss: 3339.5227\n",
            "Epoch 7806/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2315.4766 - val_loss: 3338.6274\n",
            "Epoch 7807/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.3882 - val_loss: 3339.2556\n",
            "Epoch 7808/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.2554 - val_loss: 3339.0432\n",
            "Epoch 7809/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2312.4802 - val_loss: 3338.7493\n",
            "Epoch 7810/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2312.3047 - val_loss: 3338.1152\n",
            "Epoch 7811/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.8379 - val_loss: 3337.0232\n",
            "Epoch 7812/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2314.1208 - val_loss: 3336.7715\n",
            "Epoch 7813/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.9856 - val_loss: 3337.0056\n",
            "Epoch 7814/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.0081 - val_loss: 3341.2949\n",
            "Epoch 7815/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2312.7383 - val_loss: 3343.9304\n",
            "Epoch 7816/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2313.6333 - val_loss: 3344.9177\n",
            "Epoch 7817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.5486 - val_loss: 3343.1116\n",
            "Epoch 7818/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.3374 - val_loss: 3344.7026\n",
            "Epoch 7819/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2315.2551 - val_loss: 3350.0759\n",
            "Epoch 7820/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2316.0483 - val_loss: 3347.5796\n",
            "Epoch 7821/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2315.4062 - val_loss: 3351.7380\n",
            "Epoch 7822/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2323.2695 - val_loss: 3358.4678\n",
            "Epoch 7823/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2323.7329 - val_loss: 3357.0059\n",
            "Epoch 7824/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2323.5571 - val_loss: 3358.3884\n",
            "Epoch 7825/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2324.2561 - val_loss: 3359.6313\n",
            "Epoch 7826/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2322.1873 - val_loss: 3353.8047\n",
            "Epoch 7827/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.0505 - val_loss: 3348.9758\n",
            "Epoch 7828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.8579 - val_loss: 3346.3953\n",
            "Epoch 7829/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2316.1743 - val_loss: 3344.3782\n",
            "Epoch 7830/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.7400 - val_loss: 3343.7222\n",
            "Epoch 7831/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2314.1353 - val_loss: 3342.9546\n",
            "Epoch 7832/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.2893 - val_loss: 3342.0393\n",
            "Epoch 7833/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2311.7336 - val_loss: 3342.2180\n",
            "Epoch 7834/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2312.7793 - val_loss: 3342.6084\n",
            "Epoch 7835/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2314.2847 - val_loss: 3339.1284\n",
            "Epoch 7836/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.3118 - val_loss: 3337.5898\n",
            "Epoch 7837/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.9890 - val_loss: 3337.4824\n",
            "Epoch 7838/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.2400 - val_loss: 3339.0466\n",
            "Epoch 7839/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.1565 - val_loss: 3339.2190\n",
            "Epoch 7840/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2314.8123 - val_loss: 3339.6970\n",
            "Epoch 7841/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.2449 - val_loss: 3340.5032\n",
            "Epoch 7842/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.1475 - val_loss: 3340.8926\n",
            "Epoch 7843/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2312.1094 - val_loss: 3339.7991\n",
            "Epoch 7844/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.1819 - val_loss: 3339.5471\n",
            "Epoch 7845/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.2522 - val_loss: 3339.6489\n",
            "Epoch 7846/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2312.2214 - val_loss: 3339.5515\n",
            "Epoch 7847/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.1636 - val_loss: 3337.3792\n",
            "Epoch 7848/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.4456 - val_loss: 3336.7358\n",
            "Epoch 7849/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2313.2749 - val_loss: 3336.7922\n",
            "Epoch 7850/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.3403 - val_loss: 3340.7678\n",
            "Epoch 7851/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2310.4766 - val_loss: 3344.7129\n",
            "Epoch 7852/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2310.3320 - val_loss: 3347.1226\n",
            "Epoch 7853/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2311.7715 - val_loss: 3347.8174\n",
            "Epoch 7854/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.7549 - val_loss: 3346.6765\n",
            "Epoch 7855/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2315.3896 - val_loss: 3349.7097\n",
            "Epoch 7856/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.8909 - val_loss: 3348.4382\n",
            "Epoch 7857/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2319.3359 - val_loss: 3347.7605\n",
            "Epoch 7858/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2317.3274 - val_loss: 3347.1235\n",
            "Epoch 7859/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2313.0710 - val_loss: 3348.4270\n",
            "Epoch 7860/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2316.8699 - val_loss: 3349.9229\n",
            "Epoch 7861/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2310.1912 - val_loss: 3347.2810\n",
            "Epoch 7862/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.4158 - val_loss: 3345.8188\n",
            "Epoch 7863/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2312.1956 - val_loss: 3344.2498\n",
            "Epoch 7864/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2312.7583 - val_loss: 3341.4753\n",
            "Epoch 7865/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.1182 - val_loss: 3341.2573\n",
            "Epoch 7866/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.1646 - val_loss: 3340.6804\n",
            "Epoch 7867/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.9990 - val_loss: 3340.4902\n",
            "Epoch 7868/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2312.2354 - val_loss: 3340.6453\n",
            "Epoch 7869/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2311.8584 - val_loss: 3341.5442\n",
            "Epoch 7870/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.7976 - val_loss: 3341.7991\n",
            "Epoch 7871/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.8406 - val_loss: 3342.1704\n",
            "Epoch 7872/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2310.7581 - val_loss: 3342.8125\n",
            "Epoch 7873/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.7266 - val_loss: 3341.7058\n",
            "Epoch 7874/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.7993 - val_loss: 3344.5176\n",
            "Epoch 7875/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.6350 - val_loss: 3344.3872\n",
            "Epoch 7876/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2311.0435 - val_loss: 3347.1416\n",
            "Epoch 7877/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2311.4573 - val_loss: 3348.9866\n",
            "Epoch 7878/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.8601 - val_loss: 3352.7004\n",
            "Epoch 7879/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.5708 - val_loss: 3352.2722\n",
            "Epoch 7880/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.4893 - val_loss: 3351.2971\n",
            "Epoch 7881/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2312.5120 - val_loss: 3348.6633\n",
            "Epoch 7882/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2312.5796 - val_loss: 3347.2998\n",
            "Epoch 7883/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.6648 - val_loss: 3346.1841\n",
            "Epoch 7884/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2310.7554 - val_loss: 3346.3396\n",
            "Epoch 7885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.6389 - val_loss: 3346.5085\n",
            "Epoch 7886/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.7717 - val_loss: 3346.9946\n",
            "Epoch 7887/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.1133 - val_loss: 3348.4807\n",
            "Epoch 7888/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.8047 - val_loss: 3350.7009\n",
            "Epoch 7889/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2313.8186 - val_loss: 3355.5400\n",
            "Epoch 7890/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2315.1665 - val_loss: 3355.7346\n",
            "Epoch 7891/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2314.6892 - val_loss: 3355.4854\n",
            "Epoch 7892/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2315.6494 - val_loss: 3353.1748\n",
            "Epoch 7893/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.9294 - val_loss: 3345.6509\n",
            "Epoch 7894/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.7129 - val_loss: 3343.3157\n",
            "Epoch 7895/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.6162 - val_loss: 3342.3196\n",
            "Epoch 7896/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.9912 - val_loss: 3341.4663\n",
            "Epoch 7897/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2313.1174 - val_loss: 3342.1963\n",
            "Epoch 7898/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.3367 - val_loss: 3344.2683\n",
            "Epoch 7899/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.8010 - val_loss: 3344.1843\n",
            "Epoch 7900/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.2949 - val_loss: 3344.9751\n",
            "Epoch 7901/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.6316 - val_loss: 3349.5168\n",
            "Epoch 7902/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2311.0293 - val_loss: 3350.6633\n",
            "Epoch 7903/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2310.5190 - val_loss: 3350.4062\n",
            "Epoch 7904/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.7019 - val_loss: 3350.2258\n",
            "Epoch 7905/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.0938 - val_loss: 3348.5857\n",
            "Epoch 7906/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2310.5320 - val_loss: 3348.6970\n",
            "Epoch 7907/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2310.2104 - val_loss: 3347.9285\n",
            "Epoch 7908/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.5830 - val_loss: 3347.6396\n",
            "Epoch 7909/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2311.0159 - val_loss: 3348.0979\n",
            "Epoch 7910/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.5166 - val_loss: 3352.9753\n",
            "Epoch 7911/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.4404 - val_loss: 3351.9832\n",
            "Epoch 7912/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.4292 - val_loss: 3345.9175\n",
            "Epoch 7913/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2308.2671 - val_loss: 3343.3862\n",
            "Epoch 7914/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.1064 - val_loss: 3345.5371\n",
            "Epoch 7915/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.0732 - val_loss: 3347.7878\n",
            "Epoch 7916/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.2883 - val_loss: 3354.2236\n",
            "Epoch 7917/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.1550 - val_loss: 3356.1897\n",
            "Epoch 7918/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.8909 - val_loss: 3361.4111\n",
            "Epoch 7919/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2313.1519 - val_loss: 3354.8740\n",
            "Epoch 7920/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.0698 - val_loss: 3351.4797\n",
            "Epoch 7921/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.4365 - val_loss: 3351.6443\n",
            "Epoch 7922/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2311.2671 - val_loss: 3351.6562\n",
            "Epoch 7923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.0061 - val_loss: 3348.8459\n",
            "Epoch 7924/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2310.1182 - val_loss: 3347.4009\n",
            "Epoch 7925/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2308.8774 - val_loss: 3348.8940\n",
            "Epoch 7926/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.9453 - val_loss: 3353.0454\n",
            "Epoch 7927/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2313.9778 - val_loss: 3353.1616\n",
            "Epoch 7928/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2313.6362 - val_loss: 3351.6724\n",
            "Epoch 7929/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2313.3123 - val_loss: 3351.1733\n",
            "Epoch 7930/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2313.7124 - val_loss: 3348.7861\n",
            "Epoch 7931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.9634 - val_loss: 3348.0015\n",
            "Epoch 7932/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2312.0464 - val_loss: 3346.4558\n",
            "Epoch 7933/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.7070 - val_loss: 3345.7285\n",
            "Epoch 7934/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.2310 - val_loss: 3345.5872\n",
            "Epoch 7935/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.0559 - val_loss: 3343.5562\n",
            "Epoch 7936/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.9658 - val_loss: 3343.1145\n",
            "Epoch 7937/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.4326 - val_loss: 3342.6721\n",
            "Epoch 7938/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2309.5525 - val_loss: 3341.6812\n",
            "Epoch 7939/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.9946 - val_loss: 3345.3093\n",
            "Epoch 7940/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.4788 - val_loss: 3349.7483\n",
            "Epoch 7941/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.3337 - val_loss: 3350.7627\n",
            "Epoch 7942/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2308.9177 - val_loss: 3349.7795\n",
            "Epoch 7943/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2308.2798 - val_loss: 3347.6272\n",
            "Epoch 7944/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.1963 - val_loss: 3348.0654\n",
            "Epoch 7945/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.9514 - val_loss: 3348.3286\n",
            "Epoch 7946/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2311.0322 - val_loss: 3348.4622\n",
            "Epoch 7947/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.3423 - val_loss: 3349.3052\n",
            "Epoch 7948/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.3264 - val_loss: 3348.9753\n",
            "Epoch 7949/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.2109 - val_loss: 3349.2021\n",
            "Epoch 7950/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2310.0461 - val_loss: 3351.0408\n",
            "Epoch 7951/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.5974 - val_loss: 3354.8916\n",
            "Epoch 7952/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.9983 - val_loss: 3360.3540\n",
            "Epoch 7953/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2311.2151 - val_loss: 3358.2266\n",
            "Epoch 7954/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.5000 - val_loss: 3355.4248\n",
            "Epoch 7955/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.9976 - val_loss: 3354.8438\n",
            "Epoch 7956/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.8621 - val_loss: 3355.3279\n",
            "Epoch 7957/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2313.9604 - val_loss: 3352.3420\n",
            "Epoch 7958/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2315.2529 - val_loss: 3350.5232\n",
            "Epoch 7959/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2320.1411 - val_loss: 3349.1594\n",
            "Epoch 7960/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2319.0886 - val_loss: 3347.6785\n",
            "Epoch 7961/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.5745 - val_loss: 3347.6265\n",
            "Epoch 7962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.4832 - val_loss: 3347.2996\n",
            "Epoch 7963/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.6040 - val_loss: 3344.7795\n",
            "Epoch 7964/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2311.6501 - val_loss: 3344.4978\n",
            "Epoch 7965/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2311.9875 - val_loss: 3347.1562\n",
            "Epoch 7966/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.7375 - val_loss: 3347.6956\n",
            "Epoch 7967/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2309.6821 - val_loss: 3346.3904\n",
            "Epoch 7968/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2308.9580 - val_loss: 3348.4229\n",
            "Epoch 7969/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2310.9480 - val_loss: 3350.5930\n",
            "Epoch 7970/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.2185 - val_loss: 3349.2410\n",
            "Epoch 7971/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.2805 - val_loss: 3349.5415\n",
            "Epoch 7972/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2313.6816 - val_loss: 3352.2212\n",
            "Epoch 7973/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2309.0874 - val_loss: 3351.4409\n",
            "Epoch 7974/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.7769 - val_loss: 3352.3066\n",
            "Epoch 7975/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.1626 - val_loss: 3351.0811\n",
            "Epoch 7976/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2308.8330 - val_loss: 3349.4270\n",
            "Epoch 7977/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.9031 - val_loss: 3349.4116\n",
            "Epoch 7978/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.2917 - val_loss: 3344.8792\n",
            "Epoch 7979/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.2273 - val_loss: 3343.3435\n",
            "Epoch 7980/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.9138 - val_loss: 3343.7805\n",
            "Epoch 7981/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.9377 - val_loss: 3343.6514\n",
            "Epoch 7982/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.3767 - val_loss: 3343.6877\n",
            "Epoch 7983/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.7996 - val_loss: 3342.5720\n",
            "Epoch 7984/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2307.9209 - val_loss: 3342.0288\n",
            "Epoch 7985/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.8347 - val_loss: 3341.8933\n",
            "Epoch 7986/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2313.3992 - val_loss: 3342.3271\n",
            "Epoch 7987/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2315.5898 - val_loss: 3340.8547\n",
            "Epoch 7988/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.1541 - val_loss: 3340.6177\n",
            "Epoch 7989/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2318.8806 - val_loss: 3339.3804\n",
            "Epoch 7990/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2314.8367 - val_loss: 3339.1917\n",
            "Epoch 7991/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2313.3479 - val_loss: 3341.2847\n",
            "Epoch 7992/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.1709 - val_loss: 3342.0105\n",
            "Epoch 7993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.6084 - val_loss: 3342.6533\n",
            "Epoch 7994/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.4810 - val_loss: 3342.7671\n",
            "Epoch 7995/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.8025 - val_loss: 3343.6458\n",
            "Epoch 7996/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.1770 - val_loss: 3345.2212\n",
            "Epoch 7997/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.0322 - val_loss: 3345.4753\n",
            "Epoch 7998/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2307.4575 - val_loss: 3344.6907\n",
            "Epoch 7999/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.3982 - val_loss: 3342.4785\n",
            "Epoch 8000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.4124 - val_loss: 3343.6899\n",
            "Epoch 8001/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.3481 - val_loss: 3343.8010\n",
            "Epoch 8002/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.4016 - val_loss: 3344.1482\n",
            "Epoch 8003/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.9180 - val_loss: 3345.9961\n",
            "Epoch 8004/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.6353 - val_loss: 3347.2378\n",
            "Epoch 8005/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.8521 - val_loss: 3348.5400\n",
            "Epoch 8006/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.5894 - val_loss: 3346.5466\n",
            "Epoch 8007/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.9233 - val_loss: 3343.3413\n",
            "Epoch 8008/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.2710 - val_loss: 3342.2100\n",
            "Epoch 8009/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.8301 - val_loss: 3338.7502\n",
            "Epoch 8010/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2308.7053 - val_loss: 3338.8284\n",
            "Epoch 8011/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.6223 - val_loss: 3339.9753\n",
            "Epoch 8012/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.4785 - val_loss: 3340.2419\n",
            "Epoch 8013/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.6929 - val_loss: 3340.2893\n",
            "Epoch 8014/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.1616 - val_loss: 3340.5085\n",
            "Epoch 8015/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.7036 - val_loss: 3342.1279\n",
            "Epoch 8016/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.1904 - val_loss: 3343.0337\n",
            "Epoch 8017/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.7842 - val_loss: 3342.8262\n",
            "Epoch 8018/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.9475 - val_loss: 3344.8887\n",
            "Epoch 8019/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.0105 - val_loss: 3344.0928\n",
            "Epoch 8020/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.0999 - val_loss: 3342.2466\n",
            "Epoch 8021/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.3323 - val_loss: 3341.3230\n",
            "Epoch 8022/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2308.6902 - val_loss: 3340.2534\n",
            "Epoch 8023/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.9243 - val_loss: 3341.0828\n",
            "Epoch 8024/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2308.0454 - val_loss: 3341.1875\n",
            "Epoch 8025/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.9194 - val_loss: 3340.5320\n",
            "Epoch 8026/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.4622 - val_loss: 3343.1553\n",
            "Epoch 8027/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2312.3198 - val_loss: 3342.7610\n",
            "Epoch 8028/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2310.5132 - val_loss: 3342.9207\n",
            "Epoch 8029/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.7444 - val_loss: 3344.4453\n",
            "Epoch 8030/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.9817 - val_loss: 3345.6165\n",
            "Epoch 8031/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.5190 - val_loss: 3346.5105\n",
            "Epoch 8032/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2307.2761 - val_loss: 3346.3740\n",
            "Epoch 8033/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.8823 - val_loss: 3342.0208\n",
            "Epoch 8034/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.4805 - val_loss: 3341.9670\n",
            "Epoch 8035/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.3875 - val_loss: 3342.3640\n",
            "Epoch 8036/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2308.6655 - val_loss: 3341.0657\n",
            "Epoch 8037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.8901 - val_loss: 3341.2598\n",
            "Epoch 8038/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.7295 - val_loss: 3340.5422\n",
            "Epoch 8039/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.0044 - val_loss: 3340.7822\n",
            "Epoch 8040/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2307.3008 - val_loss: 3342.1475\n",
            "Epoch 8041/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.3970 - val_loss: 3342.4231\n",
            "Epoch 8042/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.8279 - val_loss: 3342.1338\n",
            "Epoch 8043/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2307.7227 - val_loss: 3343.0986\n",
            "Epoch 8044/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.5344 - val_loss: 3342.9753\n",
            "Epoch 8045/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.5442 - val_loss: 3343.6067\n",
            "Epoch 8046/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.6711 - val_loss: 3342.9954\n",
            "Epoch 8047/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.0894 - val_loss: 3337.7124\n",
            "Epoch 8048/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2310.1528 - val_loss: 3336.5811\n",
            "Epoch 8049/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.9385 - val_loss: 3336.9585\n",
            "Epoch 8050/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.3076 - val_loss: 3337.7605\n",
            "Epoch 8051/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2309.0000 - val_loss: 3337.5073\n",
            "Epoch 8052/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.9395 - val_loss: 3339.4783\n",
            "Epoch 8053/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.5774 - val_loss: 3342.8672\n",
            "Epoch 8054/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2308.8828 - val_loss: 3347.8403\n",
            "Epoch 8055/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.5249 - val_loss: 3353.1067\n",
            "Epoch 8056/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.0552 - val_loss: 3350.5303\n",
            "Epoch 8057/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.1726 - val_loss: 3353.5671\n",
            "Epoch 8058/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.1575 - val_loss: 3354.8325\n",
            "Epoch 8059/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.3872 - val_loss: 3354.2678\n",
            "Epoch 8060/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2307.6213 - val_loss: 3352.4536\n",
            "Epoch 8061/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.8108 - val_loss: 3350.7412\n",
            "Epoch 8062/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.7700 - val_loss: 3348.3511\n",
            "Epoch 8063/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2306.8953 - val_loss: 3345.2683\n",
            "Epoch 8064/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.6318 - val_loss: 3347.1172\n",
            "Epoch 8065/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.2966 - val_loss: 3346.1565\n",
            "Epoch 8066/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2308.9167 - val_loss: 3342.7468\n",
            "Epoch 8067/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.6824 - val_loss: 3342.4165\n",
            "Epoch 8068/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.6653 - val_loss: 3342.8540\n",
            "Epoch 8069/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.6870 - val_loss: 3343.1365\n",
            "Epoch 8070/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.1038 - val_loss: 3342.2097\n",
            "Epoch 8071/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2309.0454 - val_loss: 3342.4375\n",
            "Epoch 8072/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2309.9971 - val_loss: 3342.6250\n",
            "Epoch 8073/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.2375 - val_loss: 3343.1082\n",
            "Epoch 8074/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.7949 - val_loss: 3341.3103\n",
            "Epoch 8075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.9846 - val_loss: 3344.9319\n",
            "Epoch 8076/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.7791 - val_loss: 3343.8438\n",
            "Epoch 8077/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.2793 - val_loss: 3339.9680\n",
            "Epoch 8078/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.1460 - val_loss: 3338.4424\n",
            "Epoch 8079/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2311.4478 - val_loss: 3337.4915\n",
            "Epoch 8080/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2310.1296 - val_loss: 3337.3975\n",
            "Epoch 8081/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2309.3113 - val_loss: 3338.2017\n",
            "Epoch 8082/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.2708 - val_loss: 3340.1763\n",
            "Epoch 8083/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.5071 - val_loss: 3340.8276\n",
            "Epoch 8084/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2306.4700 - val_loss: 3341.4526\n",
            "Epoch 8085/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.4238 - val_loss: 3342.2314\n",
            "Epoch 8086/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2307.0513 - val_loss: 3339.7812\n",
            "Epoch 8087/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2307.9702 - val_loss: 3340.3823\n",
            "Epoch 8088/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.1284 - val_loss: 3339.9641\n",
            "Epoch 8089/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.3276 - val_loss: 3340.1885\n",
            "Epoch 8090/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.6104 - val_loss: 3340.4736\n",
            "Epoch 8091/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.6438 - val_loss: 3340.3228\n",
            "Epoch 8092/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.2517 - val_loss: 3341.1194\n",
            "Epoch 8093/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.6433 - val_loss: 3341.8809\n",
            "Epoch 8094/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.0557 - val_loss: 3345.7722\n",
            "Epoch 8095/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.4448 - val_loss: 3347.1387\n",
            "Epoch 8096/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2305.9075 - val_loss: 3342.2844\n",
            "Epoch 8097/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.5454 - val_loss: 3341.6067\n",
            "Epoch 8098/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.6912 - val_loss: 3341.0979\n",
            "Epoch 8099/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2307.2473 - val_loss: 3341.5278\n",
            "Epoch 8100/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.2095 - val_loss: 3341.3862\n",
            "Epoch 8101/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2309.1201 - val_loss: 3339.3608\n",
            "Epoch 8102/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.9067 - val_loss: 3338.8467\n",
            "Epoch 8103/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.6226 - val_loss: 3340.2358\n",
            "Epoch 8104/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.9756 - val_loss: 3340.8735\n",
            "Epoch 8105/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2308.0627 - val_loss: 3345.2595\n",
            "Epoch 8106/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.0918 - val_loss: 3346.6509\n",
            "Epoch 8107/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.3044 - val_loss: 3344.9678\n",
            "Epoch 8108/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2305.6941 - val_loss: 3344.5596\n",
            "Epoch 8109/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.9683 - val_loss: 3343.9941\n",
            "Epoch 8110/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.3882 - val_loss: 3346.7410\n",
            "Epoch 8111/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2309.5615 - val_loss: 3351.3342\n",
            "Epoch 8112/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2308.2871 - val_loss: 3345.8628\n",
            "Epoch 8113/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.1833 - val_loss: 3343.2830\n",
            "Epoch 8114/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2305.7161 - val_loss: 3341.2637\n",
            "Epoch 8115/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.9319 - val_loss: 3341.5920\n",
            "Epoch 8116/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2305.3188 - val_loss: 3339.4663\n",
            "Epoch 8117/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.0127 - val_loss: 3339.0796\n",
            "Epoch 8118/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2305.0503 - val_loss: 3338.5054\n",
            "Epoch 8119/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.0229 - val_loss: 3336.5273\n",
            "Epoch 8120/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.8335 - val_loss: 3335.9080\n",
            "Epoch 8121/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.7227 - val_loss: 3336.0889\n",
            "Epoch 8122/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2304.8696 - val_loss: 3337.7478\n",
            "Epoch 8123/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.6768 - val_loss: 3342.3091\n",
            "Epoch 8124/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2306.3237 - val_loss: 3343.7754\n",
            "Epoch 8125/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.2207 - val_loss: 3344.1465\n",
            "Epoch 8126/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.1594 - val_loss: 3343.7659\n",
            "Epoch 8127/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.8540 - val_loss: 3343.8989\n",
            "Epoch 8128/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.9778 - val_loss: 3342.9009\n",
            "Epoch 8129/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.9287 - val_loss: 3345.3511\n",
            "Epoch 8130/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.2439 - val_loss: 3345.4128\n",
            "Epoch 8131/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.2388 - val_loss: 3347.5330\n",
            "Epoch 8132/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2307.0054 - val_loss: 3348.4175\n",
            "Epoch 8133/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.2568 - val_loss: 3348.6978\n",
            "Epoch 8134/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.5525 - val_loss: 3347.7129\n",
            "Epoch 8135/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.0427 - val_loss: 3341.2979\n",
            "Epoch 8136/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2304.9668 - val_loss: 3340.0007\n",
            "Epoch 8137/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.6252 - val_loss: 3342.0190\n",
            "Epoch 8138/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.6006 - val_loss: 3345.7676\n",
            "Epoch 8139/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.5640 - val_loss: 3346.1716\n",
            "Epoch 8140/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.7742 - val_loss: 3343.8435\n",
            "Epoch 8141/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2305.3894 - val_loss: 3343.3813\n",
            "Epoch 8142/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.6707 - val_loss: 3345.5322\n",
            "Epoch 8143/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2309.3469 - val_loss: 3346.3933\n",
            "Epoch 8144/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2306.3423 - val_loss: 3342.8845\n",
            "Epoch 8145/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2305.3025 - val_loss: 3341.1355\n",
            "Epoch 8146/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.5798 - val_loss: 3336.7900\n",
            "Epoch 8147/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.7632 - val_loss: 3335.3110\n",
            "Epoch 8148/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.6284 - val_loss: 3335.4607\n",
            "Epoch 8149/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2308.1296 - val_loss: 3335.5247\n",
            "Epoch 8150/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.2219 - val_loss: 3339.8286\n",
            "Epoch 8151/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.2122 - val_loss: 3342.3259\n",
            "Epoch 8152/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.8376 - val_loss: 3342.9177\n",
            "Epoch 8153/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.3708 - val_loss: 3344.1377\n",
            "Epoch 8154/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2304.2588 - val_loss: 3343.5940\n",
            "Epoch 8155/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.2791 - val_loss: 3343.6868\n",
            "Epoch 8156/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.7773 - val_loss: 3343.8323\n",
            "Epoch 8157/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2304.1042 - val_loss: 3344.2766\n",
            "Epoch 8158/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2304.7898 - val_loss: 3344.7402\n",
            "Epoch 8159/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.0205 - val_loss: 3342.9983\n",
            "Epoch 8160/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.7842 - val_loss: 3343.7642\n",
            "Epoch 8161/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2306.8096 - val_loss: 3343.8992\n",
            "Epoch 8162/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.6125 - val_loss: 3343.2798\n",
            "Epoch 8163/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.7200 - val_loss: 3342.8398\n",
            "Epoch 8164/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.5608 - val_loss: 3342.3560\n",
            "Epoch 8165/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2304.3735 - val_loss: 3341.5115\n",
            "Epoch 8166/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2305.1709 - val_loss: 3339.7734\n",
            "Epoch 8167/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.0422 - val_loss: 3340.3567\n",
            "Epoch 8168/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.4890 - val_loss: 3342.1860\n",
            "Epoch 8169/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2304.3137 - val_loss: 3344.2458\n",
            "Epoch 8170/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.6357 - val_loss: 3346.4160\n",
            "Epoch 8171/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.0583 - val_loss: 3349.2141\n",
            "Epoch 8172/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2302.8110 - val_loss: 3346.1555\n",
            "Epoch 8173/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2304.3838 - val_loss: 3344.6904\n",
            "Epoch 8174/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.1970 - val_loss: 3344.3792\n",
            "Epoch 8175/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.7839 - val_loss: 3342.7273\n",
            "Epoch 8176/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.1826 - val_loss: 3342.3416\n",
            "Epoch 8177/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2305.0957 - val_loss: 3343.5049\n",
            "Epoch 8178/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.2517 - val_loss: 3346.0547\n",
            "Epoch 8179/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.8567 - val_loss: 3346.8091\n",
            "Epoch 8180/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.1926 - val_loss: 3345.8806\n",
            "Epoch 8181/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.6423 - val_loss: 3346.1228\n",
            "Epoch 8182/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2303.2197 - val_loss: 3347.6746\n",
            "Epoch 8183/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2308.3889 - val_loss: 3351.2180\n",
            "Epoch 8184/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.5981 - val_loss: 3349.6003\n",
            "Epoch 8185/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.1511 - val_loss: 3342.5222\n",
            "Epoch 8186/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.9614 - val_loss: 3340.8711\n",
            "Epoch 8187/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2303.9136 - val_loss: 3339.0928\n",
            "Epoch 8188/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.4377 - val_loss: 3338.5647\n",
            "Epoch 8189/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.1150 - val_loss: 3338.5127\n",
            "Epoch 8190/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.2295 - val_loss: 3342.2725\n",
            "Epoch 8191/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2305.1099 - val_loss: 3343.7788\n",
            "Epoch 8192/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.7876 - val_loss: 3342.9912\n",
            "Epoch 8193/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.1196 - val_loss: 3335.6282\n",
            "Epoch 8194/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.7371 - val_loss: 3335.0520\n",
            "Epoch 8195/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2303.8232 - val_loss: 3335.6931\n",
            "Epoch 8196/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.6987 - val_loss: 3338.3936\n",
            "Epoch 8197/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.0859 - val_loss: 3338.9629\n",
            "Epoch 8198/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2305.6179 - val_loss: 3339.9214\n",
            "Epoch 8199/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.8589 - val_loss: 3338.3223\n",
            "Epoch 8200/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2304.8689 - val_loss: 3336.0208\n",
            "Epoch 8201/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.0330 - val_loss: 3336.1523\n",
            "Epoch 8202/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.8918 - val_loss: 3336.0142\n",
            "Epoch 8203/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2303.8271 - val_loss: 3335.9509\n",
            "Epoch 8204/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.1638 - val_loss: 3335.6643\n",
            "Epoch 8205/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.3438 - val_loss: 3335.9944\n",
            "Epoch 8206/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2304.5642 - val_loss: 3338.1353\n",
            "Epoch 8207/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.4214 - val_loss: 3333.7332\n",
            "Epoch 8208/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2304.3728 - val_loss: 3334.2283\n",
            "Epoch 8209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.7981 - val_loss: 3335.8694\n",
            "Epoch 8210/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2307.0344 - val_loss: 3336.3372\n",
            "Epoch 8211/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2306.9055 - val_loss: 3337.5786\n",
            "Epoch 8212/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2306.3354 - val_loss: 3338.7563\n",
            "Epoch 8213/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2304.1409 - val_loss: 3340.0088\n",
            "Epoch 8214/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.6477 - val_loss: 3342.7261\n",
            "Epoch 8215/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.4436 - val_loss: 3342.9033\n",
            "Epoch 8216/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.6499 - val_loss: 3343.1506\n",
            "Epoch 8217/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2303.5068 - val_loss: 3343.3115\n",
            "Epoch 8218/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.2090 - val_loss: 3343.8198\n",
            "Epoch 8219/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.0159 - val_loss: 3345.1868\n",
            "Epoch 8220/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.1357 - val_loss: 3345.4375\n",
            "Epoch 8221/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2302.3809 - val_loss: 3342.4558\n",
            "Epoch 8222/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.4453 - val_loss: 3342.1392\n",
            "Epoch 8223/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.8167 - val_loss: 3341.9766\n",
            "Epoch 8224/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2303.6677 - val_loss: 3342.0120\n",
            "Epoch 8225/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.9666 - val_loss: 3344.1995\n",
            "Epoch 8226/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.9468 - val_loss: 3343.5464\n",
            "Epoch 8227/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2304.0859 - val_loss: 3343.4648\n",
            "Epoch 8228/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.1533 - val_loss: 3342.5935\n",
            "Epoch 8229/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.5066 - val_loss: 3342.0583\n",
            "Epoch 8230/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2304.8625 - val_loss: 3344.3357\n",
            "Epoch 8231/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.8850 - val_loss: 3344.6907\n",
            "Epoch 8232/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.5503 - val_loss: 3346.9946\n",
            "Epoch 8233/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.3997 - val_loss: 3349.0730\n",
            "Epoch 8234/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.9409 - val_loss: 3344.1228\n",
            "Epoch 8235/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.6233 - val_loss: 3345.4810\n",
            "Epoch 8236/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.9182 - val_loss: 3343.7903\n",
            "Epoch 8237/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.9514 - val_loss: 3342.1897\n",
            "Epoch 8238/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.7966 - val_loss: 3341.1243\n",
            "Epoch 8239/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.3354 - val_loss: 3341.1804\n",
            "Epoch 8240/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2302.3850 - val_loss: 3344.0610\n",
            "Epoch 8241/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.6467 - val_loss: 3348.0347\n",
            "Epoch 8242/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.2419 - val_loss: 3350.0737\n",
            "Epoch 8243/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.7488 - val_loss: 3350.6072\n",
            "Epoch 8244/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2304.0989 - val_loss: 3347.2886\n",
            "Epoch 8245/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.7952 - val_loss: 3345.0759\n",
            "Epoch 8246/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2302.5200 - val_loss: 3341.0864\n",
            "Epoch 8247/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.7114 - val_loss: 3340.7588\n",
            "Epoch 8248/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2302.7986 - val_loss: 3340.7471\n",
            "Epoch 8249/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.7085 - val_loss: 3342.3518\n",
            "Epoch 8250/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.7041 - val_loss: 3342.5464\n",
            "Epoch 8251/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.4265 - val_loss: 3346.2686\n",
            "Epoch 8252/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.5808 - val_loss: 3347.8071\n",
            "Epoch 8253/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.8315 - val_loss: 3345.9944\n",
            "Epoch 8254/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.8264 - val_loss: 3345.9277\n",
            "Epoch 8255/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.8269 - val_loss: 3346.0737\n",
            "Epoch 8256/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2301.7686 - val_loss: 3346.9473\n",
            "Epoch 8257/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2304.7522 - val_loss: 3346.2578\n",
            "Epoch 8258/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2304.5679 - val_loss: 3346.2197\n",
            "Epoch 8259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.0427 - val_loss: 3347.6829\n",
            "Epoch 8260/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2301.2581 - val_loss: 3351.3108\n",
            "Epoch 8261/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.8984 - val_loss: 3352.1978\n",
            "Epoch 8262/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.7085 - val_loss: 3351.7773\n",
            "Epoch 8263/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.6445 - val_loss: 3352.1677\n",
            "Epoch 8264/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.3110 - val_loss: 3353.9866\n",
            "Epoch 8265/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.4299 - val_loss: 3355.4807\n",
            "Epoch 8266/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.0081 - val_loss: 3355.9209\n",
            "Epoch 8267/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.8123 - val_loss: 3356.3823\n",
            "Epoch 8268/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.5210 - val_loss: 3355.6846\n",
            "Epoch 8269/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2301.1765 - val_loss: 3358.5874\n",
            "Epoch 8270/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2306.2251 - val_loss: 3359.3623\n",
            "Epoch 8271/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2303.6997 - val_loss: 3360.4829\n",
            "Epoch 8272/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2303.0876 - val_loss: 3360.7983\n",
            "Epoch 8273/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.3938 - val_loss: 3365.9304\n",
            "Epoch 8274/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2302.5061 - val_loss: 3369.4133\n",
            "Epoch 8275/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2302.9773 - val_loss: 3369.7004\n",
            "Epoch 8276/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.9121 - val_loss: 3368.4866\n",
            "Epoch 8277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.8955 - val_loss: 3369.9546\n",
            "Epoch 8278/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2305.9055 - val_loss: 3372.5947\n",
            "Epoch 8279/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2310.2361 - val_loss: 3379.5435\n",
            "Epoch 8280/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2310.4548 - val_loss: 3374.5593\n",
            "Epoch 8281/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.6919 - val_loss: 3372.4968\n",
            "Epoch 8282/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.5605 - val_loss: 3369.8564\n",
            "Epoch 8283/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.4023 - val_loss: 3368.9958\n",
            "Epoch 8284/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2306.7190 - val_loss: 3372.3486\n",
            "Epoch 8285/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2305.9253 - val_loss: 3368.8789\n",
            "Epoch 8286/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.5947 - val_loss: 3366.1685\n",
            "Epoch 8287/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2302.9890 - val_loss: 3363.6921\n",
            "Epoch 8288/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2300.9033 - val_loss: 3360.4551\n",
            "Epoch 8289/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.0625 - val_loss: 3359.9060\n",
            "Epoch 8290/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.4180 - val_loss: 3359.1025\n",
            "Epoch 8291/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2300.5649 - val_loss: 3358.7876\n",
            "Epoch 8292/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.8728 - val_loss: 3355.3374\n",
            "Epoch 8293/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2300.4709 - val_loss: 3355.0784\n",
            "Epoch 8294/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.1538 - val_loss: 3355.9092\n",
            "Epoch 8295/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.5642 - val_loss: 3356.5657\n",
            "Epoch 8296/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.9685 - val_loss: 3356.9309\n",
            "Epoch 8297/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.7903 - val_loss: 3355.8621\n",
            "Epoch 8298/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.9397 - val_loss: 3354.3926\n",
            "Epoch 8299/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2302.0029 - val_loss: 3354.6257\n",
            "Epoch 8300/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.6831 - val_loss: 3354.7227\n",
            "Epoch 8301/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2298.1665 - val_loss: 3357.7471\n",
            "Epoch 8302/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.5566 - val_loss: 3361.2454\n",
            "Epoch 8303/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.1389 - val_loss: 3358.1692\n",
            "Epoch 8304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.0461 - val_loss: 3355.4241\n",
            "Epoch 8305/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.6111 - val_loss: 3352.8054\n",
            "Epoch 8306/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.3293 - val_loss: 3350.2102\n",
            "Epoch 8307/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.4583 - val_loss: 3349.0403\n",
            "Epoch 8308/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2301.9856 - val_loss: 3349.4873\n",
            "Epoch 8309/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.8716 - val_loss: 3349.4167\n",
            "Epoch 8310/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.8250 - val_loss: 3348.7092\n",
            "Epoch 8311/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.4204 - val_loss: 3347.8892\n",
            "Epoch 8312/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2302.8752 - val_loss: 3349.3496\n",
            "Epoch 8313/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.1008 - val_loss: 3348.3665\n",
            "Epoch 8314/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.2085 - val_loss: 3350.0603\n",
            "Epoch 8315/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.1455 - val_loss: 3352.8447\n",
            "Epoch 8316/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.9558 - val_loss: 3353.5090\n",
            "Epoch 8317/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.1685 - val_loss: 3353.5073\n",
            "Epoch 8318/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.1973 - val_loss: 3353.6257\n",
            "Epoch 8319/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2300.8711 - val_loss: 3357.4009\n",
            "Epoch 8320/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.8315 - val_loss: 3356.8792\n",
            "Epoch 8321/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2301.5920 - val_loss: 3355.2092\n",
            "Epoch 8322/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.5376 - val_loss: 3355.2747\n",
            "Epoch 8323/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.4968 - val_loss: 3357.0168\n",
            "Epoch 8324/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.9985 - val_loss: 3356.4839\n",
            "Epoch 8325/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2301.7727 - val_loss: 3355.2324\n",
            "Epoch 8326/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2302.3347 - val_loss: 3356.9031\n",
            "Epoch 8327/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.8853 - val_loss: 3354.9678\n",
            "Epoch 8328/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.2900 - val_loss: 3354.7395\n",
            "Epoch 8329/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2300.0200 - val_loss: 3351.5522\n",
            "Epoch 8330/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.4023 - val_loss: 3348.5232\n",
            "Epoch 8331/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.5547 - val_loss: 3345.4368\n",
            "Epoch 8332/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.4143 - val_loss: 3346.8855\n",
            "Epoch 8333/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2300.5139 - val_loss: 3344.8599\n",
            "Epoch 8334/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2301.0312 - val_loss: 3344.9568\n",
            "Epoch 8335/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2299.2583 - val_loss: 3345.6016\n",
            "Epoch 8336/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.0764 - val_loss: 3344.9705\n",
            "Epoch 8337/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2299.9722 - val_loss: 3345.5281\n",
            "Epoch 8338/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2300.4153 - val_loss: 3346.0498\n",
            "Epoch 8339/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2299.4534 - val_loss: 3348.7219\n",
            "Epoch 8340/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.4265 - val_loss: 3350.2581\n",
            "Epoch 8341/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.4790 - val_loss: 3354.7922\n",
            "Epoch 8342/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2301.9458 - val_loss: 3356.0947\n",
            "Epoch 8343/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.8506 - val_loss: 3351.3943\n",
            "Epoch 8344/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.0503 - val_loss: 3349.9102\n",
            "Epoch 8345/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2299.1602 - val_loss: 3348.9751\n",
            "Epoch 8346/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2300.8269 - val_loss: 3353.1545\n",
            "Epoch 8347/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.3662 - val_loss: 3354.7354\n",
            "Epoch 8348/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2303.2695 - val_loss: 3357.7659\n",
            "Epoch 8349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.0603 - val_loss: 3356.1377\n",
            "Epoch 8350/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.0410 - val_loss: 3356.0049\n",
            "Epoch 8351/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2301.4807 - val_loss: 3359.6250\n",
            "Epoch 8352/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2303.1924 - val_loss: 3361.7891\n",
            "Epoch 8353/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.0269 - val_loss: 3360.1211\n",
            "Epoch 8354/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2301.0366 - val_loss: 3358.9404\n",
            "Epoch 8355/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2299.6921 - val_loss: 3356.6707\n",
            "Epoch 8356/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.3599 - val_loss: 3352.8037\n",
            "Epoch 8357/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2299.9153 - val_loss: 3349.0154\n",
            "Epoch 8358/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.0159 - val_loss: 3349.0991\n",
            "Epoch 8359/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.3074 - val_loss: 3349.0337\n",
            "Epoch 8360/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2300.5620 - val_loss: 3349.0654\n",
            "Epoch 8361/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.9412 - val_loss: 3347.3860\n",
            "Epoch 8362/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.4519 - val_loss: 3346.6194\n",
            "Epoch 8363/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.9146 - val_loss: 3345.2078\n",
            "Epoch 8364/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.8242 - val_loss: 3346.2410\n",
            "Epoch 8365/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2304.4241 - val_loss: 3345.8821\n",
            "Epoch 8366/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2302.3342 - val_loss: 3344.7922\n",
            "Epoch 8367/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2300.9597 - val_loss: 3344.6184\n",
            "Epoch 8368/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.5068 - val_loss: 3344.7666\n",
            "Epoch 8369/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.1047 - val_loss: 3345.9927\n",
            "Epoch 8370/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2298.7620 - val_loss: 3348.0867\n",
            "Epoch 8371/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.1174 - val_loss: 3349.7834\n",
            "Epoch 8372/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2298.1653 - val_loss: 3350.7266\n",
            "Epoch 8373/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2298.0244 - val_loss: 3350.4722\n",
            "Epoch 8374/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.8667 - val_loss: 3349.9634\n",
            "Epoch 8375/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2298.3840 - val_loss: 3350.3184\n",
            "Epoch 8376/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2298.9761 - val_loss: 3349.9719\n",
            "Epoch 8377/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.0903 - val_loss: 3350.3357\n",
            "Epoch 8378/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.4868 - val_loss: 3351.9321\n",
            "Epoch 8379/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2298.4758 - val_loss: 3354.1667\n",
            "Epoch 8380/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.8831 - val_loss: 3357.3123\n",
            "Epoch 8381/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2298.7373 - val_loss: 3362.4646\n",
            "Epoch 8382/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2302.1570 - val_loss: 3370.9897\n",
            "Epoch 8383/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2304.5803 - val_loss: 3372.7698\n",
            "Epoch 8384/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2307.8486 - val_loss: 3377.6396\n",
            "Epoch 8385/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2310.3689 - val_loss: 3379.1892\n",
            "Epoch 8386/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.1084 - val_loss: 3378.0073\n",
            "Epoch 8387/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2310.6138 - val_loss: 3375.7927\n",
            "Epoch 8388/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2308.8213 - val_loss: 3370.0371\n",
            "Epoch 8389/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2304.5911 - val_loss: 3366.4915\n",
            "Epoch 8390/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2302.4644 - val_loss: 3362.9697\n",
            "Epoch 8391/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.0281 - val_loss: 3356.6946\n",
            "Epoch 8392/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.6414 - val_loss: 3355.1978\n",
            "Epoch 8393/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2298.7388 - val_loss: 3353.7988\n",
            "Epoch 8394/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.3813 - val_loss: 3355.3101\n",
            "Epoch 8395/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.3157 - val_loss: 3352.4502\n",
            "Epoch 8396/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2298.6028 - val_loss: 3351.5447\n",
            "Epoch 8397/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2298.2568 - val_loss: 3353.4480\n",
            "Epoch 8398/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.2329 - val_loss: 3354.9080\n",
            "Epoch 8399/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.2856 - val_loss: 3355.8416\n",
            "Epoch 8400/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2299.5410 - val_loss: 3356.0737\n",
            "Epoch 8401/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2298.3359 - val_loss: 3354.3040\n",
            "Epoch 8402/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.6602 - val_loss: 3352.3159\n",
            "Epoch 8403/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.9089 - val_loss: 3348.6841\n",
            "Epoch 8404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.3657 - val_loss: 3350.7346\n",
            "Epoch 8405/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2297.6975 - val_loss: 3350.6960\n",
            "Epoch 8406/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.7634 - val_loss: 3348.6748\n",
            "Epoch 8407/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7065 - val_loss: 3348.8416\n",
            "Epoch 8408/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7490 - val_loss: 3348.3967\n",
            "Epoch 8409/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2299.2559 - val_loss: 3346.6396\n",
            "Epoch 8410/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2298.1377 - val_loss: 3347.4807\n",
            "Epoch 8411/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.2686 - val_loss: 3347.7903\n",
            "Epoch 8412/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.5337 - val_loss: 3348.2620\n",
            "Epoch 8413/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.1558 - val_loss: 3352.3967\n",
            "Epoch 8414/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.8335 - val_loss: 3354.1091\n",
            "Epoch 8415/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.0752 - val_loss: 3360.4504\n",
            "Epoch 8416/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.4119 - val_loss: 3354.1846\n",
            "Epoch 8417/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.4253 - val_loss: 3351.7910\n",
            "Epoch 8418/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.9841 - val_loss: 3352.5015\n",
            "Epoch 8419/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.6294 - val_loss: 3351.9497\n",
            "Epoch 8420/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.4912 - val_loss: 3347.9014\n",
            "Epoch 8421/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.3052 - val_loss: 3347.8384\n",
            "Epoch 8422/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2297.8225 - val_loss: 3350.3206\n",
            "Epoch 8423/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.2239 - val_loss: 3350.0994\n",
            "Epoch 8424/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.3113 - val_loss: 3350.4646\n",
            "Epoch 8425/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.8096 - val_loss: 3350.9246\n",
            "Epoch 8426/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2297.8293 - val_loss: 3354.1956\n",
            "Epoch 8427/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2298.9497 - val_loss: 3357.1028\n",
            "Epoch 8428/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.1382 - val_loss: 3357.1050\n",
            "Epoch 8429/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2299.1553 - val_loss: 3353.0793\n",
            "Epoch 8430/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.3696 - val_loss: 3351.1946\n",
            "Epoch 8431/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.9907 - val_loss: 3350.2043\n",
            "Epoch 8432/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.7004 - val_loss: 3352.5479\n",
            "Epoch 8433/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.2632 - val_loss: 3348.3862\n",
            "Epoch 8434/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.9065 - val_loss: 3347.5344\n",
            "Epoch 8435/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7961 - val_loss: 3347.1843\n",
            "Epoch 8436/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2299.0942 - val_loss: 3347.0872\n",
            "Epoch 8437/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.2068 - val_loss: 3347.5488\n",
            "Epoch 8438/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7983 - val_loss: 3348.4910\n",
            "Epoch 8439/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.5876 - val_loss: 3348.8435\n",
            "Epoch 8440/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.6267 - val_loss: 3350.1777\n",
            "Epoch 8441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.9075 - val_loss: 3350.4409\n",
            "Epoch 8442/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.0698 - val_loss: 3350.2227\n",
            "Epoch 8443/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.7773 - val_loss: 3350.9446\n",
            "Epoch 8444/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.3645 - val_loss: 3351.2483\n",
            "Epoch 8445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7522 - val_loss: 3348.7085\n",
            "Epoch 8446/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.7678 - val_loss: 3346.1497\n",
            "Epoch 8447/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.0679 - val_loss: 3345.1721\n",
            "Epoch 8448/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.1379 - val_loss: 3344.1514\n",
            "Epoch 8449/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.7004 - val_loss: 3344.8716\n",
            "Epoch 8450/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.7922 - val_loss: 3345.2009\n",
            "Epoch 8451/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.7920 - val_loss: 3345.3645\n",
            "Epoch 8452/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2297.0764 - val_loss: 3344.3767\n",
            "Epoch 8453/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.4075 - val_loss: 3343.7454\n",
            "Epoch 8454/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.1204 - val_loss: 3340.4966\n",
            "Epoch 8455/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.4824 - val_loss: 3340.1409\n",
            "Epoch 8456/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2298.1191 - val_loss: 3342.3333\n",
            "Epoch 8457/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.8835 - val_loss: 3343.2380\n",
            "Epoch 8458/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.8066 - val_loss: 3346.6890\n",
            "Epoch 8459/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2299.2588 - val_loss: 3348.5176\n",
            "Epoch 8460/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2301.6775 - val_loss: 3352.0408\n",
            "Epoch 8461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.5442 - val_loss: 3351.2253\n",
            "Epoch 8462/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2296.9487 - val_loss: 3343.4768\n",
            "Epoch 8463/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.6863 - val_loss: 3339.5828\n",
            "Epoch 8464/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.2964 - val_loss: 3339.0022\n",
            "Epoch 8465/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.1685 - val_loss: 3339.7878\n",
            "Epoch 8466/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.7896 - val_loss: 3343.3271\n",
            "Epoch 8467/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.5254 - val_loss: 3344.8679\n",
            "Epoch 8468/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.0588 - val_loss: 3345.0320\n",
            "Epoch 8469/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.9810 - val_loss: 3345.3367\n",
            "Epoch 8470/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2295.8970 - val_loss: 3349.3694\n",
            "Epoch 8471/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.3755 - val_loss: 3351.3447\n",
            "Epoch 8472/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7605 - val_loss: 3350.8511\n",
            "Epoch 8473/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.8494 - val_loss: 3351.8828\n",
            "Epoch 8474/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2297.6497 - val_loss: 3352.5127\n",
            "Epoch 8475/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.1257 - val_loss: 3358.0442\n",
            "Epoch 8476/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.4536 - val_loss: 3359.4167\n",
            "Epoch 8477/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2302.9778 - val_loss: 3363.0183\n",
            "Epoch 8478/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.2102 - val_loss: 3365.8662\n",
            "Epoch 8479/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.0886 - val_loss: 3362.3118\n",
            "Epoch 8480/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.5112 - val_loss: 3354.4102\n",
            "Epoch 8481/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2295.3823 - val_loss: 3352.3372\n",
            "Epoch 8482/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.0034 - val_loss: 3351.9504\n",
            "Epoch 8483/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.8020 - val_loss: 3352.8989\n",
            "Epoch 8484/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.5410 - val_loss: 3355.6411\n",
            "Epoch 8485/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.3120 - val_loss: 3355.9517\n",
            "Epoch 8486/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.3333 - val_loss: 3356.8271\n",
            "Epoch 8487/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.0854 - val_loss: 3360.8716\n",
            "Epoch 8488/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.3247 - val_loss: 3362.2134\n",
            "Epoch 8489/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.9590 - val_loss: 3362.3638\n",
            "Epoch 8490/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.4534 - val_loss: 3361.1809\n",
            "Epoch 8491/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.1362 - val_loss: 3359.9121\n",
            "Epoch 8492/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.2244 - val_loss: 3360.5249\n",
            "Epoch 8493/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.3672 - val_loss: 3360.0808\n",
            "Epoch 8494/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.5229 - val_loss: 3362.4204\n",
            "Epoch 8495/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.7700 - val_loss: 3361.3552\n",
            "Epoch 8496/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2295.3425 - val_loss: 3359.8379\n",
            "Epoch 8497/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2296.2153 - val_loss: 3359.4849\n",
            "Epoch 8498/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.8945 - val_loss: 3357.2732\n",
            "Epoch 8499/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.5793 - val_loss: 3359.1306\n",
            "Epoch 8500/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.7754 - val_loss: 3357.3047\n",
            "Epoch 8501/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.8318 - val_loss: 3357.6099\n",
            "Epoch 8502/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.5447 - val_loss: 3358.0330\n",
            "Epoch 8503/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.1660 - val_loss: 3358.4448\n",
            "Epoch 8504/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.3174 - val_loss: 3356.0217\n",
            "Epoch 8505/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.2273 - val_loss: 3358.6123\n",
            "Epoch 8506/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.8560 - val_loss: 3358.6714\n",
            "Epoch 8507/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.5667 - val_loss: 3358.9392\n",
            "Epoch 8508/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2296.0034 - val_loss: 3359.1707\n",
            "Epoch 8509/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2295.4707 - val_loss: 3359.4248\n",
            "Epoch 8510/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2297.3728 - val_loss: 3358.2610\n",
            "Epoch 8511/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2295.2449 - val_loss: 3358.2451\n",
            "Epoch 8512/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.1897 - val_loss: 3359.4463\n",
            "Epoch 8513/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2295.2661 - val_loss: 3358.8660\n",
            "Epoch 8514/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.8643 - val_loss: 3357.3535\n",
            "Epoch 8515/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2295.2461 - val_loss: 3357.1965\n",
            "Epoch 8516/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.6448 - val_loss: 3356.8286\n",
            "Epoch 8517/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.3872 - val_loss: 3355.6074\n",
            "Epoch 8518/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2294.2581 - val_loss: 3354.6284\n",
            "Epoch 8519/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.5088 - val_loss: 3356.1553\n",
            "Epoch 8520/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.6555 - val_loss: 3356.5591\n",
            "Epoch 8521/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2295.9648 - val_loss: 3357.6003\n",
            "Epoch 8522/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.3123 - val_loss: 3361.1829\n",
            "Epoch 8523/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2300.2625 - val_loss: 3354.6377\n",
            "Epoch 8524/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2298.6709 - val_loss: 3352.1274\n",
            "Epoch 8525/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.7065 - val_loss: 3351.0225\n",
            "Epoch 8526/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2299.9470 - val_loss: 3353.2329\n",
            "Epoch 8527/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.7510 - val_loss: 3353.2153\n",
            "Epoch 8528/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2300.8555 - val_loss: 3356.9038\n",
            "Epoch 8529/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.7876 - val_loss: 3360.7754\n",
            "Epoch 8530/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2304.8560 - val_loss: 3362.3359\n",
            "Epoch 8531/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2303.4500 - val_loss: 3359.7202\n",
            "Epoch 8532/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.2766 - val_loss: 3359.4175\n",
            "Epoch 8533/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2305.3213 - val_loss: 3365.7852\n",
            "Epoch 8534/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2305.6287 - val_loss: 3366.7083\n",
            "Epoch 8535/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2302.6550 - val_loss: 3360.4031\n",
            "Epoch 8536/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.7312 - val_loss: 3354.5952\n",
            "Epoch 8537/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2295.9973 - val_loss: 3354.4634\n",
            "Epoch 8538/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.1555 - val_loss: 3359.5408\n",
            "Epoch 8539/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2297.5063 - val_loss: 3367.8865\n",
            "Epoch 8540/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2301.5916 - val_loss: 3367.4175\n",
            "Epoch 8541/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2298.3008 - val_loss: 3359.5576\n",
            "Epoch 8542/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2294.8979 - val_loss: 3354.3479\n",
            "Epoch 8543/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.5679 - val_loss: 3353.3560\n",
            "Epoch 8544/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.7407 - val_loss: 3356.0642\n",
            "Epoch 8545/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2293.6846 - val_loss: 3359.5632\n",
            "Epoch 8546/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.2087 - val_loss: 3360.9031\n",
            "Epoch 8547/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.8435 - val_loss: 3361.9817\n",
            "Epoch 8548/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2297.0933 - val_loss: 3364.2507\n",
            "Epoch 8549/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2297.4526 - val_loss: 3363.3567\n",
            "Epoch 8550/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.7146 - val_loss: 3362.3708\n",
            "Epoch 8551/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2298.1533 - val_loss: 3364.1467\n",
            "Epoch 8552/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2297.4231 - val_loss: 3364.0535\n",
            "Epoch 8553/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.3669 - val_loss: 3364.5835\n",
            "Epoch 8554/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.0464 - val_loss: 3362.7253\n",
            "Epoch 8555/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2296.2664 - val_loss: 3361.5906\n",
            "Epoch 8556/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2296.2759 - val_loss: 3361.6714\n",
            "Epoch 8557/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.3081 - val_loss: 3360.5562\n",
            "Epoch 8558/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2295.8689 - val_loss: 3363.7253\n",
            "Epoch 8559/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2295.7690 - val_loss: 3365.7788\n",
            "Epoch 8560/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.1047 - val_loss: 3363.5208\n",
            "Epoch 8561/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.5376 - val_loss: 3365.9849\n",
            "Epoch 8562/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2295.6602 - val_loss: 3370.8809\n",
            "Epoch 8563/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2295.3359 - val_loss: 3369.8977\n",
            "Epoch 8564/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.0066 - val_loss: 3372.5398\n",
            "Epoch 8565/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2296.6135 - val_loss: 3372.4233\n",
            "Epoch 8566/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.8933 - val_loss: 3373.4375\n",
            "Epoch 8567/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2295.5068 - val_loss: 3373.5266\n",
            "Epoch 8568/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.8130 - val_loss: 3371.9746\n",
            "Epoch 8569/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.4441 - val_loss: 3370.9546\n",
            "Epoch 8570/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2295.4939 - val_loss: 3368.7644\n",
            "Epoch 8571/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.2322 - val_loss: 3368.4592\n",
            "Epoch 8572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.5430 - val_loss: 3367.5586\n",
            "Epoch 8573/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.2625 - val_loss: 3366.7092\n",
            "Epoch 8574/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.2725 - val_loss: 3367.2324\n",
            "Epoch 8575/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2295.9641 - val_loss: 3367.7510\n",
            "Epoch 8576/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2295.1121 - val_loss: 3368.5972\n",
            "Epoch 8577/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.7170 - val_loss: 3369.1362\n",
            "Epoch 8578/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.2891 - val_loss: 3371.3547\n",
            "Epoch 8579/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2294.3250 - val_loss: 3372.1162\n",
            "Epoch 8580/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.5764 - val_loss: 3367.2759\n",
            "Epoch 8581/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2294.0474 - val_loss: 3362.1001\n",
            "Epoch 8582/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.1985 - val_loss: 3361.3132\n",
            "Epoch 8583/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2295.3403 - val_loss: 3361.6885\n",
            "Epoch 8584/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2295.3496 - val_loss: 3360.7571\n",
            "Epoch 8585/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.2100 - val_loss: 3361.5259\n",
            "Epoch 8586/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.8274 - val_loss: 3361.4824\n",
            "Epoch 8587/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2293.5608 - val_loss: 3362.2483\n",
            "Epoch 8588/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2293.5239 - val_loss: 3362.8728\n",
            "Epoch 8589/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.7537 - val_loss: 3362.3191\n",
            "Epoch 8590/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2292.8418 - val_loss: 3363.0632\n",
            "Epoch 8591/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.1016 - val_loss: 3362.8816\n",
            "Epoch 8592/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.6797 - val_loss: 3361.8884\n",
            "Epoch 8593/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.1555 - val_loss: 3361.3621\n",
            "Epoch 8594/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2292.6196 - val_loss: 3363.6682\n",
            "Epoch 8595/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.7053 - val_loss: 3364.5503\n",
            "Epoch 8596/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.8020 - val_loss: 3365.1372\n",
            "Epoch 8597/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.4402 - val_loss: 3367.5383\n",
            "Epoch 8598/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.5491 - val_loss: 3368.0112\n",
            "Epoch 8599/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2295.6079 - val_loss: 3368.3547\n",
            "Epoch 8600/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.6555 - val_loss: 3368.1216\n",
            "Epoch 8601/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2294.4153 - val_loss: 3366.0576\n",
            "Epoch 8602/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2296.7166 - val_loss: 3366.3083\n",
            "Epoch 8603/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2299.8281 - val_loss: 3365.7778\n",
            "Epoch 8604/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.9160 - val_loss: 3365.5408\n",
            "Epoch 8605/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.2783 - val_loss: 3365.7798\n",
            "Epoch 8606/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.1729 - val_loss: 3367.1553\n",
            "Epoch 8607/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2297.0764 - val_loss: 3364.8428\n",
            "Epoch 8608/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2298.9021 - val_loss: 3364.8069\n",
            "Epoch 8609/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2299.8591 - val_loss: 3364.3604\n",
            "Epoch 8610/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2301.2200 - val_loss: 3364.7683\n",
            "Epoch 8611/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2300.6230 - val_loss: 3363.8413\n",
            "Epoch 8612/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2292.3525 - val_loss: 3364.2129\n",
            "Epoch 8613/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.7144 - val_loss: 3365.8462\n",
            "Epoch 8614/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.5525 - val_loss: 3366.3179\n",
            "Epoch 8615/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.5620 - val_loss: 3366.2078\n",
            "Epoch 8616/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2293.6306 - val_loss: 3364.2358\n",
            "Epoch 8617/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.0918 - val_loss: 3362.9529\n",
            "Epoch 8618/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.5481 - val_loss: 3362.9521\n",
            "Epoch 8619/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2292.1870 - val_loss: 3364.6697\n",
            "Epoch 8620/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.3684 - val_loss: 3364.3462\n",
            "Epoch 8621/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.4368 - val_loss: 3364.6760\n",
            "Epoch 8622/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.1992 - val_loss: 3364.7437\n",
            "Epoch 8623/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2292.5281 - val_loss: 3364.2783\n",
            "Epoch 8624/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2292.3813 - val_loss: 3363.7722\n",
            "Epoch 8625/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2292.1670 - val_loss: 3363.9778\n",
            "Epoch 8626/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2292.2383 - val_loss: 3370.5337\n",
            "Epoch 8627/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2297.9739 - val_loss: 3379.0449\n",
            "Epoch 8628/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2297.2380 - val_loss: 3378.2170\n",
            "Epoch 8629/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.0933 - val_loss: 3368.2810\n",
            "Epoch 8630/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.5525 - val_loss: 3362.3877\n",
            "Epoch 8631/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.4768 - val_loss: 3359.9670\n",
            "Epoch 8632/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.1621 - val_loss: 3358.7891\n",
            "Epoch 8633/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.6819 - val_loss: 3358.8608\n",
            "Epoch 8634/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.6790 - val_loss: 3359.6089\n",
            "Epoch 8635/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.2568 - val_loss: 3361.7815\n",
            "Epoch 8636/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.9897 - val_loss: 3364.9983\n",
            "Epoch 8637/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.4661 - val_loss: 3370.2627\n",
            "Epoch 8638/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.2803 - val_loss: 3371.0483\n",
            "Epoch 8639/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2293.6528 - val_loss: 3375.6553\n",
            "Epoch 8640/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.4509 - val_loss: 3377.3855\n",
            "Epoch 8641/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.0266 - val_loss: 3372.0449\n",
            "Epoch 8642/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.0291 - val_loss: 3368.8831\n",
            "Epoch 8643/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.3601 - val_loss: 3367.2781\n",
            "Epoch 8644/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.8469 - val_loss: 3366.4922\n",
            "Epoch 8645/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.0266 - val_loss: 3365.8721\n",
            "Epoch 8646/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.5894 - val_loss: 3366.6643\n",
            "Epoch 8647/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.0737 - val_loss: 3366.9065\n",
            "Epoch 8648/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.3284 - val_loss: 3366.0825\n",
            "Epoch 8649/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.1465 - val_loss: 3366.4558\n",
            "Epoch 8650/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.1685 - val_loss: 3366.4214\n",
            "Epoch 8651/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2296.5701 - val_loss: 3366.5315\n",
            "Epoch 8652/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2297.7415 - val_loss: 3367.3503\n",
            "Epoch 8653/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.9343 - val_loss: 3366.4233\n",
            "Epoch 8654/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2295.8826 - val_loss: 3365.2156\n",
            "Epoch 8655/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2294.8840 - val_loss: 3365.5403\n",
            "Epoch 8656/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.7373 - val_loss: 3366.3672\n",
            "Epoch 8657/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.2900 - val_loss: 3367.8230\n",
            "Epoch 8658/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2291.1587 - val_loss: 3368.6868\n",
            "Epoch 8659/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.4038 - val_loss: 3369.7366\n",
            "Epoch 8660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.5667 - val_loss: 3369.7244\n",
            "Epoch 8661/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2293.7812 - val_loss: 3369.7859\n",
            "Epoch 8662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.5437 - val_loss: 3369.0134\n",
            "Epoch 8663/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.7715 - val_loss: 3366.4873\n",
            "Epoch 8664/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.2629 - val_loss: 3366.6748\n",
            "Epoch 8665/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2292.3057 - val_loss: 3364.5535\n",
            "Epoch 8666/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.5286 - val_loss: 3362.2366\n",
            "Epoch 8667/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.2744 - val_loss: 3362.7156\n",
            "Epoch 8668/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2292.1372 - val_loss: 3363.2810\n",
            "Epoch 8669/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2293.4832 - val_loss: 3366.2229\n",
            "Epoch 8670/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.8186 - val_loss: 3371.0791\n",
            "Epoch 8671/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.7866 - val_loss: 3370.6909\n",
            "Epoch 8672/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2291.6472 - val_loss: 3370.9102\n",
            "Epoch 8673/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.0281 - val_loss: 3370.7085\n",
            "Epoch 8674/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.3611 - val_loss: 3371.7078\n",
            "Epoch 8675/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2295.4841 - val_loss: 3381.2195\n",
            "Epoch 8676/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2297.5725 - val_loss: 3381.7810\n",
            "Epoch 8677/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2297.2617 - val_loss: 3378.7610\n",
            "Epoch 8678/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.6584 - val_loss: 3376.4360\n",
            "Epoch 8679/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2292.5500 - val_loss: 3376.5945\n",
            "Epoch 8680/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.8110 - val_loss: 3376.9111\n",
            "Epoch 8681/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.3435 - val_loss: 3376.8999\n",
            "Epoch 8682/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.6155 - val_loss: 3372.3020\n",
            "Epoch 8683/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2291.4202 - val_loss: 3369.3010\n",
            "Epoch 8684/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.5056 - val_loss: 3368.4817\n",
            "Epoch 8685/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.2070 - val_loss: 3372.9646\n",
            "Epoch 8686/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.2300 - val_loss: 3375.4578\n",
            "Epoch 8687/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2292.0571 - val_loss: 3373.6721\n",
            "Epoch 8688/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.4048 - val_loss: 3376.7307\n",
            "Epoch 8689/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.8052 - val_loss: 3380.1477\n",
            "Epoch 8690/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.2671 - val_loss: 3379.6672\n",
            "Epoch 8691/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2294.3142 - val_loss: 3379.3789\n",
            "Epoch 8692/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.5088 - val_loss: 3377.1052\n",
            "Epoch 8693/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.3611 - val_loss: 3376.3638\n",
            "Epoch 8694/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2294.8308 - val_loss: 3375.3491\n",
            "Epoch 8695/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.2661 - val_loss: 3374.6836\n",
            "Epoch 8696/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.2947 - val_loss: 3372.0354\n",
            "Epoch 8697/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.1606 - val_loss: 3374.3381\n",
            "Epoch 8698/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2291.7795 - val_loss: 3374.8118\n",
            "Epoch 8699/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.1274 - val_loss: 3374.7852\n",
            "Epoch 8700/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2294.3481 - val_loss: 3380.0408\n",
            "Epoch 8701/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.8752 - val_loss: 3380.4810\n",
            "Epoch 8702/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.6133 - val_loss: 3376.9653\n",
            "Epoch 8703/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.7539 - val_loss: 3376.3665\n",
            "Epoch 8704/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2292.6074 - val_loss: 3374.7866\n",
            "Epoch 8705/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.7839 - val_loss: 3374.9929\n",
            "Epoch 8706/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.4175 - val_loss: 3374.7686\n",
            "Epoch 8707/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2291.2852 - val_loss: 3374.7051\n",
            "Epoch 8708/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.0588 - val_loss: 3375.6724\n",
            "Epoch 8709/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.6802 - val_loss: 3377.2778\n",
            "Epoch 8710/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2294.9668 - val_loss: 3378.5618\n",
            "Epoch 8711/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.4089 - val_loss: 3376.6748\n",
            "Epoch 8712/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.5859 - val_loss: 3375.4543\n",
            "Epoch 8713/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.4180 - val_loss: 3379.4814\n",
            "Epoch 8714/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2292.7046 - val_loss: 3383.4404\n",
            "Epoch 8715/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.7229 - val_loss: 3384.4705\n",
            "Epoch 8716/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2294.2454 - val_loss: 3385.2700\n",
            "Epoch 8717/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.2744 - val_loss: 3380.1704\n",
            "Epoch 8718/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2292.2822 - val_loss: 3377.5759\n",
            "Epoch 8719/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.3706 - val_loss: 3373.5579\n",
            "Epoch 8720/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.2725 - val_loss: 3371.4736\n",
            "Epoch 8721/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.2505 - val_loss: 3370.7620\n",
            "Epoch 8722/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2290.3064 - val_loss: 3370.9565\n",
            "Epoch 8723/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2290.0591 - val_loss: 3371.6377\n",
            "Epoch 8724/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.5840 - val_loss: 3371.9622\n",
            "Epoch 8725/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.0085 - val_loss: 3373.0015\n",
            "Epoch 8726/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.5137 - val_loss: 3372.3203\n",
            "Epoch 8727/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.9275 - val_loss: 3371.3813\n",
            "Epoch 8728/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2289.7119 - val_loss: 3366.7542\n",
            "Epoch 8729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.6345 - val_loss: 3364.3953\n",
            "Epoch 8730/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.8694 - val_loss: 3365.3782\n",
            "Epoch 8731/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.7786 - val_loss: 3365.4990\n",
            "Epoch 8732/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2291.2773 - val_loss: 3367.4712\n",
            "Epoch 8733/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.9255 - val_loss: 3370.7927\n",
            "Epoch 8734/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.1492 - val_loss: 3370.5610\n",
            "Epoch 8735/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.6421 - val_loss: 3370.6699\n",
            "Epoch 8736/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2289.9648 - val_loss: 3369.9160\n",
            "Epoch 8737/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.3672 - val_loss: 3366.7485\n",
            "Epoch 8738/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2292.2297 - val_loss: 3362.8076\n",
            "Epoch 8739/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.9502 - val_loss: 3362.6978\n",
            "Epoch 8740/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.0557 - val_loss: 3362.6289\n",
            "Epoch 8741/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.9531 - val_loss: 3363.1082\n",
            "Epoch 8742/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2288.8218 - val_loss: 3366.8950\n",
            "Epoch 8743/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.1140 - val_loss: 3370.0908\n",
            "Epoch 8744/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.2930 - val_loss: 3369.6729\n",
            "Epoch 8745/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2289.5327 - val_loss: 3373.9185\n",
            "Epoch 8746/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.6943 - val_loss: 3375.1924\n",
            "Epoch 8747/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.2258 - val_loss: 3372.4351\n",
            "Epoch 8748/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.3647 - val_loss: 3371.0896\n",
            "Epoch 8749/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2289.7793 - val_loss: 3370.8462\n",
            "Epoch 8750/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.5852 - val_loss: 3369.8308\n",
            "Epoch 8751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.9019 - val_loss: 3370.9509\n",
            "Epoch 8752/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2290.9482 - val_loss: 3374.1553\n",
            "Epoch 8753/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2290.7100 - val_loss: 3374.4165\n",
            "Epoch 8754/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2290.6279 - val_loss: 3372.9543\n",
            "Epoch 8755/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.1345 - val_loss: 3373.8440\n",
            "Epoch 8756/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2290.2898 - val_loss: 3373.8477\n",
            "Epoch 8757/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.8147 - val_loss: 3374.4719\n",
            "Epoch 8758/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.3096 - val_loss: 3377.9446\n",
            "Epoch 8759/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2290.3481 - val_loss: 3378.6611\n",
            "Epoch 8760/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2291.6206 - val_loss: 3378.2803\n",
            "Epoch 8761/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.6191 - val_loss: 3380.0688\n",
            "Epoch 8762/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.0286 - val_loss: 3377.6882\n",
            "Epoch 8763/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.7634 - val_loss: 3373.5688\n",
            "Epoch 8764/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2288.9614 - val_loss: 3373.6011\n",
            "Epoch 8765/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.7942 - val_loss: 3372.9790\n",
            "Epoch 8766/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.1431 - val_loss: 3367.9753\n",
            "Epoch 8767/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.8403 - val_loss: 3366.9192\n",
            "Epoch 8768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.1499 - val_loss: 3367.2429\n",
            "Epoch 8769/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2292.6362 - val_loss: 3368.5540\n",
            "Epoch 8770/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.6638 - val_loss: 3367.5566\n",
            "Epoch 8771/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.7839 - val_loss: 3363.3604\n",
            "Epoch 8772/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.4553 - val_loss: 3361.8096\n",
            "Epoch 8773/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2290.1575 - val_loss: 3362.3726\n",
            "Epoch 8774/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2290.6106 - val_loss: 3364.5962\n",
            "Epoch 8775/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2294.0864 - val_loss: 3365.2107\n",
            "Epoch 8776/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2294.2329 - val_loss: 3365.3870\n",
            "Epoch 8777/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2294.2507 - val_loss: 3367.6609\n",
            "Epoch 8778/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2289.9688 - val_loss: 3368.3323\n",
            "Epoch 8779/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.6687 - val_loss: 3367.8518\n",
            "Epoch 8780/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2288.7820 - val_loss: 3368.0408\n",
            "Epoch 8781/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.3523 - val_loss: 3368.6321\n",
            "Epoch 8782/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.7444 - val_loss: 3370.2173\n",
            "Epoch 8783/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2288.7246 - val_loss: 3370.5615\n",
            "Epoch 8784/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2288.6440 - val_loss: 3370.5417\n",
            "Epoch 8785/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.9680 - val_loss: 3369.5920\n",
            "Epoch 8786/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2291.1953 - val_loss: 3368.5190\n",
            "Epoch 8787/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.9346 - val_loss: 3366.6965\n",
            "Epoch 8788/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2293.6453 - val_loss: 3365.7515\n",
            "Epoch 8789/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.9353 - val_loss: 3365.2234\n",
            "Epoch 8790/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2291.1377 - val_loss: 3365.6228\n",
            "Epoch 8791/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.2087 - val_loss: 3370.0378\n",
            "Epoch 8792/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.6060 - val_loss: 3371.7668\n",
            "Epoch 8793/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2290.2002 - val_loss: 3373.0029\n",
            "Epoch 8794/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2288.9102 - val_loss: 3372.8462\n",
            "Epoch 8795/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2289.2363 - val_loss: 3371.2903\n",
            "Epoch 8796/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2289.4075 - val_loss: 3368.6733\n",
            "Epoch 8797/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.3726 - val_loss: 3367.4990\n",
            "Epoch 8798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.1201 - val_loss: 3367.3447\n",
            "Epoch 8799/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2287.2014 - val_loss: 3370.5833\n",
            "Epoch 8800/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.2998 - val_loss: 3371.3215\n",
            "Epoch 8801/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.5195 - val_loss: 3372.1260\n",
            "Epoch 8802/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.6946 - val_loss: 3372.4006\n",
            "Epoch 8803/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2290.0017 - val_loss: 3374.9734\n",
            "Epoch 8804/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.0754 - val_loss: 3374.2415\n",
            "Epoch 8805/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.8762 - val_loss: 3374.0042\n",
            "Epoch 8806/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2289.5884 - val_loss: 3372.6191\n",
            "Epoch 8807/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.9600 - val_loss: 3370.2085\n",
            "Epoch 8808/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.5122 - val_loss: 3369.5078\n",
            "Epoch 8809/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.5769 - val_loss: 3365.9453\n",
            "Epoch 8810/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2288.2871 - val_loss: 3366.4316\n",
            "Epoch 8811/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.1797 - val_loss: 3368.2610\n",
            "Epoch 8812/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.4392 - val_loss: 3368.6128\n",
            "Epoch 8813/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.6506 - val_loss: 3370.2971\n",
            "Epoch 8814/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.2576 - val_loss: 3370.7581\n",
            "Epoch 8815/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.1973 - val_loss: 3370.2253\n",
            "Epoch 8816/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.5759 - val_loss: 3369.6306\n",
            "Epoch 8817/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2289.4924 - val_loss: 3368.3691\n",
            "Epoch 8818/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.3635 - val_loss: 3368.7517\n",
            "Epoch 8819/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.8115 - val_loss: 3368.1160\n",
            "Epoch 8820/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.1755 - val_loss: 3367.3140\n",
            "Epoch 8821/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2290.1970 - val_loss: 3366.2090\n",
            "Epoch 8822/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.0549 - val_loss: 3362.8645\n",
            "Epoch 8823/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2292.8650 - val_loss: 3362.0703\n",
            "Epoch 8824/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.3999 - val_loss: 3362.2217\n",
            "Epoch 8825/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2290.3828 - val_loss: 3362.6794\n",
            "Epoch 8826/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.6531 - val_loss: 3363.0664\n",
            "Epoch 8827/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.2832 - val_loss: 3362.5977\n",
            "Epoch 8828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.7397 - val_loss: 3363.5142\n",
            "Epoch 8829/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2291.3843 - val_loss: 3369.5454\n",
            "Epoch 8830/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.4636 - val_loss: 3374.7886\n",
            "Epoch 8831/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.4565 - val_loss: 3374.6082\n",
            "Epoch 8832/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.6060 - val_loss: 3364.8103\n",
            "Epoch 8833/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.7822 - val_loss: 3360.3398\n",
            "Epoch 8834/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.6150 - val_loss: 3360.5857\n",
            "Epoch 8835/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.3503 - val_loss: 3359.5684\n",
            "Epoch 8836/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2290.1201 - val_loss: 3350.9880\n",
            "Epoch 8837/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.1460 - val_loss: 3349.5520\n",
            "Epoch 8838/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.0298 - val_loss: 3350.3835\n",
            "Epoch 8839/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2290.4958 - val_loss: 3356.9214\n",
            "Epoch 8840/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.7856 - val_loss: 3355.5488\n",
            "Epoch 8841/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.4500 - val_loss: 3352.0881\n",
            "Epoch 8842/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.9434 - val_loss: 3351.0168\n",
            "Epoch 8843/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.8772 - val_loss: 3351.0398\n",
            "Epoch 8844/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2286.4275 - val_loss: 3349.5752\n",
            "Epoch 8845/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.0564 - val_loss: 3346.8484\n",
            "Epoch 8846/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2287.0718 - val_loss: 3346.6226\n",
            "Epoch 8847/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2286.6885 - val_loss: 3347.3154\n",
            "Epoch 8848/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.9946 - val_loss: 3348.8103\n",
            "Epoch 8849/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2287.9817 - val_loss: 3346.0930\n",
            "Epoch 8850/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2289.5698 - val_loss: 3347.0916\n",
            "Epoch 8851/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2290.1445 - val_loss: 3347.7766\n",
            "Epoch 8852/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2292.8618 - val_loss: 3346.5254\n",
            "Epoch 8853/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.5181 - val_loss: 3346.3630\n",
            "Epoch 8854/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.6304 - val_loss: 3347.0496\n",
            "Epoch 8855/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2287.3958 - val_loss: 3348.0283\n",
            "Epoch 8856/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2286.4880 - val_loss: 3349.1602\n",
            "Epoch 8857/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.0022 - val_loss: 3351.4600\n",
            "Epoch 8858/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.2517 - val_loss: 3352.4832\n",
            "Epoch 8859/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.1692 - val_loss: 3352.7517\n",
            "Epoch 8860/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.2178 - val_loss: 3353.5986\n",
            "Epoch 8861/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.5652 - val_loss: 3357.5962\n",
            "Epoch 8862/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.3604 - val_loss: 3354.2612\n",
            "Epoch 8863/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2286.9216 - val_loss: 3353.7366\n",
            "Epoch 8864/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.6777 - val_loss: 3352.9814\n",
            "Epoch 8865/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.2373 - val_loss: 3351.7927\n",
            "Epoch 8866/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2287.8115 - val_loss: 3347.9741\n",
            "Epoch 8867/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.8584 - val_loss: 3347.1328\n",
            "Epoch 8868/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2287.6035 - val_loss: 3344.3684\n",
            "Epoch 8869/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2289.3728 - val_loss: 3344.4807\n",
            "Epoch 8870/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.9958 - val_loss: 3345.0696\n",
            "Epoch 8871/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2289.5623 - val_loss: 3344.4573\n",
            "Epoch 8872/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.5312 - val_loss: 3344.6355\n",
            "Epoch 8873/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2287.5557 - val_loss: 3345.6218\n",
            "Epoch 8874/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.9958 - val_loss: 3346.3318\n",
            "Epoch 8875/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2287.6169 - val_loss: 3347.6360\n",
            "Epoch 8876/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.9155 - val_loss: 3351.0627\n",
            "Epoch 8877/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.0833 - val_loss: 3351.3901\n",
            "Epoch 8878/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.9622 - val_loss: 3350.7942\n",
            "Epoch 8879/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.7117 - val_loss: 3350.0967\n",
            "Epoch 8880/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.0547 - val_loss: 3349.5986\n",
            "Epoch 8881/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2286.4092 - val_loss: 3349.9133\n",
            "Epoch 8882/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.8237 - val_loss: 3351.4775\n",
            "Epoch 8883/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.5659 - val_loss: 3351.8196\n",
            "Epoch 8884/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2287.5122 - val_loss: 3351.3452\n",
            "Epoch 8885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.1006 - val_loss: 3350.5105\n",
            "Epoch 8886/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.6123 - val_loss: 3350.7258\n",
            "Epoch 8887/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2288.6003 - val_loss: 3353.6018\n",
            "Epoch 8888/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.2056 - val_loss: 3353.9517\n",
            "Epoch 8889/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.8025 - val_loss: 3356.4453\n",
            "Epoch 8890/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.0039 - val_loss: 3355.7251\n",
            "Epoch 8891/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.1697 - val_loss: 3355.8523\n",
            "Epoch 8892/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2284.5706 - val_loss: 3358.5227\n",
            "Epoch 8893/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.5598 - val_loss: 3358.0422\n",
            "Epoch 8894/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.1221 - val_loss: 3357.6970\n",
            "Epoch 8895/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.3955 - val_loss: 3359.6355\n",
            "Epoch 8896/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.6040 - val_loss: 3361.3948\n",
            "Epoch 8897/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.4036 - val_loss: 3365.4160\n",
            "Epoch 8898/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2293.6401 - val_loss: 3367.5400\n",
            "Epoch 8899/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2296.7693 - val_loss: 3370.7935\n",
            "Epoch 8900/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2298.2422 - val_loss: 3372.9778\n",
            "Epoch 8901/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2296.1213 - val_loss: 3367.9656\n",
            "Epoch 8902/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2294.3469 - val_loss: 3363.2822\n",
            "Epoch 8903/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2291.8362 - val_loss: 3360.9741\n",
            "Epoch 8904/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2290.7910 - val_loss: 3354.0073\n",
            "Epoch 8905/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.3474 - val_loss: 3352.0288\n",
            "Epoch 8906/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.1404 - val_loss: 3352.5142\n",
            "Epoch 8907/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.5681 - val_loss: 3357.3020\n",
            "Epoch 8908/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2288.2969 - val_loss: 3357.4094\n",
            "Epoch 8909/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.2031 - val_loss: 3356.6833\n",
            "Epoch 8910/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.4890 - val_loss: 3356.3091\n",
            "Epoch 8911/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2285.5408 - val_loss: 3356.2991\n",
            "Epoch 8912/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.3650 - val_loss: 3355.3933\n",
            "Epoch 8913/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.2720 - val_loss: 3353.8223\n",
            "Epoch 8914/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2285.5059 - val_loss: 3351.5479\n",
            "Epoch 8915/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.3850 - val_loss: 3350.4609\n",
            "Epoch 8916/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.8855 - val_loss: 3350.6279\n",
            "Epoch 8917/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2285.9851 - val_loss: 3351.0940\n",
            "Epoch 8918/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2285.4514 - val_loss: 3351.2246\n",
            "Epoch 8919/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.5977 - val_loss: 3351.0706\n",
            "Epoch 8920/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2284.6313 - val_loss: 3349.6531\n",
            "Epoch 8921/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.6792 - val_loss: 3350.7683\n",
            "Epoch 8922/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.2341 - val_loss: 3347.9360\n",
            "Epoch 8923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.2600 - val_loss: 3352.9004\n",
            "Epoch 8924/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2284.9519 - val_loss: 3356.7852\n",
            "Epoch 8925/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.5989 - val_loss: 3358.0923\n",
            "Epoch 8926/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.7136 - val_loss: 3360.1204\n",
            "Epoch 8927/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2287.1716 - val_loss: 3360.3047\n",
            "Epoch 8928/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2287.0508 - val_loss: 3358.4854\n",
            "Epoch 8929/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.2549 - val_loss: 3352.0920\n",
            "Epoch 8930/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.7266 - val_loss: 3350.9573\n",
            "Epoch 8931/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.0554 - val_loss: 3350.8179\n",
            "Epoch 8932/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2285.0562 - val_loss: 3352.0298\n",
            "Epoch 8933/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2284.4268 - val_loss: 3352.2964\n",
            "Epoch 8934/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2284.2542 - val_loss: 3348.5850\n",
            "Epoch 8935/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2284.7688 - val_loss: 3348.4578\n",
            "Epoch 8936/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.5698 - val_loss: 3351.2209\n",
            "Epoch 8937/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.3237 - val_loss: 3349.6252\n",
            "Epoch 8938/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.5093 - val_loss: 3349.7153\n",
            "Epoch 8939/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.7546 - val_loss: 3349.1472\n",
            "Epoch 8940/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2287.6426 - val_loss: 3348.7773\n",
            "Epoch 8941/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.4165 - val_loss: 3350.3574\n",
            "Epoch 8942/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2285.7864 - val_loss: 3351.0864\n",
            "Epoch 8943/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.2053 - val_loss: 3351.8184\n",
            "Epoch 8944/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.2910 - val_loss: 3356.5930\n",
            "Epoch 8945/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2285.4487 - val_loss: 3359.8679\n",
            "Epoch 8946/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.6399 - val_loss: 3361.7947\n",
            "Epoch 8947/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.9917 - val_loss: 3358.7671\n",
            "Epoch 8948/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.1128 - val_loss: 3358.8789\n",
            "Epoch 8949/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2285.5208 - val_loss: 3359.8103\n",
            "Epoch 8950/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.2695 - val_loss: 3360.0571\n",
            "Epoch 8951/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.2358 - val_loss: 3361.3630\n",
            "Epoch 8952/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2285.6111 - val_loss: 3359.6516\n",
            "Epoch 8953/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.0750 - val_loss: 3353.1826\n",
            "Epoch 8954/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2284.4272 - val_loss: 3351.5457\n",
            "Epoch 8955/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2284.5654 - val_loss: 3352.1821\n",
            "Epoch 8956/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.2495 - val_loss: 3354.8235\n",
            "Epoch 8957/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.7314 - val_loss: 3353.9446\n",
            "Epoch 8958/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2284.9839 - val_loss: 3355.1504\n",
            "Epoch 8959/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.5496 - val_loss: 3356.8616\n",
            "Epoch 8960/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2284.0403 - val_loss: 3357.9102\n",
            "Epoch 8961/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2284.9980 - val_loss: 3363.3918\n",
            "Epoch 8962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.4329 - val_loss: 3364.6667\n",
            "Epoch 8963/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2286.5659 - val_loss: 3363.7903\n",
            "Epoch 8964/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2286.3652 - val_loss: 3364.3271\n",
            "Epoch 8965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.7920 - val_loss: 3367.7185\n",
            "Epoch 8966/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2290.2412 - val_loss: 3367.2109\n",
            "Epoch 8967/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2289.9851 - val_loss: 3364.1194\n",
            "Epoch 8968/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2288.1948 - val_loss: 3366.3982\n",
            "Epoch 8969/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2289.6184 - val_loss: 3370.4856\n",
            "Epoch 8970/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2292.2163 - val_loss: 3372.9421\n",
            "Epoch 8971/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2293.0916 - val_loss: 3374.4094\n",
            "Epoch 8972/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2291.5312 - val_loss: 3371.0034\n",
            "Epoch 8973/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2288.6599 - val_loss: 3368.3848\n",
            "Epoch 8974/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.7083 - val_loss: 3370.8469\n",
            "Epoch 8975/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2289.0847 - val_loss: 3367.4885\n",
            "Epoch 8976/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2286.9675 - val_loss: 3365.6130\n",
            "Epoch 8977/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.2251 - val_loss: 3366.6733\n",
            "Epoch 8978/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2286.3167 - val_loss: 3366.8044\n",
            "Epoch 8979/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.5767 - val_loss: 3367.9304\n",
            "Epoch 8980/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2287.2329 - val_loss: 3367.0803\n",
            "Epoch 8981/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.9707 - val_loss: 3364.9736\n",
            "Epoch 8982/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2285.5840 - val_loss: 3361.4558\n",
            "Epoch 8983/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.4624 - val_loss: 3360.8560\n",
            "Epoch 8984/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2286.7881 - val_loss: 3363.1140\n",
            "Epoch 8985/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.8003 - val_loss: 3363.4592\n",
            "Epoch 8986/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2289.4460 - val_loss: 3357.8960\n",
            "Epoch 8987/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2285.4478 - val_loss: 3354.8267\n",
            "Epoch 8988/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2284.9275 - val_loss: 3350.6604\n",
            "Epoch 8989/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.8877 - val_loss: 3353.9199\n",
            "Epoch 8990/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2285.8198 - val_loss: 3363.8269\n",
            "Epoch 8991/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.9871 - val_loss: 3365.2009\n",
            "Epoch 8992/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.3733 - val_loss: 3361.0090\n",
            "Epoch 8993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.2278 - val_loss: 3366.3855\n",
            "Epoch 8994/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.2581 - val_loss: 3366.8679\n",
            "Epoch 8995/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.8225 - val_loss: 3363.8467\n",
            "Epoch 8996/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.8972 - val_loss: 3359.9368\n",
            "Epoch 8997/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.5532 - val_loss: 3357.9009\n",
            "Epoch 8998/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2284.1924 - val_loss: 3356.5015\n",
            "Epoch 8999/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2283.7886 - val_loss: 3356.2668\n",
            "Epoch 9000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.5686 - val_loss: 3356.3840\n",
            "Epoch 9001/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2282.6841 - val_loss: 3353.7415\n",
            "Epoch 9002/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2283.3818 - val_loss: 3353.6145\n",
            "Epoch 9003/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.3088 - val_loss: 3352.1729\n",
            "Epoch 9004/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2283.0505 - val_loss: 3351.6228\n",
            "Epoch 9005/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2282.7349 - val_loss: 3352.8503\n",
            "Epoch 9006/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2282.2605 - val_loss: 3353.9961\n",
            "Epoch 9007/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2283.9641 - val_loss: 3351.6104\n",
            "Epoch 9008/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2283.1265 - val_loss: 3356.5691\n",
            "Epoch 9009/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2283.5759 - val_loss: 3357.1931\n",
            "Epoch 9010/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2283.0078 - val_loss: 3352.6541\n",
            "Epoch 9011/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.2310 - val_loss: 3352.3245\n",
            "Epoch 9012/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.9668 - val_loss: 3352.4832\n",
            "Epoch 9013/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2283.1895 - val_loss: 3352.7275\n",
            "Epoch 9014/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2283.4468 - val_loss: 3352.8774\n",
            "Epoch 9015/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.8713 - val_loss: 3352.2332\n",
            "Epoch 9016/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2282.9817 - val_loss: 3348.2915\n",
            "Epoch 9017/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2281.9563 - val_loss: 3347.2354\n",
            "Epoch 9018/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.9541 - val_loss: 3347.4062\n",
            "Epoch 9019/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.6226 - val_loss: 3350.6218\n",
            "Epoch 9020/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2282.2529 - val_loss: 3352.0835\n",
            "Epoch 9021/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.6492 - val_loss: 3349.2227\n",
            "Epoch 9022/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.2122 - val_loss: 3348.4912\n",
            "Epoch 9023/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.9387 - val_loss: 3347.9062\n",
            "Epoch 9024/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.8728 - val_loss: 3347.4666\n",
            "Epoch 9025/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.9272 - val_loss: 3349.0696\n",
            "Epoch 9026/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2283.9902 - val_loss: 3351.5085\n",
            "Epoch 9027/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.0139 - val_loss: 3351.8767\n",
            "Epoch 9028/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.5024 - val_loss: 3349.0422\n",
            "Epoch 9029/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2285.0552 - val_loss: 3348.1187\n",
            "Epoch 9030/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.0862 - val_loss: 3348.6362\n",
            "Epoch 9031/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2282.7451 - val_loss: 3348.8267\n",
            "Epoch 9032/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2283.1956 - val_loss: 3347.8406\n",
            "Epoch 9033/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2283.0588 - val_loss: 3347.6741\n",
            "Epoch 9034/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.2974 - val_loss: 3351.3989\n",
            "Epoch 9035/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2283.8696 - val_loss: 3355.7275\n",
            "Epoch 9036/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2288.1128 - val_loss: 3361.0154\n",
            "Epoch 9037/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.4941 - val_loss: 3358.8940\n",
            "Epoch 9038/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.3684 - val_loss: 3354.6050\n",
            "Epoch 9039/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2284.7625 - val_loss: 3353.8989\n",
            "Epoch 9040/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2288.2207 - val_loss: 3361.2153\n",
            "Epoch 9041/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2288.1172 - val_loss: 3360.4377\n",
            "Epoch 9042/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.6851 - val_loss: 3359.5063\n",
            "Epoch 9043/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2287.1768 - val_loss: 3358.9490\n",
            "Epoch 9044/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2287.4255 - val_loss: 3359.9097\n",
            "Epoch 9045/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2286.7910 - val_loss: 3358.0808\n",
            "Epoch 9046/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.5615 - val_loss: 3357.4800\n",
            "Epoch 9047/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.7791 - val_loss: 3360.3765\n",
            "Epoch 9048/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2287.2683 - val_loss: 3359.5903\n",
            "Epoch 9049/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.8733 - val_loss: 3351.4231\n",
            "Epoch 9050/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.8618 - val_loss: 3344.6025\n",
            "Epoch 9051/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.0894 - val_loss: 3344.1135\n",
            "Epoch 9052/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.4163 - val_loss: 3348.6404\n",
            "Epoch 9053/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.6729 - val_loss: 3349.3750\n",
            "Epoch 9054/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.9548 - val_loss: 3349.7634\n",
            "Epoch 9055/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.3315 - val_loss: 3348.0505\n",
            "Epoch 9056/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2283.2517 - val_loss: 3343.3516\n",
            "Epoch 9057/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2281.2124 - val_loss: 3342.3103\n",
            "Epoch 9058/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2279.3572 - val_loss: 3338.5344\n",
            "Epoch 9059/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.6353 - val_loss: 3337.8840\n",
            "Epoch 9060/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.1653 - val_loss: 3337.4617\n",
            "Epoch 9061/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.8740 - val_loss: 3337.7922\n",
            "Epoch 9062/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.4739 - val_loss: 3340.5422\n",
            "Epoch 9063/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2280.8325 - val_loss: 3344.5496\n",
            "Epoch 9064/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.3899 - val_loss: 3347.7065\n",
            "Epoch 9065/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.0525 - val_loss: 3347.0344\n",
            "Epoch 9066/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.2344 - val_loss: 3346.0779\n",
            "Epoch 9067/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2283.2588 - val_loss: 3344.5823\n",
            "Epoch 9068/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.6838 - val_loss: 3345.6970\n",
            "Epoch 9069/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.8528 - val_loss: 3347.2373\n",
            "Epoch 9070/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.5776 - val_loss: 3345.3103\n",
            "Epoch 9071/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.1616 - val_loss: 3344.6860\n",
            "Epoch 9072/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.4500 - val_loss: 3344.9890\n",
            "Epoch 9073/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.4041 - val_loss: 3344.6609\n",
            "Epoch 9074/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.8374 - val_loss: 3344.7227\n",
            "Epoch 9075/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.4539 - val_loss: 3346.0225\n",
            "Epoch 9076/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.8132 - val_loss: 3347.3286\n",
            "Epoch 9077/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2282.1570 - val_loss: 3349.5190\n",
            "Epoch 9078/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.5525 - val_loss: 3348.7627\n",
            "Epoch 9079/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.8086 - val_loss: 3347.5610\n",
            "Epoch 9080/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2282.7812 - val_loss: 3351.9719\n",
            "Epoch 9081/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.6157 - val_loss: 3352.8547\n",
            "Epoch 9082/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.9211 - val_loss: 3348.6909\n",
            "Epoch 9083/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2281.8428 - val_loss: 3346.1404\n",
            "Epoch 9084/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.1548 - val_loss: 3347.1523\n",
            "Epoch 9085/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2281.6558 - val_loss: 3348.7358\n",
            "Epoch 9086/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2281.7151 - val_loss: 3348.4951\n",
            "Epoch 9087/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.9834 - val_loss: 3347.7153\n",
            "Epoch 9088/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2278.7593 - val_loss: 3344.9575\n",
            "Epoch 9089/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.7649 - val_loss: 3347.3821\n",
            "Epoch 9090/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.8562 - val_loss: 3347.7358\n",
            "Epoch 9091/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.5461 - val_loss: 3349.0554\n",
            "Epoch 9092/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.5867 - val_loss: 3348.5422\n",
            "Epoch 9093/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.3335 - val_loss: 3346.3982\n",
            "Epoch 9094/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.6511 - val_loss: 3346.3413\n",
            "Epoch 9095/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.3982 - val_loss: 3346.3796\n",
            "Epoch 9096/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2280.8721 - val_loss: 3346.6179\n",
            "Epoch 9097/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.2444 - val_loss: 3347.9915\n",
            "Epoch 9098/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.2903 - val_loss: 3349.2380\n",
            "Epoch 9099/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.2168 - val_loss: 3350.8630\n",
            "Epoch 9100/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.7271 - val_loss: 3348.4221\n",
            "Epoch 9101/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.3086 - val_loss: 3347.3911\n",
            "Epoch 9102/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.4753 - val_loss: 3346.5737\n",
            "Epoch 9103/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.5598 - val_loss: 3346.1418\n",
            "Epoch 9104/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.5364 - val_loss: 3346.3926\n",
            "Epoch 9105/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.1509 - val_loss: 3347.0879\n",
            "Epoch 9106/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2280.2490 - val_loss: 3347.2178\n",
            "Epoch 9107/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2281.8206 - val_loss: 3344.2332\n",
            "Epoch 9108/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.2642 - val_loss: 3343.8115\n",
            "Epoch 9109/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.1394 - val_loss: 3347.0471\n",
            "Epoch 9110/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2279.7512 - val_loss: 3348.4277\n",
            "Epoch 9111/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2279.2185 - val_loss: 3349.4248\n",
            "Epoch 9112/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.5681 - val_loss: 3350.2598\n",
            "Epoch 9113/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2282.2856 - val_loss: 3352.3523\n",
            "Epoch 9114/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.4604 - val_loss: 3350.6179\n",
            "Epoch 9115/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2278.5903 - val_loss: 3346.8188\n",
            "Epoch 9116/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.8354 - val_loss: 3344.8188\n",
            "Epoch 9117/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2281.6682 - val_loss: 3346.1560\n",
            "Epoch 9118/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.9956 - val_loss: 3345.5063\n",
            "Epoch 9119/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2281.9089 - val_loss: 3347.2158\n",
            "Epoch 9120/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.3425 - val_loss: 3348.1145\n",
            "Epoch 9121/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.2358 - val_loss: 3349.5745\n",
            "Epoch 9122/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.5830 - val_loss: 3348.3926\n",
            "Epoch 9123/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2279.9299 - val_loss: 3348.5906\n",
            "Epoch 9124/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.0447 - val_loss: 3349.2405\n",
            "Epoch 9125/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2279.1904 - val_loss: 3353.8147\n",
            "Epoch 9126/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2280.6951 - val_loss: 3355.1079\n",
            "Epoch 9127/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.9316 - val_loss: 3355.0562\n",
            "Epoch 9128/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.9133 - val_loss: 3356.6089\n",
            "Epoch 9129/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.7371 - val_loss: 3356.3215\n",
            "Epoch 9130/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.4873 - val_loss: 3356.2893\n",
            "Epoch 9131/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.2278 - val_loss: 3356.7549\n",
            "Epoch 9132/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2283.1472 - val_loss: 3362.9033\n",
            "Epoch 9133/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.6504 - val_loss: 3362.9648\n",
            "Epoch 9134/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2281.4968 - val_loss: 3361.6409\n",
            "Epoch 9135/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.5540 - val_loss: 3361.7266\n",
            "Epoch 9136/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.0898 - val_loss: 3360.3352\n",
            "Epoch 9137/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.8481 - val_loss: 3358.1860\n",
            "Epoch 9138/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2280.1755 - val_loss: 3358.5425\n",
            "Epoch 9139/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2279.7432 - val_loss: 3357.1790\n",
            "Epoch 9140/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.6284 - val_loss: 3358.3040\n",
            "Epoch 9141/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.9883 - val_loss: 3361.2251\n",
            "Epoch 9142/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.0186 - val_loss: 3361.7866\n",
            "Epoch 9143/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.0144 - val_loss: 3361.4456\n",
            "Epoch 9144/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2282.2046 - val_loss: 3361.7725\n",
            "Epoch 9145/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.4270 - val_loss: 3362.7195\n",
            "Epoch 9146/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.4902 - val_loss: 3360.9290\n",
            "Epoch 9147/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.5320 - val_loss: 3356.5994\n",
            "Epoch 9148/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.3755 - val_loss: 3357.1528\n",
            "Epoch 9149/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.8069 - val_loss: 3359.9529\n",
            "Epoch 9150/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.6006 - val_loss: 3364.6533\n",
            "Epoch 9151/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2285.2019 - val_loss: 3365.3101\n",
            "Epoch 9152/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2285.4067 - val_loss: 3362.3877\n",
            "Epoch 9153/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2283.0454 - val_loss: 3360.4060\n",
            "Epoch 9154/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.4106 - val_loss: 3359.1931\n",
            "Epoch 9155/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2281.3596 - val_loss: 3357.1619\n",
            "Epoch 9156/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2280.1465 - val_loss: 3356.1506\n",
            "Epoch 9157/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2280.1538 - val_loss: 3355.3662\n",
            "Epoch 9158/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.9756 - val_loss: 3355.0688\n",
            "Epoch 9159/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.6570 - val_loss: 3355.7051\n",
            "Epoch 9160/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.4060 - val_loss: 3356.4109\n",
            "Epoch 9161/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.5100 - val_loss: 3358.6790\n",
            "Epoch 9162/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.6001 - val_loss: 3359.2690\n",
            "Epoch 9163/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.8889 - val_loss: 3358.5947\n",
            "Epoch 9164/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2281.6433 - val_loss: 3355.7605\n",
            "Epoch 9165/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.8198 - val_loss: 3352.7180\n",
            "Epoch 9166/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.0498 - val_loss: 3350.8352\n",
            "Epoch 9167/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2281.3579 - val_loss: 3351.0098\n",
            "Epoch 9168/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.2051 - val_loss: 3351.4346\n",
            "Epoch 9169/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.1462 - val_loss: 3352.5410\n",
            "Epoch 9170/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.9514 - val_loss: 3348.6299\n",
            "Epoch 9171/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2279.5623 - val_loss: 3349.0632\n",
            "Epoch 9172/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.0376 - val_loss: 3348.7900\n",
            "Epoch 9173/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.7610 - val_loss: 3352.5046\n",
            "Epoch 9174/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.7021 - val_loss: 3352.8835\n",
            "Epoch 9175/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.9683 - val_loss: 3352.9241\n",
            "Epoch 9176/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.5854 - val_loss: 3349.2078\n",
            "Epoch 9177/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.3108 - val_loss: 3347.5754\n",
            "Epoch 9178/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.5117 - val_loss: 3347.6741\n",
            "Epoch 9179/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.6794 - val_loss: 3347.6924\n",
            "Epoch 9180/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.8289 - val_loss: 3344.4846\n",
            "Epoch 9181/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.7390 - val_loss: 3345.9607\n",
            "Epoch 9182/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.9050 - val_loss: 3351.8408\n",
            "Epoch 9183/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.6218 - val_loss: 3351.6721\n",
            "Epoch 9184/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.8638 - val_loss: 3348.1960\n",
            "Epoch 9185/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.7749 - val_loss: 3346.0166\n",
            "Epoch 9186/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2279.3413 - val_loss: 3345.4060\n",
            "Epoch 9187/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2277.9031 - val_loss: 3342.2092\n",
            "Epoch 9188/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.1641 - val_loss: 3342.9878\n",
            "Epoch 9189/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.1987 - val_loss: 3343.4521\n",
            "Epoch 9190/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2277.6060 - val_loss: 3345.8247\n",
            "Epoch 9191/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.9592 - val_loss: 3348.1248\n",
            "Epoch 9192/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2277.7944 - val_loss: 3347.8835\n",
            "Epoch 9193/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.0874 - val_loss: 3346.0200\n",
            "Epoch 9194/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.3584 - val_loss: 3344.7148\n",
            "Epoch 9195/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.8608 - val_loss: 3344.5203\n",
            "Epoch 9196/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.7864 - val_loss: 3344.7041\n",
            "Epoch 9197/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2279.2988 - val_loss: 3343.5591\n",
            "Epoch 9198/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.6270 - val_loss: 3343.6108\n",
            "Epoch 9199/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.4082 - val_loss: 3343.8350\n",
            "Epoch 9200/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.0042 - val_loss: 3343.3623\n",
            "Epoch 9201/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.1406 - val_loss: 3343.4333\n",
            "Epoch 9202/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.1606 - val_loss: 3344.0227\n",
            "Epoch 9203/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.1755 - val_loss: 3345.3220\n",
            "Epoch 9204/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.4951 - val_loss: 3348.6885\n",
            "Epoch 9205/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2279.5081 - val_loss: 3349.2683\n",
            "Epoch 9206/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2279.9114 - val_loss: 3354.2778\n",
            "Epoch 9207/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2282.9573 - val_loss: 3356.1860\n",
            "Epoch 9208/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2283.1240 - val_loss: 3356.3894\n",
            "Epoch 9209/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2284.3577 - val_loss: 3360.6387\n",
            "Epoch 9210/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.1370 - val_loss: 3354.9609\n",
            "Epoch 9211/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.3555 - val_loss: 3349.2781\n",
            "Epoch 9212/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2284.3501 - val_loss: 3345.9885\n",
            "Epoch 9213/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.1260 - val_loss: 3349.7949\n",
            "Epoch 9214/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.0149 - val_loss: 3349.0176\n",
            "Epoch 9215/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.3760 - val_loss: 3348.9802\n",
            "Epoch 9216/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.4585 - val_loss: 3348.6023\n",
            "Epoch 9217/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.1528 - val_loss: 3349.6265\n",
            "Epoch 9218/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.4668 - val_loss: 3348.4150\n",
            "Epoch 9219/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2279.3337 - val_loss: 3345.2141\n",
            "Epoch 9220/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2280.6704 - val_loss: 3346.3262\n",
            "Epoch 9221/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.5139 - val_loss: 3345.9578\n",
            "Epoch 9222/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.3035 - val_loss: 3346.6904\n",
            "Epoch 9223/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2280.7383 - val_loss: 3346.7668\n",
            "Epoch 9224/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2280.7004 - val_loss: 3349.1672\n",
            "Epoch 9225/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.9092 - val_loss: 3350.4209\n",
            "Epoch 9226/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2278.4790 - val_loss: 3351.8420\n",
            "Epoch 9227/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2277.8889 - val_loss: 3356.4321\n",
            "Epoch 9228/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.8240 - val_loss: 3357.6467\n",
            "Epoch 9229/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2279.3179 - val_loss: 3360.3103\n",
            "Epoch 9230/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.9373 - val_loss: 3357.9175\n",
            "Epoch 9231/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.0879 - val_loss: 3358.2241\n",
            "Epoch 9232/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.0962 - val_loss: 3363.6335\n",
            "Epoch 9233/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.6350 - val_loss: 3366.8435\n",
            "Epoch 9234/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2279.8352 - val_loss: 3371.1875\n",
            "Epoch 9235/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2282.4597 - val_loss: 3376.9045\n",
            "Epoch 9236/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2287.8354 - val_loss: 3383.8447\n",
            "Epoch 9237/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2290.2588 - val_loss: 3382.4409\n",
            "Epoch 9238/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2287.7786 - val_loss: 3376.1570\n",
            "Epoch 9239/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.0566 - val_loss: 3369.7217\n",
            "Epoch 9240/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2280.2732 - val_loss: 3368.1362\n",
            "Epoch 9241/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.8115 - val_loss: 3369.6548\n",
            "Epoch 9242/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.1736 - val_loss: 3370.5017\n",
            "Epoch 9243/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.4807 - val_loss: 3368.6248\n",
            "Epoch 9244/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.5913 - val_loss: 3368.4561\n",
            "Epoch 9245/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.9077 - val_loss: 3371.4309\n",
            "Epoch 9246/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2280.8860 - val_loss: 3367.2041\n",
            "Epoch 9247/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.1826 - val_loss: 3364.6597\n",
            "Epoch 9248/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.1277 - val_loss: 3362.4592\n",
            "Epoch 9249/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.0725 - val_loss: 3361.7017\n",
            "Epoch 9250/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2278.9265 - val_loss: 3361.5417\n",
            "Epoch 9251/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.6150 - val_loss: 3359.5190\n",
            "Epoch 9252/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.2261 - val_loss: 3363.0767\n",
            "Epoch 9253/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.1028 - val_loss: 3365.9038\n",
            "Epoch 9254/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.1829 - val_loss: 3366.5891\n",
            "Epoch 9255/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.1987 - val_loss: 3363.3491\n",
            "Epoch 9256/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.2798 - val_loss: 3360.5664\n",
            "Epoch 9257/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.4495 - val_loss: 3353.7258\n",
            "Epoch 9258/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2276.3477 - val_loss: 3350.7217\n",
            "Epoch 9259/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.8572 - val_loss: 3352.2385\n",
            "Epoch 9260/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2282.6536 - val_loss: 3353.9431\n",
            "Epoch 9261/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2279.2051 - val_loss: 3353.8896\n",
            "Epoch 9262/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2280.3911 - val_loss: 3353.2988\n",
            "Epoch 9263/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.5227 - val_loss: 3352.1228\n",
            "Epoch 9264/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.0564 - val_loss: 3352.8491\n",
            "Epoch 9265/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.8438 - val_loss: 3353.2634\n",
            "Epoch 9266/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2276.4797 - val_loss: 3353.4502\n",
            "Epoch 9267/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.8247 - val_loss: 3352.1050\n",
            "Epoch 9268/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.2407 - val_loss: 3351.6853\n",
            "Epoch 9269/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2276.4841 - val_loss: 3351.1636\n",
            "Epoch 9270/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.7588 - val_loss: 3349.7251\n",
            "Epoch 9271/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2277.1489 - val_loss: 3349.0527\n",
            "Epoch 9272/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2278.9602 - val_loss: 3348.4326\n",
            "Epoch 9273/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.5403 - val_loss: 3352.2131\n",
            "Epoch 9274/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2274.0969 - val_loss: 3354.0808\n",
            "Epoch 9275/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2277.5972 - val_loss: 3351.9158\n",
            "Epoch 9276/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.3140 - val_loss: 3351.9309\n",
            "Epoch 9277/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.2090 - val_loss: 3352.5996\n",
            "Epoch 9278/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.8169 - val_loss: 3355.9248\n",
            "Epoch 9279/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2276.1824 - val_loss: 3356.3496\n",
            "Epoch 9280/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2276.1853 - val_loss: 3356.3469\n",
            "Epoch 9281/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.9976 - val_loss: 3357.6348\n",
            "Epoch 9282/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.8105 - val_loss: 3356.9526\n",
            "Epoch 9283/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2278.0828 - val_loss: 3348.5857\n",
            "Epoch 9284/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2275.9153 - val_loss: 3346.9150\n",
            "Epoch 9285/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.0508 - val_loss: 3345.4031\n",
            "Epoch 9286/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2276.5959 - val_loss: 3340.3518\n",
            "Epoch 9287/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2277.6191 - val_loss: 3339.7798\n",
            "Epoch 9288/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2279.6519 - val_loss: 3339.2122\n",
            "Epoch 9289/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.6416 - val_loss: 3339.1650\n",
            "Epoch 9290/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2280.2092 - val_loss: 3340.4333\n",
            "Epoch 9291/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2279.6189 - val_loss: 3340.1331\n",
            "Epoch 9292/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.5564 - val_loss: 3340.7188\n",
            "Epoch 9293/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.7283 - val_loss: 3342.2205\n",
            "Epoch 9294/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.2573 - val_loss: 3343.9097\n",
            "Epoch 9295/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2276.0952 - val_loss: 3347.5625\n",
            "Epoch 9296/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2277.5288 - val_loss: 3353.7507\n",
            "Epoch 9297/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2277.6067 - val_loss: 3358.1733\n",
            "Epoch 9298/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2277.8818 - val_loss: 3361.4722\n",
            "Epoch 9299/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2281.2891 - val_loss: 3354.4285\n",
            "Epoch 9300/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2278.6208 - val_loss: 3351.5674\n",
            "Epoch 9301/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.2017 - val_loss: 3351.4766\n",
            "Epoch 9302/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2279.9973 - val_loss: 3357.0322\n",
            "Epoch 9303/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2279.9971 - val_loss: 3356.8357\n",
            "Epoch 9304/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2278.5864 - val_loss: 3356.2109\n",
            "Epoch 9305/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.0212 - val_loss: 3354.8286\n",
            "Epoch 9306/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2277.5310 - val_loss: 3349.2329\n",
            "Epoch 9307/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2275.5352 - val_loss: 3349.4341\n",
            "Epoch 9308/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2276.5996 - val_loss: 3348.8708\n",
            "Epoch 9309/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.9443 - val_loss: 3349.6489\n",
            "Epoch 9310/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.0989 - val_loss: 3350.2341\n",
            "Epoch 9311/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.7844 - val_loss: 3345.5674\n",
            "Epoch 9312/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2275.7512 - val_loss: 3344.3977\n",
            "Epoch 9313/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2275.9138 - val_loss: 3344.4189\n",
            "Epoch 9314/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2275.5317 - val_loss: 3343.9778\n",
            "Epoch 9315/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2275.3425 - val_loss: 3344.3086\n",
            "Epoch 9316/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.9270 - val_loss: 3344.6960\n",
            "Epoch 9317/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.7554 - val_loss: 3345.8062\n",
            "Epoch 9318/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2273.5635 - val_loss: 3347.5879\n",
            "Epoch 9319/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2276.9153 - val_loss: 3349.8606\n",
            "Epoch 9320/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.4875 - val_loss: 3349.6089\n",
            "Epoch 9321/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2276.1553 - val_loss: 3350.1040\n",
            "Epoch 9322/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.8804 - val_loss: 3350.1218\n",
            "Epoch 9323/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.5486 - val_loss: 3349.3328\n",
            "Epoch 9324/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2277.1099 - val_loss: 3348.3792\n",
            "Epoch 9325/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2275.6670 - val_loss: 3349.5825\n",
            "Epoch 9326/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.0190 - val_loss: 3350.0723\n",
            "Epoch 9327/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.4290 - val_loss: 3349.8271\n",
            "Epoch 9328/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2275.2200 - val_loss: 3351.8567\n",
            "Epoch 9329/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2277.5957 - val_loss: 3359.3206\n",
            "Epoch 9330/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.6360 - val_loss: 3360.9553\n",
            "Epoch 9331/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2276.7563 - val_loss: 3358.3022\n",
            "Epoch 9332/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.9851 - val_loss: 3356.6460\n",
            "Epoch 9333/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2274.9294 - val_loss: 3357.0183\n",
            "Epoch 9334/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.4763 - val_loss: 3359.0442\n",
            "Epoch 9335/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2274.7090 - val_loss: 3357.4849\n",
            "Epoch 9336/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2274.2817 - val_loss: 3351.7925\n",
            "Epoch 9337/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.5415 - val_loss: 3350.6980\n",
            "Epoch 9338/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2274.6345 - val_loss: 3350.2202\n",
            "Epoch 9339/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2274.5564 - val_loss: 3350.7620\n",
            "Epoch 9340/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2274.5730 - val_loss: 3350.2646\n",
            "Epoch 9341/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2274.2668 - val_loss: 3347.2942\n",
            "Epoch 9342/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2275.3511 - val_loss: 3349.1995\n",
            "Epoch 9343/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2274.1716 - val_loss: 3350.8726\n",
            "Epoch 9344/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2274.1897 - val_loss: 3350.7236\n",
            "Epoch 9345/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.8418 - val_loss: 3350.9871\n",
            "Epoch 9346/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2273.9441 - val_loss: 3351.6023\n",
            "Epoch 9347/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2273.4753 - val_loss: 3350.0371\n",
            "Epoch 9348/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.6343 - val_loss: 3350.4465\n",
            "Epoch 9349/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.7727 - val_loss: 3351.2131\n",
            "Epoch 9350/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2273.5566 - val_loss: 3351.3276\n",
            "Epoch 9351/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2273.6338 - val_loss: 3349.5203\n",
            "Epoch 9352/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.6111 - val_loss: 3349.2209\n",
            "Epoch 9353/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.5776 - val_loss: 3348.9934\n",
            "Epoch 9354/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2274.1106 - val_loss: 3347.0376\n",
            "Epoch 9355/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2273.3291 - val_loss: 3348.1548\n",
            "Epoch 9356/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.9751 - val_loss: 3347.7998\n",
            "Epoch 9357/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.9424 - val_loss: 3347.4846\n",
            "Epoch 9358/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2275.5557 - val_loss: 3347.7180\n",
            "Epoch 9359/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2274.8210 - val_loss: 3347.8071\n",
            "Epoch 9360/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.2175 - val_loss: 3347.5649\n",
            "Epoch 9361/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2281.1375 - val_loss: 3351.8716\n",
            "Epoch 9362/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2283.5215 - val_loss: 3352.8604\n",
            "Epoch 9363/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2281.1150 - val_loss: 3353.4265\n",
            "Epoch 9364/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2276.7588 - val_loss: 3353.8167\n",
            "Epoch 9365/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2275.4197 - val_loss: 3353.1897\n",
            "Epoch 9366/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.2434 - val_loss: 3351.7283\n",
            "Epoch 9367/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2277.9026 - val_loss: 3352.6191\n",
            "Epoch 9368/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.5632 - val_loss: 3354.3679\n",
            "Epoch 9369/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2272.9863 - val_loss: 3354.7620\n",
            "Epoch 9370/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.7871 - val_loss: 3353.6418\n",
            "Epoch 9371/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2272.8433 - val_loss: 3352.5808\n",
            "Epoch 9372/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2274.1252 - val_loss: 3354.4368\n",
            "Epoch 9373/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.9270 - val_loss: 3354.7651\n",
            "Epoch 9374/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2272.2058 - val_loss: 3355.9224\n",
            "Epoch 9375/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.9968 - val_loss: 3356.1748\n",
            "Epoch 9376/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2273.3354 - val_loss: 3354.4558\n",
            "Epoch 9377/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.7380 - val_loss: 3352.6140\n",
            "Epoch 9378/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.8237 - val_loss: 3348.2886\n",
            "Epoch 9379/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2273.4424 - val_loss: 3348.9421\n",
            "Epoch 9380/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.7739 - val_loss: 3348.6580\n",
            "Epoch 9381/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.3645 - val_loss: 3348.4814\n",
            "Epoch 9382/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2273.0369 - val_loss: 3348.2402\n",
            "Epoch 9383/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2274.5481 - val_loss: 3347.5190\n",
            "Epoch 9384/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.3726 - val_loss: 3347.0930\n",
            "Epoch 9385/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2275.7439 - val_loss: 3347.9761\n",
            "Epoch 9386/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2275.3445 - val_loss: 3349.8096\n",
            "Epoch 9387/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.8220 - val_loss: 3351.3044\n",
            "Epoch 9388/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2272.6790 - val_loss: 3352.3660\n",
            "Epoch 9389/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2272.5532 - val_loss: 3353.7083\n",
            "Epoch 9390/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.9084 - val_loss: 3358.3010\n",
            "Epoch 9391/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2272.9697 - val_loss: 3358.5691\n",
            "Epoch 9392/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2274.3477 - val_loss: 3357.9329\n",
            "Epoch 9393/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2273.9766 - val_loss: 3357.0032\n",
            "Epoch 9394/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.5674 - val_loss: 3355.0796\n",
            "Epoch 9395/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.4690 - val_loss: 3354.1421\n",
            "Epoch 9396/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2271.9995 - val_loss: 3352.1729\n",
            "Epoch 9397/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2275.2002 - val_loss: 3354.8381\n",
            "Epoch 9398/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2273.4966 - val_loss: 3355.0552\n",
            "Epoch 9399/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2271.7800 - val_loss: 3347.7957\n",
            "Epoch 9400/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.5581 - val_loss: 3344.3804\n",
            "Epoch 9401/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.9570 - val_loss: 3344.9092\n",
            "Epoch 9402/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.0930 - val_loss: 3345.6523\n",
            "Epoch 9403/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2275.3496 - val_loss: 3343.6707\n",
            "Epoch 9404/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.3228 - val_loss: 3345.5393\n",
            "Epoch 9405/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.2124 - val_loss: 3347.0178\n",
            "Epoch 9406/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.6287 - val_loss: 3351.9221\n",
            "Epoch 9407/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2274.5745 - val_loss: 3355.5002\n",
            "Epoch 9408/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2274.5955 - val_loss: 3355.5967\n",
            "Epoch 9409/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2274.7932 - val_loss: 3353.6836\n",
            "Epoch 9410/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2273.1484 - val_loss: 3349.0618\n",
            "Epoch 9411/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2273.1152 - val_loss: 3347.8406\n",
            "Epoch 9412/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2272.2061 - val_loss: 3347.0642\n",
            "Epoch 9413/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2272.3918 - val_loss: 3347.2676\n",
            "Epoch 9414/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.7195 - val_loss: 3347.3503\n",
            "Epoch 9415/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2273.7278 - val_loss: 3345.4290\n",
            "Epoch 9416/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.5667 - val_loss: 3343.5046\n",
            "Epoch 9417/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2271.5542 - val_loss: 3341.7209\n",
            "Epoch 9418/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.4958 - val_loss: 3340.0283\n",
            "Epoch 9419/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.2124 - val_loss: 3339.3694\n",
            "Epoch 9420/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.0684 - val_loss: 3339.1489\n",
            "Epoch 9421/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.4263 - val_loss: 3339.0425\n",
            "Epoch 9422/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2277.8123 - val_loss: 3340.7620\n",
            "Epoch 9423/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.2905 - val_loss: 3342.7258\n",
            "Epoch 9424/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2271.2527 - val_loss: 3343.8440\n",
            "Epoch 9425/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.3372 - val_loss: 3341.1621\n",
            "Epoch 9426/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.7556 - val_loss: 3340.8926\n",
            "Epoch 9427/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.5073 - val_loss: 3341.1809\n",
            "Epoch 9428/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2271.2998 - val_loss: 3340.9009\n",
            "Epoch 9429/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2271.4958 - val_loss: 3340.2534\n",
            "Epoch 9430/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.5969 - val_loss: 3339.8591\n",
            "Epoch 9431/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.9746 - val_loss: 3341.1428\n",
            "Epoch 9432/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.8857 - val_loss: 3341.5127\n",
            "Epoch 9433/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2272.2959 - val_loss: 3344.7302\n",
            "Epoch 9434/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.4004 - val_loss: 3345.1689\n",
            "Epoch 9435/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2272.1338 - val_loss: 3345.4204\n",
            "Epoch 9436/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2272.1704 - val_loss: 3348.0330\n",
            "Epoch 9437/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.3787 - val_loss: 3348.2297\n",
            "Epoch 9438/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.9519 - val_loss: 3346.5857\n",
            "Epoch 9439/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2271.9255 - val_loss: 3346.4873\n",
            "Epoch 9440/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.7947 - val_loss: 3347.2466\n",
            "Epoch 9441/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.5249 - val_loss: 3347.4316\n",
            "Epoch 9442/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.2195 - val_loss: 3346.7139\n",
            "Epoch 9443/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2270.6057 - val_loss: 3346.9072\n",
            "Epoch 9444/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2270.9783 - val_loss: 3346.2517\n",
            "Epoch 9445/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.3611 - val_loss: 3346.9849\n",
            "Epoch 9446/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.3738 - val_loss: 3349.6250\n",
            "Epoch 9447/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2271.3474 - val_loss: 3352.7654\n",
            "Epoch 9448/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2272.7722 - val_loss: 3353.9546\n",
            "Epoch 9449/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2268.5750 - val_loss: 3359.6667\n",
            "Epoch 9450/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.8857 - val_loss: 3360.4060\n",
            "Epoch 9451/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.0186 - val_loss: 3361.2468\n",
            "Epoch 9452/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.1370 - val_loss: 3362.0823\n",
            "Epoch 9453/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.1248 - val_loss: 3360.1106\n",
            "Epoch 9454/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.2847 - val_loss: 3353.6011\n",
            "Epoch 9455/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2270.8206 - val_loss: 3352.6165\n",
            "Epoch 9456/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.3394 - val_loss: 3349.7959\n",
            "Epoch 9457/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.6375 - val_loss: 3347.5696\n",
            "Epoch 9458/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2277.3975 - val_loss: 3348.6377\n",
            "Epoch 9459/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2269.8867 - val_loss: 3349.4983\n",
            "Epoch 9460/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.8418 - val_loss: 3351.2458\n",
            "Epoch 9461/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.7520 - val_loss: 3351.7869\n",
            "Epoch 9462/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.2361 - val_loss: 3350.4529\n",
            "Epoch 9463/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2270.4700 - val_loss: 3349.5344\n",
            "Epoch 9464/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.2205 - val_loss: 3349.5991\n",
            "Epoch 9465/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.7195 - val_loss: 3350.6714\n",
            "Epoch 9466/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.6016 - val_loss: 3350.5928\n",
            "Epoch 9467/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2270.2537 - val_loss: 3351.5322\n",
            "Epoch 9468/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.9778 - val_loss: 3351.6416\n",
            "Epoch 9469/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.5305 - val_loss: 3351.4446\n",
            "Epoch 9470/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.5427 - val_loss: 3349.3467\n",
            "Epoch 9471/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2270.3677 - val_loss: 3349.2612\n",
            "Epoch 9472/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.1262 - val_loss: 3348.8125\n",
            "Epoch 9473/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.2458 - val_loss: 3348.3916\n",
            "Epoch 9474/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2269.8748 - val_loss: 3347.2739\n",
            "Epoch 9475/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.7158 - val_loss: 3347.4944\n",
            "Epoch 9476/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.3501 - val_loss: 3348.2083\n",
            "Epoch 9477/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.5203 - val_loss: 3350.3164\n",
            "Epoch 9478/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2269.5591 - val_loss: 3344.8535\n",
            "Epoch 9479/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2270.1707 - val_loss: 3343.9326\n",
            "Epoch 9480/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.7573 - val_loss: 3343.7363\n",
            "Epoch 9481/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.4102 - val_loss: 3344.2842\n",
            "Epoch 9482/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.4238 - val_loss: 3348.8230\n",
            "Epoch 9483/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.1101 - val_loss: 3354.2515\n",
            "Epoch 9484/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2269.7185 - val_loss: 3352.5371\n",
            "Epoch 9485/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.9253 - val_loss: 3350.7219\n",
            "Epoch 9486/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.9978 - val_loss: 3350.1177\n",
            "Epoch 9487/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2271.1577 - val_loss: 3350.3069\n",
            "Epoch 9488/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2269.1074 - val_loss: 3350.0752\n",
            "Epoch 9489/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.7432 - val_loss: 3348.7468\n",
            "Epoch 9490/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.5410 - val_loss: 3348.4209\n",
            "Epoch 9491/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.2966 - val_loss: 3347.8101\n",
            "Epoch 9492/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2269.1304 - val_loss: 3345.0129\n",
            "Epoch 9493/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.7695 - val_loss: 3342.0767\n",
            "Epoch 9494/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.5273 - val_loss: 3340.7349\n",
            "Epoch 9495/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.9219 - val_loss: 3340.0442\n",
            "Epoch 9496/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2270.7432 - val_loss: 3342.2957\n",
            "Epoch 9497/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.8701 - val_loss: 3344.0740\n",
            "Epoch 9498/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.0894 - val_loss: 3341.1829\n",
            "Epoch 9499/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2270.4785 - val_loss: 3340.9087\n",
            "Epoch 9500/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.4409 - val_loss: 3341.1931\n",
            "Epoch 9501/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.8823 - val_loss: 3343.0723\n",
            "Epoch 9502/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.8167 - val_loss: 3344.4797\n",
            "Epoch 9503/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2268.4922 - val_loss: 3346.4053\n",
            "Epoch 9504/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.4072 - val_loss: 3345.9417\n",
            "Epoch 9505/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2268.9939 - val_loss: 3346.1216\n",
            "Epoch 9506/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2269.2244 - val_loss: 3345.0842\n",
            "Epoch 9507/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2268.1035 - val_loss: 3341.3435\n",
            "Epoch 9508/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.2742 - val_loss: 3338.9460\n",
            "Epoch 9509/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.6357 - val_loss: 3338.5281\n",
            "Epoch 9510/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.9404 - val_loss: 3338.3862\n",
            "Epoch 9511/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.5513 - val_loss: 3338.7241\n",
            "Epoch 9512/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.2876 - val_loss: 3338.8669\n",
            "Epoch 9513/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.9888 - val_loss: 3341.1289\n",
            "Epoch 9514/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2268.3699 - val_loss: 3341.1873\n",
            "Epoch 9515/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2267.1899 - val_loss: 3340.2661\n",
            "Epoch 9516/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.4734 - val_loss: 3340.0320\n",
            "Epoch 9517/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.3311 - val_loss: 3340.1338\n",
            "Epoch 9518/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2269.0305 - val_loss: 3340.5227\n",
            "Epoch 9519/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.5281 - val_loss: 3341.1440\n",
            "Epoch 9520/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2268.4502 - val_loss: 3341.1956\n",
            "Epoch 9521/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2268.4714 - val_loss: 3341.3384\n",
            "Epoch 9522/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.1716 - val_loss: 3341.7571\n",
            "Epoch 9523/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2270.8706 - val_loss: 3340.8132\n",
            "Epoch 9524/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2269.2671 - val_loss: 3341.6707\n",
            "Epoch 9525/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.9338 - val_loss: 3342.1697\n",
            "Epoch 9526/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.6072 - val_loss: 3339.5771\n",
            "Epoch 9527/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.7063 - val_loss: 3340.1191\n",
            "Epoch 9528/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.3804 - val_loss: 3340.5347\n",
            "Epoch 9529/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.9106 - val_loss: 3344.2458\n",
            "Epoch 9530/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.3279 - val_loss: 3345.5120\n",
            "Epoch 9531/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.4465 - val_loss: 3345.5198\n",
            "Epoch 9532/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.6731 - val_loss: 3344.4065\n",
            "Epoch 9533/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2267.9873 - val_loss: 3342.8672\n",
            "Epoch 9534/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.7202 - val_loss: 3342.1516\n",
            "Epoch 9535/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.8347 - val_loss: 3342.6987\n",
            "Epoch 9536/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.6182 - val_loss: 3343.4800\n",
            "Epoch 9537/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2267.6582 - val_loss: 3345.1604\n",
            "Epoch 9538/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.1750 - val_loss: 3345.5640\n",
            "Epoch 9539/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2268.3970 - val_loss: 3345.9233\n",
            "Epoch 9540/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2268.7158 - val_loss: 3345.8567\n",
            "Epoch 9541/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2268.7341 - val_loss: 3346.6052\n",
            "Epoch 9542/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.3315 - val_loss: 3348.6165\n",
            "Epoch 9543/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2270.6582 - val_loss: 3349.7668\n",
            "Epoch 9544/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2269.3025 - val_loss: 3351.0281\n",
            "Epoch 9545/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2266.9724 - val_loss: 3353.5273\n",
            "Epoch 9546/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.0681 - val_loss: 3356.3384\n",
            "Epoch 9547/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.0422 - val_loss: 3356.8989\n",
            "Epoch 9548/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2269.5562 - val_loss: 3356.5906\n",
            "Epoch 9549/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.2991 - val_loss: 3354.7886\n",
            "Epoch 9550/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.9084 - val_loss: 3357.6123\n",
            "Epoch 9551/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.0999 - val_loss: 3358.5425\n",
            "Epoch 9552/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.2502 - val_loss: 3357.2458\n",
            "Epoch 9553/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2267.1072 - val_loss: 3356.2725\n",
            "Epoch 9554/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.6409 - val_loss: 3359.3149\n",
            "Epoch 9555/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2267.3914 - val_loss: 3361.3181\n",
            "Epoch 9556/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2267.4871 - val_loss: 3362.4009\n",
            "Epoch 9557/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2267.6208 - val_loss: 3364.1580\n",
            "Epoch 9558/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.2959 - val_loss: 3363.6833\n",
            "Epoch 9559/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2266.5923 - val_loss: 3364.7195\n",
            "Epoch 9560/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.9707 - val_loss: 3366.7188\n",
            "Epoch 9561/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2267.7422 - val_loss: 3366.9126\n",
            "Epoch 9562/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2267.0635 - val_loss: 3366.2859\n",
            "Epoch 9563/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.2561 - val_loss: 3361.5227\n",
            "Epoch 9564/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2265.8889 - val_loss: 3360.9641\n",
            "Epoch 9565/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.8901 - val_loss: 3358.4055\n",
            "Epoch 9566/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.5305 - val_loss: 3359.6748\n",
            "Epoch 9567/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2267.2522 - val_loss: 3359.3328\n",
            "Epoch 9568/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2268.5398 - val_loss: 3353.9172\n",
            "Epoch 9569/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2268.8667 - val_loss: 3354.9385\n",
            "Epoch 9570/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2265.7490 - val_loss: 3349.5762\n",
            "Epoch 9571/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2266.2527 - val_loss: 3349.3406\n",
            "Epoch 9572/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.1572 - val_loss: 3348.1440\n",
            "Epoch 9573/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.2744 - val_loss: 3347.8379\n",
            "Epoch 9574/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.8701 - val_loss: 3348.8157\n",
            "Epoch 9575/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2266.9180 - val_loss: 3349.8403\n",
            "Epoch 9576/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2265.9705 - val_loss: 3350.0996\n",
            "Epoch 9577/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2266.3354 - val_loss: 3350.0271\n",
            "Epoch 9578/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2266.5510 - val_loss: 3348.6335\n",
            "Epoch 9579/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.4746 - val_loss: 3347.6704\n",
            "Epoch 9580/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.9785 - val_loss: 3345.7886\n",
            "Epoch 9581/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.5002 - val_loss: 3344.6113\n",
            "Epoch 9582/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2267.1975 - val_loss: 3341.5569\n",
            "Epoch 9583/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2271.1370 - val_loss: 3341.0273\n",
            "Epoch 9584/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2272.0784 - val_loss: 3340.7317\n",
            "Epoch 9585/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.1082 - val_loss: 3340.8132\n",
            "Epoch 9586/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2269.8359 - val_loss: 3340.9951\n",
            "Epoch 9587/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.1255 - val_loss: 3341.0203\n",
            "Epoch 9588/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2267.6704 - val_loss: 3342.9033\n",
            "Epoch 9589/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2267.2124 - val_loss: 3342.8118\n",
            "Epoch 9590/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2267.2188 - val_loss: 3344.0264\n",
            "Epoch 9591/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2266.4941 - val_loss: 3344.3440\n",
            "Epoch 9592/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.9500 - val_loss: 3344.0186\n",
            "Epoch 9593/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.9727 - val_loss: 3345.4265\n",
            "Epoch 9594/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.4197 - val_loss: 3346.9290\n",
            "Epoch 9595/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.9260 - val_loss: 3348.1453\n",
            "Epoch 9596/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2265.5474 - val_loss: 3348.7908\n",
            "Epoch 9597/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2265.9307 - val_loss: 3351.0740\n",
            "Epoch 9598/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2266.4719 - val_loss: 3351.5579\n",
            "Epoch 9599/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2265.8245 - val_loss: 3353.9504\n",
            "Epoch 9600/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2266.6619 - val_loss: 3355.3638\n",
            "Epoch 9601/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.9578 - val_loss: 3359.1804\n",
            "Epoch 9602/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2267.4150 - val_loss: 3354.9102\n",
            "Epoch 9603/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2266.9668 - val_loss: 3354.3547\n",
            "Epoch 9604/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2266.2200 - val_loss: 3353.8127\n",
            "Epoch 9605/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.4541 - val_loss: 3355.5000\n",
            "Epoch 9606/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2265.9570 - val_loss: 3352.8303\n",
            "Epoch 9607/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.4453 - val_loss: 3352.0417\n",
            "Epoch 9608/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2264.9011 - val_loss: 3352.6553\n",
            "Epoch 9609/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2266.4736 - val_loss: 3354.3540\n",
            "Epoch 9610/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2266.5776 - val_loss: 3354.0247\n",
            "Epoch 9611/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.5024 - val_loss: 3354.2009\n",
            "Epoch 9612/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2268.7761 - val_loss: 3357.2949\n",
            "Epoch 9613/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.1426 - val_loss: 3352.1265\n",
            "Epoch 9614/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2264.1150 - val_loss: 3346.9473\n",
            "Epoch 9615/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2264.0271 - val_loss: 3346.1833\n",
            "Epoch 9616/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2270.4719 - val_loss: 3348.9521\n",
            "Epoch 9617/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2276.9333 - val_loss: 3349.4414\n",
            "Epoch 9618/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2278.8098 - val_loss: 3350.8848\n",
            "Epoch 9619/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2287.3145 - val_loss: 3354.1917\n",
            "Epoch 9620/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2288.6338 - val_loss: 3354.2261\n",
            "Epoch 9621/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2284.7136 - val_loss: 3353.1868\n",
            "Epoch 9622/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2273.9968 - val_loss: 3352.2739\n",
            "Epoch 9623/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2270.4578 - val_loss: 3352.5959\n",
            "Epoch 9624/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2269.3567 - val_loss: 3351.5740\n",
            "Epoch 9625/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.7573 - val_loss: 3350.5649\n",
            "Epoch 9626/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2269.5286 - val_loss: 3353.1292\n",
            "Epoch 9627/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2269.2708 - val_loss: 3354.1602\n",
            "Epoch 9628/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.7505 - val_loss: 3355.5059\n",
            "Epoch 9629/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.9668 - val_loss: 3355.2471\n",
            "Epoch 9630/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2266.0605 - val_loss: 3354.3528\n",
            "Epoch 9631/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2264.8896 - val_loss: 3356.9031\n",
            "Epoch 9632/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2265.0608 - val_loss: 3358.4368\n",
            "Epoch 9633/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2267.5969 - val_loss: 3356.5105\n",
            "Epoch 9634/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2264.9678 - val_loss: 3356.6787\n",
            "Epoch 9635/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2264.7231 - val_loss: 3357.3118\n",
            "Epoch 9636/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2264.2686 - val_loss: 3356.9607\n",
            "Epoch 9637/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2266.0352 - val_loss: 3357.5190\n",
            "Epoch 9638/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2265.6853 - val_loss: 3358.7085\n",
            "Epoch 9639/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2268.7432 - val_loss: 3362.8125\n",
            "Epoch 9640/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.7915 - val_loss: 3360.3630\n",
            "Epoch 9641/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2264.6467 - val_loss: 3358.3835\n",
            "Epoch 9642/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2264.6479 - val_loss: 3357.9646\n",
            "Epoch 9643/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2264.4714 - val_loss: 3358.7083\n",
            "Epoch 9644/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2264.2510 - val_loss: 3358.0378\n",
            "Epoch 9645/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.9675 - val_loss: 3358.3015\n",
            "Epoch 9646/10000\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 2263.2271 - val_loss: 3357.7556\n",
            "Epoch 9647/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2264.2336 - val_loss: 3355.5562\n",
            "Epoch 9648/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2263.4275 - val_loss: 3353.8286\n",
            "Epoch 9649/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.0002 - val_loss: 3353.9585\n",
            "Epoch 9650/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.7551 - val_loss: 3355.8721\n",
            "Epoch 9651/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2265.8965 - val_loss: 3359.7683\n",
            "Epoch 9652/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.6240 - val_loss: 3360.7947\n",
            "Epoch 9653/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.2310 - val_loss: 3360.2747\n",
            "Epoch 9654/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2265.1846 - val_loss: 3360.8672\n",
            "Epoch 9655/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2266.2507 - val_loss: 3360.8252\n",
            "Epoch 9656/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2265.2473 - val_loss: 3359.0547\n",
            "Epoch 9657/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2266.4565 - val_loss: 3360.1123\n",
            "Epoch 9658/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2268.9766 - val_loss: 3370.9097\n",
            "Epoch 9659/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2274.2471 - val_loss: 3375.9062\n",
            "Epoch 9660/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2275.8911 - val_loss: 3375.3496\n",
            "Epoch 9661/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2278.3970 - val_loss: 3377.6155\n",
            "Epoch 9662/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2277.4231 - val_loss: 3373.8384\n",
            "Epoch 9663/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2269.3931 - val_loss: 3367.8799\n",
            "Epoch 9664/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2268.9468 - val_loss: 3364.6868\n",
            "Epoch 9665/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2267.9668 - val_loss: 3361.5847\n",
            "Epoch 9666/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2265.8406 - val_loss: 3357.5403\n",
            "Epoch 9667/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2264.3296 - val_loss: 3354.3535\n",
            "Epoch 9668/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2265.1377 - val_loss: 3352.5811\n",
            "Epoch 9669/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.4375 - val_loss: 3352.7788\n",
            "Epoch 9670/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2264.4478 - val_loss: 3354.4470\n",
            "Epoch 9671/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2264.0950 - val_loss: 3356.8950\n",
            "Epoch 9672/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2264.4504 - val_loss: 3358.2515\n",
            "Epoch 9673/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2263.1865 - val_loss: 3358.5249\n",
            "Epoch 9674/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.9136 - val_loss: 3358.4080\n",
            "Epoch 9675/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.2468 - val_loss: 3355.6978\n",
            "Epoch 9676/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2263.0728 - val_loss: 3356.9358\n",
            "Epoch 9677/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.3342 - val_loss: 3361.4543\n",
            "Epoch 9678/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2263.5806 - val_loss: 3362.1272\n",
            "Epoch 9679/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.9866 - val_loss: 3361.1692\n",
            "Epoch 9680/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.3135 - val_loss: 3355.5664\n",
            "Epoch 9681/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.3142 - val_loss: 3354.7781\n",
            "Epoch 9682/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.0010 - val_loss: 3355.4214\n",
            "Epoch 9683/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.3469 - val_loss: 3357.2615\n",
            "Epoch 9684/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.6091 - val_loss: 3356.1504\n",
            "Epoch 9685/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.3418 - val_loss: 3355.1099\n",
            "Epoch 9686/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2263.2612 - val_loss: 3353.6516\n",
            "Epoch 9687/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.2102 - val_loss: 3353.5535\n",
            "Epoch 9688/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2262.6821 - val_loss: 3353.8030\n",
            "Epoch 9689/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2262.9712 - val_loss: 3359.3413\n",
            "Epoch 9690/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2262.1924 - val_loss: 3359.9565\n",
            "Epoch 9691/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2262.4243 - val_loss: 3359.7261\n",
            "Epoch 9692/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2262.0051 - val_loss: 3359.7781\n",
            "Epoch 9693/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.9673 - val_loss: 3361.7302\n",
            "Epoch 9694/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.5454 - val_loss: 3364.3230\n",
            "Epoch 9695/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2264.9446 - val_loss: 3363.8838\n",
            "Epoch 9696/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2263.9556 - val_loss: 3362.9622\n",
            "Epoch 9697/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.6528 - val_loss: 3362.6257\n",
            "Epoch 9698/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.0574 - val_loss: 3361.8835\n",
            "Epoch 9699/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.7000 - val_loss: 3366.0249\n",
            "Epoch 9700/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2265.2026 - val_loss: 3368.4878\n",
            "Epoch 9701/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2264.6887 - val_loss: 3364.6592\n",
            "Epoch 9702/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.1963 - val_loss: 3361.2158\n",
            "Epoch 9703/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.9558 - val_loss: 3360.6042\n",
            "Epoch 9704/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2262.8584 - val_loss: 3358.1484\n",
            "Epoch 9705/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.5493 - val_loss: 3355.0786\n",
            "Epoch 9706/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2261.8228 - val_loss: 3355.2219\n",
            "Epoch 9707/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.2568 - val_loss: 3355.1643\n",
            "Epoch 9708/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.6445 - val_loss: 3353.6555\n",
            "Epoch 9709/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2265.1763 - val_loss: 3352.6809\n",
            "Epoch 9710/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2262.6257 - val_loss: 3352.7493\n",
            "Epoch 9711/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2263.1455 - val_loss: 3358.2722\n",
            "Epoch 9712/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.3845 - val_loss: 3356.6265\n",
            "Epoch 9713/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.1980 - val_loss: 3355.5815\n",
            "Epoch 9714/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2261.3357 - val_loss: 3354.9382\n",
            "Epoch 9715/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2261.8472 - val_loss: 3355.0352\n",
            "Epoch 9716/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2263.2148 - val_loss: 3358.0498\n",
            "Epoch 9717/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.0256 - val_loss: 3356.6035\n",
            "Epoch 9718/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.4412 - val_loss: 3357.7141\n",
            "Epoch 9719/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2261.5442 - val_loss: 3357.8608\n",
            "Epoch 9720/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.2981 - val_loss: 3357.0510\n",
            "Epoch 9721/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2262.4731 - val_loss: 3355.4543\n",
            "Epoch 9722/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2262.1802 - val_loss: 3357.5210\n",
            "Epoch 9723/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.0403 - val_loss: 3358.9053\n",
            "Epoch 9724/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2260.7227 - val_loss: 3359.5767\n",
            "Epoch 9725/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.8088 - val_loss: 3359.8691\n",
            "Epoch 9726/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.3345 - val_loss: 3359.6475\n",
            "Epoch 9727/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2261.4016 - val_loss: 3359.3164\n",
            "Epoch 9728/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.9785 - val_loss: 3358.3164\n",
            "Epoch 9729/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.1169 - val_loss: 3358.0498\n",
            "Epoch 9730/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2261.5732 - val_loss: 3355.5530\n",
            "Epoch 9731/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.8064 - val_loss: 3355.6130\n",
            "Epoch 9732/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.8171 - val_loss: 3355.6018\n",
            "Epoch 9733/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2264.1292 - val_loss: 3356.9927\n",
            "Epoch 9734/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2264.4819 - val_loss: 3357.8167\n",
            "Epoch 9735/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.0874 - val_loss: 3363.3684\n",
            "Epoch 9736/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.5010 - val_loss: 3366.0105\n",
            "Epoch 9737/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.3245 - val_loss: 3365.5566\n",
            "Epoch 9738/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2261.7200 - val_loss: 3364.8752\n",
            "Epoch 9739/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2259.6831 - val_loss: 3366.3408\n",
            "Epoch 9740/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.1775 - val_loss: 3366.4126\n",
            "Epoch 9741/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.9917 - val_loss: 3364.3721\n",
            "Epoch 9742/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2260.7747 - val_loss: 3363.0977\n",
            "Epoch 9743/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.8755 - val_loss: 3365.4841\n",
            "Epoch 9744/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2260.7668 - val_loss: 3365.0159\n",
            "Epoch 9745/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.9546 - val_loss: 3362.4060\n",
            "Epoch 9746/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.2322 - val_loss: 3361.6785\n",
            "Epoch 9747/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2261.5244 - val_loss: 3361.8779\n",
            "Epoch 9748/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.0066 - val_loss: 3362.1465\n",
            "Epoch 9749/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.6646 - val_loss: 3362.1997\n",
            "Epoch 9750/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2261.7124 - val_loss: 3364.9565\n",
            "Epoch 9751/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.3750 - val_loss: 3363.0254\n",
            "Epoch 9752/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.3081 - val_loss: 3360.0291\n",
            "Epoch 9753/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.8975 - val_loss: 3359.3921\n",
            "Epoch 9754/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2261.0164 - val_loss: 3359.4390\n",
            "Epoch 9755/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2261.3389 - val_loss: 3360.8848\n",
            "Epoch 9756/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.1755 - val_loss: 3361.3743\n",
            "Epoch 9757/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.0730 - val_loss: 3362.5496\n",
            "Epoch 9758/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2259.0093 - val_loss: 3365.3203\n",
            "Epoch 9759/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2260.3245 - val_loss: 3365.5200\n",
            "Epoch 9760/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2260.5608 - val_loss: 3365.3091\n",
            "Epoch 9761/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.1016 - val_loss: 3363.2605\n",
            "Epoch 9762/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2259.9143 - val_loss: 3362.4912\n",
            "Epoch 9763/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.0776 - val_loss: 3362.4824\n",
            "Epoch 9764/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.0513 - val_loss: 3362.2310\n",
            "Epoch 9765/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2259.7915 - val_loss: 3362.0947\n",
            "Epoch 9766/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2259.1375 - val_loss: 3362.1377\n",
            "Epoch 9767/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.5623 - val_loss: 3359.6619\n",
            "Epoch 9768/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.7356 - val_loss: 3358.0703\n",
            "Epoch 9769/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2260.2598 - val_loss: 3357.2139\n",
            "Epoch 9770/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2260.5190 - val_loss: 3357.4143\n",
            "Epoch 9771/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.4756 - val_loss: 3358.9849\n",
            "Epoch 9772/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.1562 - val_loss: 3354.7627\n",
            "Epoch 9773/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2263.0127 - val_loss: 3351.7634\n",
            "Epoch 9774/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2265.2278 - val_loss: 3352.9978\n",
            "Epoch 9775/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2265.2812 - val_loss: 3353.1145\n",
            "Epoch 9776/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2264.1328 - val_loss: 3352.2241\n",
            "Epoch 9777/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.8618 - val_loss: 3349.9937\n",
            "Epoch 9778/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2261.2769 - val_loss: 3348.9231\n",
            "Epoch 9779/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2264.8289 - val_loss: 3349.0242\n",
            "Epoch 9780/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.1553 - val_loss: 3349.0859\n",
            "Epoch 9781/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2262.3779 - val_loss: 3349.4165\n",
            "Epoch 9782/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2262.6948 - val_loss: 3350.2156\n",
            "Epoch 9783/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2264.8198 - val_loss: 3351.7397\n",
            "Epoch 9784/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2271.1562 - val_loss: 3354.6309\n",
            "Epoch 9785/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2274.4585 - val_loss: 3354.2051\n",
            "Epoch 9786/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2271.3694 - val_loss: 3353.3901\n",
            "Epoch 9787/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2267.6208 - val_loss: 3352.1497\n",
            "Epoch 9788/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.7249 - val_loss: 3350.6379\n",
            "Epoch 9789/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2264.9297 - val_loss: 3347.5930\n",
            "Epoch 9790/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2264.5254 - val_loss: 3347.1616\n",
            "Epoch 9791/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2261.8374 - val_loss: 3347.4297\n",
            "Epoch 9792/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.7053 - val_loss: 3348.1665\n",
            "Epoch 9793/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2259.9385 - val_loss: 3348.5015\n",
            "Epoch 9794/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.3062 - val_loss: 3348.9824\n",
            "Epoch 9795/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.3691 - val_loss: 3348.7805\n",
            "Epoch 9796/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.9038 - val_loss: 3349.3665\n",
            "Epoch 9797/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2262.7124 - val_loss: 3349.5161\n",
            "Epoch 9798/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.3728 - val_loss: 3351.4045\n",
            "Epoch 9799/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2260.0793 - val_loss: 3351.8391\n",
            "Epoch 9800/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.8669 - val_loss: 3349.7026\n",
            "Epoch 9801/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2262.6165 - val_loss: 3351.5254\n",
            "Epoch 9802/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2262.1558 - val_loss: 3352.5103\n",
            "Epoch 9803/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.6665 - val_loss: 3352.7820\n",
            "Epoch 9804/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.9275 - val_loss: 3352.4009\n",
            "Epoch 9805/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2259.5562 - val_loss: 3352.3638\n",
            "Epoch 9806/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2258.5674 - val_loss: 3353.0735\n",
            "Epoch 9807/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.2458 - val_loss: 3352.1570\n",
            "Epoch 9808/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.9771 - val_loss: 3351.0898\n",
            "Epoch 9809/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.6318 - val_loss: 3350.5547\n",
            "Epoch 9810/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.4680 - val_loss: 3349.3296\n",
            "Epoch 9811/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2258.2239 - val_loss: 3349.4094\n",
            "Epoch 9812/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2258.7607 - val_loss: 3347.6392\n",
            "Epoch 9813/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2257.8662 - val_loss: 3349.0198\n",
            "Epoch 9814/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2257.5371 - val_loss: 3345.1853\n",
            "Epoch 9815/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2264.1362 - val_loss: 3343.5398\n",
            "Epoch 9816/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.2300 - val_loss: 3343.4084\n",
            "Epoch 9817/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2263.5227 - val_loss: 3342.1741\n",
            "Epoch 9818/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2263.4688 - val_loss: 3341.5979\n",
            "Epoch 9819/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.3394 - val_loss: 3340.4062\n",
            "Epoch 9820/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2257.9106 - val_loss: 3338.7549\n",
            "Epoch 9821/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.6189 - val_loss: 3340.0359\n",
            "Epoch 9822/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.9294 - val_loss: 3341.9312\n",
            "Epoch 9823/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2260.0789 - val_loss: 3345.7310\n",
            "Epoch 9824/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2257.5120 - val_loss: 3346.6609\n",
            "Epoch 9825/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2257.6279 - val_loss: 3347.0601\n",
            "Epoch 9826/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2260.4998 - val_loss: 3348.0728\n",
            "Epoch 9827/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.5962 - val_loss: 3353.6292\n",
            "Epoch 9828/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.4722 - val_loss: 3361.2598\n",
            "Epoch 9829/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2258.8098 - val_loss: 3361.1011\n",
            "Epoch 9830/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.2317 - val_loss: 3360.1931\n",
            "Epoch 9831/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2257.9812 - val_loss: 3359.8359\n",
            "Epoch 9832/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2258.6479 - val_loss: 3358.1489\n",
            "Epoch 9833/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2259.1028 - val_loss: 3364.4617\n",
            "Epoch 9834/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.6741 - val_loss: 3371.4150\n",
            "Epoch 9835/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.2034 - val_loss: 3370.3740\n",
            "Epoch 9836/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2260.4697 - val_loss: 3369.7065\n",
            "Epoch 9837/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2263.5952 - val_loss: 3374.1931\n",
            "Epoch 9838/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.8994 - val_loss: 3373.5142\n",
            "Epoch 9839/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2262.4551 - val_loss: 3371.8096\n",
            "Epoch 9840/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.2896 - val_loss: 3368.1826\n",
            "Epoch 9841/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2259.6428 - val_loss: 3365.7781\n",
            "Epoch 9842/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.1035 - val_loss: 3358.8845\n",
            "Epoch 9843/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2257.2400 - val_loss: 3357.3550\n",
            "Epoch 9844/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.8047 - val_loss: 3357.2651\n",
            "Epoch 9845/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.0310 - val_loss: 3358.3022\n",
            "Epoch 9846/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2257.5403 - val_loss: 3359.3589\n",
            "Epoch 9847/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.6182 - val_loss: 3359.0330\n",
            "Epoch 9848/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.0513 - val_loss: 3361.0242\n",
            "Epoch 9849/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2257.8340 - val_loss: 3363.9304\n",
            "Epoch 9850/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2257.3157 - val_loss: 3363.0354\n",
            "Epoch 9851/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2256.8794 - val_loss: 3361.9880\n",
            "Epoch 9852/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.4263 - val_loss: 3360.0906\n",
            "Epoch 9853/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2255.7539 - val_loss: 3355.7676\n",
            "Epoch 9854/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2256.1157 - val_loss: 3352.9241\n",
            "Epoch 9855/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2260.8967 - val_loss: 3352.9167\n",
            "Epoch 9856/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2261.2651 - val_loss: 3353.0879\n",
            "Epoch 9857/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2260.1392 - val_loss: 3353.6985\n",
            "Epoch 9858/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.4197 - val_loss: 3353.7346\n",
            "Epoch 9859/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.9868 - val_loss: 3353.5547\n",
            "Epoch 9860/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.7661 - val_loss: 3353.4229\n",
            "Epoch 9861/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2259.4600 - val_loss: 3353.6028\n",
            "Epoch 9862/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2256.6035 - val_loss: 3354.7114\n",
            "Epoch 9863/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2256.8066 - val_loss: 3356.0242\n",
            "Epoch 9864/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2257.0674 - val_loss: 3357.0122\n",
            "Epoch 9865/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.2839 - val_loss: 3357.9390\n",
            "Epoch 9866/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.7217 - val_loss: 3361.1797\n",
            "Epoch 9867/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2260.3367 - val_loss: 3355.6411\n",
            "Epoch 9868/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.1411 - val_loss: 3355.7917\n",
            "Epoch 9869/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.0017 - val_loss: 3356.2939\n",
            "Epoch 9870/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.2400 - val_loss: 3359.9077\n",
            "Epoch 9871/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2257.2876 - val_loss: 3360.6580\n",
            "Epoch 9872/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2255.9893 - val_loss: 3362.8040\n",
            "Epoch 9873/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.0808 - val_loss: 3364.8247\n",
            "Epoch 9874/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2256.8145 - val_loss: 3363.8591\n",
            "Epoch 9875/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.1353 - val_loss: 3365.3721\n",
            "Epoch 9876/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.7339 - val_loss: 3364.3379\n",
            "Epoch 9877/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.1912 - val_loss: 3361.7339\n",
            "Epoch 9878/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.7715 - val_loss: 3360.7668\n",
            "Epoch 9879/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.6394 - val_loss: 3359.5969\n",
            "Epoch 9880/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2255.8005 - val_loss: 3358.5127\n",
            "Epoch 9881/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2254.9507 - val_loss: 3355.5540\n",
            "Epoch 9882/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2256.2815 - val_loss: 3355.1592\n",
            "Epoch 9883/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.6018 - val_loss: 3357.5142\n",
            "Epoch 9884/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2260.7412 - val_loss: 3357.1152\n",
            "Epoch 9885/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2262.3958 - val_loss: 3358.0767\n",
            "Epoch 9886/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.5874 - val_loss: 3358.7466\n",
            "Epoch 9887/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.8838 - val_loss: 3358.1538\n",
            "Epoch 9888/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2257.2261 - val_loss: 3357.4309\n",
            "Epoch 9889/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.8516 - val_loss: 3358.0376\n",
            "Epoch 9890/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2254.7156 - val_loss: 3361.6089\n",
            "Epoch 9891/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2254.7102 - val_loss: 3364.3740\n",
            "Epoch 9892/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.6504 - val_loss: 3369.2795\n",
            "Epoch 9893/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2256.4597 - val_loss: 3369.4277\n",
            "Epoch 9894/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.7129 - val_loss: 3368.4016\n",
            "Epoch 9895/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2255.2695 - val_loss: 3366.4417\n",
            "Epoch 9896/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.0808 - val_loss: 3366.2212\n",
            "Epoch 9897/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2255.4126 - val_loss: 3365.8982\n",
            "Epoch 9898/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.4719 - val_loss: 3363.2690\n",
            "Epoch 9899/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2257.3225 - val_loss: 3363.4727\n",
            "Epoch 9900/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2257.3708 - val_loss: 3363.6331\n",
            "Epoch 9901/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2257.0364 - val_loss: 3367.2935\n",
            "Epoch 9902/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2257.0371 - val_loss: 3371.8701\n",
            "Epoch 9903/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.7537 - val_loss: 3372.6433\n",
            "Epoch 9904/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2256.4719 - val_loss: 3370.9622\n",
            "Epoch 9905/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2255.8721 - val_loss: 3370.7180\n",
            "Epoch 9906/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.4417 - val_loss: 3370.2739\n",
            "Epoch 9907/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2255.0706 - val_loss: 3373.8171\n",
            "Epoch 9908/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.8044 - val_loss: 3379.8528\n",
            "Epoch 9909/10000\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2259.3921 - val_loss: 3381.5305\n",
            "Epoch 9910/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.2556 - val_loss: 3376.1123\n",
            "Epoch 9911/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.4370 - val_loss: 3372.3384\n",
            "Epoch 9912/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.8035 - val_loss: 3370.4705\n",
            "Epoch 9913/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2255.3257 - val_loss: 3369.2781\n",
            "Epoch 9914/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.6899 - val_loss: 3368.6541\n",
            "Epoch 9915/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.7891 - val_loss: 3368.2292\n",
            "Epoch 9916/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2258.1926 - val_loss: 3367.4358\n",
            "Epoch 9917/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2259.1350 - val_loss: 3368.4021\n",
            "Epoch 9918/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2257.4973 - val_loss: 3367.7590\n",
            "Epoch 9919/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2257.4404 - val_loss: 3368.1409\n",
            "Epoch 9920/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.6841 - val_loss: 3371.4673\n",
            "Epoch 9921/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2255.8533 - val_loss: 3372.5591\n",
            "Epoch 9922/10000\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2256.8748 - val_loss: 3369.0029\n",
            "Epoch 9923/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2258.4443 - val_loss: 3368.2534\n",
            "Epoch 9924/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2261.1196 - val_loss: 3366.8115\n",
            "Epoch 9925/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.2097 - val_loss: 3368.0547\n",
            "Epoch 9926/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2260.9062 - val_loss: 3368.3096\n",
            "Epoch 9927/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2259.3792 - val_loss: 3369.5864\n",
            "Epoch 9928/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2256.5647 - val_loss: 3370.3245\n",
            "Epoch 9929/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2255.0718 - val_loss: 3371.0540\n",
            "Epoch 9930/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.2019 - val_loss: 3370.9578\n",
            "Epoch 9931/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.8379 - val_loss: 3371.6809\n",
            "Epoch 9932/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2253.8289 - val_loss: 3371.9712\n",
            "Epoch 9933/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.7390 - val_loss: 3371.5771\n",
            "Epoch 9934/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.7507 - val_loss: 3370.4453\n",
            "Epoch 9935/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2255.3335 - val_loss: 3368.4878\n",
            "Epoch 9936/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2254.3030 - val_loss: 3366.7058\n",
            "Epoch 9937/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2255.2256 - val_loss: 3366.9971\n",
            "Epoch 9938/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.4998 - val_loss: 3367.5603\n",
            "Epoch 9939/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2260.3174 - val_loss: 3368.8140\n",
            "Epoch 9940/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2261.1604 - val_loss: 3369.1057\n",
            "Epoch 9941/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2262.1626 - val_loss: 3368.2676\n",
            "Epoch 9942/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.2439 - val_loss: 3368.1987\n",
            "Epoch 9943/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.7185 - val_loss: 3367.4060\n",
            "Epoch 9944/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.8330 - val_loss: 3365.4978\n",
            "Epoch 9945/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2256.3020 - val_loss: 3364.9102\n",
            "Epoch 9946/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2256.9614 - val_loss: 3364.9785\n",
            "Epoch 9947/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2257.2761 - val_loss: 3369.0288\n",
            "Epoch 9948/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2258.9629 - val_loss: 3370.4971\n",
            "Epoch 9949/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2257.1121 - val_loss: 3371.3301\n",
            "Epoch 9950/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2259.5947 - val_loss: 3374.0073\n",
            "Epoch 9951/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2254.9868 - val_loss: 3374.0369\n",
            "Epoch 9952/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.3508 - val_loss: 3374.0022\n",
            "Epoch 9953/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.2769 - val_loss: 3377.9172\n",
            "Epoch 9954/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2257.7400 - val_loss: 3379.5225\n",
            "Epoch 9955/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2257.5608 - val_loss: 3378.7283\n",
            "Epoch 9956/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2256.8999 - val_loss: 3377.6541\n",
            "Epoch 9957/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2257.2634 - val_loss: 3377.0042\n",
            "Epoch 9958/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2256.5007 - val_loss: 3375.9663\n",
            "Epoch 9959/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.4688 - val_loss: 3374.2869\n",
            "Epoch 9960/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2255.3896 - val_loss: 3372.5278\n",
            "Epoch 9961/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.6389 - val_loss: 3372.7549\n",
            "Epoch 9962/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.1621 - val_loss: 3371.5994\n",
            "Epoch 9963/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.8044 - val_loss: 3371.9192\n",
            "Epoch 9964/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2254.7375 - val_loss: 3371.1746\n",
            "Epoch 9965/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.8992 - val_loss: 3370.9312\n",
            "Epoch 9966/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2254.4692 - val_loss: 3372.6475\n",
            "Epoch 9967/10000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2254.5981 - val_loss: 3375.2065\n",
            "Epoch 9968/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2255.3257 - val_loss: 3372.8975\n",
            "Epoch 9969/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2252.1545 - val_loss: 3366.5610\n",
            "Epoch 9970/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2252.7412 - val_loss: 3365.3245\n",
            "Epoch 9971/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2252.9829 - val_loss: 3364.7710\n",
            "Epoch 9972/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2252.5513 - val_loss: 3365.9343\n",
            "Epoch 9973/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2252.9771 - val_loss: 3367.7578\n",
            "Epoch 9974/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2252.2876 - val_loss: 3367.8655\n",
            "Epoch 9975/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2252.7188 - val_loss: 3368.5728\n",
            "Epoch 9976/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.0461 - val_loss: 3369.3494\n",
            "Epoch 9977/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2254.9368 - val_loss: 3365.0544\n",
            "Epoch 9978/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.0828 - val_loss: 3364.5547\n",
            "Epoch 9979/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2252.3328 - val_loss: 3363.8237\n",
            "Epoch 9980/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2252.9580 - val_loss: 3363.9060\n",
            "Epoch 9981/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2252.4346 - val_loss: 3367.4775\n",
            "Epoch 9982/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.0840 - val_loss: 3368.2134\n",
            "Epoch 9983/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2252.4104 - val_loss: 3367.9185\n",
            "Epoch 9984/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2252.2012 - val_loss: 3370.3484\n",
            "Epoch 9985/10000\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2256.1575 - val_loss: 3377.3821\n",
            "Epoch 9986/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2255.0029 - val_loss: 3379.5049\n",
            "Epoch 9987/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2254.4546 - val_loss: 3378.9241\n",
            "Epoch 9988/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2253.1902 - val_loss: 3371.7021\n",
            "Epoch 9989/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2252.2148 - val_loss: 3370.3552\n",
            "Epoch 9990/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2252.5649 - val_loss: 3370.9121\n",
            "Epoch 9991/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2252.0500 - val_loss: 3370.4094\n",
            "Epoch 9992/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2251.5898 - val_loss: 3369.3533\n",
            "Epoch 9993/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2251.4307 - val_loss: 3368.3020\n",
            "Epoch 9994/10000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2252.1331 - val_loss: 3364.5879\n",
            "Epoch 9995/10000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2250.9329 - val_loss: 3366.4309\n",
            "Epoch 9996/10000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2251.9194 - val_loss: 3371.2173\n",
            "Epoch 9997/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2250.3987 - val_loss: 3370.2041\n",
            "Epoch 9998/10000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2250.2097 - val_loss: 3369.0183\n",
            "Epoch 9999/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2251.1826 - val_loss: 3369.0972\n",
            "Epoch 10000/10000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2251.8240 - val_loss: 3370.1187\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_transformed.T, Y_train, epochs = 10000, batch_size = 32, validation_data=(X_test_transformed.T, Y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d74Ke_D9CXc"
      },
      "source": [
        "---\n",
        "\n",
        "Plot train and test loss as a function of epoch\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "qTvGrHgF9071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss vs. Epoch for reg. strength 1.0')"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot( history.history['loss'], 'b', label = 'Train')\n",
        "ax.plot( history.history['val_loss'], 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 1.0', fontsize = 14)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtOAXO2n9GMr"
      },
      "source": [
        "---\n",
        "\n",
        "Compare the true and predicted values\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qz9ORVFV3HPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 78.  , 142.83, 149.36],\n",
              "       [152.  , 101.37, 134.86],\n",
              "       [200.  , 163.38, 177.57],\n",
              "       [ 59.  ,  76.47,  91.61],\n",
              "       [311.  , 149.67, 148.47],\n",
              "       [178.  , 159.39, 198.05],\n",
              "       [332.  , 161.16, 231.99],\n",
              "       [132.  ,  77.24, 104.52],\n",
              "       [156.  ,  87.49, 144.66],\n",
              "       [135.  , 147.42, 105.52],\n",
              "       [220.  , 163.39, 206.25],\n",
              "       [233.  , 163.25, 212.65],\n",
              "       [ 91.  ,  78.61,  95.77],\n",
              "       [ 51.  ,  77.83,  79.6 ],\n",
              "       [195.  , 163.34, 242.22],\n",
              "       [109.  , 162.99, 200.12],\n",
              "       [217.  , 155.57, 183.45],\n",
              "       [ 94.  , 155.4 , 100.99],\n",
              "       [ 89.  ,  74.61, 130.19],\n",
              "       [111.  , 153.4 , 139.58],\n",
              "       [129.  , 157.84, 191.32],\n",
              "       [181.  ,  80.57,  57.28],\n",
              "       [168.  , 160.6 , 124.74],\n",
              "       [ 97.  , 133.72, 140.41],\n",
              "       [115.  , 140.45, 112.33],\n",
              "       [202.  , 162.72, 199.09],\n",
              "       [ 84.  ,  73.53,  56.45],\n",
              "       [147.  , 162.86, 154.01],\n",
              "       [253.  , 139.45,  99.61],\n",
              "       [144.  , 152.21, 186.34],\n",
              "       [262.  , 141.48, 139.54],\n",
              "       [115.  , 143.07, 152.2 ],\n",
              "       [ 68.  , 163.47, 202.6 ],\n",
              "       [ 65.  ,  70.64,  72.92],\n",
              "       [252.  , 158.16, 188.57],\n",
              "       [212.  , 162.48, 168.23],\n",
              "       [142.  , 121.67, 171.96],\n",
              "       [215.  , 163.41, 232.01],\n",
              "       [180.  , 141.85, 165.63],\n",
              "       [163.  , 163.3 , 165.46],\n",
              "       [151.  ,  84.37, 150.39],\n",
              "       [283.  ,  82.97, 217.82],\n",
              "       [ 66.  ,  69.38, 135.77],\n",
              "       [ 83.  ,  76.85, 101.96],\n",
              "       [214.  , 136.94, 125.19],\n",
              "       [189.  , 162.73, 209.92],\n",
              "       [302.  , 163.32, 161.47],\n",
              "       [ 93.  , 162.85, 171.48],\n",
              "       [178.  , 163.19, 182.14],\n",
              "       [241.  , 162.5 , 200.83],\n",
              "       [ 52.  ,  77.52,  46.55],\n",
              "       [144.  , 151.44, 151.29],\n",
              "       [102.  ,  80.74,  92.61],\n",
              "       [200.  , 147.61, 138.62],\n",
              "       [232.  , 153.53, 199.35],\n",
              "       [ 97.  ,  80.06, 108.3 ],\n",
              "       [109.  , 136.03, 176.58],\n",
              "       [ 55.  ,  79.29,  63.28],\n",
              "       [ 63.  ,  75.72,  66.77],\n",
              "       [ 98.  ,  69.93,  60.43],\n",
              "       [ 88.  ,  75.35,  85.51],\n",
              "       [233.  , 163.47, 238.55],\n",
              "       [235.  , 159.55, 154.49],\n",
              "       [ 97.  , 127.79, 103.58],\n",
              "       [243.  , 163.49, 296.1 ],\n",
              "       [ 59.  ,  81.54,  65.24],\n",
              "       [138.  ,  82.95,  58.04],\n",
              "       [220.  , 163.44, 195.63],\n",
              "       [137.  , 163.48, 189.57],\n",
              "       [ 72.  ,  87.46,  58.16],\n",
              "       [109.  , 163.37, 206.01],\n",
              "       [ 71.  , 100.08, 123.26],\n",
              "       [ 74.  ,  78.02, 108.34],\n",
              "       [219.  ,  79.83, 123.6 ],\n",
              "       [196.  , 158.87, 135.63],\n",
              "       [170.  ,  76.95, 107.8 ],\n",
              "       [199.  ,  61.98, 115.87],\n",
              "       [ 71.  ,  96.19,  84.21],\n",
              "       [155.  , 162.73, 237.91],\n",
              "       [ 52.  , 155.41, 165.42],\n",
              "       [ 63.  ,  86.95,  73.82],\n",
              "       [ 88.  ,  79.8 ,  87.63],\n",
              "       [ 97.  , 161.5 , 141.41],\n",
              "       [100.  , 161.95, 145.02],\n",
              "       [ 64.  ,  99.87,  86.38],\n",
              "       [107.  ,  93.66,  94.65],\n",
              "       [ 49.  ,  99.26, 100.4 ],\n",
              "       [ 60.  ,  72.83,  76.56],\n",
              "       [346.  , 163.3 , 269.34]])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Compare the true and predicted values\n",
        "ypred_TF = model.predict(X_test_transformed.T)\n",
        "np.column_stack((Y_test, ypred, ypred_TF))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dropout layer\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the neural network model\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-04)\n",
        "model.compile(optimizer = opt, loss = 'mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29070.7480 - val_loss: 26529.2637\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29069.8359 - val_loss: 26523.8594\n",
            "Epoch 3/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 34418.5352"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 4ms/step - loss: 29022.3711 - val_loss: 26518.7773\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29045.1504 - val_loss: 26513.8320\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 29045.0547 - val_loss: 26508.8574\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 29045.5293 - val_loss: 26503.6855\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29022.4160 - val_loss: 26498.6582\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29028.5801 - val_loss: 26493.4082\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29006.8008 - val_loss: 26487.8652\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28995.2383 - val_loss: 26482.6738\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28999.6738 - val_loss: 26477.3203\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28961.5078 - val_loss: 26471.6816\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28951.0566 - val_loss: 26466.3633\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28957.3477 - val_loss: 26460.9316\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28963.8301 - val_loss: 26455.6973\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28965.7871 - val_loss: 26450.6074\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28962.1523 - val_loss: 26445.2559\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29007.0820 - val_loss: 26439.9082\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28902.0176 - val_loss: 26434.5820\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28974.0684 - val_loss: 26429.2637\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 28947.0293 - val_loss: 26424.0332\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28959.9121 - val_loss: 26418.8262\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28934.9434 - val_loss: 26413.4863\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28918.5078 - val_loss: 26408.0469\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28953.2441 - val_loss: 26402.8008\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28929.5742 - val_loss: 26397.0781\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28892.7227 - val_loss: 26391.3926\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28842.9668 - val_loss: 26385.7812\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28894.3066 - val_loss: 26380.2578\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28899.9688 - val_loss: 26374.6465\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28920.8320 - val_loss: 26369.0254\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28823.2910 - val_loss: 26362.8027\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28883.5723 - val_loss: 26357.1035\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28845.2656 - val_loss: 26351.7031\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28880.5645 - val_loss: 26346.3828\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28858.2949 - val_loss: 26340.8066\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28819.8535 - val_loss: 26335.2734\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28795.4023 - val_loss: 26329.3652\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28804.3027 - val_loss: 26323.4688\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28829.0449 - val_loss: 26317.7578\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28782.7754 - val_loss: 26311.9414\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28776.2832 - val_loss: 26306.2969\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28804.5898 - val_loss: 26300.5684\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28815.8379 - val_loss: 26294.2871\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28727.8984 - val_loss: 26288.4238\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28756.6230 - val_loss: 26282.5840\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 28805.0801 - val_loss: 26276.8262\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28686.9883 - val_loss: 26270.9883\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28764.0508 - val_loss: 26265.0684\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28745.4648 - val_loss: 26259.0176\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28776.7910 - val_loss: 26253.0566\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28728.5605 - val_loss: 26246.9102\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28727.1523 - val_loss: 26240.0332\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28743.5410 - val_loss: 26233.4180\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28693.9141 - val_loss: 26227.5371\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28747.7500 - val_loss: 26221.6777\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28715.6094 - val_loss: 26215.9121\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28698.4570 - val_loss: 26209.5918\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28670.4648 - val_loss: 26202.7617\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28652.2715 - val_loss: 26195.9297\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28691.3457 - val_loss: 26189.8516\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28677.3262 - val_loss: 26184.0020\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28676.8750 - val_loss: 26178.0781\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28633.4844 - val_loss: 26172.0273\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28743.4043 - val_loss: 26165.7812\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28655.3320 - val_loss: 26160.0898\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28638.8047 - val_loss: 26154.2520\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28647.7051 - val_loss: 26148.4355\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28626.5723 - val_loss: 26142.6016\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28636.8125 - val_loss: 26136.6602\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28637.1309 - val_loss: 26130.8418\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28549.0742 - val_loss: 26123.6465\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28641.5234 - val_loss: 26117.1133\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 28570.5137 - val_loss: 26110.9570\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28585.0938 - val_loss: 26105.0508\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28582.6621 - val_loss: 26099.1934\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28551.8066 - val_loss: 26092.9863\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28542.0645 - val_loss: 26085.9473\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28584.1504 - val_loss: 26079.5781\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28512.5273 - val_loss: 26073.4941\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28528.1777 - val_loss: 26067.2422\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28495.0586 - val_loss: 26060.9434\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28442.4160 - val_loss: 26054.3418\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28484.0762 - val_loss: 26048.0840\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28479.3223 - val_loss: 26041.7109\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28522.4277 - val_loss: 26035.6035\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28495.6660 - val_loss: 26029.3906\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28469.2695 - val_loss: 26023.3066\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28516.8730 - val_loss: 26017.2305\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28475.9492 - val_loss: 26010.3086\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28483.7480 - val_loss: 26003.1230\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28434.0566 - val_loss: 25996.3594\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28430.2715 - val_loss: 25990.0000\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28453.3574 - val_loss: 25983.7559\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28487.4277 - val_loss: 25977.5781\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28462.3828 - val_loss: 25971.3008\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28387.2246 - val_loss: 25965.0176\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28409.8496 - val_loss: 25958.4863\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 28362.5273 - val_loss: 25952.0957\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 28416.4473 - val_loss: 25945.7129\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_transformed.T, Y_train, epochs = 100, batch_size = 32, validation_data=(X_test_transformed.T, Y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss vs. Epoch for reg. strength 1.0')"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot( history.history['loss'], 'b', label = 'Train')\n",
        "ax.plot( history.history['val_loss'], 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 1.0', fontsize = 14)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10  4  1  2  3 17 16 14]\n",
            " [19 18  6 10 11  2 19  8]\n",
            " [ 9  9  5  4 15 12  6 19]\n",
            " [13 12  7 19 13  7 14 10]\n",
            " [13  3 18 14 16  3 13 15]\n",
            " [19 18  2  8  4  7  4 18]\n",
            " [ 5  2 12  8  9  6  1 11]\n",
            " [18  1 18  5 11 15 11  1]\n",
            " [13  2 10 12 10 13 16 12]\n",
            " [ 1  6 12  6 18  1 19  1]]\n",
            "---------------------------------------------------\n",
            "[[3.57e-01 9.76e-01 2.19e-01 6.14e-01 1.93e-02 3.72e-01 1.09e-01 9.51e-01]\n",
            " [1.14e-02 4.63e-01 5.56e-01 5.67e-01 4.55e-01 8.81e-01 8.33e-02 7.32e-01]\n",
            " [9.14e-01 7.05e-01 1.22e-01 8.25e-02 6.81e-01 1.96e-01 4.90e-01 8.22e-01]\n",
            " [1.09e-02 1.83e-01 1.68e-01 4.95e-01 8.44e-01 4.15e-01 9.48e-01 8.87e-02]\n",
            " [4.36e-01 9.23e-01 2.85e-01 8.91e-01 8.15e-01 4.88e-01 2.43e-01 4.42e-01]\n",
            " [6.76e-01 2.56e-02 4.14e-02 8.63e-01 3.70e-01 8.94e-01 8.60e-01 4.25e-01]\n",
            " [2.64e-01 9.26e-01 7.70e-01 8.70e-01 3.97e-01 7.00e-01 1.96e-01 7.54e-02]\n",
            " [1.35e-01 7.79e-01 3.07e-01 4.10e-01 6.08e-02 2.51e-01 3.39e-01 7.86e-01]\n",
            " [9.18e-01 5.47e-01 5.40e-01 7.18e-01 5.78e-01 6.37e-01 8.52e-01 2.43e-01]\n",
            " [6.05e-01 9.96e-01 8.68e-01 3.22e-04 3.65e-02 3.51e-01 5.03e-01 9.77e-01]]\n",
            "---------------------------------------------------\n",
            "[[ True False  True  True  True  True  True False]\n",
            " [ True  True  True  True  True False  True  True]\n",
            " [False  True  True  True  True  True  True False]\n",
            " [ True  True  True  True False  True False  True]\n",
            " [ True False  True False False  True  True  True]\n",
            " [ True  True  True False  True False False  True]\n",
            " [ True False  True False  True  True  True  True]\n",
            " [ True  True  True  True  True  True  True  True]\n",
            " [False  True  True  True  True  True False  True]\n",
            " [ True False False  True  True  True  True False]]\n",
            "---------------------------------------------------\n",
            "[[10  0  1  2  3 17 16  0]\n",
            " [19 18  6 10 11  0 19  8]\n",
            " [ 0  9  5  4 15 12  6  0]\n",
            " [13 12  7 19  0  7  0 10]\n",
            " [13  0 18  0  0  3 13 15]\n",
            " [19 18  2  0  4  0  0 18]\n",
            " [ 5  0 12  0  9  6  1 11]\n",
            " [18  1 18  5 11 15 11  1]\n",
            " [ 0  2 10 12 10 13  0 12]\n",
            " [ 1  0  0  6 18  1 19  0]]\n"
          ]
        }
      ],
      "source": [
        "# sir madsid\n",
        "b = 8 # batch size\n",
        "nl = 10 # number of nodes in layer l\n",
        "probability_dropout = 0.2 # probability of dropout\n",
        "\n",
        "# simulate an activated scores matrix\n",
        "z = np.random.randint(1, 20, ( nl, b))\n",
        "print(z)\n",
        "print('---------------------------------------------------')\n",
        "\n",
        "# dropout matrix\n",
        "dropout_matrix = np.random.rand(z.shape[0], z.shape[1]) #< probability_dropout #rand kotre mention madirad include agalla\n",
        "print(dropout_matrix)\n",
        "print('---------------------------------------------------')\n",
        "\n",
        "# binary dropout matrix\n",
        "binary_dropout_matrix = (dropout_matrix <(1- probability_dropout))\n",
        "print(binary_dropout_matrix)\n",
        "print('---------------------------------------------------')\n",
        "# dropout applied to the activated scores matrix\n",
        "z = z * binary_dropout_matrix\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[12.5   0.    1.25  2.5   3.75 21.25 20.    0.  ]\n",
            " [23.75 22.5   7.5  12.5  13.75  0.   23.75 10.  ]\n",
            " [ 0.   11.25  6.25  5.   18.75 15.    7.5   0.  ]\n",
            " [16.25 15.    8.75 23.75  0.    8.75  0.   12.5 ]\n",
            " [16.25  0.   22.5   0.    0.    3.75 16.25 18.75]\n",
            " [23.75 22.5   2.5   0.    5.    0.    0.   22.5 ]\n",
            " [ 6.25  0.   15.    0.   11.25  7.5   1.25 13.75]\n",
            " [22.5   1.25 22.5   6.25 13.75 18.75 13.75  1.25]\n",
            " [ 0.    2.5  12.5  15.   12.5  16.25  0.   15.  ]\n",
            " [ 1.25  0.    0.    7.5  22.5   1.25 23.75  0.  ]]\n"
          ]
        }
      ],
      "source": [
        "z = (z * binary_dropout_matrix)/(1 - probability_dropout)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# on house price dataset, lm model ge close iro ond model build madbeku\n",
        "# implement a dropout layer\n",
        "# build a nn model to mnist dataset using tensorflow and keras use only dropuout layer. any architecture. plot train and test loss\n",
        "# backward propogation madbeku "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
